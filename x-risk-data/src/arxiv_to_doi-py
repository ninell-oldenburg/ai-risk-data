import os
import pandas as pd
import json
import re
from pathlib import Path
import kagglehub
from typing import Dict, List, Optional
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ArxivDOIExtractor:
    def __init__(self, folder_path: str):
        """
        Initialize the ArXiv DOI extractor.
        
        Args:
            folder_path (str): Path to the folder containing year subfolders with CSV files
        """
        self.folder_path = Path(folder_path)
        self.arxiv_data = {}
        self.arxiv_pattern = re.compile(r'arxiv\.org/(?:abs/|pdf/)?([0-9]{4}\.[0-9]{4,5}(?:v[0-9]+)?)', re.IGNORECASE)
        # Pattern to extract DOIs directly from text
        self.doi_pattern = re.compile(r'(?:doi:|doi\.org/|dx\.doi\.org/)([0-9]{2}\.[0-9]{4,}/[^\s,;)]+)', re.IGNORECASE)
        
    def download_arxiv_dataset(self) -> str:
        logger.info("Downloading arXiv dataset from Kaggle...")
        path = kagglehub.dataset_download("Cornell-University/arxiv")
        logger.info(f"Dataset downloaded to: {path}")
        return path
    
    def load_arxiv_data(self, dataset_path: str):
        logger.info("Loading arXiv dataset...")
        
        # Find the JSON file in the dataset
        dataset_dir = Path(dataset_path)
        json_files = list(dataset_dir.glob("*.json"))
        
        if not json_files:
            raise FileNotFoundError("No JSON file found in the arXiv dataset")
        
        json_file = json_files[0]  # Assume the first JSON file is the main dataset
        logger.info(f"Loading data from: {json_file}")
        
        # Load and index the arXiv data by ID
        with open(json_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    paper = json.loads(line.strip())
                    paper_id = paper.get('id', '').strip()
                    if paper_id:
                        self.arxiv_data[paper_id] = paper
                except json.JSONDecodeError as e:
                    logger.warning(f"Error parsing JSON on line {line_num}: {e}")
                    continue
                
                # Log progress every 100k entries
                if line_num % 100000 == 0:
                    logger.info(f"Loaded {line_num} entries...")
        
        logger.info(f"Loaded {len(self.arxiv_data)} arXiv papers")
    
    def extract_arxiv_ids(self, text: str) -> List[str]:
        if pd.isna(text) or not isinstance(text, str):
            return []
        
        matches = self.arxiv_pattern.findall(text)
        # remove version numbers (e.g., v1, v2) to match the base ID
        clean_ids = [match.split('v')[0] for match in matches]
        return list(set(clean_ids))
    
    def extract_direct_dois(self, text: str) -> List[str]:
        """
        Extract DOIs directly from text.
        
        Args:
            text (str): Text potentially containing DOI links
            
        Returns:
            List[str]: List of extracted DOIs
        """
        if pd.isna(text) or not isinstance(text, str):
            return []
        
        matches = self.doi_pattern.findall(text)
        clean_dois = []
        for doi in matches:
            doi = re.sub(r'[.,;:)\]}>"\'\s]+', doi)
            if doi:
                clean_dois.append(doi)
        
        return list(set(clean_dois)) 
    
    def find_csv_files(self) -> List[Path]:
        """
        Find all CSV files matching the pattern year/year-month.csv
        
        Returns:
            List[Path]: List of CSV file paths
        """
        csv_files = []
        
        for year_folder in self.folder_path.iterdir():
            if year_folder.is_dir() and year_folder.name.isdigit():
                year = year_folder.name
                pattern = f"{year}-*.csv"
                csv_matches = list(year_folder.glob(pattern))
                csv_files.extend(csv_matches)
        
        logger.info(f"Found {len(csv_files)} CSV files")
        return csv_files
    
    def extract_all_dois(self, text: str) -> List[str]:
        """
        Extract all DOIs from text: both direct DOIs and DOIs from arXiv links.
        
        Args:
            text (str): Text potentially containing links
            
        Returns:
            List[str]: List of all extracted DOIs
        """
        all_dois = []
        
        # Extract direct DOIs
        direct_dois = self.extract_direct_dois(text)
        all_dois.extend(direct_dois)
        
        # Extract DOIs from arXiv links
        arxiv_ids = self.extract_arxiv_ids(text)
        for arxiv_id in arxiv_ids:
            doi = self.get_doi_for_arxiv_id(arxiv_id)
            if doi:
                all_dois.append(doi)
        
    def get_doi_for_arxiv_id(self, arxiv_id: str) -> Optional[str]:
        """
        Get DOI for a given arXiv ID.
        
        Args:
            arxiv_id (str): arXiv paper ID
            
        Returns:
            Optional[str]: DOI if found, None otherwise
        """
        paper = self.arxiv_data.get(arxiv_id)
        if paper:
            doi = paper.get('doi', '').strip()
            return doi if doi else None
        return None
    
    def process_csv_file(self, csv_path: Path) -> pd.DataFrame:
        """
        Process a single CSV file to extract all DOIs from the extracted_links column.
        
        Args:
            csv_path (Path): Path to the CSV file
            
        Returns:
            pd.DataFrame: Processed DataFrame with extracted_dois column
        """
        logger.info(f"Processing: {csv_path}")
        
        try:
            df = pd.read_csv(csv_path)
        except Exception as e:
            logger.error(f"Error reading {csv_path}: {e}")
            return pd.DataFrame()
        
        if 'extracted_links' not in df.columns:
            logger.warning(f"'extracted_links' column not found in {csv_path}")
            return df
        
        # Extract all DOIs (both direct and from arXiv)
        df['extracted_dois'] = df['extracted_links'].apply(self.extract_all_dois)
        
        return df
    
    def process_all_files(self) -> Dict[str, pd.DataFrame]:
        """
        Process all CSV files in the folder structure.
        
        Returns:
            Dict[str, pd.DataFrame]: Dictionary mapping file paths to processed DataFrames
        """
        # Download and load arXiv dataset
        dataset_path = self.download_arxiv_dataset()
        self.load_arxiv_data(dataset_path)
        
        # Find and process CSV files
        csv_files = self.find_csv_files()
        results = {}
        
        for csv_file in csv_files:
            processed_df = self.process_csv_file(csv_file)
            if not processed_df.empty:
                results[str(csv_file)] = processed_df
        
        return results
    
    def generate_summary_report(self, results: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        Generate a summary report of the processing results.
        
        Args:
            results (Dict[str, pd.DataFrame]): Processing results
            
        Returns:
            pd.DataFrame: Summary report
        """
        summary_data = []
        
        for file_path, df in results.items():
            total_rows = len(df)
            rows_with_arxiv = (df['arxiv_count'] > 0).sum()
            total_arxiv_links = df['arxiv_count'].sum()
            rows_with_dois = (df['doi_count'] > 0).sum()
            total_dois_found = df['doi_count'].sum()
            
            summary_data.append({
                'file': file_path,
                'total_rows': total_rows,
                'rows_with_arxiv': rows_with_arxiv,
                'total_arxiv_links': total_arxiv_links,
                'rows_with_dois': rows_with_dois,
                'total_dois_found': total_dois_found,
                'doi_success_rate': total_dois_found / total_arxiv_links if total_arxiv_links > 0 else 0
            })
        
        return pd.DataFrame(summary_data)

# Example usage
def main():
    """Example usage of the ArxivDOIExtractor"""
    
    folder_path = "x-risk-data/data/lw_csv_cleaned_topic"
    extractor = ArxivDOIExtractor(folder_path)
    
    results = extractor.process_all_files()
    
    summary = extractor.generate_summary_report(results)
    print("\nSummary Report:")
    print(summary.to_string(index=False))
    
    # Save processed files (optional)
    output_folder = Path("processed_output")
    output_folder.mkdir(exist_ok=True)
    
    for file_path, df in results.items():
        original_name = Path(file_path).stem
        output_path = output_folder / f"{original_name}_with_dois.csv"
        df.to_csv(output_path, index=False)
        logger.info(f"Saved processed file: {output_path}")
    
    # Save summary report
    summary_path = output_folder / "processing_summary.csv"
    summary.to_csv(summary_path, index=False)
    logger.info(f"Saved summary report: {summary_path}")

if __name__ == "__main__":
    main()