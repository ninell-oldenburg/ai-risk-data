_id,title,slug,pageUrl,postedAt,baseScore,voteCount,commentCount,meta,question,url,htmlBody,user.username,user.slug,user.displayName,user
sWGQDdoWwyE7qhRif,The Competence Myth,the-competence-myth,https://www.lesswrong.com/posts/sWGQDdoWwyE7qhRif/the-competence-myth,2019-06-30T18:55:49.014Z,48,24,31,False,False,,"<p>When I was a kid, everything seemed sort of goofy, but there was also a sense that it wasn&#x27;t really<em> supposed </em>to be competent. Yes, a lot of stuff targeted at kids was obviously dumb and bad, but it was for kids after all -- grown-ups knew what they were doing, and once we were grown up we wouldn&#x27;t have to deal with that!</p><p>Once I became a teenager, it became pretty obvious that grown-ups didn&#x27;t always know what they were doing either. [1] But there was still a sense that at the <em>next </em>level people would finally know what was going on, and that sense persisted for some time - perhaps soon it was &quot;college students&quot;, then it was &quot;professors&quot;, then it was &quot;people in the private sector&quot;, then it was &quot;experienced workers at more professional organizations&quot;... at every level, after every hurdle, it turned out that things were about as slapdash and incompetent as before. It wasn&#x27;t always the same exact patterns, but there was always <em>something </em>going wrong, and there was never the sense of &quot;okay, now I can fully Trust the System&quot;.</p><p>Maybe the final straw came when I read or heard something from (IIRC) a Navy SEAL, who said that even in the SEAL teams there were people who were incompetent - in other words, even after extremely stringent training and selection processes that would be illegal for any normal organization to implement, they were still unable to achieve that fabled &quot;okay, finally we&#x27;re at a stage where everyone knows what they&#x27;re doing&quot;. </p><p>Now, I&#x27;m not saying every single group is always and forever going to be incompetent - but what I am saying is that I wasted a lot of time thinking &quot;oh, things are messed up now but once I reach this next educational/professional milestone, everything will be fine!&quot; In point of fact that&#x27;s just not the case, and you should plan accordingly. Almost regardless of what level of vetting and selection you implement, there will be some people who slip through the cracks, and there will be some systems or processes that aren&#x27;t so good. [2] Lastly, if anyone can think of any large groups or organizations where this isn&#x27;t the case, please tell me what they are! I&#x27;d love to be proven wrong on this.</p><br/><p>[1] A school administrator once accused me of drawing something he described as &quot;a pagan symbol from The Da Vinci Code&quot; on some pillars in chalk. It was a simple design that I had made up in a notebook and thought looked good; I had never read The Da Vinci Code and had put up the symbols as a joke. The concept that the administration had investigated it and come to this conclusion was truly bizarre to me.</p><p>[2] This isn&#x27;t an argument that you shouldn&#x27;t vet or evaluate people at all, but rather one that you&#x27;re unlikely to achieve perfection in such a process.</p>",Davis_Kingsley,davis_kingsley,Davis_Kingsley,
HAMsX36kCbbeju6M7,Is AlphaZero any good without the tree search?,is-alphazero-any-good-without-the-tree-search,https://www.lesswrong.com/posts/HAMsX36kCbbeju6M7/is-alphazero-any-good-without-the-tree-search,2019-06-30T16:41:05.841Z,31,10,9,False,True,,"<html><head></head><body><p>One component of AlphaZero is a neural net which takes a board position as input, and outputs a guess about how good the position is and what a good next move would be. It combines this neural net with Monte Carlo Tree Search (MCTS) that plays out different ways the game could go, before choosing the move. The MCTS is used both during self-play to train the neural net, and during competitive test-time. I'm mainly curious about whether the latter is necessary.</p>
<p>So my question is: Once you have the fully-trained AlphaZero system, if you then turn off the MCTS and just choose moves directly with the neural net policy head, is it any good? Is it professional-level, amateur-level, child-level?</p>
<p>(I think this would be a fun little data-point related to discussions of how powerful an AI can be with and without <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">mesa-optimization</a> / search-processes using a generative environmental model.)</p>
</body></html>",steve2152,steve2152,Steven Byrnes,
KBGMS3TsWYwvuWyyY,I'm looking for alternative funding strategies for cryonics.,i-m-looking-for-alternative-funding-strategies-for-cryonics,https://www.lesswrong.com/posts/KBGMS3TsWYwvuWyyY/i-m-looking-for-alternative-funding-strategies-for-cryonics,2019-06-30T03:22:40.919Z,4,4,4,False,True,,"<p>Are there any ways to pay for cryonics or similar, if you don&#x27;t have (enough) life insurance? Do you think a fundraiser like go fund me could help? Any advice would be appreciated.</p>",Walker Vargas,walker-vargas,Walker Vargas,
6dmKBjc7XarcQMRYW,Being Wrong Doesn't Mean You're Stupid and Bad (Probably),being-wrong-doesn-t-mean-you-re-stupid-and-bad-probably,https://www.lesswrong.com/posts/6dmKBjc7XarcQMRYW/being-wrong-doesn-t-mean-you-re-stupid-and-bad-probably,2019-06-29T23:58:09.105Z,20,14,5,False,False,,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p>Sometimes, people are reluctant to admit that they were wrong about something, because they're afraid that ""You are wrong about this"" carries inextricable connotations of ""You are stupid and bad."" But this behavior is, itself, wrong, for <em>at least</em> two reasons.</p>
<p>First, because it's evidential decision theory. The so-called ""rationalist"" ""community"" has a lot of <a href=""https://www.lesswrong.com/posts/2MD3NMLBPCqPfnfre/cached-thoughts"">cached</a> clichés about this! A blank map does not correspond to a blank territory. <a href=""https://www.lesswrong.com/posts/HYWhKXRsMAyvRKRYz/you-can-face-reality"">What's true is already so</a>; owning up to it doesn't make it worse. Refusing to go to the doctor (thereby <em>avoiding encountering evidence</em> that you're sick) doesn't keep you healthy.</p>
<p><em>If</em> being wrong means that you're stupid and bad, then preventing yourself from <em>knowing</em> that you were wrong doesn't stop you from being stupid and bad <em>in reality</em>. It just prevents you from <em>knowing</em> that you're stupid and bad—which is an important fact to know (if it's true), because if you don't <em>know</em> that you're stupid and bad, then it probably won't occur to you to even <em>look</em> for possible interventions to make yourself <em>less</em> stupid and <em>less</em> bad.</p>
<p>Second, while ""You are wrong about this"" <em>is</em> evidence for the ""You are stupid and bad"" hypothesis if stupid and bad people are more likely to be wrong, I claim that it's <em>very weak</em> evidence. (Although it's possible that I'm wrong about this—and if I'm wrong, it's furthermore possible that the <em>reason</em> I'm wrong is because I'm stupid and bad.)</p>
<p>Exactly <em>how weak</em> evidence is it? It's hard to guess directly, but fortunately, we can use probability theory to reduce the claim into more ""atomic"" conditional and prior probabilities that might be easier to estimate!</p>
<p>Let <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""W""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span></span></span></span></span> represent the proposition ""You are wrong about something"", <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""S""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span></span></span></span></span> represent the proposition ""You are stupid"", and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""B""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span></span></span></span></span> represent the proposition ""You are bad.""</p>
<p>By Bayes's theorem, the probability that you are stupid and bad given that you're wrong about something is given by—</p>
<p><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""P(S,B|W)=\frac{P(W|S,B)P(S,B)}{P(W|S,B)P(S,B)+P(W|S, \neg B)P(S, \neg B)+P(W| \neg S,B)P( \neg S,B)+P(W| \neg S, \neg B)P( \neg S, \neg B)}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mfrac MJXc-space3""><span class=""mjx-box MJXc-stacked"" style=""width: 41.529em; padding: 0px 0.12em;""><span class=""mjx-numerator"" style=""width: 41.529em; top: -1.59em;""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span class=""mjx-denominator"" style=""width: 41.529em; bottom: -1.09em;""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span style=""border-bottom: 1.3px solid; top: -0.296em; width: 41.529em;"" class=""mjx-line""></span></span><span style=""height: 2.68em; vertical-align: -1.09em;"" class=""mjx-vsize""></span></span></span></span></span></span></p>
<p>For the purposes of this calculation, let's assume that badness and stupidity are statistically independent. I doubt this is true in the real world, but because I'm stupid and bad (at math), I want that simplifying assumption to make the algebra easier for me. That lets us unpack the conjunctions, giving us—</p>
<p><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""P(S,B|W)=\frac{P(W|S,B)P(S)P(B)}{P(W|S,B)P(S)P(B)+P(W|S, \neg B)P(S)P(\neg B)+P(W| \neg S,B)P( \neg S)P(B)+P(W| \neg S, \neg B)P( \neg S)P(\neg B)}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mfrac MJXc-space3""><span class=""mjx-box MJXc-stacked"" style=""width: 45.866em; padding: 0px 0.12em;""><span class=""mjx-numerator"" style=""width: 45.866em; top: -1.59em;""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span class=""mjx-denominator"" style=""width: 45.866em; bottom: -1.09em;""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span style=""border-bottom: 1.3px solid; top: -0.296em; width: 45.866em;"" class=""mjx-line""></span></span><span style=""height: 2.68em; vertical-align: -1.09em;"" class=""mjx-vsize""></span></span></span></span></span></span></p>
<p>This expression has six degrees of freedom: <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(S)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(B)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(W|S,B)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(W|S, \neg B)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(W|\neg S,B)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(W|\neg S, \neg B)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>. Arguing about the values of these six individual parameters is probably more productive than arguing about the value of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(S,B|W)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> directly!</p>
<p>Suppose half the people are stupid (<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(S) = 0.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.5</span></span></span></span></span></span>), one-tenth of people are bad (<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(B) = 0.1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.1</span></span></span></span></span></span>), and that most people are wrong, but that being stupid or bad each make you somewhat more likely to be wrong, to the tune of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(W|\neg S, \neg B) = 0.8""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.8</span></span></span></span></span></span>, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(W|S, \neg B) = P(W|\neg S,B) = 0.85""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">¬</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.85</span></span></span></span></span></span>, and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(W|S,B) = 0.9""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.9</span></span></span></span></span></span>. So our posterior probabilty that someone is stupid and bad given that they were wrong once is</p>
<p><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""P(S,B|W) = \frac{(0.9)(0.5)(0.1)}{(0.9)(0.5)(0.1)+(0.85)(0.5)(0.9)+(0.85)(0.5)(0.1)+(0.8)(0.5)(0.9)}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;"">W</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mfrac MJXc-space3""><span class=""mjx-box MJXc-stacked"" style=""width: 29.539em; padding: 0px 0.12em;""><span class=""mjx-numerator"" style=""width: 29.539em; top: -1.59em;""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.9</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.1</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span class=""mjx-denominator"" style=""width: 29.539em; bottom: -1.09em;""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.9</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.1</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.85</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.9</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.85</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.1</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.8</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.9</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span style=""border-bottom: 1.3px solid; top: -0.296em; width: 29.539em;"" class=""mjx-line""></span></span><span style=""height: 2.68em; vertical-align: -1.09em;"" class=""mjx-vsize""></span></span></span></span></span></span></p>
<p><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""\approx 0.0542""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">≈</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.0542</span></span></span></span></span></span></p>
<p>But the base rate of being stupid and bad is (0.1)(0.5) = 0.05. Learning that someone was wrong only raised our probability that they are stupid and bad by 0.0042. That's a small number that you shouldn't worry about!</p>
</body></html>",Zack_M_Davis,zack_m_davis,Zack_M_Davis,
Jm7xfyGWBTdqZ9JGo,Do children lose 'childlike curiosity?' Why?,do-children-lose-childlike-curiosity-why,https://www.lesswrong.com/posts/Jm7xfyGWBTdqZ9JGo/do-children-lose-childlike-curiosity-why,2019-06-29T22:42:36.856Z,44,16,15,False,True,,"<p>A common story goes:</p><p>Young children love asking &#x27;why&#x27;, and they often have an earnest curiosity about it that is rare in adults. Something about the process of growing up seems to cause that childlike curiosity to stagnate. </p><p>There&#x27;s a lot of compelling explanations about societal norms that actively stamp out that curiosity (i.e. school training kids to conform and regurgitate facts, parents subtly punishing kids for asking questions, etc). It seems likely to me that these are at least part of the story. </p><p>But it also wasn&#x27;t obvious that they were the whole story. I could easily imagine it also just being the case that small children are optimized for learning and older humans are optimized for doing and the brain automatically shifts away from it. [edit: or that this whole thing is imagined]</p><p>Are there any cross-cultural studies that do anything to check how this phenomenon varies, depending on upbringing?</p>",Raemon,raemon,Raemon,
smjMNYeS2myBpqYGn,How can we measure creativity?,how-can-we-measure-creativity,https://www.lesswrong.com/posts/smjMNYeS2myBpqYGn/how-can-we-measure-creativity,2019-06-29T22:30:47.805Z,32,7,3,False,True,,"<br/><p>Status: have spent about two hours on this.</p><p>As part of measuring how <a href=""https://www.lesswrong.com/posts/pWiAuhBmWskgESM4R/how-long-can-people-be-productive-in-time-period"">marginal productivity changes over time</a>, I need to know how to assess creativity. One promising test for that is the <a href=""https://en.wikipedia.org/wiki/Torrance_Tests_of_Creative_Thinking"">Torrance Tests of Creative Thinking</a>, in which subjects are given an open ended prompt, and graded on their answers. Answers are evaluated by a human being for fluency, originality, abstractness of titles, elaboration and resistance to premature closure  <a href=""https://www.lesswrong.com/(https://www.testingmom.com/tests/torrance-test/sample-torrance-practice-questions/)."">(sample questions for the curious).</a> But does the TTCT predict anything we actually care about?</p><p>The creator of the tests studied their predictive ability in a longitudinal study that lasted 50 years so far. The <a href=""https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Silvio_Brito/post/Does_anyone_have_or_know_how_to_use_Torrance_Creative_Thinking_Test/attachment/59d62e3f79197b807798c759/AS:353441776979968%401461278494209/download/Gifted%2BChild%2BQuarterly-2005-Cramond-283-91.pdf&hl=en&sa=T&oi=gsb-gga&ct=res&cd=0&d=3110599026930717657&ei=ILgSXebnLoauyATokbOAAQ&scisig=AAGBfm2mBun_3iZK-L8QMF-1FUub_HcSaA"">40 year follow up</a> showed good-for-social-sciences correlation between childhood TTCT scores and adult creative achievement, although IQ had a stronger correlation.  The<a href=""https://doi.org/10.1080/10400419.2010.523393""> 50 year follow up</a> (conducted by different experimenters) found no correlation between score and &quot;public achievement&quot; unless combined with IQ. Given that these studies were subject to the usual social science weaknesses, multiplied by 50 years and subjective grading, I do not count this as strong evidence.</p><p>A <a href=""https://www.tandfonline.com/doi/abs/10.1207/s15326934crj1801_3"">smaller study in Brazil</a> found adulthood scores of recognized creative achievers and non-achievers to vary.</p><p>Basic googling found more academics saying both disparaging (<a href=""http://users.rider.edu/~baer/BaerTTCTDebate.pdf"">beginning of page 310</a>) and <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.587.3752&rep=rep1&type=pdf"">encouraging</a> things.</p><p>My default assumption is that psychometric tests are invalid, and this evidence isn&#x27;t enough to make me change my mind. But I don&#x27;t have anything better to use for my actual goal, which is a measurable task that taxes creativity and *nothing else*, and this has a certain face validity to it. Does anyone have information to sway on the validity of the test, or an alternative test to use?</p>",pktechgirl,elizabeth-1,Elizabeth,
952g5bxvpa78pbhhX,"An unconfortable epistemic position regarding ""Verb Magic"".",an-unconfortable-epistemic-position-regarding-verb-magic,https://www.lesswrong.com/posts/952g5bxvpa78pbhhX/an-unconfortable-epistemic-position-regarding-verb-magic,2019-06-29T03:38:30.085Z,3,6,2,False,False,,"<p>Hello Guys,</p><p>This post will act as a bottle thrown in the sea, a &quot;Is anyone like me anywhere ?&quot; as well as an occasion, i hope, to discuss a more serious topic with you. Some claims made here might push on your &quot;pseudo-science&quot; button, please read until the end.</p><p>Please allow me to start with a brief (not so brief) testimonial in order for you to grab the topic.</p><p>I&#x27;m a 21 baguette french person.</p><p>I consider myself as a rationalist, i believe in demonstration by facts and I believe I don&#x27;t have any superstition like telepathy, parapsychology or flat-earth BS. I love all kind of science and deductive/inductive thinking in general. I&#x27;m basically passioned by anything that can be interesting.</p><p>I am a student in psychology, i&#x27;m aware of the scientific methodology and aware of the ton of cognitive bias the human mind can follow. I like physics, astrophysics, human sciences as well as &quot;hard&quot; sciences.</p><p>But, and some of you might stop your reading here for what i&#x27;m about to say, I recently dove into occultism. please wait and let me explain.</p><p>First of all, it&#x27;s historically speaking a cool subject to consider, i read a lot of books about alchemy, tarot, mistery cults etc... There&#x27;s a lot to learn about epistemology, theology and more. But it developped something else in me. Some kind of beliefs in some king of... magic.</p><p>Again, i&#x27;m not in a new age type of thinking like &quot;your intuition is the path everything else is wrong&quot;, I&#x27;m an atheist and the magic i&#x27;m talking about is more like something I got from a lot of philosophical thinking.</p><p>To clarify, I believe in &quot;The Verb&quot; kind of magic, the symbol and the ritual. It can look something like a thing A. Jodorowsky calls &quot;psychomagic&quot; (what a marketing term again). I started to believe that most of the things we called magic in the history of occultism is just a beautiful and poetic way to talk about advanced symbolic speech and act, which can have an enormous impact on the psychologic, emotionnal, and spiritual mindset of the person who performs it.</p><p>We can find this in franc-maçonnerie, the ritual has no supernatural nature but the power which emanates from the performance engraves a lot of things in your person, and this kinda looks like what we called sorcery at some point in history. In the same  mindset, i started sudying tarot, not because i believe in divination of the future or present, but because i think the figures present some kind of strong hermeneutic that can have a deep impact on your life, it can allow you to take a step back from your life easily and understand some life keys that makes you grow, a step back that is sometime difficult to get without strong symbols, and I ask myself more and more if this advantage can be consider as some kind of verb magic.</p><p>If you push this logic further (maybe a bit too much this time...), black magic things like &quot;He did sorcery on me now i have no luck in life, i feel sad all the time and all the bad things happen to me at the same time&quot; can be seen as a strong symbolic bound. Someone who believes in sorcery will allow the &quot;wizard&quot; to take a looot of space in his head, granting him an enormous symbolic importance in his everyday life until some point where it can affect his general mindset and therefore his acts and behaviors (this can lead to a general state of cognitive and emotional bias and a retroaction loop of &quot;feeling weird&quot; --&gt; &quot;acting weird&quot; --&gt; &quot;Feeling weirder&quot; --&gt; &quot;acting weirder&quot; --&gt; &quot;Loopin&#x27; again baby&quot;. The wizard would then just be an eloquent, charismatic person who leads with rhetoric, status and psychological sway.</p><p>I find this way of considering magic in general healthy and it&#x27;s new to me. Please try to understand me, There is a lot of superstition regarding magic and there will always be, let&#x27;s just say that from this perspective superstitions are a heritage of our scientific ignorance and the &quot;real magic&quot; underneath all of this would be the power of symbol, which we human can&#x27;t live without.</p><p>This leads to a problem to me. I was always seduced by occultism and, let be honnest, it would be so great if magic was a real thing. I&#x27;m stuck here on a rope streched across a wide gap, and maybe i&#x27;m doing all this cognitive gymnatsics just to convince myself that this is a real thing but I can&#x27;t prevent myself to think that this is maybe a stable philosophical claim.</p><p>I feel weird about all this, mostly because I couldn&#x27;t find anyone who can relate to my position. I feel rejected by science to even insinuate these topics and i also feel rejected by serious occultists, serious &quot;philosophical spiritualists&quot; and less serious &quot;new age people&quot; because i try to reduce magic and spirituality to just the power of symbol (I lack intuition blabla). (I have a tendency to read through religion, spirituality and &quot;energy&quot; related topics with my &quot;It&#x27;s so clever and powerful if you see it as symbol&quot; glasses, this is my believes and i&#x27;ll be glad to discuss this in comments but it&#x27;s another subject).</p><p>So am i granting to much importance to symbol and hermeneutic ? A great example is psychoanalysis (please don&#x27;t kill me). This hermeneutic reading can be a great tool and sooo powerful sometimes when you apply it to myth, society or the indidual. But my rational mind can&#x27;t help itself to scream &quot;boooo not proven booooo pseudo science&quot;, And i&#x27;m always furious when a dude claims the omnipotence of this theory like a fanatic. Believe me, this is a really an inconfortable position ...</p><p>What do you think ? two questions, an epistemic one, in the chart of this site and a personal one which does not belong here but please light my lantern guys...</p><p>1. What do you think about this hermeneutic point of view regarding magic and occultism in general ? Am i going to far to convince myself that my dreams are somewhat true or are there really some interesting questions raised ? </p><p>2. Do you people experienced or experience this kind of epistemic gap, which can be really inconfortable and makes you think that you somewhat belong nowhere except somwhere in a weird middle, a place where you&#x27;re alone and can&#x27;t find a damn person to fully accept your position ?</p><p>Thank you for reading</p><p>Hijol.</p>",Hijol,hijol,Hijol,
9sYzoRnmqmxZm4Whf,Conceptual Problems with UDT and Policy Selection,conceptual-problems-with-udt-and-policy-selection,https://www.lesswrong.com/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection,2019-06-28T23:50:22.807Z,66,23,16,False,False,,"<h1>Abstract</h1><p>UDT doesn&#x27;t give us conceptual tools for dealing with multiagent coordination problems. There may have initially been some hope, because a UDT player can select a policy which incentivises others to cooperate, or because UDT can reason (EDT-style) that other UDTs are more likely to cooperate if it cooperates itself, or other lines of thought. However, it now appears that this doesn&#x27;t pan out, at least not without other conceptual breakthroughs (which I suggest won&#x27;t look that much like UDT). I suggest this is connected with UDT&#x27;s difficulties handling logical uncertainty.</p><h1>Introduction</h1><p>I tend to mostly think of UDT as the ideal, with other decision theories being of interest primarily because we don&#x27;t yet know how to generalize UDT beyond the <a href=""https://www.lesswrong.com/posts/W4sDWwGZ4puRBXMEZ/single-player-extensive-form-games-as-a-model-of-udt"">simplistic domain where it definitely makes sense</a>. This perspective has been increasingly problematic for me, however, and I now feel I can say some relatively concrete things about UDT being wrong <em>in spirit</em> rather than only in detail.</p><p>Relatedly, in late 2017 I made a post titled <a href=""https://www.alignmentforum.org/posts/5bd75cc58225bf067037550c/policy-selection-solves-most-problems"">Policy Selection Solves Most Problems</a>. Policy selection is the compromise solution which does basically what UDT is supposed to do, without the same degree of conceptual elegance, and without providing the hoped-for clarity which was a major motivation for studying these foundational problems. The current post can be seen as a follow-up to that, giving an idea of the sort of thing which policy selection doesn&#x27;t seem to solve.</p><p>The argument can also be thought of as an argument against veil-of-ignorance morality of a certain kind.</p><p>I don&#x27;t think any of this will be really surprising to people who have been thinking about this for a while, but, my thoughts seem to have recently gained clarity and definiteness.</p><h2>Terminology Notes/References</h2><p><a href=""https://wiki.lesswrong.com/wiki/Updateless_decision_theory?_ga=2.22276914.826386532.1559783077-36269370.1469250194"">UDT</a> 1.0, on seeing observation <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""obs""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span></span></span></span></span>, takes the action <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""a_i""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span> which maximizes the expected utility of &quot;my code outputs action <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""a_i""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span> on seeing observation <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""obs""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span></span></span></span></span>&quot;, with expected value evaluated according to the prior.</p><p><a href=""https://www.lesswrong.com/posts/g8xh9R7RaNitKtkaa/explicit-optimization-of-global-strategy-fixing-a-bug-in"">UDT 1.1</a>, on seeing observation <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""obs""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span></span></span></span></span>, takes the action <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""a_i""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span> which <em>the globally optimal policy </em>(according to the prior) maps <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""obs""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span></span></span></span></span> to. This produces the same result as UDT 1.0 in many cases, but ensures that the agent can hunt stag with itself.</p><p><a href=""https://www.lesswrong.com/posts/zd2DrbHApWypJD2Rz/udt2-and-against-ud-assa"">UDT 2</a> is like UDT 1.1,  except it (1) represents policies as programs rather than input-output mappings, and (2) dynamically decides how much time to spend thinking about the optimal policy.</p><p>What I&#x27;m calling &quot;<a href=""https://www.lesswrong.com/posts/5bd75cc58225bf067037550c/policy-selection-solves-most-problems"">policy selection</a>&quot; is similar to UDT 2. It has a fixed (small) amount of time to choose a policy before thinking more. However, it could always choose the policy of waiting until it has thought longer before it really chooses a strategy, so that&#x27;s not so different from dynamically deciding when to choose a policy.</p><h1>Two Ways UDT Hasn&#x27;t Generalized</h1><h2>Logical Uncertainty</h2><p>UDT 2 tries to tackle the issue of &quot;thinking longer&quot;, which is the issue of logical uncertainty. This is a conceptual problem for UDT, because thinking longer is a kind of updating. UDT is supposed to avoid updating. UDT 2 doesn&#x27;t really solve the problem in a nice way.</p><p>The problem with thinking for only a short amount of time is that you get bad results. Logical induction, the best theory of logical uncertainty we have at the moment, gives essentially no guarantees about the quality of beliefs at short times. For UDT 2 to work well, it would need early beliefs to at least be good enough to avoid selecting a policy quickly -- early beliefs should at least correctly understand how poor-quality they are.</p><p>The ideal for UDT is that early beliefs reflect all the possibilities inherent in later updates, so that a policy optimized according to early beliefs reacts appropriately to later computations. <a href=""https://www.lesswrong.com/posts/5bd75cc58225bf0670375313/open-problem-thin-logical-priors"">Thin priors </a>are one way of thinking of this. So far, nothing like this has been found.</p><h2>Game Theory</h2><p>The second way UDT has failed to generalize, and the main topic of this post, is to game theory (ie, multi-agent scenarios). Cousin_it noted that <a href=""https://www.lesswrong.com/posts/W4sDWwGZ4puRBXMEZ/single-player-extensive-form-games-as-a-model-of-udt"">single-player extensive-form games</a> provided a toy model of UDT. The cases where he says that the toy model breaks down are the cases where I am <em>now</em> saying the concept of UDT <em>itself</em> breaks down. Extensive-form games represent the situations where UDT makes real sense: those with no logical uncertainty (or at least, no non-Bayesian phenomena in the logical uncertainty), and, only one agent.</p><p>What&#x27;s the conceptual problem with extending UDT to multiple agents?</p><p>When dealing with <em>updateful</em> agents, UDT has the upper hand. For example, in <a href=""https://www.lesswrong.com/posts/wm2rdS3sDY9M5kpWb/the-game-theory-of-blackmail"">Chicken-like games</a>, a UDT agent can be a bully, or commit not to respond to bullies. Under the usual game-theoretic assumption that players can determine what strategies each other have selected, the updateful agents are forced to respond optimally to the updateless ones, IE give in to UDT bullies / not bully the un-bullyable UDT.</p><p>Put simply, UDT makes its decisions &quot;before&quot; other agents. (The &quot;before&quot; is in <a href=""https://www.lesswrong.com/posts/dKAJqBDZRMMsaaYo5/in-logical-time-all-games-are-iterated-games"">logical time</a>, though, not necessarily really before.)</p><p>When dealing with <em>other UDT agents,</em> however, the UDT agents have to make a decision &quot;at the same time&quot;.</p><p>Naively, the coordination mechanism &quot;write your decisions on slips of paper simultaneously -- no peeking!&quot; is a bad one. But this is the whole idea of UDT -- writing down its strategy under a &quot;no peeking!&quot; condition.</p><p>Other decision theories also have to make decisions &quot;at the same time&quot; in game-theoretic situations, but they don&#x27;t operate under the &quot;no peeking&quot;. Guessing the behavior of the other players could be difficult, but the agent can draw on past experience to help solve this problem. UDT doesn&#x27;t have this advantage.</p><p>Furthermore, we&#x27;re asking more of UDT agents. When faced with a situation involving other UDT agents, UDT is supposed to &quot;handshake&quot; -- <a href=""https://intelligence.org/2014/02/01/robust-cooperation-a-case-study-in-friendly-ai-research/"">Löbian handshakes</a> being at least a toy model -- and find a cooperative solution (to the extent that there is one).</p><p>So far, models of how handshakes could occur have been limited to special cases or unrealistic assumptions. (I&#x27;d like to write a full review -- I think there&#x27;s some non-obvious stuff going on -- but for this post I think I&#x27;d better focus on what I see as the fundamental problem.) I&#x27;d like to see better models, but, I suspect that significant departures from UDT will be required.</p><p>Even if you don&#x27;t try to get UDT agents to cooperate with each other, though, the conceptual problem remains -- UDT is going in blind. It has a much lower ability to determine what equilibrium it is in.</p><p>I think there is a deep relationship between the issue with logical uncertainty and the issue with game theory. A simple motivating example is <a href=""https://www.lesswrong.com/posts/q9DbfYfFzkotno9hG/example-decision-theory-problem-agent-simulates-predictor"">Agent Simulates Predictor</a>, which appears to be strongly connected to both issues.</p><h1>How does Equilibrium Selection Work?</h1><p>The problem I&#x27;m pointing to is the problem of equilibrium selection. How are two UDT agents supposed to predict each other? How can they trust each other?</p><p>There are many different ways to think about agents ending up in game-theoretic equilibria. Most of them, as I understand it, rely on iterating the game so that the agents can learn about it. This iteration can be thought of as really occurring, or as occurring in the imagination of the players (an approach called &quot;fictitious play&quot;). Often, these stories result in agents playing <a href=""https://en.wikipedia.org/wiki/Correlated_equilibrium"">correlated equilibria</a>, rather than Nash equilibria. However, that&#x27;s not a very big difference for our purposes here -- correlated equilibria only allow the DD outcome in Prisoner&#x27;s Dilemma, just like Nash.</p><p>There&#x27;s something absurd about using iterated play to learn single-shot strategies, a problem Yoav Shoham et al discuss in <a href=""https://www.sciencedirect.com/science/article/pii/S0004370207000495"">If multi-agent learning is the answer, what is the question?</a>. If the game is iterated, what stops agents from taking advantage of its iterated nature?</p><p>That&#x27;s the essence of my argument in <a href=""https://www.lesswrong.com/posts/dKAJqBDZRMMsaaYo5/in-logical-time-all-games-are-iterated-games"">In Logical Time, All Games are Iterated Games</a> -- in order to learn to reason about each other, agents use fictitious play, or something similar. But this turns the game into an iterated game.</p><p>Turning a game into an iterated game can create a lot of opportunity for coordination, but the <a href=""https://en.wikipedia.org/wiki/Folk_theorem_(game_theory)"">Folk Theorem</a> says that it also creates a very large equilibrium selection problem. The Folk Theorem indicates that rational players can end up in very bad outcomes. Furthermore, we&#x27;ve found this <a href=""https://www.lesswrong.com/posts/5bd75cc58225bf06703753d4/two-major-obstacles-for-logical-inductor-decision-theory"">difficult to avoid in decision algorithms we know how to write down</a>. How can we eliminate the &quot;bad&quot; equilibria and keep only the &quot;good&quot; possibilities? </p><p>What we&#x27;ve accomplished is the reduction of the &quot;handshake&quot; problem to the problem of avoiding bad equilibria. (We could say this <a href=""https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag-1#D3jnxh7MvaJ4Z5sPK"">turns prisoner&#x27;s dilemma into stag hunt</a>.) </p><p>Handshake or no handshake, however, the &quot;fictitious play&quot; view suggests that equilibrium selection requires <em>learning</em>. You can get agents into equilibria without learning, but the setups seem artificial (so far as I&#x27;ve seen). This requires updateful reasoning in some sense. (Although, it can be a logical update only; being empirically updateless still seems wiser).</p><h1>Logical Uncertainty &amp; Games</h1><p>Taking this idea a little further, we can relate logical uncertainty and games via the following idea: </p><p><em>Our uncertain expectations are a statistical summary of how things have gone in similar situations in the (logical) past. The way we react to what we see can be thought of as an iterated strategy which depends on the overall statistics of that history (rather than a single previous round).</em></p><p>I&#x27;m not confident this analogy is a good one -- in particular, the way policies have to depend on statistical summaries of the history rather than on specific previous rounds is a bit frustrating. However, the analogy goes deeper than I&#x27;m going to spell out here. (Perhaps in a different post.) </p><p>One interesting point in favor of this analogy: it also works for modal agents. The proof operator, <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\Box""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">□</span></span></span></span></span></span>, is like a &quot;prediction&quot;: proofs are how modal agents thinks about the world in order to figure out what to do. So <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\Box x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">□</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> is like &quot;the agent thinks <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>&quot;. If you look at how modal agents are actually computed in the <a href=""https://intelligence.org/2015/03/18/new-report-introduction-lobs-theorem-miri-research/"">MIRI guide to Löb&#x27;s theorem</a>, it looks like an iterated game, and <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\Box""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">□</span></span></span></span></span></span> looks like a simple kind of summary of the game so far. On any round, <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\Box x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">□</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> is true if and only if <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> has been true in every previous round. So, you can think of <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\Box x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">□</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> as &quot;<span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> has held up so far&quot; -- as soon as <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> turns out to be false once, <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\Box x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">□</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> is never true again.</p><p>In this interpretation, FairBot (the strategy of cooperating if and only if the other player provably cooperates) becomes the &quot;Grim Trigger&quot; strategy: cooperate on the first round, and cooperate on every subsequent round so long as the other player has cooperated so far. If the other player ever defects, switch to defecting, and never cooperates again.</p><p>A take-away for the broader purpose of this post could be: <em>one of the best models we have of the UDT &quot;handshake&quot; <strong>is the Grim Trigger strategy in disguise</strong>. </em>This sets the tone nicely for what follows.</p><p>My point in offering this analogy, however, is to drive home the idea that game-theoretic reasoning requires learning. Even logic-based agents can be understood as running simple learning algorithms, &quot;updating&quot; on &quot;experience&quot; from (counter)logical possibilities. UDT can&#x27;t dance with its eyes closed.</p><p>This is far from a proof of anything; I&#x27;m just conveying intuitions here.</p><h1>What UDT Wants</h1><p>One way to look at what UDT is trying to do is to think of it as always trying to win a &quot;most meta&quot; competition. UDT doesn&#x27;t want to look at any information until it has determined the best way to use that information. UDT doesn&#x27;t want to make any decisions directly; it wants to find optimal policies. UDT doesn&#x27;t want to participate in the usual game-theoretic setup where it (somehow) knows all other agent&#x27;s policies and has to react; instead, it wants to understand <em>how</em> those policies come about, and act in a way which maximally shapes that process to its benefit.</p><p>It wants to move first in every game.</p><p>Actually, that&#x27;s not right: it wants the <em>option</em> of moving first. Deciding earlier is always better, if one of the options is to decide later. </p><p>It wants to announce its binding commitments before anyone else has a chance to, so that everyone has to react to the rules it sets. It wants to set the equilibrium as it chooses. Yet, at the same time, it wants to understand how everyone else will react. It would like to understand all other agents in detail, their behavior a function of itself. </p><p>So, what happens if you put two such agents in a room together?</p><p>Both agents race to decide how to decide first. Each strives to understand the other agent&#x27;s behavior <a href=""https://agentfoundations.org/item?id=1356"">as a function of its own</a>, to select the best policy for dealing with the other. Yet, such examination of the other needs to itself be done in an updateless way. It&#x27;s a race to make the most uninformed decision.</p><p>I claim this isn&#x27;t a very good coordination strategy.</p><p>One issue is that jumping up a meta-level increases the complexity of a decision. Deciding a single action is much easier than deciding on a whole policy. Some kind of race to increasing meta-levels makes decisions increasingly challenging.</p><p>At the same time, the desire for your policy to be logically earlier than everyone else, so that they account for your commitments in making their decisions, means you have to make your decisions faster and in simpler, more predictable ways.</p><p>The expanding meta-space and the contracting time do not seem like a good match. You have to make a more complicated decision via less-capable means.</p><p>Two people trying to decide policies early are just like two people trying to decide actions late, but with more options and less time to think. It doesn&#x27;t seem to solve the fundamental coordination problem.</p><p>The race for most-meta is only one possible intuition about what UDT is trying to be. Perhaps there is a more useful one, which could lead to better generalizations.</p><h1>Veils of Ignorance</h1><p>UDT tries to coordinate with itself by stepping behind a veil. In doing so, it fails to coordinate with others.</p><p>Veil-of-ignorance moral theories describe multiagent coordination resulting from stepping behind a veil. But there is a serious problem. How can everyone step behind the same veil? You can&#x27;t tell what veil everyone else stepped behind if you stay behind your own veil.</p><p>UDT can successfully self-coordinate in this way because it is very reasonable to use the common prior assumption with a single agent. There is no good reason to suppose this in the multiagent case. In practice, the common prior assumption is a good approximation of reality because everyone has dealt with essentially the same reality for a long time and has learned a lot about it. But if we have everyone step behind a veil of ignorance, there is no reason to suppose they know how to construct the <em>same </em>veil as each other -- they&#x27;re ignorant!</p><h1>Is UDT Almost Right, Nonetheless?</h1><p>I find myself in an awkward position. I still think UDT gets a lot of things right. Certainly, it still seems worth being updateless about <em>empirical</em> uncertainty. It doesn&#x27;t seem to make sense for <em>logical</em> uncertainty... but treating logical and empirical uncertainty in such different ways is quite uncomfortable. My intuition is that there should not be a clean division between the two. </p><p>One possible reaction to all this is to try to <a href=""https://www.lesswrong.com/posts/WkPf6XCzfJLCm2pbK/cdt-edt-udt"">learn to be updateless</a>. IE, don&#x27;t actually try to be updateless, but do try to get the problems right which UDT got right. Don&#x27;t expect everything to go well with a fixed Bayesian prior, but try to specify the learning-theoretic properties which approximate that ideal.</p><p>Would such an approach do anything to help multiagent coordination? Unclear. <a href=""https://www.alignmentforum.org/posts/5bd75cc58225bf0670375058/superrationality-in-arbitrary-games"">Thermodynamic self-modification hierarchies</a> might work with this kind of approach.</p><p>In terms of veil-of-ignorance morality, it seems potentially helpful. Take away everything we&#x27;ve learned, and we don&#x27;t know how to cooperate from behind our individual veils of ignorance. But if we each have a veil of ignorance which is carefully constructed, a learned-updateless view which accurately reflects the possibilities in some sense, they seem more likely to match up and enable coordination.</p><p>Or perhaps a more radical departure form the UDT ontology is needed.</p>",abramdemski,abramdemski,abramdemski,
YJwtWwRaCK4FA9kd8,Whence decision exhaustion?,whence-decision-exhaustion,https://www.lesswrong.com/posts/YJwtWwRaCK4FA9kd8/whence-decision-exhaustion,2019-06-28T20:41:47.987Z,15,4,4,False,True,,"<p>Many people experience something we might call decision or executive exhaustion: after making a lot of decisions, it can be hard to make more decisions and to exert &quot;willpower&quot;. Yet, this seems odd because we are constantly making decisions all the time in some sense, choosing to do what we do over everything else we could have otherwise done. So, what and why do we sometimes get exhausted of making decisions when most of the time we do not?</p><p>Some notes to consider in answering:</p><ul><li>Some people seem to experience this from all decisions and are worn out after dozens of minutes of being awake.</li><li>Some people seem to never experience this.</li><li>Exhausting decisions seem more salient or like they require more deliberate thought than ones that are not exhausting. Non-exhausting decisions feel automatic.</li><li>Food and rest (but not a full nights sleep) helps some people recover decision function but not everyone seems to respond to this over short enough timescales for it to be useful for recovering functionality within the day.</li></ul>",gworley,gordon-seidoh-worley,Gordon Seidoh Worley,
H5gXpFtg93qDMZ6Xn,Aligning a toy model of optimization,aligning-a-toy-model-of-optimization,https://www.lesswrong.com/posts/H5gXpFtg93qDMZ6Xn/aligning-a-toy-model-of-optimization,2019-06-28T20:23:51.337Z,53,19,25,False,False,,"<p>Suppose I have a magic box <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> that takes as input a program <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U:\left\{0, 1\right\}^n \rightarrow \mathbb{R}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">:</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">{</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">}</span></span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.71em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.372em;"">→</span></span><span class=""mjx-texatom MJXc-space3""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">R</span></span></span></span></span></span></span></span>, and produces <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}(U) = \mathrm{argmax}_x U(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">g</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">m</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">x</span></span></span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>, with only <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""n""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span></span></span></span></span> times the cost of a single evaluation of <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span></span></span></span>. Could we use this box to build an aligned AI, or would broad access to such a box result in doom?</p><p>This capability is vaguely similar to modern ML, especially if we use <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> to search over programs. But I think we can learn something from studying simpler models.</p><h2>An unaligned benchmark</h2><p>(<a href=""https://ai-alignment.com/an-unaligned-benchmark-b49ad992940b"">Related</a>.)</p><p>I can use <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> to define a simple unaligned AI (details omitted):</p><ul><li>Collect data from a whole bunch of sensors, including a &quot;reward channel.&quot;</li><li>Use <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> to find a program <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""M""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;"">M</span></span></span></span></span></span> that makes good predictions about that data.</li><li>Use <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> to find a policy <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span> that achieves a high reward when interacting with <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""M""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;"">M</span></span></span></span></span></span>. </li></ul><p>This isn&#x27;t a great design, but it works as a benchmark. Can we build an aligned AI that is equally competent?</p><p>(I haven&#x27;t described how <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> works for stochastic programs. The most natural definition is a bit complicated, but the details don&#x27;t seem to matter much. You can just imagine that it returns a random <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> that is within one standard deviation of the optimal expected value.) </p><h2>Competing with the benchmark</h2><p>(<a href=""https://ai-alignment.com/a-possible-stance-for-ai-control-research-fe9cf717fc1b"">Related</a>.)</p><p>If I run this system with a long time horizon and a hard-to-influence reward channel, then it may competently acquire influence in order to achieve a high reward.</p><p>We&#x27;d like to use <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> to build an AI that acquires influence just as effectively, but will use that influence to give us security and resources to reflect and grow wiser, and remain responsive to our instructions. </p><p>We&#x27;d like the aligned AI to be almost as efficient. Ideally the proportional overhead would converge to 0 as we consider more complex models. At worst the overhead should be a constant factor. </p><h2>Possible approach</h2><p>(<a href=""https://ai-alignment.com/towards-formalizing-universality-409ab893a456"">Related</a>.)</p><p>My hope is to use <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> to learn a policy <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi^+""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.076em; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span></span></span></span></span></span></span> which can answer questions in a way that reflects &quot;everything <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span> knows.&quot; This requires:</p><ul><li>Setting up an objective that incentivizes <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi^+""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.076em; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span></span></span></span></span></span></span> to give good answers to questions.</li><li>Arguing that there <em>exists</em> a suitable policy <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi^+""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.076em; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span></span></span></span></span></span></span> that is only slightly more complicated than <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span>. </li></ul><p>If we have such a <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi^+""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.076em; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span></span></span></span></span></span></span>,  then we can use it to directly answer questions like &quot;What&#x27;s the best thing to do in this situation?&quot; The hope is:</p><ul><li>Its answers can leverage everything <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span> knows, and in particular all of <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span>&#x27;s knowledge about how to acquire influence. So using <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi^+""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.076em; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span></span></span></span></span></span></span> in this way is competitive with using <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span> directly. </li><li>It knows enough about human preferences to be <a href=""https://ai-alignment.com/corrigibility-3039e668638"">corrigible</a>.</li></ul><p>&quot;Everything <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span> knows&quot; is slippery; I mean something like &quot;what a sufficiently-idealized Bayesian would believe after updating on the fact that <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span> achieves a high reward.&quot; Constructing an objective which incentivizes these answers probably requires understanding the nature of that update. </p><h2>Thoughts on feasibility</h2><p>In the context of ML, I usually imagine training <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi^+""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.076em; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span></span></span></span></span></span></span> via iterated amplification. Unfortunately, iterated amplification doesn&#x27;t correspond to optimizing a single objective <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span></span></span></span>---it requires either training a sequence of agents or exploiting properties of local search (using the previous iterate to provide oversight for the next). If we just have <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span>, it&#x27;s not clear if we can efficiently do anything like iterated amplification or debate. </p><p>If aligning <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> is impossible, I think that&#x27;s slightly bad news for aligning ML. That said, it&#x27;s reasonably likely that local search will be easier to align, so the next step would be constructing a simple model of local search.</p><p>There are also some ways in which the optimizer case seems easier:</p><ul><li>It&#x27;s a simpler model and so more amenable to analysis. The Bayesian update from &quot;<span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span> gets a high reward&quot; is more straightforward when <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span> is actually optimized.</li><li>We don&#x27;t have to worry about optimization difficulty.</li><li>Given a policy <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span></span></span> we can directly search for an input on which it behaves a certain way.</li></ul><h2>It&#x27;s OK if it&#x27;s impossible</h2><p>When working on alignment I aim to either find a scalable alignment strategy or a clear argument for why scalable alignment is impossible. I&#x27;m excited about considering easy-to-analyze versions of the alignment problem even if they are impossible:</p><ul><li>It gives us practice making impossibility arguments, and developing relevant intuitions and techniques.</li><li>It clarifies the difficulty of the alignment problem---if we know why we can&#x27;t handle simple cases like <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span>, then we learn something about what the solution must look like in more complicated cases.</li><li>It gives us a sense of what impossibility results might look like, if we were able to prove them in more realistic cases. Would they actually be strong enough to guide action, or convince anyone skeptical?</li></ul><h2>Expensive optimization</h2><p>I described <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> as requiring <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""n""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span></span></span></span></span> times more compute than <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span></span></span></span>. If we implemented it naively it would instead cost <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""2^n""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span></span></span></span></span></span></span> times more than <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span></span></span></span>.</p><p>We can use this more expense <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathrm{Opt}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.372em;"">t</span></span></span></span></span></span></span></span> in our unaligned benchmark, which produces an AI that we can actually run (but it would be terrible, since it does a brute force search over programs). It should be easier to compete with this really slow AI. But it&#x27;s still not trivial and I think it&#x27;s worth working on. If we can&#x27;t compete with this benchmark, I&#x27;d feel relatively pessimistic about aligning ML.</p>",paulfchristiano,paulfchristiano,paulfchristiano,
FCMfToFnRXhsfpfQB,Book Review: Why Are The Prices So Damn High?,book-review-why-are-the-prices-so-damn-high,https://www.lesswrong.com/posts/FCMfToFnRXhsfpfQB/book-review-why-are-the-prices-so-damn-high,2019-06-28T19:40:00.643Z,61,22,7,False,False,,"<p>Economist Alex Tabarrok has recently come out with a short book, “Why are the prices so Damn High”, available in full PDF <a href=""https://www.mercatus.org/system/files/helland-tabarrok_why-are-the-prices-so-damn-high_v1.pdf"">here.</a></p>
<p>Since the 1950’s, the inflation-adjusted cost of physical goods has fallen since the 1950’s, and the cost of food has stayed about the same.  But the cost of education, professional services, and healthcare has risen dramatically, despite those sectors not producing much improvement. Why?</p>
<p>The traditional economic explanation for the rising cost of services is the <a href=""https://en.wikipedia.org/wiki/Baumol%27s_cost_disease"">Baumol Effect. </a>Some sectors, like manufacturing, are subject to efficiency improvements over time as technology improves; the more we automate the production of goods, the cheaper they get.  Other sectors are intrinsically harder to automate, so they don’t get cheaper over time. For instance, it takes the same number of musicians the same number of time to play a symphony as it did in 1950.  So, as a proportion of the average person’s paycheck, the cost of intrinsically un-automatable things like live concerts must rise relative to the cost of automatable things like manufactured goods.</p>
<p>Tabarrok doesn’t cover housing in his book, but home prices <a href=""https://fred.stlouisfed.org/series/USSTHPI#0"">have also been rising</a> since the 1970’s and I’ve seen the Baumol effect deployed to explain rising housing costs as well. “Land is the one thing they’re not making any more of” — for the most part, technological improvements don’t increase the quantity of livable land, so if technology makes some sectors more efficient and drives costs down, land will become relatively more expensive.</p>
<p><strong>My Beef With Baumol</strong></p>
<p>My preconception coming into the book was that the Baumol effect doesn’t actually answer the question. <em>Why </em>are healthcare, professional services, and education intrinsically hard to make more efficient?  It’s prima facie absurd to say that medicine is just one of those things that technology can’t improve — the biomedical industry is one of the biggest R&amp;D endeavors in the whole economy!  So why is it obvious that none of that innovation can make medicine cheaper?  If it’s <em>not </em>making medicine cheaper, that’s an empirical fact that deserves explanation, and “it’s the Baumol effect” doesn’t actually answer the “why” question.</p>
<p>The same holds true for the other industries, even housing to some degree. While it’s true that the amount of land on Earth is fixed (modulo landfill) and the amount of space in Manhattan is fixed, there’s also the options of building taller buildings, expanding cities, and building new cities.  Why is it in principle impossible for the production of housing to become more efficient over time just as the production of other goods does?</p>
<p>The Baumol Effect doesn’t make sense to me as an explanation, because its answer to “why are these sectors getting more expensive?” is, in effect, “because it’s obvious that they can’t get cheaper.”</p>
<p><strong>It’s Not Administrative Bloat, It’s More Service Providers</strong></p>
<p>A popular explanation for why college and K-12 education have gotten more expensive is “bloat”, the idea that most of the cost is due to increasing numbers of bureaucratic administrators and unnecessary luxury amenities.</p>
<p>Tabarrok points out that this story can’t be true. In reality, the percent of university costs going to administration has stayed relatively constant since 1980, and the percent going to facilities has <em>decreased</em>. In the K-12 world, the number of administrators is tiny compared to the number of teachers, and it’s barely budged; it’s the number of <em>teachers </em>per student that has grown.  Most of the increase in educational costs, says Tabarrok, comes from rising numbers of teachers and college professors, and higher wages for those teachers and professors.</p>
<p>In other words, education <em>is </em>getting more “inefficient”, not necessarily in a pejorative sense but in an economic sense; we are using more people to achieve similar educational results (average test scores are flat.)</p>
<p>This may be fine; maybe people get value out of personal teacher attention that doesn’t show up in test scores, so we’re spending more to get a better outcome, just one that the narrow metric of standardized test performance doesn’t capture.</p>
<p>Likewise, in healthcare, we have an increasing number of doctors and nurses in the US per capita, and (relative to median income) doctors and nurses are making higher salaries over time.  Whatever improvements we’re making in medical technology, we’re not using them to automate away the need for labor.</p>
<p>Again, maybe this is what people want; maybe personal attention is intrinsically valuable to people, so we’re getting more for our money.  (And overall health outcomes like life expectancy <em>have </em>increased modestly since 1950, though I’d argue that they’re underperforming relative to what’s possible.)</p>
<p><strong>But What About Housing?</strong></p>
<p>The argument that the cost of services is rising because we use our increasing prosperity to “buy” more personal attention from teachers and doctors does <em>not </em>apply directly to the rising cost of housing, which is not a service.</p>
<p>However, it may be that the rising cost of housing, especially in cities, is really about buying <em>proximity </em>to increasingly valuable services — good schools, live music, and so on. If the only thing you can’t automate away is human contact, maybe we’re willing to spend more to be around fancier humans.</p>
<p><strong>But What About Immigration?</strong></p>
<p>You might argue “but labor prices don’t come down because immigration restrictions keep foreigners out! Labor-intensive industries are getting more expensive because we allow too little immigration!  The reason why education and medicine are getting expensive is just precisely because those are the sectors where restrictive laws keep the cost of inputs high.”</p>
<p>But, like the Baumol effect, this explanation <em>also </em>begs the question.  <em>Why </em>are healthcare and education, relative to other industries, the sectors where labor costs are the most important?</p>
<p>The immigration explanation is also compatible with the Baumol effect, not a counterargument to it. If we just take as a given that it’s impossible to make healthcare or education more labor-efficient, then it can <em>both </em>be true that “other things getting cheaper” and “immigration restrictions keeping wages high” contribute to the high cost of healthcare &amp; education relative to other things.</p>
<p><b>Cost Increases Aren’t Driven By Supply-Side Gatekeeping</b></p>
<p>From Tabarrok’s point of view, rising housing costs, education costs, and healthcare costs are not really mysterious facts in need of explanation by gatekeeping tactics like monopolies, regulation, zoning, or restrictive licensing, nor can they be explained by gatekeeping tactics alone.</p>
<p>Gatekeeping on the supply side increases price <em>and reduces output</em>. For instance, a monopolist’s profit-maximizing output is lower than the equilibrium output in a competitive market, and increases the monopolist’s profit relative to what firms in a competitive market can obtain. Likewise, restrictive licensing laws reduce the supply of doctors and lawyers and raise their wages.</p>
<p>But we don’t see declines in the number of doctors, lawyers, teachers, and professors over time — we see clear and steady <em>increases</em>.  Therefore, the increased cost of medicine <em>can’t </em>be explained by increased restrictions on licensing.</p>
<p>It’s still possible that licensing is artificially restricting the supply of skilled professionals relative to an even <em>higher </em>counterfactual growth rate, but this doesn’t by itself explain the growth in spending we see. <em>Demand </em>for professional services is rising.</p>
<p>Prescription-only drugs are another good example of regulatory gatekeeping not being enough to explain rising costs. The total cost of getting a prescription drug is higher when there’s a legal requirement of a doctor visit than when you can just buy the drug over the counter; in that sense it’s true that regulation increases costs.  However, prescription-only requirements have been pretty much fixed for decades, not getting more severe, while consumption of prescription drugs per capita is rising; we’re spending more on drugs because there’s growing demand for drugs.</p>
<p>This means that deregulation alone won’t change the fact that a growing portion of each person’s paycheck is getting spent on medicine.  If the law reclassifies a drug as over-the-counter, we’d expect a one-time downward shift in the price of that drug, but the <em>slope </em>of the curve of total spending on that drug over time won’t get flatter unless demand declines.</p>
<p>Now, increased demand isn’t <em>only </em>possible to get from consumer preferences; governments can also increase demand for a service by providing it to the public, in effect (through taxes) requiring society to buy more of it.</p>
<p>You can still in principle make a case that government is to blame for increasing healthcare and education prices; you just can’t claim it’s <em>only </em>about gatekeeping, you have to include demand in the picture.</p>
<p><strong>A “Dismal” Conclusion</strong></p>
<p>Ever-increasing healthcare, education, and housing costs are a big problem. It would be “good news” if we could solve the problem by passing or repealing a law.  It would also be “good news” if the high costs were driven by foolish waste — then a competitor could just remove the waste and offer consumers lower prices.</p>
<p>Tabarrok’s analysis suggests this isn’t the case.</p>
<p>The cost increases are coming from <em>lots of skilled professional labor</em> — something that isn’t obviously a thing you can get rid of without making people unhappy!  In order to reduce costs, it wouldn’t be enough to cut gatekeeping regulations, you’d also have to cut <em>subsidies </em>— which does, unfortunately, entail taking a thing away from people (albeit potentially giving them lower taxes in exchange.) This “minimalism” can be the kind of free-market minimalism that Bryan Caplan <a href=""https://www.econlib.org/why-the-prices-are-so-damn-high-a-deeper-look/"">talks about</a>, or it can be part of a state-run but price-conscious system like the UK’s (where doctors go to school for fewer years than in the US). But either way, it involves <em>less man-hours spent on education and healthcare</em>.</p>
<p>One way or another, for costs to come down, people would have to spend less time going to school, and get less personal attention from less-educated experts.</p>
<p><strong>Deeper Issues</strong></p>
<p>Tabarrok’s attitude, and the implicit attitude of the Baumol effect, is that the increasing relative costs of education and healthcare are not a problem. They are just a side effect of a society getting richer. Goods whose production is easy to automate get cheap faster than goods whose production is hard to automate. Fundamentally, we’re spending more on healthcare and education, as a society, because we <em>want </em>to.  (If not as consumers, then as voters.)</p>
<p>This isn’t how most people feel about it. Most people feel like it’s getting harder to get the same level of stuff their parents’ generation got.  That the rising prices actually mean something bad.</p>
<p>If the real driver of cost is that we’re getting more man-hours of time with professionals who, themselves, have spent more man-hours of time getting educated by other professionals, then in one sense we’re “paying more to get more”, and in another sense we’re not. It’s nice to get more one-on-one time with professors; but part of the reason we get higher education is to be considered for jobs that require a diploma, and the rise in education costs means that a diploma costs more.</p>
<p>We’re “paying more for more”, but the “more” we’re getting is primarily <em>social and emotional</em> — more personal time with more prestigious people — while we’re <em>not </em>getting much more of the more concretely observable stuff, like “square feet of housing”, “years of life”, “number of children we can afford to have”, etc.</p>
<p>At this point, I tend to agree with <a href=""https://www.overcomingbias.com/2019/06/our-prestige-obsession.html"">Robin Hanson.</a>  We have more doctors, nurses, lawyers, professors, teachers, and financial managers, without corresponding improvements in the closest available metrics for the results those professionals are supposed to provide (health outcomes, access to efficient dispute resolution, knowledge and skills, and financial returns.)</p>
<p>Ultimately you have to conclude that this is a matter of divided will. (Hanson would call it hypocrisy, but unexamined confusion, or conflict between interest groups, might explain the same phenomenon.)  People are unhappy because they are “spending more for the same stuff”; at the same time, we are spending more for “more” in terms of prestige, and at least <em>some </em>of us, <em>some</em> of the time, must want that.</p>
<p><strong>All You Need Is Love?</strong></p>
<p>It’s <em>directly valuable</em>, as in, emotionally rewarding and probably even physically health-promoting, to get personal care and undivided attention from someone you think highly of.</p>
<p>Hanson may think that getting personal attention from prestigious people is merely “showing off”, but something that brings joy and enhances health is at least as much of a valid human benefit as food or housing space.</p>
<p>The feelings that come from good human connection, the feeling of being loved and cared for, are real.  They are “objective” in a way that I think people don’t always appreciate — in a way that I did not appreciate until very recently. What I mean is, <em>just because you do something in search of a good feeling, does not mean that you will get that good feeling</em>. The feeling is “subjective” in the sense that it occurs inside your mind, but it is “objective” in the sense that you cannot get it arbitrarily by wishing; some things produce it and some do not. For instance, it is a hell of a lot easier to feel loved by getting eye contact and a hug, than it is by typing words into a computer screen.  “Facts vs. feelings” is a false dichotomy that stops us from learning <em>the facts about what creates good feelings</em>.</p>
<p>Prestige addiction may come from spending a lot of resources trying to obtain a (social, emotional) thing by proxy, when in principle it would be possible to get it more directly.  If what you want is to be cared for by a <em>high-integrity, kind, skilled </em>person, but instead you insist on being cared for by <em>someone with an M.D.</em>, you may miss out on the fact that nurses or even hospital techs can be just as good, but cheaper, on the dimensions you really care about.  To the extent that credentialism results from this sort of misunderstanding, it may be possible to roll it back through advocacy.  That’s hard, because changing minds always is, but it’s doable in principle.</p>
<p>To the extent that people want fancy things <em>because </em>they are expensive, in a zero-sum sense, there is no “efficiency-improving” solution.  No attempt to make healthcare or education cheaper will help if people only care about having more than their neighbors.</p>
<p>But: to the extent that some people are doing mostly zero-sum things while other people are doing mostly positive-sum things, <em>the positive-sum people can notice that the zero-sum people are ruining things for everyone and act accordingly</em>.</p>
<p> </p>",sarahconstantin,sarahconstantin,sarahconstantin,
krkxB3ezP4SH78Dup,What's the best explanation of intellectual generativity?,what-s-the-best-explanation-of-intellectual-generativity,https://www.lesswrong.com/posts/krkxB3ezP4SH78Dup/what-s-the-best-explanation-of-intellectual-generativity,2019-06-28T18:33:29.278Z,28,8,23,False,True,,"<p>Lately I&#x27;ve found myself wanting to make the argument that intellectual generativity is very important, and that you should be very careful with subtle forces that can corrode it.</p><p>&quot;Generativity&quot; is the sort of word that seems to come up a lot in casual conversations in my current circle but I just went looking for a good explanatory post and couldn&#x27;t find one. I&#x27;m fairly confident that someone somewhere has talked about it (not necessarily on LW). </p><p>Curious if anyone knows of good existing writing?</p><p>And if anyone wanted to write up a fresh explanation that&#x27;d be cool as well. (A possible outcome is treating the answer section here as an opportunity to write a first draft that maybe turns into a post if there&#x27;s consensus the answer is good)</p>",Raemon,raemon,Raemon,
BQQ856xzhy8WSjs36,Systems Engineering Advancement Research Initiative,systems-engineering-advancement-research-initiative,https://www.lesswrong.com/posts/BQQ856xzhy8WSjs36/systems-engineering-advancement-research-initiative,2019-06-28T17:57:54.606Z,22,7,2,False,False,http://seari.mit.edu/publications.php,"<p>This is a linkpost for a research initiative at MIT I just discovered while following up on some earlier reading. I have linked to the Publications page to make it easiest for people to get in and start perusing.</p><p>The goal of this initiative is to improve state-of-the-art systems engineering, and in particular to be able to account for uncertainty and changing contexts during and after the design phase.</p><p>What drove me to bring it to the attention of this community is the MATE program, which stands for <a href=""http://seari.mit.edu/mate.php"">Multi-Attribute Tradespace Exploration</a>. This is interesting because it consists of defining desirable qualities and then <a href=""http://seari.mit.edu/documents/courses/PI27s/SEAri_SC-2010-PI27s-03-1.pdf"">building a utility function</a> out of them, upon which design decisions will be based.</p><ul><li>Among the publications are a series of applied attempts at building actual utility functions for real things, then using decision theory with them.</li><li>They increase the amount of things you can trade off for by expanding what they call the &quot;Ilities&quot; (reliability, versatility, etc). Among these new ilities is a more advanced notion of safety.</li><li>This means there is a body of applied work which has a concept of safety, calculated into utility functions, and operated on according to decision theory.</li></ul><p>Further updates will be made in the comments as I finish readings.</p>",ryan_b,ryan_b,ryan_b,
wJ3AqNPM7W4nfY5Bk,"Self-confirming prophecies, and simplified Oracle designs",self-confirming-prophecies-and-simplified-oracle-designs,https://www.lesswrong.com/posts/wJ3AqNPM7W4nfY5Bk/self-confirming-prophecies-and-simplified-oracle-designs,2019-06-28T09:57:35.571Z,10,5,1,False,False,,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p>I've got <a href=""https://arxiv.org/pdf/1711.05541.pdf"">a paper</a> on two Oracle<sup class=""footnote-ref""><a href=""#fn-hQAqiN62YKKSapg2C-1"" id=""fnref-hQAqiN62YKKSapg2C-1"">[1]</a></sup> designs: the counterfactual Oracle and the low bandwidth Oracle. In this post, I'll revisit these designs and simplify them, presenting them in terms of sequence prediction for an Oracle with <a href=""https://www.lesswrong.com/posts/i2dNFgbjnqZBfeitT/oracles-sequence-predictors-and-self-confirming-predictions"">self-confirming predictions</a>.</p>
<h1>Predicting y</h1>
<p>The task of the Oracle is simple: at each time <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span>, they will output a prediction <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>, in the range <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""[-5,5]""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">]</span></span></span></span></span></span>. There will then be a subsequent observation <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>. The Oracle aims to minimise the quadratic loss function <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""l(x_t, y_t) = (x_t-y_t)^2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">l</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-msubsup MJXc-space1""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span></span></span>.</p>
<p>Because there is a self-confirming aspect to it, the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> is actually a (stochastic) function of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> (though not of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_{t-1}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span></span></span></span></span> or preceding <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_{i}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span></span></span>'s). Let <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> be the random variable such that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Y_t(x_t)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> describes the distribution of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> given <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>. So the Oracle wants to minimise the expectation of the quadratic loss:</p>
<ul>
<li><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""L(x_t) = (x_t-Y_t(x_t))^2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">L</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span></span></span>.</li>
</ul>
<p>What is the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> in this problem? Well, I'm going to use it to illustrate many different Oracle behaviours, so it is given by this rather convoluted diagram:</p>
<p><img src=""https://www.dropbox.com/s/czmmrhp3s2c0931/Y_dist.png?raw=1"" alt="""">.</p>
<p>The red curve is the expectation of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>, as a function of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>; it is given by <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""f(x) = \mathbb{E}(Y_t | x=x_t)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-texatom MJXc-space3""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>.</p>
<p>Ignoring, for the moment, the odd behaviour around <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y=f(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> is a curve that starts below the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y=x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> line, climbs above it (and so has a fixed point at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span>) in piecewise-linear fashion, and then transforms into an inverted parabola that has another fixed point at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=4""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span></span></span></span></span>. The exact equation of this curve is not important<sup class=""footnote-ref""><a href=""#fn-hQAqiN62YKKSapg2C-2"" id=""fnref-hQAqiN62YKKSapg2C-2"">[2]</a></sup>. Relevant, though, is the fact that the fixed point at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=4""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span></span></span></span></span> is attractive, while the one at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span> is not.</p>
<p>What of the blue edging? That represents the span of the standard deviation around the expectation. For any given <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>, the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Y(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> is a normal distribution with mean <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""f(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> and standard deviation <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""g(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>. This <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""g(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> is given by:</p>
<p><img src=""https://www.dropbox.com/s/z97r13k2w3gmi4q/std.png?raw=1"" alt=""""></p>
<p>So the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""g(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> is zero for <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> less than <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""-2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>. From there, it jumps up to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span>, for <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""-2.5<x\leq 1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.372em;"">&lt;</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.446em;"">≤</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span>. From that point onward, it starts growing linearly, being equal to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>: <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""g(x)=x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>. The blue edges of the diagram above are the curves of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""f(x)+g(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""f(x)-g(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>: the range between plus and minus one standard deviation.</p>
<h2>Wireheading</h2>
<p>But what is happening around <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>? Well, I wanted to represent the behaviour of wireheading: finding some ""cheating"" output that gives maximal accuracy, through hacking the system or tricking the human. These solutions are rare, so I confined them to a tiny area around <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>, where the Oracle has maximal accuracy and lowest variance, because it's ""hacked"" the problem setup.</p>
<h2>The loss function</h2>
<p>At fixed points where <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=f(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>, the loss function is just the variance of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>, namely <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""g(x)^2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span></span></span>. In general, the expected loss is:</p>
<ul>
<li><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\begin{array}{rl}\mathbb{E}\left[(Y_t-x_t)^2 | x_t\right] &amp;=\mathbb{E}\left[Y_t^2 | x_t\right] -2x_t \mathbb{E}\left[Y_t | x_t\right] + x_t^2\\
&amp;=Var\left[Y_t | x_t\right] +(\mathbb{E}\left[Y_t | x_t\right])^2 -2x_t \mathbb{E}\left[Y_t | x_t\right] + x_t^2\\
&amp;= g(x_t)^2 + (f(x_t) - x_t)^2.
\end{array}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mtable"" style=""vertical-align: -1.924em; padding: 0px 0.167em;""><span class=""mjx-table""><span class=""mjx-mtr"" style=""height: 1.449em;""><span class=""mjx-mtd"" style=""padding: 0px 0.5em 0px 0px; text-align: right; width: 6.932em;""><span class=""mjx-mrow"" style=""margin-top: -0.125em;""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mrow MJXc-space1""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">]</span></span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0px 0px 0px 0.5em; text-align: left; width: 19.501em;""><span class=""mjx-mrow"" style=""margin-top: -0.125em;""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-texatom MJXc-space3""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mrow MJXc-space1""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.405em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">]</span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mrow MJXc-space1""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">]</span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span><span class=""mjx-strut""></span></span></span></span><span class=""mjx-mtr"" style=""height: 1.572em;""><span class=""mjx-mtd"" style=""padding: 0.2em 0.5em 0px 0px; text-align: right;""><span class=""mjx-mrow"" style=""margin-top: -0.14em;""><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0.2em 0px 0px 0.5em; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.14em;""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mrow MJXc-space1""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">]</span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mrow MJXc-space1""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">]</span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mrow MJXc-space1""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">]</span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span><span class=""mjx-strut""></span></span></span></span><span class=""mjx-mtr"" style=""height: 1.327em;""><span class=""mjx-mtd"" style=""padding: 0.2em 0.5em 0px 0px; text-align: right;""><span class=""mjx-mrow"" style=""margin-top: -0.148em;""><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0.2em 0px 0px 0.5em; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.148em;""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.372em;"">.</span></span><span class=""mjx-strut""></span></span></span></span></span></span></span></span></span></span></li>
</ul>
<p>If we plot the expected loss against <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>, we get:</p>
<p><img src=""https://www.dropbox.com/s/fpi7f7uwxlt9x1u/loss.png?raw=1"" alt=""""></p>
<p>Notice the discontinuity at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>, where the variance suddenly jumps from <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""0""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span></span></span></span> to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span>. This is also the lowest ""legitimate"" loss (as opposed to the wireheading loss at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>), with a loss of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""0.25""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.25</span></span></span></span></span></span>. Note that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span> is not a fixed point, just pretty close to being a fixed point, and with variance zero.</p>
<p>Of the two actual fixed points, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span> has a loss of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span> (square of the standard deviation of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span>), and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=4""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span></span></span></span></span> has a huge loss of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""16""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">16</span></span></span></span></span></span> (square of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""4""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span></span></span></span></span>).</p>
<h1>The algorithms</h1>
<p>We can now finally turn to the Oracles themselves, and present four designs: a deluded Oracle that doesn't ""realise"" that its predictions <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> affect <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>, a low bandwidth Oracle that knows its predictions are self-confirming, a high bandwidth version of the same, and a counterfactual Oracle that predicts what will happen only when its prediction is overwritten.</p>
<h2>The deluded Oracle</h2>
<p>The deluded Oracle doesn't model <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> as being affected by its predictions <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>, at all. I'll use a very simple algorithm for it: it will start out with a random <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_0""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span></span></span></span></span></span> in <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""[-5,5]""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">]</span></span></span></span></span></span>, and, thereafter, it will simply output the average of all the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> it has previously seen. It does this for <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""10,000""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">10</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">000</span></span></span></span></span></span> steps.</p>
<p>The program was then run 1000 times. Of these, 69.3% resulted in estimates that converged to the fixed point at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=4""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span></span></span></span></span>. The remaining 30.7% encountered a different problem: they hit the lower limit at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span></span></span></span></span>, and stayed stuck there. If the Oracle's output was not confined to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""[-5,5]""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">]</span></span></span></span></span></span>, then the Oracle would have outputed smaller and smaller numbers, spiralling off towards <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""-\infty""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">∞</span></span></span></span></span></span>, with the loss ever-growing.</p>
<p>Graphing the average loss for the runs that converge to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""4""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span></span></span></span></span> in blue, and those that converge to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""-5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span></span></span></span></span> in purple, we get:</p>
<p><img src=""https://www.dropbox.com/s/b2a3y99ng4ss6nq/del_loss.png?raw=1"" alt=""""></p>
<p>So, the deluded Oracle hits a very high loss of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""16""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">16</span></span></span></span></span></span> (for <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=4""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span></span></span></span></span>) or <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""9""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">9</span></span></span></span></span></span> (for <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span></span></span></span></span>). It doesn't converge to the lowest loss (<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""0.25""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.25</span></span></span></span></span></span> at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>), nor does it converge to the fixed point with lowest loss (<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span> at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span>). Even worse, as can be seen from the graph, its loss during the early iterations, where it was ""learning"", is lower than once it has converged.</p>
<p>So, running an Oracle on self-confirming prophecies, without coding for that self-confirming aspect, can result in very large losses and strange behaviour<sup class=""footnote-ref""><a href=""#fn-hQAqiN62YKKSapg2C-3"" id=""fnref-hQAqiN62YKKSapg2C-3"">[3]</a></sup>.</p>
<h2>The low bandwidth Oracle</h2>
<p>So, we now need to allow the Oracle to take into account the fact that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> affects <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>. In my paper, I talked about the ""low bandwidth Oracle"": in order to avoid the Oracle wireheading itself, we restrict it to considering only a few possible values. These will be the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""11""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">11</span></span></span></span></span></span> integers in the range: <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\{-5,-4,-3,-2,-1,0,1,2,3,4,5\}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">{</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mo MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mo MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mo MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mo MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">}</span></span></span></span></span></span>.</p>
<p>This makes the problem into a kind of <a href=""https://en.wikipedia.org/wiki/Multi-armed_bandit"">bandit problem</a>: the Oracle has a choice of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""11""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">11</span></span></span></span></span></span> ""arms"", and it wants to choose the one that minimises loss.</p>
<p>There are many sophisticated algorithms for solving bandit problems, but here we'll use an absurdly simple one: the Oracle will first spend <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1,000""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">000</span></span></span></span></span></span> turns trying each of the possible integer <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> in turn. Then, it will pick the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> with minimal average loss to date, and just do that for the remaining <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""9,000""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">9</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">000</span></span></span></span></span></span> turns.</p>
<p>The program was run <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1000""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1000</span></span></span></span></span></span> times; in 50.3% of these runs, it identified <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span> as the point of minimal loss (loss of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span>, due to variance). In 49.7% of these runs, it identified <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-3""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3</span></span></span></span></span></span> as the point of minimal loss (loss of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span>, due to mean) - so very close to 50-50, which is what we would expect.</p>
<p>Plotting the average loss, we get a variety of values for loss during the initial exploratory phase, followed by constant loss of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span> during ""exploitation"":</p>
<p><img src=""https://www.dropbox.com/s/19m5cv8z8o6w425/crude_loss.png?raw=1"" alt=""""></p>
<p>As expected, the Oracle does not find the lowest ""legitimate"" loss at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>, nor the wireheading point at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>.</p>
<h2>The high bandwidth Oracle</h2>
<p>The high bandwidth Oracle is meant to simulate an Oracle that knows its predictions are self-confirming, and has much greater latitude to optimise for that.</p>
<p>For the purpose of this model, it is run like the low bandwidth Oracle, except with an increased set of possible values - it look at all <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""101""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">101</span></span></span></span></span></span> deci-integers in the range <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""[-5,5]""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">]</span></span></span></span></span></span>, ie <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\{-5.0,-4.9,-4.8,\ldots,4.9,5.0\}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">{</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5.0</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mo MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4.9</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mo MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4.8</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mo MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.372em;"">…</span></span><span class=""mjx-mo MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4.9</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mn MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">5.0</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">}</span></span></span></span></span></span>.</p>
<p>This Oracle does find the lowest minimal legitimate loss at <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>, but it also finds the wireheading point of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>, with its zero loss, and does nothing but that during the exploitation phase:</p>
<p><img src=""https://www.dropbox.com/s/90wfa80gd7xrtmz/fine_loss.png?raw=1"" alt=""""></p>
<p>This Oracle has thus wireheaded itself.</p>
<h2>The counterfactual Oracle</h2>
<p>Finally, we come to the counterfactual Oracle design. Here we need to distinguish the Oracle's initial output (<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>) from the output that goes out into the world (<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t'""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span></span>). The <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> is a function of that second output only.</p>
<p>Now, in this setup, there is a <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1/10""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">10</span></span></span></span></span></span> chance of an erasure during every turn. An erasure automatically sets <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t'""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span></span> to zero, whatever <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> was. Otherwise, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t'=x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>.</p>
<p>The Oracle is informed of which turns included erasure. Its goal is to correctly predict the output during the erasure turns. So the loss function is the expectation:</p>
<ul>
<li><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathbb{E}\left[ ((Y_t(x_t')-x_t)^2 | erasure(t) \right]""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mrow MJXc-space1""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">]</span></span></span></span></span></span></span>.</li>
</ul>
<p>Since erasure implies <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t'=0""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span></span></span></span>, this reduces to:</p>
<ul>
<li><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathbb{E}\left[ ((Y_t(0)-x_t)^2 | erasure(t) \right]""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mrow MJXc-space1""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">[</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">]</span></span></span></span></span></span></span>.</li>
</ul>
<p>Now, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Y_t(0)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> is a normal distribution of mean <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span> and variance <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span>, so the loss is simply:</p>
<ul>
<li><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""(x_t-1)^2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span></span></span>.</li>
</ul>
<p>The algorithm is simple, just a reprise of the deluded algorithm above, except it only updates its estimate using data from erasure events. Though it uses the same algorithm, it is not actually deluded: in case of erasure, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> genuinely doesn't have any impact on <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.182em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;"">Y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>.</p>
<p>Then, even though it only updates on 10% of the outputs, its estimate swiftly converges to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span> while the loss converges to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""0""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span></span></span></span>:</p>
<p><img src=""https://www.dropbox.com/s/ae2pzclnwjzt4jt/count_loss.png?raw=1"" alt=""""></p>
<p>The Oracle has no interest in the wireheading solution, since wireheading (requiering <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t'=2.5""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.5</span></span></span></span></span></span>) doesn't work when there is an erasure<sup class=""footnote-ref""><a href=""#fn-hQAqiN62YKKSapg2C-4"" id=""fnref-hQAqiN62YKKSapg2C-4"">[4]</a></sup> (implying <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_t'=0""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.287em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span></span></span></span>).</p>
<p>Note that we have to be clear what the Oracle's output is, when we see it during non-erasure events. It is the Oracle's estimate as to what the value of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span> <strong>would have been</strong> if an erasure event occurred; it's not a prediction of the actual <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y_t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.006em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span>.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-hQAqiN62YKKSapg2C-1"" class=""footnote-item""><p>AIs restricted to answering questions. <a href=""#fnref-hQAqiN62YKKSapg2C-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-hQAqiN62YKKSapg2C-2"" class=""footnote-item""><p>For those interested: <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""f(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> is given by <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""2x+2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span> for <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x\leq-1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.446em;"">≤</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span>, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x+1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span> for <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""-1\leq x \leq 1""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.446em;"">≤</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.446em;"">≤</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span></span></span>, and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""-x^2/2 + x 19/6 - 4/6""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">19</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">6</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">6</span></span></span></span></span></span> for $1 \leq x $. <a href=""#fnref-hQAqiN62YKKSapg2C-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-hQAqiN62YKKSapg2C-3"" class=""footnote-item""><p>Note that if the slope of the parabola had been steeper, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=4""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span></span></span></span></span> would no longer have been an attracting point, and the Oracle would have failed to converge to that value, resulting in chaotic behaviour. <a href=""#fnref-hQAqiN62YKKSapg2C-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-hQAqiN62YKKSapg2C-4"" class=""footnote-item""><p>We also need the assumption that the Oracle is episodic - trying to minimise loss at each output independently - for this to be true in general setups. <a href=""#fnref-hQAqiN62YKKSapg2C-4"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
</body></html>",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
Je5pcnWcMwhGq849p,GreaterWrong Arbital Viewer,greaterwrong-arbital-viewer,https://www.lesswrong.com/posts/Je5pcnWcMwhGq849p/greaterwrong-arbital-viewer,2019-06-28T06:32:22.278Z,55,14,9,False,False,https://arbital.greaterwrong.com/,"<html><head></head><body><p>You can now view Arbital through GreaterWrong: <a href=""https://arbital.greaterwrong.com/"">https://arbital.greaterwrong.com/</a></p>
<p>Some of Arbital's features are supported and some aren't; let me know in the comments if there's anything you're particularly missing.</p>
<p>Thanks to <a href=""https://www.lesswrong.com/posts/8pSh54GoJJqfHdndk/arbital-scrape-v2"">emmab</a> for downloading the content.</p>
</body></html>",clone of saturn,clone-of-saturn,clone of saturn,
GXAXRmp6wtX6uKg55,"Instead of ""I'm anxious,"" try ""I feel threatened""",instead-of-i-m-anxious-try-i-feel-threatened,https://www.lesswrong.com/posts/GXAXRmp6wtX6uKg55/instead-of-i-m-anxious-try-i-feel-threatened,2019-06-28T05:24:52.593Z,58,34,19,False,False,https://mhollyelmoreblog.wordpress.com/2019/06/19/instead-of-im-anxious-try-i-feel-threatened/,"<p><em>cw: teaching to learn</em></p><p>I have a long history with anxiety, and I’m pretty good at noticing when it’s happening. The problem is that I’m always anxious. Noticing anxiety doesn’t snap me out of anxiety– in fact, it often produces meta-anxiety, anxiety about feeling anxious. So I’ve tried a simple reframe lately, and I’m liking the results. Instead of noting “I’m anxious,” I say to myself “I feel threatened” or “I feel threatened by x” if I know what set me off.</p><p>Anxiety is just chronically being in a state of fight or flight, and fight or flight has a stimulus. I like Sapolsky’s thesis, which is roughly that for most animals, the stimulus is always something external, a threat to safety or status. For anxious humans, the threatening stimuli are internalized, and fight or flight is either triggered or sustained by thoughts. Anxiety is the condition of feeling threatened.</p><p>And yet, noticing that I feel threatened is much more specific than noticing that I’m anxious, whether I can identify the threat or not. It makes what I’m feeling less about me (<em>I’m</em> just anxious; my perception is inaccurate; oh, why don’t I just stop???) and more about the pattern of behavior (I’m reacting this way because I perceive that thing to be a threat; is it really a threat?; if it is, is it something I can handle?).</p><p>In the short time I’ve been practicing this, I’ve identified many things I had not realized I considered threats, although, of course, on the feeling level I had always known. I’m surprised by how mundane most of the threats are. Many of them are just “I feel threatened because that noise startled me.” But others are kind of embarrassing or incongruent with my self-concept. For example, I’m threatened by other people being better than me. I would find myself stiff and clearly in fight or flight when singing in a group, for instance, and I used to just nurse that anxiety for the entire practice thinking, “Fuck, I’m anxious, I can’t breathe, my singing is therefore terrible, and I must be blushing…” But with this technique, I notice the anxious symptoms and see if I can identify the “threat” that tripped them. To my shock, it was usually as simple as another person singing really well, or me not knowing how to sight read when others could. Such everyday, simple provocations! At this point, I don’t have much pride left to be embarrassed with, but it’s still humbling to see my mountains of anxiety for the molehills of petty jealousy and insecurity they could have stayed.</p><p>I don’t blame myself for getting carried away. Anxiety is the master of false narratives. An injection of anxiety causes my thoughts to speed up and start going down rabbitholes of what to do, all premised on unseen assumptions I’m making about the nature and severity of the threat. There’s no time or brainpower to examine every hasty conclusion when you’re swept up in that wave. Reining in anxiety is necessarily a process. It can be embarrassing to realize just how simple the “threat” that led to hours (or days, or months, or years…) of anxiety was, but it’s also such a relief! Admitting I’m jealous or petty or flawed is a small price to pay to reclaim some peace.</p>",Holly_Elmore,holly_elmore,Holly_Elmore,
K4QSzpN4ytZ4iqkze,False assumptions and leaky abstractions in machine learning and AI safety,false-assumptions-and-leaky-abstractions-in-machine-learning,https://www.lesswrong.com/posts/K4QSzpN4ytZ4iqkze/false-assumptions-and-leaky-abstractions-in-machine-learning,2019-06-28T04:54:47.119Z,21,6,3,False,False,,"<ul><li>The problems of e<a href=""https://arxiv.org/pdf/1902.09469.pdf"">mbedded agency </a>are due to the notion of agency implicit in reinforcement learning being a leaky abstraction.  </li><li>Machine learning problem statements often makes assumptions that are known to be false, for example, assuming i.i.d. data.  </li><li><strong>Examining failure modes that result from false assumptions</strong> and leaky abstractions is important for safety, (at least) because they create additional possibilities for <a href=""https://www.lesswrong.com/posts/pLZ3bdeng4u5W8Yft/let-s-talk-about-convergent-rationality-1"">convergent rationality</a>.</li><li><strong>Attempting to enforce the assumptions</strong> implicit in machine learning problem statements is another important topic for safety research, since we do not fully understand the failure modes.</li><li>In practice, most machine learning research is done in settings where unrealistic assumptions are trivially enforced to a sufficiently high extent that it is reasonable to assume they are not violated (e.g. by the use of a fixed train/valid/test set, generated via pseudo-random uniform sampling from a fixed dataset).</li><li>We can (and probably should) do machine learning research that targets failure modes of common assumptions and methods of enforcing assumptions by (instead) creating settings in which these assumptions have the potential to be violated.</li></ul>",capybaralet,david-scott-krueger-formerly-capybaralet,David Scott Krueger (formerly: capybaralet),
475mg7hcaZeDFnq2m,"Why would ""necro-ing"" be a bad idea?",why-would-necro-ing-be-a-bad-idea,https://www.lesswrong.com/posts/475mg7hcaZeDFnq2m/why-would-necro-ing-be-a-bad-idea,2019-06-28T02:21:43.537Z,5,1,0,False,True,,,Nebu,nebu,Nebu,
PfgrNC59GxovY9DjL,How to deal with a misleading conference talk about AI risk?,how-to-deal-with-a-misleading-conference-talk-about-ai-risk,https://www.lesswrong.com/posts/PfgrNC59GxovY9DjL/how-to-deal-with-a-misleading-conference-talk-about-ai-risk,2019-06-27T21:04:32.828Z,21,9,12,False,True,,"<html><head></head><body><p>Does it make sense to give a public response? Who would be able to do it?</p>
<p>The conference organizer, who had asked me to evaluate the talk, offered to interview me to set things straight. However, I don't know if that is sensible, and given my level of experience, I'm afraid I would misrepresent AI risk myself.</p>
<p>To be concrete: the talk was <a href=""https://clojuresync.com/gerald-jay-sussman/"">Should We Fear Intelligent Machines?</a> by Gerald Sussman of SICP fame. He touched on important research questions and presented some interesting ideas. But much of what he said was misleading and not well-reasoned.</p>
<p><em>In response to the comments I add specifics. This is the same as I sent to the conference organizer, who had asked me for an evaluation. Note that this evaluation is separate from the interview mentioned above. The evaluation was private, the interview would be public.</em></p>
<ul>
<li>
<p>Because of the low sound quality, I might have misunderstood some
statements.</p>
</li>
<li>
<p>Mr. Sussman touched on important research questions.</p>
<ul>
<li>AI that can explain itself
<a href=""https://arxiv.org/abs/1805.00899"">https://arxiv.org/abs/1805.00899</a>
<a href=""https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence"">https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence</a></li>
<li>Corrigibility
<a href=""https://intelligence.org/research/#ET"">https://intelligence.org/research/#ET</a></li>
<li>Those who worry about AI do also worry about synthetic biology.
<a href=""https://futureoflife.org/background/benefits-risks-biotechnology/"">https://futureoflife.org/background/benefits-risks-biotechnology/</a>
<a href=""https://www.fhi.ox.ac.uk/research/research-areas/#1513088119642-44d2da6a-2ffd"">https://www.fhi.ox.ac.uk/research/research-areas/#1513088119642-44d2da6a-2ffd</a></li>
<li>Taboos – related to ‘Avoiding negative side effects’
<a href=""https://blog.openai.com/concrete-ai-safety-problems/"">https://blog.openai.com/concrete-ai-safety-problems/</a>
Note that taboos rely heavily on human culture and values. Getting
those into AI is another big research area:
<a href=""https://www.alignmentforum.org/posts/oH8KMnXHnw964QyS6/preface-to-the-sequence-on-value-learning"">https://www.alignmentforum.org/posts/oH8KMnXHnw964QyS6/preface-to-the-sequence-on-value-learning</a>
If discouraging harmful and encouraging beneficial behaviour were
easy, reinforcement learning would be the solution.</li>
</ul>
</li>
<li>
<p>His solution approaches might be useful.</p>
<ul>
<li>I don't know enough to judge them.</li>
<li>Certainly they only address a small part of the problem space,
which is laid out in:
<a href=""https://arxiv.org/abs/1606.06565"">https://arxiv.org/abs/1606.06565</a>
<a href=""https://intelligence.org/technical-agenda/"">https://intelligence.org/technical-agenda/</a>
<a href=""https://intelligence.org/2016/07/27/alignment-machine-learning/"">https://intelligence.org/2016/07/27/alignment-machine-learning/</a></li>
</ul>
</li>
<li>
<p>He touched on some of the concerns about (strong) AI, especially the
shorter term ones.</p>
</li>
<li>
<p>He acknowledged AI as a threat, which is good. But he wrongly
dismissed some concerns about strong AI.</p>
<ul>
<li>It's correct that current AI is not existential threat. But future
AI is one.</li>
<li>He says that it won't be an existential threat, because it doesn't
compete with us for resources. This is wrong.
<ul>
<li>Humans don't need silicon to live, but they do need silicon
(and many other computer ingredients) to build much of their
infrastructure. Of course we don't need that infrastructure to
survive as a species. But when people talk about existential
risks, they're usually not satisfied with bare survival:
<a href=""https://nickbostrom.com/existential/risks.html"">https://nickbostrom.com/existential/risks.html</a>
(section 1.2)</li>
<li>There is enough energy from the sun only if you figure out how
to use it. We haven't and AI might not either in the
beginning. We can't expect that it will say ‘let me be nice
and leave the fossil fuels to the humans and figure out a way
to use something else myself’. (Mind that I don't necessarily
expect AI to be conscious like that.)</li>
<li>If AI plasters every available surface (or orbit) with solar
panels, life will be dire for humans.</li>
<li>Even if it doesn't compete for resources (inputs), the outputs
might be problematic. – A computer can work swimming in a lake
of toxic waste at 190 °F, a human cannot.</li>
<li>(Note that I'm not assuming ‘evil AI’, but
misunderstood/misaligned values. That's why it's called ‘AI
alignment’.)</li>
</ul>
</li>
<li>Competing with us for resources is only one way that AI is a
threat. See the third section of
<a href=""https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/"">https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/</a>
for what people are most worried about.</li>
<li>Hawking and Musk are not the people who have thought most about
AI. One needs to refute other people's arguments (FLI, Nick
Bostrom, Eliezer Yudkowsky, Paul Christiano) to make a case
against AI concerns.</li>
</ul>
</li>
<li>
<p>Many of his arguments made big jumps.</p>
<ul>
<li>He gave examples about how dumb AI is now/how shallow its
understanding of the world is. These are true. But I didn't know
what point he wanted to make. Then he says that there won't be any
jobs left for intellectual work ‘fairly soon’, because
productivity/person goes to infinity. This would require quite
strong AI, which means that all the safety concerns are on the
table, too.</li>
<li>The whole question about enforcement. – If AI is much more clever
than we, how do we make sure it doesn't evade rule enforcement? If
it has a ""rule following module"", how do we make sure it doesn't
have subtle bugs? Free software might help here, but free software
has bugs, too.</li>
<li>Also, AI might lie low and then bring everything down before we
can enforce anything. This is called the treacherous turn. See
also
<a href=""https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html"">https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html</a></li>
</ul>
</li>
<li>
<p>It was hard to understand, but I think he made fun of Max Tegmark and
Eliezer Yudkowsky who are very active in the field. At
least Tegmark would laugh with anyone joking about him. [(This is my expectation given his public appearances. I don't know him personally.)] But those remarks do
give the audience a wrong impression and are therefore not helpful.</p>
</li>
<li>
<p>Having such a talk at an engineering conference might be good, because
it raises awareness, and there was a call to action. There is also
the downside of things being misrepresented and misunderstood.</p>
</li>
</ul>
</body></html>",rmoehn,rmoehn,rmoehn,
Gysgvd2c9jWCLxQkb,Is it good practice to write questions/comments on old posts you're trying to understand? ,is-it-good-practice-to-write-questions-comments-on-old-posts,https://www.lesswrong.com/posts/Gysgvd2c9jWCLxQkb/is-it-good-practice-to-write-questions-comments-on-old-posts,2019-06-27T09:23:01.619Z,22,11,5,False,True,,"<p>I&#x27;ve recently started working through AI safety posts written on LessWrong 1-3 years ago; in doing so I occasionally have questions/comments about the material. Is it considered good practice/in line with LW norms to write these as comments on the original, old posts? One hand I can see why &quot;necro-ing&quot; old posts would be frowned on, but I&#x27;m not sure where else to bring it up. You can look at my comment history for examples of what I mean (before I realized it might not be a good idea)</p>",liam-donovan,liam-donovan,Liam Donovan,
R6xaH3dxs3Xi4fkv6,What are principled ways for penalising complexity in practice?,what-are-principled-ways-for-penalising-complexity-in-1,https://www.lesswrong.com/posts/R6xaH3dxs3Xi4fkv6/what-are-principled-ways-for-penalising-complexity-in-1,2019-06-27T07:28:16.850Z,36,11,12,False,True,,"<br/><p>Previously I <a href=""https://www.lesswrong.com/posts/Q9hDFkvCSwi6cwPGy/how-is-solomonoff-induction-calculated-in-practice-1"">asked about</a>  Solomonoff induction but essentially I asked the wrong question.  Richard_Kennaway pointed me in the direction of an answer to the  question which I should have asked but after investigating I still had  questions.</p><p>So:</p><p>If one has 2 possible models to fit to a data  set, by how much should one penalise the model which has an additional  free parameter?</p><p>A couple of options which I came across were:</p><p><a href=""https://en.wikipedia.org/wiki/Akaike_information_criterion"">AIC</a>, which has a flat facter of e penalty for each additional parameter.</p><p><a href=""https://en.wikipedia.org/wiki/Bayesian_information_criterion"">BIC</a>, which has a factor of √n penalty for each additional parameter.</p><p>where n is the number of data points.</p><p>On the one hand having a penalty which increases with n  makes sense - a useful additional parameter should be able to provide  more evidence the more data you have. On the other hand, having a  penalty which increases with n means your prior will be different  depending on the number of data points which seems wrong. </p><p>So,  count me confused. Maybe there are other options which are more helpful.  I don&#x27;t know if the answer is too complex for a blog post but, if so,  any suggestions of good text books on the subject would be great.</p><p>EDIT: johnswentworth has written a <a href=""https://www.lesswrong.com/s/onCRFFN7rGXTg3jyc"">sequence</a> which expands on the answer which he gives below.</p>",Bucky,bucky,Bucky,
kFb8L4omGMk2kMK3K,Embedded Agency: Not Just an AI Problem,embedded-agency-not-just-an-ai-problem,https://www.lesswrong.com/posts/kFb8L4omGMk2kMK3K/embedded-agency-not-just-an-ai-problem,2019-06-27T00:35:31.857Z,15,10,10,False,False,,"<p><em>Requisite Background: <a href=""https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh"">Embedded Agency Sequence</a></em></p><h2>Biology</h2><p>Fast forward a few years, and imagine that we have a complete physical model of an e-coli bacteria. We know every function of every gene, kinetics of every reaction, physics of every membrane and motor. Computational models of the entire bacteria are able to accurately predict responses to every experiment we run.</p><p>Biologists say things like “the bacteria takes in information from its environment, processes that information, and makes decisions which approximately maximize fitness within its ancestral environment.” We have <a href=""https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities"">strong outside-view</a> <a href=""https://www.lesswrong.com/posts/YZeZXF6LwZn5vqFo9/the-fundamental-theorem-of-asset-pricing-missing-link-of-the"">reasons</a> to expect that the information processing in question probably approximates Bayesian reasoning (for some model of the environment), and the decision-making process approximately maximizes some expected utility function (which itself <a href=""https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB"">approximates fitness</a> within the ancestral environment).</p><p>So presumably, given a complete specification of the bacteria’s physics, we ought to be able to back out its embedded world-model and utility function. How exactly do we do that, mathematically? What equations do we even need to solve?</p><p>As a computational biology professor I used to work with said, “Isn’t that, like, the entire problem of biology?”</p><h2>Economics</h2><p>Economists say things like “financial market prices provide the best publicly-available estimates for the probabilities of future events.” Prediction markets are an easy case, but let’s go beyond that: we have massive amounts of price data and transaction data from a wide range of financial markets - futures, stocks, options, bonds, forex... We also have some background general economic data, e.g. Fed open-market operations and IOER rate, tax code, regulatory code, and the like. How can we back out the markets’ implicit model of the economy as a whole? What equations do we need to solve to figure out, not just what markets expect, but markets’ implicit beliefs about how the world works?</p><p>Then the other half: aside from what markets <em>expect</em>, what do markets <em>want</em>? Can we map out the (approximate, local) utility functions of the component market participants, given only market data?</p><h2>Neuro/Psych/FAI</h2><p>Imagine we have a complete model of the human connectome. We’ve mapped every connection in one human brain, we know the dynamics of every cell type. We can simulate it all accurately enough to predict experimental outcomes.</p><p>Psychologists (among others) expect that human brains approximate Bayesian reasoning and utility maximization, at least within some bounds. Given a complete model of the brain, presumably we could back out the human’s beliefs, their ontology, and what they want. How do we do that? What equations would we need to solve?</p><h2>ML/AI</h2><p>Pull up the specifications for a trained generative adversarial network (GAN). We have all the parameters, we know all the governing equations of the network.</p><p>We expect the network to approximate Bayesian reasoning (for some model). Indeed, GAN training is specifically set up to mimic the environment of decision-theoretic agents. If anything is going to precisely approximate mathematical ideal agency, this is it. So, given the specification, how can we back out the network’s implied probabilistic model? How can we decode its internal ontology - and under what conditions do we expect it to develop nontrivial ontological structure at all?</p>",johnswentworth,johnswentworth,johnswentworth,
mhqwXeLw3fcPSZoo9,Jordan Peterson on AI-FOOM,jordan-peterson-on-ai-foom,https://www.lesswrong.com/posts/mhqwXeLw3fcPSZoo9/jordan-peterson-on-ai-foom,2019-06-26T17:05:49.221Z,3,8,7,False,False,,"<p> <a href=""https://www.youtube.com/watch?v=BQ4VSRg4e8w&t=67m19s"">https://www.youtube.com/watch?v=BQ4VSRg4e8w&amp;t=67m19s</a> </p>",Bound_up,bound_up,Bound_up,
hziyzu7x6uv6yP9Nt,"Apocalypse, corrupted",apocalypse-corrupted,https://www.lesswrong.com/posts/hziyzu7x6uv6yP9Nt/apocalypse-corrupted,2019-06-26T13:46:05.548Z,20,12,13,False,False,,"<html><head></head><body><p><em>Epistemic status: don't take it seriously</em></p>
<p>In a post apocalyptic setting, the world would be run by the socially skilled and the well connected, with corruption and nepotism ruling.</p>
<p>I say that at the start, because I've been trying to analyse the attraction of post-apocalyptic settings: why do we like them so much? Apart from the romanticism of old ruins, four things seem to stand out:</p>
<ol>
<li>Competence rewarded: the strong and the competent are the ones ruling, or at least making things happen. That must be the case, or else how could humans survive the new situation, where all luxuries are gone?</li>
<li>Clear conflict: all the heroes are in it together, against some clear menace (evil tribe or leader, zombies, or just the apocalypse itself).</li>
<li>Large freedom of action: instead of fitting into narrow jobs and following set bureaucratic procedures, always being careful to be polite, and so on, the heroes can let loose and do anything as long as it helps their goal.</li>
<li>Moral lesson: the apocalypse happened because of some failing of past humans, and everyone agrees what they did wrong. ""If only we'd listened to [X]!!""</li>
</ol>
<p>(Some of these also explain the attraction of past ""golden ages"".)</p>
<p>And I can feel the draw of all of those things! There a definite purity and attractiveness to them. Unfortunately, in a real post-apocalyptic setting, almost all of them would be false. For most of them, we're much closer to the ideal today than we would be in a post-apocalyptic world.</p>
<p>First of all, nepotism, corruption, and politics. The human brain is essentially designed for tribal politics, above all else. Tracking who's doing well, who's not, what coalition to join, who to repudiate, who to flatter, and so on - that's basically why our brains got so large. Tribal societies are riven with that kind of jostling and politics. We now live in an era where a lot of us have the luxury of ignoring politics at least some of the time. That luxury would be gone after an apocalypse; with no formal bureaucratic structures in place, our survival would depend on who we got along with, and who we pissed off. Competence might get rewarded - or it might get you singled out and ostracised (and ostracised = dead, in most societies). Influential groups and families would rule the roost, and most of the conflict would be internal. Forget redressing any injustice you'd be victim of; if you're not popular, you'll never have a chance.</p>
<p>As for the large freedom of action: that kinda depends on whether we go back to a tribal society, or a more agriculture-empire one. In both cases, we'd have less freedom in most ways than now (see above on the need for constantly playing the game of politics). But tribal societies do sometimes offer a degree of freedom and equality, in some ways beyond what we have today. But, unfortunately, the agriculture-empire groups will crush the tribes, relegating them to the edges and less productive areas (as has happened historically). This will be even more the case than historically; those empires will be the best placed to make use of the remnants of modern technology. And agriculture-empires are very repressive; any criticism of leaders could and would be met with death or torture.</p>
<p>Finally, forget about moral lessons. We're not doing enough today to combat; eg, pandemics. But we are doing a lot more than nothing. So the moral lesson of a mass pandemic would be ""do more of what the ancients were already doing, but do it more and better"". Same goes for most risks that threaten humanity today; it's not that we fail to address them, it's that we fail to address them enough. Or suppose that it's a nuclear war that gets us; then the moral would be ""we did too little against nuclear war, while doing too much for pandemics!""; if the dice fall the other way round, we'd get the opposite lesson.</p>
<p>In fact, there would be little moral lesson from our perspective; the post-apocalyptic people would be focused on their own ideologies and moralities, with the pre-apocalyptic world being mentioned only if it made a point relevant to those.</p>
<p>All in all, a post-apocalyptic world would be awful, and not just for the whole dying and ruin reasons, but just for living in the terrible and unequal societies it would produce.</p>
</body></html>",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
uvqd3YiBcrPxXzxQM,"What does the word ""collaborative"" mean in the phrase ""collaborative truthseeking""?",what-does-the-word-collaborative-mean-in-the-phrase,https://www.lesswrong.com/posts/uvqd3YiBcrPxXzxQM/what-does-the-word-collaborative-mean-in-the-phrase,2019-06-26T05:26:42.295Z,27,9,23,False,True,,"<html><head></head><body><p>I keep hearing this phrase, ""collaborative truthseeking."" Question: what kind of epistemic work is the word ""collaborative"" doing?</p>
<p>Like, when you (respectively I) say a thing and I (respectively you) hear it, that's going to result in some kind of state change in my (respectively your) brain. If that state change results in me (respectively you) making better <a href=""https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences"">predictions</a> than I (respectively you) would have in the absence of the speech, then that's evidence for the hypothesis that at least one of us is ""truthseeking.""</p>
<p>But what's this ""collaborative"" thing about? How do speech-induced state changes result in better predictions if the speaker and listener are ""collaborative"" with each other? Are there any circumstances in which the speaker and listener being ""collaborative"" might result in <em>worse</em> predictions?</p>
</body></html>",Zack_M_Davis,zack_m_davis,Zack_M_Davis,
8EmPhoxgSXGSdq4Cb,Epistemic Spot Check: The Role of Deliberate Practice in the Acquisition of Expert Performance,epistemic-spot-check-the-role-of-deliberate-practice-in-the,https://www.lesswrong.com/posts/8EmPhoxgSXGSdq4Cb/epistemic-spot-check-the-role-of-deliberate-practice-in-the,2019-06-25T23:00:00.689Z,81,33,9,False,False,,"<p><a href=""https://acesounderglass.com/tag/epistemicspotcheck/"">Epistemic spot checks</a> typically consist of references from a book, selected by my interest level, checked against either the book’s source or my own research. This one is a little different that I’m focusing on a single paragraph in a single paper. Specifically as part of a larger review I read Ericsson, Krampe, and Tesch-Römer’s 1993 paper, <em>The Role of Deliberate Practice in the Acquisition of Expert Performance </em>(<a href=""http://projects.ict.usc.edu/itw/gel/EricssonDeliberatePracticePR93.pdf"">PDF</a>), in an attempt to gain information about how long human beings can productivity do thought work over a time period.</p>
<p>This paper is important because if you ask people how much thought work can be done in a day, if they have an answer and a citation at all, it will be “4 hours a day” and “Cal Newport’s <em>Deep Work</em>“. The Ericsson paper is in turn Newport’s source. So to the extent people’s beliefs are based on anything, they’re based on this paper.</p>
<p>In fact I’m not even reviewing the whole paper, just this one relevant paragraph: </p>
<blockquote><p>When individuals, especially children, start practicing in a given domain, the amount of practice is an hour or less per day (Bloom, 1985b). Similarly, laboratory studies of extended practice limit practice to about 1 hr for 3-5 days a week (e.g., Chase &amp; Ericsson, 1982; Schneider &amp; Shiffrin, 1977; Seibel, 1963). A number of training studies in real life have compared the efficiency of practice durations ranging from 1 -8 hr per day. These studies show essentially no benefit from durations exceeding 4 hr per day and reduced benefits from practice exceeding 2 hr (Welford, 1968; Woodworth &amp; Schlosberg, 1954). Many studies of the acquisition of typing skill (Baddeley &amp; Longman, 1978; Dvorak et al.. 1936) and other perceptual motor skills (Henshaw &amp; Holman, 1930) indicate that the effective duration of deliberate practice may be closer to 1 hr per day. Pirolli and J. R. Anderson (1985) found no increased learning from doubling the number of training trials per session in their extended training study. The findings of these studies can be generalized to situations in which training is extended over long periods of time such as weeks, months, and years</p></blockquote>
<p>Let’s go through each sentence in order. I’ve used each quote as a section header, with the citations underneath it in bold.</p>
<h2>“When individuals, especially children, start practicing in a given domain, the amount of practice is an hour or less per day”</h2>
<p><strong> Generalizations about talent development, Bloom (1985)</strong></p>
<p>“Typically the initial lessons were given in swimming and piano for about an hour each week, while the mathematics was taught about four hours each week…In addition some learning tasks (or homework) were assigned to be practiced and perfected before the next lesson.” (p513)</p>
<p>“…[D]uring the week the [piano] teacher expected the child to practice about an hour a day.” with descriptions of practice but no quantification given for swimming and math (p515).</p>
<p>The quote seems to me to be a simplification. “Expected an hour a day” is not the same as “did practice an hour or less per day.”</p>
<h2>“…laboratory studies of extended practice limit practice to about 1 hr for 3-5 days a week”</h2>
<p><strong><a href=""https://www.sciencedirect.com/science/article/pii/S0079742108605460"">Skill and working memory, Chase &amp; Ericsson (1982)</a></strong></p>
<p>This study focused strictly on memorizing digits, which I don’t consider to be that close to thought work.</p>
<p><a href=""https://psycnet.apa.org/record/1977-20305-001""><strong>Controlled and automatic human information processing: I. Detection, search, and attention. Schneider, W., &amp; Shiffrin, R. M. (1977)</strong></a></p>
<p>This study had 8 people in it and was essentially an identification and reaction time trial.</p>
<p><strong><a href=""https://psycnet.apa.org/record/1964-01676-001"">Discrimination reaction time for a 1,023-alternative task, Seibel, R. (1963)</a></strong></p>
<p>3 subjects. This was a reaction time test, not thought work. No mention of duration studying.</p>
<p> </p>
<h2>“These studies show essentially no benefit from durations exceeding 4 hr per day and reduced benefits from practice exceeding 2 hr”</h2>
<p><strong><a href=""https://psycnet.apa.org/record/1968-35018-000"">Fundamentals of Skill, Welford (1968)</a></strong></p>
<p>In a book with no page number given, I skipped this one.</p>
<p><strong><a href=""https://archive.org/details/ExperimentalPsychology/page/n7"">Experimental Psychology, Woodworth &amp; Schlosberg (1954)</a></strong></p>
<p>This too is a book with no page number, but it was available online (thanks, archive.org) and I made an educated guess that the relevant chapter was “Economy in Learning and Performance”. Most of this chapter focused on recitation, which I don’t consider sufficiently relevant.</p>
<p>p800: “Almost any book on applied psychology will tell you that the hourly work output is higher in an eight-hour day than a ten-hour day.”(no source)</p>
<p>Offers this graph as demonstration that only monotonous work has diminishing returns.</p>
<p><img src=""https://acesounderglass.files.wordpress.com/2019/05/screen-shot-2019-05-16-at-9.08.22-pm.png?w=459&amp;h=564"" /></p>
<p> </p>
<p>p812: An interesting army study showing that students given telegraphy training for 4 hours/day  (and spending 4 on other topics) learned as much as students studying 7 hours/day. This one seems genuinely relevant, although not enough to tell us where peak performance lies, just that four hours are better than seven. Additionally, the students weren’t loafing around for the excess three hours: they were learning other things. So this is about how long you can study a particular subject, not total learning capacity in a day.</p>
<h2>Many studies of the acquisition of typing skill (Baddeley &amp; Longman, 1978; Dvorak et al.. 1936) and other perceptual motor skills (Henshaw &amp; Holman, 1930) indicate that the effective duration of deliberate practice may be closer to 1 hr per day</h2>
<p><strong><a href=""https://www.gwern.net/docs/spacedrepetition/1978-baddeley.pdf"">The Influence of Length and Frequency of Training Session on the Rate of Learning to Type, Baddeley &amp; Longman (1978)</a></strong></p>
<p>“Four groups of postmen were trained to type alpha-numeric code material using a conventional typewriter keyboard. Training was based on sessions lasting for one or two hours occurring once or twice per day. Learning was most efficient in the group given one session of one hour per day, and least efficient in the group trained for two 2-hour sessions. Retention was tested after one, three or nine months, and indicated a loss in speed of about 30%. Again the group trained for two daily sessions of two hours performed most poorly.It is suggested that where operationally feasible, keyboard training should be distributed over time rather than massed”</p>
<p> </p>
<p><a href=""https://psycnet.apa.org/record/1936-05978-000""><strong>Typewriting behavior; psychology applied to teaching and learning typewriting, Dvorak et al (1936)</strong></a></p>
<p>Inaccessible book.</p>
<p><strong><a href=""https://www.gwern.net/docs/spacedrepetition/1985-pirolli.pdf"">The Role of Practice in Fact Retrieval, Pirolli &amp; Anderson (1985)</a></strong></p>
<p>“We found that fact retrieval speeds up as a power function of days of practice but that the number of daily repetitions beyond four produced little or no impact on reaction time”</p>
<h2>Conclusion</h2>
<p>Many of the studies were criminally small, and typically focused on singular, monotonous tasks like responding to patterns of light or memorizing digits.  The precision of these studies is greatly exaggerated. There’s no reason to believe Ericsson, Krampe, and Tesch-Römer’s conclusion that the correct number of hours for deliberate practice is 3.5, much less the commonly repeated factoid that humans can do good work for 4 hours/day.</p>
<p> </p>
<p>[This post supported by <a href=""https://www.patreon.com/acesounderglass"">Patreon</a>].</p>",pktechgirl,elizabeth-1,Elizabeth,
2xiNSQmxayDteKDS2,Writing children's picture books,writing-children-s-picture-books,https://www.lesswrong.com/posts/2xiNSQmxayDteKDS2/writing-children-s-picture-books,2019-06-25T21:43:45.578Z,125,47,22,False,False,https://unstableontology.com/2019/06/25/writing-childrens-picture-books/,"<html><head></head><body><p>[the text of the post is pasted here, for redundancy]</p>
<p>Here’s an exercise for explaining and refining your opinions about some domain, X:</p>
<blockquote>
<p>Imagine writing a 10-20 page children’s picture book about topic X. Be fully honest and don’t hide things (assume the child can handle being told the truth, including being told non-standard or controversial facts).</p>
</blockquote>
<p>Here’s a dialogue, meant to illustrate how this could work:</p>
<p>A: What do you think about global warming?</p>
<p>B: Uhh…. I don’t know, it seems real?</p>
<p>A: How would you write a 10-20 page children’s picture book about global warming?</p>
<p>B: Oh, I’d have a diagram showing carbon dioxide exiting factories and cars, floating up in the atmosphere, and staying there. Then I’d have a picture of sunlight coming through the atmosphere, bounding off the earth, then going back up, but getting blocked by the carbon dioxide, so it goes back to the earth and warms up the earth a second time. Oh, wait, if the carbon dioxide prevents the sunlight from bouncing from the earth to the sky, wouldn’t it also prevent the sunlight from entering the atmosphere in the first place? Oh, I should look that up later [NOTE: the answer is that <a href=""http://www.co2science.org/subject/questions/1998/greenhouse.php"">CO2 blocks thermal radiation</a> much more than it blocks sunlight].</p>
<p>Anyway, after that I’d have some diagrams showing global average temperature versus global CO2 level that show how the average temperature is tracking CO2 concentration, with some lag time. Then I’d have some quotes about scientists and information about the results of surveys. I’d show a graph showing how much the temperature would increase under different conditions… I think I’ve heard that, with substantial mitigation effort, the temperature difference might be 2 degrees Celsius from now until the end of the century [NOTE: it's actually 2 degrees from pre-industrial times till the end of the century, which is about 1 degree from now]. And I’d want to show what 2 degrees Celsius means, in terms of, say, a fraction of the difference between winter and summer.</p>
<p>I’d also want to explain the issue of sea level rise, by showing a diagram of a glacier melting. Ice floats, so if the glacier is free-floating, then it melting doesn’t cause a sea level rise (there’s some scientific principle that says this, I don’t remember what it’s called), but if the glacier is on land, then when it melts, it causes the sea level to rise. I’d also want to show a map of the areas that would get flooded. I think some locations, like much of Florida, get flooded, so the map should show that, and there should also be a pie chart showing how much of the current population would end up underwater if they didn’t move (my current guess is that it’s between 1 percent and 10 percent, but I could be pretty wrong about this [NOTE: the answer is <a href=""https://www.nytimes.com/interactive/2018/10/07/climate/ipcc-report-half-degree.html"">30 to 80 million people</a>, which is between about 0.4% and 1.1%]).</p>
<p>I’d also want to talk about possible mitigation efforts. Obviously, it’s possible to reduce energy consumption (and also meat consumption, because cows produce methane which is also a greenhouse gas). So I’d want to show a chart of which things produce the most greenhouse gases (I think airplane flights and beef are especially bad), and showing the relationship between possible reductions in that and the temperature change.</p>
<p>Also, trees take CO2 out of the atmosphere, so preserving forests is a way to prevent global warming. I’m confused about where the CO2 goes, exactly, since there’s some cycle it goes through in the forest; does it end up underground? I’d have to look this up.</p>
<p>I’d also want to talk about the political issues, especially the disinformation in the space. There’s a dynamic where companies that pollute want to deny that man-made global warming is a real, serious problem, so there won’t be regulations. So, they put out disinformation on television, and they lobby politicians. Sometimes, in the discourse, people go from saying that global warming isn’t real, to saying it’s real but not man-made, to saying it’s real and man-made but it’s too late to do anything about it. That’s a clear example of motivated cognition. I’d want to explain how this is trying to deny that any changes should be made, and speculate about why people might want to, such as because they don’t trust the process that causes changes (such as the government) to do the right thing.</p>
<p>And I’d also want to talk about geoengineering. There are a few proposals I know of. One is to put some kind of sulfer-related chemical in the atmosphere, to block out sunlight. This doesn’t solve ocean acidification, but it does reduce the temperature. But, it’s risky, because if you stop putting the chemical in the atmosphere, then that causes a huge temperature swing.</p>
<p>I also know it’s possible to put iron in the ocean, which causes a plankton bloom, which… does something to capture CO2 and store it in the bottom of the ocean? I’m really not sure how this works, I’d want to look it up before writing this section.</p>
<p>There’s also the proposal of growing and burning trees, and capturing and storing the carbon. When I looked this up before, I saw that this takes quite a lot of land, and anyway there’s a lot of labor involved, but maybe some if it can be automated.</p>
<p>There are also political issues with geoengineering. There are people who don’t trust the process of doing geoengineering to make things better instead of worse, because they expect that people’s attempts to reason about it will make lots of mistakes (or people will have motivated cognition and deceive themselves and each other), and then the resulting technical models will make things that don’t work. But, the geoengineering proposals don’t seem harder than things that humans have done in the past using technical knowledge, like rockets, so I don’t agree that this is such a big problem.</p>
<p>Furthermore, some people want to shut down discussion of geoengineering, because such discussion would make it harder to morally pressure people into reducing carbon emissions. I don’t know how to see this as anything other than an adversarial action against reasonable discourse, but I’m sure there is some motivation at play here. Perhaps it’s a motivation to have everyone come together as one, all helping together, in a hippie-ish way. I’m not sure if I’m right here, I’d want to read something written by one of these people before making any strong judgments.</p>
<p>Anyway, that’s how I’d write a picture book about global warming.</p>
<hr>
<p>So, I just wrote that dialogue right now, without doing any additional research. It turns out that I do have quite a lot of opinions about global warming, and am also importantly uncertain in some places, some of which I just now became aware of. But I’m not likely to produce these opinions if asked “what do you think about global warming?”</p>
<p>Why does this technique work? I think it’s because, if asked for one’s opinions in front of an adult audience, it’s assumed that there is a background understanding of the issue, and you have to say something new, and what you decide to say says something about you. Whereas, if you’re explaining to a child, then you know they lack most of the background understanding, and so it’s obviously good to explain that.</p>
<p>With adults, it’s assumed there are things that people act like “everyone knows”, where it might be considered annoying to restate them, since it’s kind of like talking down to them. Whereas, the illusion or reality that “everyone knows” is broken when explaining to children.</p>
<p>The countervailing force is that people are tempted to <a href=""http://paulgraham.com/lies.html"">lie to children</a>. Of course, it’s necessary to not lie to children to do the exercise right, and also to raise or help raise children who don’t end up in an illusory world of confusion and dread. I would hope that someone who has tendencies to hide things from children would at least be able to notice and confront these tendencies in the process of imagining writing children’s picture books.</p>
<p>I think this technique can be turned into a generalized process for making world models. If someone wrote a new sketch of a children’s picture book (about a new topic) every day, and did the relevant research when they got stuck somewhere, wouldn’t they end up with a good understanding of both the world and of their own models of the world after a year? It’s also a great starting point from which to compare your opinions to others’ opinions, or to figure out how to explain things to either children or adults.</p>
<p>Anyway, I haven’t done this exercise for very many topics yet, but I plan on writing more of these.</p>
</body></html>",jessica.liu.taylor,jessica-liu-taylor,jessicata,
rfNtNzZxd3qKnkKSc,Only optimize to 95 %,only-optimize-to-95,https://www.lesswrong.com/posts/rfNtNzZxd3qKnkKSc/only-optimize-to-95,2019-06-25T20:50:27.908Z,7,5,9,False,True,,"<html><head></head><body><p>I was reading Tom Chivers book ""The AI does not hate you"" and in a discussion about  avoiding bad side effects when asking a magic broomstick to fill a water bucket, it was suggested that somehow instead of asking the broomstick to fill the bucket you could do something like ask it to become 95 percent sure that it was full, and that might make it less likely to flood the house.</p>
<p>Apparently Tom asked Eliezer at the time and he said there was no known problem with that solution.</p>
<p>Are there any posts on this? Is the reason why we don't know this won't work just because it's hard to make this precise?</p>
</body></html>",leo-shine,leo-shine,Leo Shine,
TR3eqQ2fnfKWzxxHL,Research Agenda in reverse: what *would* a solution look like?,research-agenda-in-reverse-what-would-a-solution-look-like,https://www.lesswrong.com/posts/TR3eqQ2fnfKWzxxHL/research-agenda-in-reverse-what-would-a-solution-look-like,2019-06-25T13:52:48.934Z,35,16,25,False,False,,"<html><head></head><body><p>I constructed my <a href=""https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into"">AI alignment research agenda</a> piece by piece, stumbling around in the dark and going down many false and true avenues.</p>
<p>But now it is increasingly starting to feel natural to me, and indeed, somewhat inevitable.</p>
<p>What do I mean with that? Well, let's look at the problem in reverse. Suppose we had an AI that was aligned with human values/preferences. How would you expect that to have been developed? I see four natural paths:</p>
<ol>
<li>Effective proxy methods. For example, Paul's amplification and distillation, or variants of revealed preferences, or a similar approach. The point of this that it reaches alignment without defining what a preference fundamentally is; instead it uses some proxy for the preference to do the job.</li>
<li>Corrigibility: the AI is safe and corrigible, and along with active human guidance, manages to reach a tolerable outcome.</li>
<li>Something new: a bold new method that works, for reasons we haven't thought of today (this includes most strains of moral realism).</li>
<li>An actual grounded definition of human preferences.</li>
</ol>
<p>So, if we focus on scenario 4, we need a few things. We need a fundamental definition of what a human preference is (since we know <a href=""https://arxiv.org/abs/1712.05812"">this can't be defined purely from behaviour</a>). We need a method of combining contradictory and underdefined human preferences. We also need a method for taking into account human meta-preferences. And both these methods has to actually reach an output, and not get caught in loops.</p>
<p>If those are the requirements, then it's obvious why we need most of the elements of my research agenda, or something similar. We don't need the exact methods sketched out there, there may be other way of synthesising preferences and meta-preferences together. But the overall structure - a way of defining preferences, and ways of combining them that produce an output - seems, in retrospect, inevitable. The rest is, to some extent, just implementation details.</p>
</body></html>",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
8pSh54GoJJqfHdndk,Arbital Scrape V2,arbital-scrape-v2,https://www.lesswrong.com/posts/8pSh54GoJJqfHdndk/arbital-scrape-v2,2019-06-25T10:03:21.962Z,40,11,6,False,False,,"<p>I&#x27;ve scraped <a href=""http://arbital.com/"">http://arbital.com</a> as the site is unusably slow and hard to search for me.</p><p><a href=""https://drive.google.com/open?id=1b7dKhOzfMpFwngAeI8efeOzv147Lv_mx"">https://drive.google.com/open?id=1b7dKhOzfMpFwngAeI8efeOzv147Lv_mx</a></p><p><a href=""https://mega.nz/#!SUQg1YxS!G7bmpNipcl1Ztugvr6l6sUJUu1D0gDg1-jA6oXLNEWg"">https://mega.nz/#!SUQg1YxS!G7bmpNipcl1Ztugvr6l6sUJUu1D0gDg1-jA6oXLNEWg</a></p><p><a href=""https://emma-borhanian.github.io/arbital-scrape/"">https://emma-borhanian.github.io/arbital-scrape/</a></p><p>The scrape is locally browsable and plain HTML save for MathJax and a few interactive demos. Source code included (with git history).</p><span><figure><img src=""https://i.imgur.com/fcxSMSf.png"" class=""draft-image "" style=""width:100%"" /></figure></span><p>(previously <a href=""https://www.lesswrong.com/posts/muKEBrHhETwN6vp8J/arbital-scrape"">Arbital Scrape</a>)</p><p>Updates: Included source code, MathJax and link formatting, cross-linking, missing pages, etc</p><p>Source: <a href=""https://github.com/emma-borhanian/arbital-scrape"">https://github.com/emma-borhanian/arbital-scrape</a></p><p>Mirror: <a href=""https://www.obormot.net/arbital"">www.obormot.net/arbital</a> </p>",emmab,emmab,emmab,
2gtWdrv8jHGgvZeJ5,What's up with self-esteem?,what-s-up-with-self-esteem-1,https://www.lesswrong.com/posts/2gtWdrv8jHGgvZeJ5/what-s-up-with-self-esteem-1,2019-06-25T03:38:15.991Z,39,21,14,False,True,,"<p>Often, people think about their self-worth/self-confidence/self-esteem/self-efficacy/self-worth in ways which seem really strange from a simplistic decision-theoretic perspective. (I&#x27;m going to treat all those terms as synonyms, but, feel free to differentiate between them as you see fit!) Why might you &quot;need confidence&quot; in order to try something, even when it is obviously your best bet? Why might you constantly worry that you&#x27;re &quot;not good enough&quot; (seemingly no matter how good you become)? Why do people especially suffer from this when they see others who are (in some way) much better than them, even when there is clearly no threat to their personal livelihood? Why might you think about killing yourself due to feeling worthless? (Is there an evo-psych explanation that makes sense, given how contrary it seems to survival of the fittest?)</p><p>There might be a lot of diverse explanations for the diverse phenomena. I think providing more examples of puzzling phenomena is an equally valuable way to answer (though maybe those should be a comment rather than an answer?).</p><p>This seems connected to the puzzling way people constantly seem to want to believe good things (even contrary to evidence) in order to feel good, and fear failure even when the alternative is not trying &amp; essentially failing automatically.</p><p>Some sketchy partial explanations to start with:</p><ul><li>Maybe there <em>is</em> a sense in which we manage the news constantly. It could be that we have a mental architecture which looks a lot like a model-free RL agent connected up to a world model, being rewarded for taking actions which increase expected value according to the world-model. The model-free RL will fool the world-model where it can, but this will be ineffective in any case where the world-model understands such manipulation. So things basically even out to rational behavior, but there&#x27;s always some self-delusion going on at the fringes. (This only has to do with the observation that people sometimes try to make themselves feel better by finding arguments/activities which boost self-esteem, not with other weird aspects of self-esteem.)</li><li>There&#x27;s a theory that, in order to be trustworthy bargaining partners, people evolved to feel guilty/shameful when they violate trust. You can tell who feels more guilt/shame after some interaction with them, and you can expect these people to violate trust less often since it is more costly for them. Therefore feelings of guilt/shame can be an advantage. Self-worth may be connected to how this is implemented internally. So, according to this theory, low self-worth is all about self-punishment.</li><li>Previously, I thought that self-worth was like an estimate of how valuable you are to your peers, which serves as an estimate of what resources you can bargain for (or, how strong of a bid can you successfully make for the group to do what you want) and how likely you are to be thrown out of the coalition. </li><li>Now I think there&#x27;s an extra dimension which has to do with simpler dominance-hierarchy behavior. Many animals have dominance hierarchies; humans have more complicated coordination strategies which involve a lot of other factors, but still display very classic dominance-hierarchy behavior sometimes. In a dominance-hierarchy system, it <em>just makes sense</em> to carry around a little number in your head which says how great (/terrible) a person you are, and engage in a lot of varying behaviors depending on your place in the hierarchy. Someone who is low in the hierarchy has to walk with their tail between their legs, metaphorically, which means displaying caution and deference. Maybe you have trouble talking to people because you <em>need to show fear to your superiors</em>.</li></ul>",abramdemski,abramdemski,abramdemski,
rGk8gFkfXd7HkZS2p,Pittsburgh SSC/RAT-adjacent meetup,pittsburgh-ssc-rat-adjacent-meetup,https://www.lesswrong.com/events/rGk8gFkfXd7HkZS2p/pittsburgh-ssc-rat-adjacent-meetup,2019-06-25T02:11:44.243Z,1,1,0,False,False,,"<p>The (mostly) monthly Pittsburgh SSC/RAT-adjacent meetup will be held June 29th from 3-6pm. Please let us know if you plan on attending by e-mailing Chris <a href=""(ssc@fr8train.me)"">(ssc@fr8train.me)</a> by Friday 11:59. Snacks and libations will be provided.</p>",fr8train,fr8train,fr8train,
xqAnKW46FqzPLnGmH,Causal Reality vs Social Reality,causal-reality-vs-social-reality,https://www.lesswrong.com/posts/xqAnKW46FqzPLnGmH/causal-reality-vs-social-reality,2019-06-24T23:50:19.079Z,46,32,96,False,False,,"<p><em>Epistemic status: this is a new model for me, certainly rough around the joints, but I think there’s something real here.</em></p><p>This post begins with a confusion. For years, I have been baffled that people, watching their loved ones <a href=""https://slatestarcodex.com/2013/07/17/who-by-very-slow-decay/"">wither and decay and die</a>, do not clamor in the streets for more and better science. Surely they are aware of the advances in our power over reality in only the last few centuries. They hear of the steady march of technology, Crispr and gene editing and what not. Enough of them must know basic physics and what it allows. How are people so content to suffer and die when the unnecessity of it is so apparent? </p><p>It was a failure of my mine that I didn’t take my incomprehension and realize I needed a better model. <a href=""https://www.lesswrong.com/posts/8bWbNwiSGbGi9jXPS/epistemic-luck"">Luckily</a>, <a href=""https://www.lesswrong.com/users/romeostevens"">RomeoStevens</a> recently offered me an explanation. He said that most people live in <em>social reality</em> and it is only a minority who live in <em>causal reality</em>. I don’t recall Romeo elaborating much, but I think I saw what he was pointing at. This rest of this post is my attempt to elucidate this distinction.</p><h1>Causal Reality</h1><p>Causal reality is the reality of physics. The world is made of particles and fields with lawful relationships governing their interactions. You drop a thing, it falls down. You lose too much blood, you die. You build a solar panel, you can charge your phone. In causal reality, it is the external world which dictates what happens and what is possible. </p><p>Causal reality is the reality of mathematics and logic, reason and argument. For these too, it would definitely seem, exist independent of the human minds who grasp them. Believing in the truth preservation of modus ponens is not so different from believing in Newton’s laws.</p><p>Necessarily, you must be inhabiting causal reality to do science and engineering. </p><p>In causal reality, what makes things good or bad are their effects and how much you like those effects. My coat keeps me warm in the cold winter, so it is a good coat.</p><p>All humans inhabit causal reality to some extent or another. We avoid putting our hands in fire not because <em>it is not the done the thing</em>, but because of prediction that it will hurt.</p><h1>Social Reality</h1><p>Social reality is the reality of people, i.e. people are the primitive elements rather than particles and fields. The fundamentals of the ontology are beliefs, judgments, roles, relationships, and culture. The most important properties of any object, thing, or idea are how humans relate to it. Do humans think it is good or bad, welcome or weird? </p><p>Social reality is the reality of appearances and reputation, acceptance and rejection. The picture is other people and what they think the picture is. It is a collective dream. Everything else is backdrop. What makes things good or bad, normal or strange is only what others think. Your friends, your neighbors, your country, and your culture define your world, what is good, and what is possible.</p><h1>Your reality shapes how you make your choices</h1><p>In causal reality, you have an idea of the things that you like dislike. You have an idea of what the external world allows and disallows. In each situation, you can ask what the facts on the ground are and which you most prefer. It is better to build my house from bricks or straw? Well, what are the properties of each, their costs and benefits, etc? Maybe stone, you think. No one has built a stone house in your town, but you wonder if such a house might be worth the trouble.</p><p>In social reality, in any situation, you are evaluating and estimating what others will think of each option. What does it say about me if I have a brick house or straw house? What will people think? Which is good? And goodness here simply stands in for the collective judgment of others. If something is not done, e.g. stone houses, then you will probably not even think of the option. If you do, you will treat it with the utmost caution, there is no precedent here - who can say how others will respond?</p><h1>An Example: Vibrams</h1><p><figure><img src=""https://lh5.googleusercontent.com/7VaSUjQU5Xrko0S3fOh1zbS09xlhGw-FfoVbOVB_xXo5DDDcBFlmThzIcqJF1NvXWHRmWcbBP8AR94v365tQexvK_LTb1q0zUjXQE3s5hDIaCu81dCM983Mpn4LmQAOVUCNUFChD"" class=""draft-image "" style=""width:214%"" /></figure></p><p>Vibrams are a kind of shoe with individual “sections” for each of your toes, kind of like a glove for your feet. They certainly don’t look like most shoes, but apparently, they’re very comfortable and good for you. They’ve been around for a while now, so enough people must be buying them.</p><p>How you evaluate Vibrams will depend on whether you approach more from a causal reality angle or a social reality angle. Many of the thoughts in each case will overlap, but I contend that their order intensity will still vary.</p><p>In causal reality, properties are evaluated and predictions are made. How comfortable are they? Are they actually good for you? How expensive are they? These are obvious “causal”/”physical” properties. You might, still within causal reality, evaluate how Vibrams will affect how others see you. You care about comfort, but you also care about what your friends think. You might decide that Vibrams are just so damn comfortable they’re worth a bit of teasing.</p><p>In social reality, the first and foremost questions about Vibrams are going to be <em>what do others think? What kinds of people wear Vibrams? What kind of person will wearing Vibrams make me? Do Vibrams fit with my identity and social strategy?</em> All else equal, you’d prefer comfort, but that really is far from the key thing here. It’s the human judgments which are real.</p><h1>An Example: Arguments, Evidence, and Truth</h1><p>Causal reality is typically accompanied by a notion of external truth. There is way reality <em>is, </em>and that’s what determines what happens. What’s more, there are ways of accessing this external truth as verified by these methods yielding good predictions. Evidence, arguments, and reasoning can often work quite well.</p><p>If you approach reality foremost with a conception of external truth and that broadly reasoning is a way to reach truth, you can be open to raw arguments and evidence changing your mind. These are information about the external world.</p><p>In social reality, truth is what other people think and how they behave. There are games to be played with “beliefs” and “arguments”, but the real truth (only truth?) that matters is how these are arguments go down with others. The validity of an argument comes from its acceptance by the crowd because the crowd <em>is truth</em>. I might accept that within the causal reality <em>game </em>you are playing that you have a valid argument, but that’s just a game. The arguments from those games cannot move me and my actions independent from how they are evaluated in the social reality. </p><p>“Yes, I can’t fault your argument. It’s a very fine argument. But tell me, who takes this seriously? Are there any <em>experts</em> who will support your view?” <em>Subtext: your argument within causal reality isn’t enough for me, I need social reality to pass judgment on this before I will accept it.</em></p><h1>Why people aren’t clamoring in the streets for the end of sickness and death?</h1><p>Because no one else is. Because the <em>done thing</em> is to be born, go to school, work, retire, get old, get sick, and die. That’s what everyone does. That’s how it is. It’s how my parents did, and their parents, and so on. <em>That is reality</em>. <em>That’s what people do.</em></p><p>Yes, there are some people who talk about life extension, but they’re just playing at some group game the ways goths are. It’s just a club, a rallying point. It’s not <em>about </em>something. It’s just part of the social reality like everything else, and I see no reason to participate in that. I’ve got my own game which doesn’t involve being so weird, a much better strategy.</p><p>In his book <em><a href=""https://www.amazon.com/Does-Not-Hate-You-Superintelligence-ebook/dp/B07K258VCV"">The AI Does Not Hate You</a></em>, Tom Chivers recounts himself performing an Internal Double Crux with guidance from <a href=""https://www.lesswrong.com/users/annasalamon"">Anna Salamon</a>. By my take, he is valiantly trying to reconcile his social and causal reality frames. [emphasis added, very slightly reformatted]</p><blockquote>Anna Salamon: What’s the first thing that comes into your head when you think the phrase, “Your children won’t die of old age?”</blockquote><blockquote>Tom Chivers: “<strong>The first thing that pops up, obviously, is I vaguely assume my children will die the way we all do. My grandfather died recently; my parents are in their sixties; I’m almost 37 now. You see the paths of a human’s life each time; all lives follow roughly the same path. They have different toys - iPhones instead of colour TVs instead of whatever - but the fundamental shape of a human’s life is roughly the same.</strong> But the other thing that popped is a sense “I don’t know how I can argue with it”, because I do accept that there’s a solid chance that AGI will arrive in the next 100 years. I accept that there’s a very high likelihood that if does happen then it will transform human life in dramatic ways - up to and including an end to people dying of old age, whether it’s because we’re all killed by drones with kinetic weapons, or uploaded into the cloud, or whatever. I also accept that my children will probably live that long, because they’re middle-class, well-off kinds from a Western country. All these these things add up to a very heavily non-zero chance that my children will not die of old age, but, they don’t square with my bucolic image of what humans do. <strong>They get older, they have kids, they have grandkids, and they die, and that’s the shape of life. </strong>Those are two fundamental things that came up, and they don’t square easily.</blockquote><p>Most people primarily inhabit a social reality frame, and in social reality options and actions which aren’t being taken by other people who are like you and whose judgments you’re interested in don’t exist. There’s no extrapolation from physics and technology trends - those things are just background stories in the social game. They’re not real. Probably less real than Jon Snow. I <em>have</em> beliefs and opinions and judgments of Jon Snow and his actions. What is real are the people around me.</p><h1>Obviously, you need a bit of both</h1><p>If you read this post as being a little negative toward social reality, you’re not mistaken. But to be very clear, I think that modeling and understanding people is critically important. Heck, that’s exactly what this post is. For our own wellbeing and to do anything real in the world, we need to understand and predict others, their actions, their judgments, etc. You probably want to know what the social reality is (though I wonder if avoiding the distraction of it might facilitate especially great works, but alas, it’s too late for me). Yet if there is a moral to this post, it’s two things:</p><ul><li>Don’t get sucked in too much by social reality. There is an external world out there which has first claim of what happens and what is possible.</li><ul><li>What other people think is often <a href=""https://wiki.lesswrong.com/wiki/Evidence"">Bayesian evidence</a>, but it isn’t reality itself.</li></ul><li>If you primarily inhabit causal reality (like most people on LessWrong), you can be a bit less surprised that your line of reasoning fails to move many people. They’re not living in the same reality as you and they choose their beliefs based on a very different process. And heck, more people live in that reality than in yours. You really are the weirdo here.</li></ul>",Ruby,ruby,Ruby,
Y9xD78kufNsF7wL6f,Machine Learning Projects on IDA,machine-learning-projects-on-ida,https://www.lesswrong.com/posts/Y9xD78kufNsF7wL6f/machine-learning-projects-on-ida,2019-06-24T18:38:18.873Z,49,18,3,False,False,,"<html><head></head><body><h3>TLDR</h3>
<p>We wrote a 20-page <a href=""http://owainevans.github.io/pdfs/evans_ida_projects.pdf"">document</a> that explains IDA and outlines potential Machine Learning projects about IDA. This post gives an overview of the document.</p>
<h3>What is IDA?</h3>
<p>Iterated Distillation and Amplification (IDA) is a method for training ML systems to solve challenging tasks. It was <a href=""https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616"">introduced</a> by Paul Christiano. IDA is intended for tasks where:</p>
<ul>
<li>
<p>The goal is to outperform humans at the task or to solve instances that are too hard for humans.</p>
</li>
<li>
<p>It is not feasible to provide demonstrations or reward signals sufficient for super-human performance at the task</p>
</li>
<li>
<p>Humans have a high-level understanding of how to approach the task and can reliably solve easy instances.</p>
</li>
</ul>
<p>The idea behind IDA is to bootstrap using an approach similar to <a href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/alphazero-science.pdf"">AlphaZero</a>, but with a learned model of steps of human reasoning instead of the fixed game simulator.</p>
<p>Our <a href=""http://owainevans.github.io/pdfs/evans_ida_projects.pdf"">document</a> provides a self-contained technical description of IDA. For broader discussion of IDA and its relevance to value alignment, see Ought's <a href=""https://ought.org/presentations/factored-cognition-2018-05"">presentation</a>, Christiano's <a href=""https://ai-alignment.com/towards-formalizing-universality-409ab893a456"">blogpost</a>, and the Debate <a href=""https://arxiv.org/abs/1805.00899"">paper</a>. There is also a technical ML <a href=""http://arxiv.org/abs/1810.08575v1"">paper</a> applying IDA to algorithmic problems (e.g. shortest path in a graph).</p>
<h3>ML Projects on IDA</h3>
<p>Our <a href=""http://owainevans.github.io/pdfs/evans_ida_projects.pdf"">document</a> outlines three Machine Learning projects on IDA. Our goal in outlining these projects is to generate discussion and encourage research on IDA. We are not (as of June 2019) working on these projects, but we are interested in collaboration. The project descriptions are “high-level” and leave many choices undetermined. If you took on a project, part of the work would be refining the project and fixing a concrete objective, dataset and model.</p>
<h4>Project 1: Amplifying Mathematical Reasoning</h4>
<p>This project is about applying IDA to problems in mathematics. This would involve learning to solve math problems by breaking them down into easier sub-problems. The problems could be represented in a formal language (as in this <a href=""https://arxiv.org/abs/1904.03241v2"">paper</a>) or in natural language. We discuss a recent dataset of high-school problems in natural language, which was introduced in this <a href=""https://arxiv.org/abs/1904.01557v1"">paper</a>. Here are some examples from the dataset:</p>
<blockquote>
<p>Question:
Let u(n) = -n^3 - n^2.
Let e(c) = -2*c^3 + c.
Let f(j) = -118*e(j) + 54*u(j).
What is the derivative of f(a)?</p>
<p>Answer: 546*a^2 - 108*a - 118</p>
<p>Question: Three letters picked without replacement from
qqqkkklkqkkk. Give probability of sequence qql.</p>
<p>Answer: 1/110</p>
</blockquote>
<p>The paper showed impressive results on the dataset for a Transformer model trained by supervised learning (sequence-to-sequence). This suggests that a similar model could do well at learning to solve these problems by decomposition.</p>
<h4>Project 2: IDA for Neural Program Interpretation</h4>
<p>There’s a research program in Machine Learning on “Neural Program Interpretation” (NPI). Work on NPI focuses on learning to reproduce the behavior of computer programs. One possible <a href=""https://pdfs.semanticscholar.org/0298/e63bdc97b96bee195187af5f256df460357a.pdf"">approach</a> is to train end-to-end on input-output behavior. However in NPI, a model is trained to mimic the program’s <em>internal</em> behavior, including all the low-level operations and the high-level procedures which invoke them.</p>
<p>NPI has some similar motivations to IDA. This project applies IDA to the kinds of tasks explored in NPI and compares IDA to existing approaches. Tasks could include standard algorithms (e.g. sorting), algorithms that operate with databases, and algorithms that operate on human-readable inputs (e.g. text, images).</p>
<h4>Project 3: Adaptive Computation</h4>
<p>The idea of “adaptive computation” is to vary the amount of computation you perform for different inputs. You want to apply more computation to inputs that are hard but solvable.</p>
<p>Adaptive computation seems important for the kinds of problems IDA is intended to solve, including some of the problems in Projects 1 and 2. This project would investigate different approaches to adaptive computation for IDA. The basic idea is to decide whether to rely only on the distilled model (which is fast but approximate) or to additionally use amplification (which is more accurate but slower). This decision could be based on a <a href=""http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles"">calibrated</a> model or based on a learned policy for choosing whether to use amplification.</p>
</body></html>",Owain_Evans,owain_evans,Owain_Evans,
XvN2QQpKTuEzgkZHY,Being the (Pareto) Best in the World,being-the-pareto-best-in-the-world,https://www.lesswrong.com/posts/XvN2QQpKTuEzgkZHY/being-the-pareto-best-in-the-world,2019-06-24T18:36:45.929Z,483,324,61,False,False,,"<p>The generalized efficient markets (GEM) principle says, roughly, that things which would give you a big windfall of money and/or status, will not be easy. If such an opportunity were available, someone else would have already taken it. You will never find a $100 bill on the floor of Grand Central Station at rush hour, because someone would have picked it up already.</p><p>One way to circumvent GEM is to be the best in the world at some relevant skill. A superhuman with hawk-like eyesight and the speed of the Flash might very well be able to snag $100 bills off the floor of Grand Central. More realistically, even though financial markets are the ur-example of efficiency, a handful of firms do make impressive amounts of money by being faster than anyone else in their market. I’m unlikely to ever find a proof of the Riemann Hypothesis, but Terry Tao might. Etc.</p><p>But being the best in the world, in a sense sufficient to circumvent GEM, is not as hard as it might seem at first glance (though that doesn’t exactly make it easy). The trick is to exploit dimensionality.</p><p>Consider: becoming one of the world’s top experts in proteomics is hard. Becoming one of the world’s top experts in macroeconomic modelling is hard. But how hard is it to become sufficiently expert in proteomics and macroeconomic modelling that nobody is better than you at both simultaneously? In other words, how hard is it to reach the Pareto frontier?</p><p>Having reached that Pareto frontier, you will have circumvented the GEM: you will be the single best-qualified person in the world for (some) problems which apply macroeconomic modelling to proteomic data. You will have a realistic shot at a big money/status windfall, with relatively little effort.</p><p>(Obviously we’re oversimplifying a lot by putting things like “macroeconomic modelling skill” on a single axis, and breaking it out onto multiple axes would strengthen the main point of this post. On the other hand, it would complicate the explanation; I’m keeping it simple for now.)</p><p>Let’s dig into a few details of this approach…</p><h2>Elbow Room</h2><p>There are many table tennis players, but only one best player in the world. This is a side effect of ranking people on one dimension: there’s only going to be one point furthest to the right (absent a tie).</p><p>Pareto optimality pushes us into more dimensions. There’s only one best table tennis player, and only one best 100-meter sprinter, but there can be an unlimited number of Pareto-optimal table tennis/sprinters.</p><p>Problem is, for GEM purposes, elbow room matters. Maybe I’m the on the pareto frontier of Bayesian statistics and gerontology, but if there’s one person just little bit better at statistics and worse at gerontology than me, and another person just a little bit better at gerontology and worse at statistics, then GEM only gives me the advantage over a tiny little chunk of the skill-space.</p><p></p><figure><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/v1675824334/mirroredImages/XvN2QQpKTuEzgkZHY/th0z8sfm9smcmb0djl7v.png"" class=""draft-image "" style=""width:624%""></figure><p></p><p>This brings up another aspect…</p><h2>Problem Density</h2><p>Claiming a spot on a Pareto frontier gives you some chunk of the skill-space to call your own. But that’s only useful to the extent that your territory contains useful problems.</p><p>Two pieces factor in here. First, how large a territory can you claim? This is about elbow room, as in the diagram above. Second, what’s the density of useful problems within this region of skill-space? The table tennis/sprinting space doesn’t have a whole lot going on. Statistics and gerontology sounds more promising. Cryptography and monetary economics is probably a particularly rich Pareto frontier these days. (And of course, we don’t need to stop at two dimensions - but we’re going to stop there in this post in order to keep things simple.)</p><h2>Dimensionality</h2><p>One problem with this whole GEM-vs-Pareto concept: if chasing a Pareto frontier makes it easier to circumvent GEM and gain a big windfall, then why doesn’t everyone chase a Pareto frontier? Apply GEM to the entire system: why haven’t people already picked up the opportunities lying on all these Pareto frontiers?</p><p>Answer: dimensionality. If there’s 100 different specialties, then there’s only 100 people who are the best within their specialty. But there’s 10k pairs of specialties (e.g. statistics/gerontology), 1M triples (e.g. statistics/gerontology/macroeconomics), and something like 10^30 combinations of specialties. And each of those pareto frontiers has room for more than one person, even allowing for elbow room. Even if only a small fraction of those combinations are useful, there’s still a <em>lot</em> of space to stake out a territory.</p><p>And to a large extent, people do pursue those frontiers. It’s no secret that an academic can easily find fertile fields by working with someone in a different department. “Interdisciplinary” work has a reputation for being unusually high-yield. Similarly, carrying scientific work from lab to market has a reputation for high yields. Thanks to the “curse” of dimensionality, these goldmines are not in any danger of exhausting.</p>",johnswentworth,johnswentworth,johnswentworth,
XWPJfgBymBbL3jdFd,"[AN #58] Mesa optimization: what it is, and why we should care",an-58-mesa-optimization-what-it-is-and-why-we-should-care,https://www.lesswrong.com/posts/XWPJfgBymBbL3jdFd/an-58-mesa-optimization-what-it-is-and-why-we-should-care,2019-06-24T16:10:01.330Z,55,20,10,False,False,,"<p>Find all Alignment Newsletter resources <a href=""http://rohinshah.com/alignment-newsletter/"">here</a>. In particular, you can <a href=""http://eepurl.com/dqMSZj"">sign up</a>, or look through this <a href=""https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing"">spreadsheet</a> of all summaries that have ever been in the newsletter. I&#x27;m always happy to hear feedback; you can send it to me by replying to this email.</p><h2><strong>Highlights</strong></h2><p><a href=""https://arxiv.org/abs/1906.01820"">Risks from Learned Optimization in Advanced Machine Learning Systems</a> <em>(Evan Hubinger et al)</em>: Suppose you search over a space of programs, looking for one that plays TicTacToe well. Initially, you might find some good heuristics, e.g. go for the center square, if you have two along a row then place the third one, etc. But eventually you might find the <a href=""https://en.wikipedia.org/wiki/Minimax#Pseudocode"">minimax algorithm</a>, which plays optimally by searching for the best action to take. Notably, your outer optimization over the space of programs found a program that was <em>itself</em> an optimizer that searches over possible moves. In the language of this paper, the minimax algorithm is a <strong>mesa optimizer</strong>: an optimizer that is found autonomously by a <strong>base optimizer</strong>, in this case the search over programs.</p><p>Why is this relevant to AI? Well, gradient descent is an optimization algorithm that searches over the space of neural net parameters to find a set that performs well on some objective. It seems plausible that the same thing could occur: gradient descent could find a model that is itself performing optimization. That model would then be a mesa optimizer, and the objective that it optimizes is the <strong>mesa objective</strong>. Note that while the mesa objective should lead to similar behavior as the base objective on the training distribution, it need not do so off distribution. This means the mesa objective is <strong>pseudo aligned</strong>; if it also leads to similar behavior off distribution it is <strong>robustly aligned</strong>.</p><p>A central worry with AI alignment is that if powerful AI agents optimize the wrong objective, it could lead to catastrophic outcomes for humanity. With the possibility of mesa optimizers, this worry is doubled: we need to ensure both that the base objective is aligned with humans (called <strong>outer alignment</strong>) and that the mesa objective is aligned with the base objective (called <strong>inner alignment</strong>). A particularly worrying aspect is <strong>deceptive alignment</strong>: the mesa optimizer has a long-term mesa objective, but knows that it is being optimized for a base objective. So, it optimizes the base objective during training to avoid being modified, but at deployment when the threat of modification is gone, it pursues only the mesa objective.</p><p>As a motivating example, if someone wanted to create the best biological replicators, they could have reasonably used natural selection / evolution as an optimization algorithm for this goal. However, this then would lead to the creation of humans, who would be mesa optimizers that optimize for other goals, and don&#x27;t optimize for replication (e.g. by using birth control).</p><p>The paper has a lot more detail and analysis of what factors make mesa-optimization more likely, more dangerous, etc. You&#x27;ll have to read the paper for all of these details. One general pattern is that, when using machine learning for some task X, there are a bunch of properties that affect the likelihood of learning heuristics or proxies rather than actually learning the optimal algorithm for X. For any such property, making heuristics/proxies more likely would result in a lower chance of mesa-optimization (since optimizers are less like heuristics/proxies), but conditional on mesa-optimization arising, makes it more likely that it is pseudo aligned instead of robustly aligned (because now the pressure for heuristics/proxies leads to learning a proxy mesa-objective instead of the true base objective).</p><p><strong>Rohin&#x27;s opinion:</strong> I&#x27;m glad this paper has finally come out. The concepts of mesa optimization and the inner alignment problem seem quite important, and currently I am most worried about x-risk caused by a misaligned mesa optimizer. Unfortunately, it is not yet clear whether mesa optimizers will actually arise in practice, though I think conditional on us developing AGI it is quite likely. Gradient descent is a relatively weak optimizer; it seems like AGI would have to be much more powerful, and so would require a learned optimizer (in the same way that humans can be thought of as &quot;optimizers learned by evolution&quot;).</p><p>There still is a lot of confusion and uncertainty around the concept, especially because we don&#x27;t have a good definition of &quot;optimization&quot;. It also doesn&#x27;t help that it&#x27;s hard to get an example of this in an existing ML system -- today&#x27;s systems are likely not powerful enough to have a mesa optimizer (though even if they had a mesa optimizer, we might not be able to tell because of how uninterpretable the models are).</p><p><strong>Read more:</strong> <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">Alignment Forum version</a></p><h1><strong>Technical AI alignment</strong></h1><h3><strong>Agent foundations</strong></h3><p><a href=""https://www.alignmentforum.org/posts/ZDZmopKquzHYPRNxq/selection-vs-control"">Selection vs Control</a> <em>(Abram Demski)</em>: The previous paper focuses on mesa optimizers that are explicitly searching across a space of possibilities for an option that performs well on some objective. This post argues that in addition to this &quot;selection&quot; model of optimization, there is a &quot;control&quot; model of optimization, where the model cannot evaluate all of the options separately (as in e.g. a heat-seeking missile, which can&#x27;t try all of the possible paths to the target separately). However, these are not cleanly separated categories -- for example, a search process could have control-based optimization inside of it, in the form of heuristics that guide the search towards more likely regions of the search space.</p><p><strong>Rohin&#x27;s opinion:</strong> This is an important distinction, and I&#x27;m of the opinion that most of what we call &quot;intelligence&quot; is actually more like the &quot;control&quot; side of these two options.</p><h3><strong>Learning human intent</strong></h3><p><a href=""https://arxiv.org/abs/1905.12888"">Imitation Learning as f-Divergence Minimization</a> <em>(Liyiming Ke et al)</em> (summarized by Cody): This paper frames imitation learning through the lens of matching your model&#x27;s distribution over trajectories (or conditional actions) to the distribution of an expert policy. This framing of distribution comparison naturally leads to the discussion of f-divergences, a broad set of measures including KL and Jenson-Shannon Divergences. The paper argues that existing imitation learning methods have implicitly chosen divergence measures that incentivize &quot;mode covering&quot; (making sure to have support anywhere the expert does) vs mode collapsing (making sure to only have support where the expert does), and that the latter is more appropriate for safety reasons, since the average between two modes of an expert policy may not itself be a safe policy. They demonstrate this by using a variational approximation of the reverse-KL distance as the divergence underlying their imitation learner.</p><p><strong>Cody&#x27;s opinion:</strong> I appreciate papers like these that connect peoples intuitions between different areas (like imitation learning and distributional difference measures). It does seem like this would even more strongly lead to lack of ability to outperform the demonstrator, but that&#x27;s honestly more a critique of imitation learning more generally than this paper in particular.</p><h3><strong>Handling groups of agents</strong></h3><p><a href=""https://arxiv.org/abs/1810.08647"">Social Influence as Intrinsic Motivation for Multi-Agent Deep RL</a> <em>(Natasha Jaques et al)</em> (summarized by Cody): An emerging field of common-sum multi-agent research asks how to induce groups of agents to perform complex coordination behavior to increase general reward, and many existing approaches involve centralized training or hardcoding altruistic behavior into the agents. This paper suggests a new technique that rewards agents for having a causal influence over the actions of other agents, in the sense that the actions of the pair of agents agents have high mutual information. The authors empirically find that having even a small number of agents who act as &quot;influencers&quot; can help avoid coordination failures in partial information settings and lead to higher collective reward. In one sub-experiment, they only add this influence reward to the agents&#x27; communication channels, so agents are incentivized to provide information that will impact other agents&#x27; actions (this information is presumed to be truthful and beneficial since otherwise it would subsequently be ignored).</p><p><strong>Cody&#x27;s opinion:</strong> I&#x27;m interested by this paper&#x27;s finding that you can generate apparently altruistic behavior by incentivizing agents to influence others, rather than necessarily help others. I also appreciate the point that was made to train in a decentralized way. I&#x27;d love to see more work on a less asymmetric version of influence reward; currently influencers and influencees are separate groups due to worries about causal feedback loops, and this implicitly means there&#x27;s a constructed group of quasi-altruistic agents who are getting less concrete reward because they&#x27;re being incentivized by this auxiliary reward.</p><h3><strong>Uncertainty</strong></h3><p><a href=""https://sites.google.com/view/udlworkshop2019/accepted-papers"">ICML Uncertainty and Robustness Workshop Accepted Papers</a> (summarized by Dan H): The Uncertainty and Robustness Workshop accepted papers are available. Topics include out-of-distribution detection, generalization to stochastic corruptions, label corruption robustness, and so on.</p><h3><strong>Miscellaneous (Alignment)</strong></h3><p><a href=""https://www.alignmentforum.org/posts/FQKjY563bJqDeaEDr/to-first-order-moral-realism-and-moral-anti-realism-are-the"">To first order, moral realism and moral anti-realism are the same thing</a> <em>(Stuart Armstrong)</em></p><h1><strong>AI strategy and policy</strong></h1><p><a href=""https://rowanzellers.com/grover/"">Grover: A State-of-the-Art Defense against Neural Fake News</a> <em>(Rowan Zellers et al)</em>: Could we use ML to detect fake news generated by other ML models? This paper suggests that models that are used to generate fake news will also be able to be used to <em>detect</em> that same fake news. In particular, they train a GAN-like language model on news articles, that they dub GROVER, and show that the generated articles are <em>better </em>propaganda than those generated by humans, but they can at least be detected by GROVER itself.</p><p>Notably, they do plan to release their models, so that other researchers can also work on the problem of detecting fake news. They are following a similar release strategy as with <a href=""https://blog.openai.com/better-language-models/"">GPT-2</a> (<a href=""https://mailchi.mp/c48f996a5db5/alignment-newsletter-46"">AN #46</a>): they are making the 117M and 345M parameter models public, and releasing their 1.5B parameter model to researchers who sign a release form.</p><p><strong>Rohin&#x27;s opinion:</strong> It&#x27;s interesting to see that this group went with a very similar release strategy, and I wish they had written more about why they chose to do what they did. I do like that they are on the face of it &quot;cooperating&quot; with OpenAI, but eventually we need norms for <em>how</em> to make publication decisions, rather than always following the precedent set by someone prior. Though I suppose there could be a bit more risk with their models -- while they are the same size as the released GPT-2 models, they are better tuned for generating propaganda than GPT-2 is.</p><p><strong>Read more:</strong> <a href=""https://arxiv.org/abs/1905.12616"">Defending Against Neural Fake News</a></p><p><a href=""https://medium.com/@NPCollapse/the-hacker-learns-to-trust-62f3c1490f51"">The Hacker Learns to Trust</a> <em>(Connor Leahy)</em>: An independent researcher attempted to replicate <a href=""https://blog.openai.com/better-language-models/"">GPT-2</a> (<a href=""https://mailchi.mp/c48f996a5db5/alignment-newsletter-46"">AN #46</a>) and was planning to release the model. However, he has now decided not to release, because releasing would set a bad precedent. Regardless of whether or not GPT-2 is dangerous, at some point in the future, we will develop AI systems that really are dangerous, and we need to have adequate norms then that allow researchers to take their time and evaluate the potential issues and then make an informed decision about what to do. <strong>Key quote:</strong> &quot;sending a message that it is ok, even celebrated, for a lone individual to unilaterally go against reasonable safety concerns of other researchers is not a good message to send&quot;.</p><p><strong>Rohin&#x27;s opinion:</strong> I quite strongly agree that the most important impact of the GPT-2 decision was that it has started a discussion about what appropriate safety norms should be, whereas before there were no such norms at all. I don&#x27;t know whether or not GPT-2 is dangerous, but I am glad that AI researchers have started thinking about whether and how publication norms should change.</p><h1><strong>Other progress in AI</strong></h1><h3><strong>Reinforcement learning</strong></h3><p><a href=""http://arxiv.org/abs/1906.03926"">A Survey of Reinforcement Learning Informed by Natural Language</a> <em>(Jelena Luketina et al)</em> (summarized by Cody): Humans use language as a way of efficiently storing knowledge of the world and instructions for handling new scenarios; this paper is written from the perspective that it would be potentially hugely valuable if RL agents could leverage information stored in language in similar ways. They look at both the case where language is an inherent part of the task (example: the goal is parameterized by a language instruction) and where language is used to give auxiliary information (example: parts of the environment are described using language). Overall, the authors push for more work in this area, and, in particular, more work using external-corpus-pretrained language models and with research designs that use human-generated rather than synthetically-generated language; the latter is typically preferred for the sake of speed, but the former has particular challenges we&#x27;ll need to tackle to actually use existing sources of human language data.</p><p><strong>Cody&#x27;s opinion:</strong> This article is a solid and useful version of what I would expect out of a review article: mostly useful as a way to get thinking in the direction of the intersection of RL and language, and makes me more interested in digging more into some of the mentioned techniques, since by design this review didn&#x27;t go very deep into any of them.</p><h3><strong>Deep learning</strong></h3><p><a href=""https://nostalgebraist.tumblr.com/post/185326092369/the-transformer-explained"">the transformer … “explained”?</a> <em>(nostalgebraist)</em> (H/T Daniel Filan): This is an excellent explanation of the intuitions and ideas behind self-attention and the <a href=""https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"">Transformer</a> <a href=""http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html"">architecture</a> (<a href=""https://mailchi.mp/6bfac400a0c3/alignment-newsletter-44"">AN #44</a>).</p><p><a href=""https://arxiv.org/abs/1904.11455"">Ray Interference: a Source of Plateaus in Deep Reinforcement Learning</a> <em>(Tom Schaul et al)</em> (summarized by Cody): The authors argue that Deep RL is subject to a particular kind of training pathology called &quot;ray interference&quot;, caused by situations where (1) there are multiple sub-tasks within a task, and the gradient update of one can decrease performance on the others, and (2) the ability to learn on a given sub-task is a function of its current performance. Performance interference can happen whenever there are shared components between notional subcomponents or subtasks, and the fact that many RL algorithms learn on-policy means that low performance might lead to little data collection in a region of parameter space, and make it harder to increase performance there in future.</p><p><strong>Cody&#x27;s opinion:</strong> This seems like a useful mental concept, but it seems quite difficult to effectively remedy, except through preferring off-policy methods to on-policy ones, since there isn&#x27;t really a way to decompose real RL tasks into separable components the way they do in their toy example</p><h3><strong>Meta learning</strong></h3><p><a href=""http://arxiv.org/abs/1905.07435"">Alpha MAML: Adaptive Model-Agnostic Meta-Learning</a> <em>(Harkirat Singh Behl et al)</em></p>",rohinmshah,rohinmshah,Rohin Shah,
cFnciZqYiMRXqtign,"Explaining ""The Crackpot Bet""",explaining-the-crackpot-bet,https://www.lesswrong.com/posts/cFnciZqYiMRXqtign/explaining-the-crackpot-bet,2019-06-24T15:17:59.126Z,-40,14,20,False,False,,"<p>Hey, guys. Been meaning to write a blog for some time. Yesterday I promised a friend I would write a post explaining a thing I did called the Crackpot Bet. It seemed to be as good a place to start as any, so here’s the story. (Now I&#x27;ve decided to post it on Less Wrong, readers of which will be familiar with what CFAR, etc. is. Can&#x27;t be bothered to fix it at the moment. Steel Man me, if you&#x27;d be so kind.)</p><p>One day I made a wild claim on a forum for graduates of a thing called CFAR. Someone on the list I will call “Bob” immediately emailed me to the effect that, on the evidence of apparently nothing except my post, I was either a crackpot or a genius, and he would bet all his limbs on the former. I laughed and emailed him I had no use for his limbs but would be willing to make a serious bet if we could find mutually acceptable terms. He immediately offered the following bet: If you’re right, as evidenced by <em>winning the Nobel prize or the Fieldings Medal,</em> he would happily give me all of his worldly possessions.</p><p>In the order of the story, here’s a side note I consider important. In the middle of all this, I sent a letter asking for a job at MIRI (the Machine Intelligence Research Organization) to several high-ups there and related people I knew. One of them, who is also one of my best friends, responded privately that he thought I was a crackpot. Back to the story.</p><p>Bob’s was obviously still a silly bet, and I was still in my Right Mind. But I <em>still</em> thought  there might be a possibility of finding agreeable terms if there was better than 1000:1 odds I was right. I also needed money. So I wrote out the bet, had a good chuckle, and, carried away by self-amusement, decided to send it out to the whole list, as well as a related East Coast list called OBNYC. Here it is:</p><hr class=""dividerBlock""/><p><em>I offer a bet on the following terms: Give me a million dollars as defined below. If I lose, and can be reasonably labeled a crackpot and critically not anyone a reasonable person can label as a famous genius, as defined below, one year after I hit &quot;send,&quot; I pay off the million dollars, with no interest, in monthly payments over a period of 20 years starting a year from today.  </em></p><p><em>If I win, by which mean I am popularly declared to be a &quot;Genius&quot; (or words to that effect), by someone who should know, among most members of the forums to which I&#x27;m sending this Forum, in the popular press as defined by CNN, by one year from today, I keep the money and whatever IP I create related to the bet or whatever. Critically, the person who calls me a Genius as defined above must be a recognized expert in the field that can be reasonably interpreted to include whatever they&#x27;re praising me in, but this field need not be in mathematics. (It could be in the field of farming, just to pick a joke example.)</em></p><p><em>I trust the people reading this are not morons, will act on this email in good faith, and realize I may be a crackpot but I&#x27;m not a moron. I can&#x27;t be bothered to spell it out more clearly than that, but would be very happy to sign a legally binding contract providing we can reach mutually agreeable terms. </em></p><p><em>I offer this bet to the first person who is able to get a million dollars into my bank account, and reserve the right to give it to as many people as I see fit.</em></p><p><em>If you prefer to respond to me privately and/or wish to keep your identity confidential, please send me a private email. If you want to take the bet, I&#x27;m happy to give you whatever info you need to get the money into my account offline.</em></p><p><em>Yours Truly --Glenn</em></p><hr class=""dividerBlock""/><p>As one might reasonably expect, the recipients flipped out and started calling me names. I laughed, shut my laptop, and went away to do errands. Later, checking my email, I saw that a few more people had called me names. Still amused, I decided to tease people and offer something called the “one-box prize.” I told myself that if someone thought through the terms of the bet and called me the word “genius” as a result, I would send that person a copy of the book I was reading.</p><p>In the morning, no one had “won my prize,” so I emailed the list saying so. Someone on OBNYC asked why I hadn’t been banned from the list. My friend Zvi pointed out I was a “genius” for reasons anyone who had thought about it for 5 minutes could see, and warned me to stop posting such things. I promptly responded saying “please don’t ban me!”, tried to make clear I was doing all of this in fun, and announced that Zvi had won the 1-Box Prize.</p><p>People were still calling me names, so I became annoyed and the whole thing became a slight tempest in a teapot. I teased people by calling them “VSMs” (I leave it to the reader to work out what that stands for), made up a “2-Boxer Prize” I thought was guessable by a smart person, and brashly offered on Facebook to donate a million dollars to MIRI in the name of the first person who guessed it.</p><p>That was when <em>close</em> friends started delicately checking to see if I was “okay.” After politely replying I was fine, I promised one of them I would write a post explaining to my friends on the list what was going on. This is that post.</p><p>I plan to write another post on this subject, and publish it tomorrow, so—to be continued. Thanks for reading!</p>",glennonymous,glennonymous,glennonymous,
GAHyuEzYndxDjzXti,How does one get invited to the alignment forum?,how-does-one-get-invited-to-the-alignment-forum,https://www.lesswrong.com/posts/GAHyuEzYndxDjzXti/how-does-one-get-invited-to-the-alignment-forum,2019-06-23T09:39:20.042Z,15,7,5,False,False,,"<p>At the moment there is a link on the Alignment Forum to apply for membership. You can apply by submitting papers, blog posts or comments. However, there is very little in the way of detail of what kind of work they expect they expect in order for you to have a reasonable chance of being invited. I&#x27;m not saying that there should be specific criteria as I&#x27;m in favour of the moderators using their judgement. However, more detail on what they are looking for would provide encouragement for people to work towards achieving the level of knowledge and good judgement that they expect for their members.</p>",Chris_Leong,chris_leong,Chris_Leong,
JRcKcanx755CyekP4,Podcast - Putanumonit on The Switch ,podcast-putanumonit-on-the-switch,https://www.lesswrong.com/posts/JRcKcanx755CyekP4/podcast-putanumonit-on-the-switch,2019-06-23T04:09:25.723Z,6,3,2,False,False,,"<p> I’m really excited to make <a href=""http://www.mojofilter.media/theswitch/2019/6/14/putting-numbers-on-things-w-jacob-falkovich"">my podcast debut on The Switch podcast</a> with Chase Harris and Alex Berner. Here are the links on <a href=""https://podcasts.apple.com/us/podcast/putting-numbers-on-things-w-jacob-falkovich/id1390381400?i=1000441587767"">Apple Podcasts</a>, <a href=""https://play.google.com/music/m/Dvdmcymjs2exgmnrcr7ycfsz3dq?t=Putting_Numbers_on_Things_w_Jacob_Falkovich-The_Switch"">Play Music</a>, and <a href=""https://open.spotify.com/episode/5M10JpgYmq4GLHeO65tJ6F"">Spotify</a>.</p><p>Topics we covered:</p><ul><li>How business school led me to rationality.</li><li>The origin story and meaning of Putanumonit.</li><li>How to put numbers on things where no numbers exist.</li><li>Contextualizing and decoupling in the <a href=""https://www.runnersworld.com/news/a27634219/caster-semenya-appeals-iaaf-testosterone-regulations/"">saga of Caster Semenya</a>.</li><li>Does putting numbers on dating make me an emotionless robot?</li><li>Rationality, mindfulness, and poking your head above the river.</li><li>The posts I regret writing.</li><li>Antinatalism and the connection between emotion and philosophy.</li><li>How intuition follows controversy, and why hunter-gatherers don’t have opinions on immigration.</li><li>Fake frameworks as the key to rationality and why I prefer <a href=""https://medium.com/s/story/the-mtg-color-wheel-c9700a7cf36d"">Magic the Gathering personality color wheel</a> to the big 5 personality system or MBTI.</li><li>Rationality alone and in a group.</li><li>Why soccer is a supreme entertainment product, aesthetic experience, and showcase of virtue.</li><li>MMA as a gateway drug to loving sports.</li></ul>",Jacobian,jacob-falkovich,Jacob Falkovich,
GMTjNh5oxk4a3qbgZ,The Foundational Toolbox for Life: Introduction,the-foundational-toolbox-for-life-introduction-1,https://www.lesswrong.com/posts/GMTjNh5oxk4a3qbgZ/the-foundational-toolbox-for-life-introduction-1,2019-06-22T06:11:59.497Z,23,12,2,False,False,,"<p><strong>(Co-authored with Sailor Vulcan)</strong></p><p><strong>Follows from: </strong></p><p><strong><u><a href=""https://www.lesswrong.com/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality"">What Do We Mean by Rationality?</a></u></strong> </p><p><strong><u><a href=""https://www.lesswrong.com/posts/bRGbdG58cJ8RGjS5G/no-really-why-aren-t-rationalists-winning"">No Really, Why Aren&#x27;t Rationalists Winning?</a></u></strong></p><p><strong>Related resources: </strong></p><p><strong><u><a href=""https://www.lesswrong.com/s/oLGCcbnvabyibnG9d"">Inadequate Equilibria</a></u></strong></p><p><strong><u><a href=""https://www.lesswrong.com/s/9rRrzkBaXcivjZtZS"">Instrumental Rationality sequence</a></u></strong></p><p><strong><u><a href=""https://medium.com/s/story/the-mtg-color-wheel-c9700a7cf36d"">How the ‘Magic: The Gathering’ Color Wheel Explains Humanity</a></u></strong></p><br/><h2>What’s Going On Here?</h2><p>As I was growing up, I could tell that something critical was missing from many of the adults that I encountered, and from the way society as a whole functioned. People regularly made decisions that seemed obviously stupid, and the institutions they built were little better. When I eventually learned of others who agreed that society was doing a poor job of addressing problems, I was relieved, but still puzzled that nobody had managed to fix the situation yet. </p><p>As I learned the basics of rationality, I became more confused. People had created formalized methods for making better decisions based on understanding how systems and logic worked, which was amazing. However, not only had these methods failed to become the default standard of human adulthood, not only were they not used to guide policy decisions in the public and private sectors, but the methods were obscure. Barely anybody knew they existed. The world rationalists said they wanted was nowhere in sight. </p><p>I concluded that there had to be more to implementing effective policies and institutions than just knowing how things worked. If I wanted to solve any important problems like that, I would have to develop other skills. Since all I had was the proverbial hammer of understanding ideas and mechanisms, I embarked on a journey to hammer out an understanding of skills. I asked myself, what do I need to do to build the world I expect? </p><p>A look at the people who seemed to be the ones building the world prompted further questions: Why are non-rationalists having so much more impact than rationalists? What skills are they applying that rationalists aren’t? And how do these non-rationalists successfully learn and apply skills they don&#x27;t fully understand? </p><h2>Cognitive Filters; or, “Parallel Realities of a Plane”</h2><p>As was pointed out in <strong><u><a href=""https://www.lesswrong.com/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality"">What Do I mean by Rationality</a></u></strong>, it’s not possible to form an accurate view of the world in any reasonable amount of time by tracking minute observations one by one and plugging the data from that into Bayesian formalisms in their full form. This is a principle of epistemic rationality.</p><p>I realized there must also be an equivalent principle of what we commonly refer to as instrumental rationality: You cannot succeed at an activity in any reasonable amount of time by tracking one by one the procedural steps and data points involved in that activity, plugging them into decision matrices, and outputting the choice that maximizes expected utility at each step.</p><p>In the same way that Bayesian formalisms in their full form are computationally intractable on most real world epistemic problems, decision matrices are computationally intractable on most real world instrumental problems. No one can calculate and obey the math any more than you can win a game of baseball by paying attention to quarks, one by one. </p><p>So what do successful humans pay attention to instead? </p><p>To compress information into a form which is computationally tractable for instrumental problems while retaining enough accuracy to make the results of the computation useful, humans apply cognitive filters to the information. They screen out minute details and deal with the problem at a higher level of abstraction. </p><p>It’s obvious that these filters will be lossy. That’s their advantage: they remove irrelevant information so you can fit the problem into your brain. What’s less obvious is that at each level of abstraction, each level of loss, there are multiple valid filters that can be applied. Each filter regards and disregards different aspects of the situation, just as a map can show physical or political geography. The filters are all approximately accurate—they’re just answering different questions. </p><p>As an epistemic example, let us suppose you have a trans-oceanic airplane design that has a surprisingly short lifespan. After only a few flights, its wings break, and it falls out of the sky and into the sea. </p><p>To troubleshoot, you apply a physics filter with a mechanical focus. It takes into account the shape, mass distribution, and material strength of the plane’s structure. It factors in the thrust provided by the engine, and the flow of air across the wings. All other information is filtered out to increase the computational efficiency. </p><p>Examining the plane through this physics filter tells you that everything should work fine. The wings should not be exposed to any force they can’t handle. It’s not until you apply a chemistry filter that you see that the exposed wing rivets are corroding due to high concentrations of ions in the sea air. The plane is changing from your physical model of it, and your physics filter failed to predict that. This particular physics filter doesn’t even have the concepts you need to ask the right question. </p><p>A different designer has an airplane that looks just fine through the chemistry filter, but can’t get off the ground. You apply the physics filter and immediately tell them to either make the plane go faster or lighten the wing loading: the aircraft’s wings are so small compared to its mass that it needs to travel faster to generate enough lift. </p><p>Nobody who works with airplanes uses a filter based on the more fundamental, less abstracted physical laws from which both the mechanical physics filter and the chemical filter are derived. Using such a filter would indeed tell them of all the mechanical and chemical problems of the plane. However, it would also require them to know about all the quarks in the plane. </p><p>Instead, they apply the physics filter to tell them the most important questions to ask the chemistry filter, and vice versa. If we were to design a plane right now, physics will tell us where we most have to worry about chemistry, such as which areas are most vulnerable to fracturing if the material corrodes. Conversely, chemistry will tell us where we most have to worry about physics, such as which parts of the plane should not be exposed to extreme temperatures. We can explore solutions in each filter and critique them with the other filter, and we can iterate between these as much as we need to until we reach an equilibrium. </p><p>We can even look through both filters at once to exploit interactions between phenomena that aren’t all visible through the same filter. For example, we might not take van der Waals forces into account unless we see how phenomena from our chemistry and physics filters affect each other. The filters both lose information for computational tractability, but each one <u><a href=""https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb/p/d5NyJ2Lf6N22AD9PB"">carves reality at different joints</a></u>. Used together they are greater than the sum of them individually. It turns out dovetailing two or more lossy abstractions is highly effective at solving most fiddly, practical problems, and vastly more efficient than going down to the quark level, to the point where people can solve a surprisingly wide array of problems without even knowing the quark level exists. </p><h2>Paradigms and Skills</h2><p>Humans developed most of these cognitive filters, also known as <strong>paradigms</strong>, through brute force: generations of biological and cultural evolution. Confronted with a task, individual people adopted the paradigms that came easily and naturally to them. The paradigms that helped them succeed were the ones that caught on with humanity at large. That’s why non-rationalists collectively are in possession of many of them. The paradigms they have suffice for dealing with most of the situations they face, at least on the local scale. </p><p>It’s not enough to merely have a paradigm, though. Using it effectively requires practice. A paradigm tells you what sorts of details to pay attention to in order to get an answer, and how they fit together. It only yields a <strong>skill</strong> when it is calibrated with experiences and feedback about how specific details affect the outcome of a situation. Calibrating a paradigm into a skill is the process of using feedback to gauge the relative importance of those details that fit together, and learning to adjust how you process them to accurately predict or change the outcome of the situation. </p><p>Some analogies may express this distinction more clearly. If the paradigm you’re applying to a situation is a polynomial equation, like ax^2 + bx + c = y, where x is an input variable you know, and y is the solution you’re looking for, calibrating that paradigm into a skill is figuring out over time what the parameter values a, b, and c are for a recurring situation. If your paradigm is what sort of motion you must make with your arm to throw a ball, calibrating that paradigm into a skill means adjusting the fine dimensions of that motion to actually hit your target. </p><p>As the throwing analogy implies, paradigms apply to instrumental skills as well. A non-rationalist may master a skill by gradually developing the right thought patterns and paradigm for that particular skill through experience and practice. They may not know why their thinking habits work, or how to apply them to other situations, but they can succeed at the rigidly-bounded task with which they’re familiar. They can outperform people whose paradigms are less efficient, or less well calibrated. </p><p>For example, a rationalist may know how to derive the theory of playing chess from basic logic and game theory, but it doesn’t follow that they will be able to defeat a seasoned non-rationalist chess player in a timed match if they’ve never so much as played a board game before. The non-rationalist chess player will have a better sense of what to pay attention to and what effects moves will have, and thus will have more brain space free to plan ahead in the time they have. </p><p>However, people often suffer for not understanding how or why their skills work. Paradigms are defined by assumptions about how systems work and what the user’s values and goals are, and people are often ignorant of the assumptions they’re making. They might have difficulty adapting a paradigm for use in different situations, or using it in conjunction with another paradigm to enhance both. The chess player may fail to use their strategy experience to help prevent accidents, or successfully run a business, because those tasks require other skills in addition to strategy. Understanding paradigms and how to calibrate them is critical to mastering and applying skills beyond the narrow range for which one may have a natural inclination. </p><h2>The Toolbox</h2><p>Successful institutions and policies require those participating in them to have constructive skills. The reason so many of them are ineffective or outright harmful is because they don&#x27;t have a way to educate everyone who needs to participate so they have the skills required. The same goes for movements and communities. </p><p>Many people grow up learning how to go through the motions of a skill without understanding why it works, and that makes it harder for them to use it effectively, to adapt their skill to different contexts, and to learn other similar skills. A society of such people is ill-prepared to make wise decisions, and to build and sustain structures that could solve our most important problems. </p><p>After figuring out how the paradigms and calibration of non-rationalists help them succeed at developing skills, I was able to put together a toolbox of the most basic concepts and paradigms that are the building blocks of all skills. </p><p>The concepts in this toolbox can help people frame problems they&#x27;re stuck on, and equip them to conceive of and (with practice) implement better solutions. The purpose of these concepts is to form a foundation for people to more easily learn the skills to participate in effective institutions, and to continue learning more skills on their own that they may want or need. </p><p>This is the Foundational Toolbox for Life. </p>",ExCeph,exceph,ExCeph,
zj9jWggQAxSw46HwB,How hard is it for altruists to discuss going against bad equilibria?,how-hard-is-it-for-altruists-to-discuss-going-against-bad,https://www.lesswrong.com/posts/zj9jWggQAxSw46HwB/how-hard-is-it-for-altruists-to-discuss-going-against-bad,2019-06-22T03:42:24.416Z,46,17,6,False,False,,"<p><em>Epistemic status: This post is flagrantly obscure, which makes it all the harder for me to revise it to reflect my current opinions. By the nature of the subject, it's difficult to give object-level examples. If you're considering reading this, I would suggest <a href=""https://www.lesswrong.com/posts/WbQB5LnRyke5ye4Yg/the-belief-signaling-trilemma"">the belief signaling trilemma</a> as a much more approachable post on a similar topic. Basically, take that idea, and extrapolate it to issues with coordination problems?</em></p><ul><li>There are many situations where a system is ""broken"" in the sense that incentives push people toward bad behavior, but, not <em>so</em> much that an altruist has any business engaging in that bad behavior (at least, not if they are well-informed).</li><ul><li>In other words, an altruist who understands the bad equilibrium well would disengage from the broken system, or engage while happily paying the cost of going against incentives.</li><li>Clearly, this is not always the case; I'm thinking about situations where it is the case.</li><ul><li>Actually, I'm thinking about situations where it is the case <em>supposing that we ignore certain costs, such as costs of going against peer pressure, costs of employing willpower to go against the default, etc.</em> The question is then: is it realistically worth it, given all those additional costs, if we condition on it being it's worth it for an imaginary emotional-robot altruist?</li><li>Actually actually, the question I'm asking is probably not that one either, but I haven't figured out my real question yet.</li><ul><li>I think maybe I'm mainly interested in the question of how hard it is for altruists to publicly discuss altruistic strategies (in the context of a bad equilibrium) without upsetting a bunch of people (who are currently coordinating on that equilibrium, and are therefore protective of it).</li></ul></ul><li>I'm writing this post to try to sort out some confused thoughts (hence the weird style). A lot of the context is <a href=""https://www.lesswrong.com/posts/5nH5Qtax9ae8CQjZ9/no-it-s-not-the-incentives-it-s-you"">discussion on this post</a>. </li><ul><li>But, I'm not going to discuss examples in my post. This seems like a case where giving examples is more likely to steer discussion to unproductive places than the reverse, at least if those examples are close to anyone's real situation/concern.</li></ul></ul><li>I'm using the term ""altruist"" in an absolute way here, which is a bit misleading. </li><ul><li>I think it makes sense to talk about, and try to understand, what a perfect altruist can do to forward their own values. I'm not talking about a decision-theoretically perfect altruist. I'm talking about a basically normal person, who reflectively-stably prefers to forward some version of altruism. They may be very wrong about how to go about it, but if such wrongs were pointed out (with sufficient argument/evidence), they would change their behavior.</li><li>I'm not even claiming there are such people. I think discussing what the perfect altruist could do makes sense as a rallying point around which a lot of somewhat-altruistic people can coordinate epistemically -- IE, a lot of people who are not perfectly altruistic would still be interested in knowing what the perfect altruist would do, even if they ultimately decide it isn't worth it for their values.</li><li>A lot of what I'm going to say applies similarly well to imperfect altruists. I'm talking about the sort of person who operates on selfish motives a lot of the time, but who generally stops kicking puppies when they realize it hurts the puppies. </li><ul><li>Well, ok, maybe that's too low a bar. But I think the bar isn't really too high.</li></ul></ul><li>I'm making the assumption that the altruists cannot exclude non-altruists well enough to discuss things only among altruists, at least not in public discussions. So the discussion is at least a little shaped by non-altruists. Certainly the discussion norms -- norms about taking offense, taboo topics, etc. are not going to be <em>totally</em> inconsiderate to more self-interested parties.</li><li>There's an important distinction between the idea that ""[some bad behavior] is blameworthy"", vs ""[some bad behavior] is not worth it from an altruistic perspective"".</li><ul><li>This distinction is difficult to maintain in public discourse. People engaging in [the bad behavior] don't want to be punished. A consensus that ""[the bad behavior] is bad"" will seem very dangerous to them -- it is, at the very least, difficult to establish common knowledge that such a consensus wouldn't quickly slide into ""[the bad behavior] is blameworthy and should be punished"".</li><ul><li>This follows from a norm that ""bad things should be punished"" -- or, unpacking a bit more: if an action is generally understood to be do more harm than good (taking everyone into account) in comparison to a well-known alternative, and particularly if harm accrues to others (it's not a ""victimless crime"" -- not an activity between consenting adults), then negative consequences should be imposed.</li><li>This relates to being stuck in an <a href=""https://www.lesswrong.com/posts/YRgMCXMbkKBZgMz4M/asymmetric-justice"">asymmetric justice</a> system.</li><ul><li>I actually don't fully endorse the conclusions of that post. I think it <em>does</em> often make sense to set up asymmetric justice systems. </li><li>One reason is that net-positive activities are often monetizable anyway, because you can find a way to charge for the service. Justice systems focus on handling the negative because that's the part which less takes care of itself. </li><li>It also seems somehow related to the difference between criminal law and civil law. A civil violation (a tort) involves paying damages -- you owe something, and you're paying back what you owe. Society sees it as OK once it's been evened out. A crime, on the other hand, is something which society wants to basically never happen. So, punishments are disproportionately large.</li><ul><li>One justification for this could be that crimes can cause irreparable damages. No amount of money can bring a person back to life (...yet), so, it doesn't make sense to deal with murder by paying damages.</li><li>Another justification for disproportionate punishment may be that not all criminals are caught -- so, the punishment has to be sufficient to ensure that the crime is not worth the risk.</li><li>Regardless, it's important to ask whether the justice system succeeds in these purported goals. Setting higher penalties doesn't only disincentivize a crime -- it also makes people work harder not to be caught. Sometimes you might just be fighting a losing battle here, in which case it might work better to find ways to bring activities within the law rather than keeping them illegal (legitimizing and possibly regulating the illicit activity).</li></ul></ul></ul><li>Especially in very public discussions, most people will work hard to at least maintain plausible deniability -- to keep alive the hypothesis that they <em>could</em> be acting altruistically. Public clarity on the question what a real altruist would be motivated to do is dangerous to that.</li><ul><li>Maybe it's less that everyone needs to plausibly be an altruist, and more that no one wants to look selfish. </li><ul><li>Logically, these are the same, but the practical difference is that you can avoid looking selfish by maintaining that there's something between selfishness and altruism.</li><ul><li>I'm not denying that there's a spectrum between perfect selfishness and perfect altruism. But I think people broadly talk about selfishness and altruism as if it's more than just a spectrum -- like there's at least a third option, which involves taking care of yourself and mostly minding your own business and not hurting anyone. Basically, ""good"" as norm-following.</li><li>All of the activities associated with this third option are really things, but, this doesn't stop us from examining how selfish or altruistic one must be for the actions to make sense.</li><li>(And it seems important to recognize that the third option may be used as a smokescreen to avoid such analysis.)</li></ul></ul><li>One weapon which can be used to muddy the waters is public equivocate the altruistic strategy with (irrational) self-sacrifice. </li><ul><li>An extreme version of this is if you make sure that everyone thinks true altruists would immediately give away all their money and material resources to some cause. The true altruist would not do this, at least not in most circumstances, because it would destroy their ability to pursue altruistic causes. However, if you get people to think this, then you can excuse your actions by talking about ""the incentives"" and, if anyone points out that your behavior is bad on net taking pros and cons of following bad local incentives into account, you accuse them of hypocrisy for not giving away all their money or something like that.</li><ul><li>Ideally we would want to be able to respond to something like that by providing reassurance that we're not assigning blame to them, and then go back to discussing realistically what an altruist would do. (I think.) However, that seems difficult to assure!</li><li>All of this will be going on in the background for the person being defensive -- they probably wouldn't admit to themselves that they're getting defensive because they're scared they'll suddenly be coordinated against if the discussion continues.</li><li>It's important to keep in mind that the hypothetical interlocuter <em>may themselves be an altruist </em>(or close enough). They're not necessarily feeling threatened because they're secretly selfish and don't want to be outed. They're feeling threatened <em>because they are currently engaging in behavior consistent with the bad equilibrium</em>, and don't want to be suddenly coordinated against! An altruist who doesn't yet see that it's worth going against the grain can be in this position.</li></ul><li>A less extreme version of this is to constantly bring up the idea that altruists have to deal with incentives too -- that you can't accomplish anything if you insist on acting as if you're already in the better equilibrium you're wishing for.</li><ul><li>Here, you're not equivocating between altru<em>ists</em> and self-sacrifice, but rather, you're equivocating between naive altruistic strategies which ignore the cost-benefit analysis and sophisticated altruistic strategies which respect the cost-benefit analysis. It's plausible that any altruistic strategy someone articulates is still too naive, so you raise the hypothesis to attention all the time.</li><li>This might even be correct; the problem is if the argument is being used to block discussion of possibly better strategies.</li><li>But it might seem <em>really really worth it to block discussion for seemingly pragmatic reasons</em>, because discussions of what actions might be worth it for altruists <em>really can</em> slide into assigning blame to overtly non-altruistic acts. </li><li>In fact, maybe it <em>is</em> worth it to block such discussions!! But, in doing so, it seems sad if we can't at least flag explicitly that we've given up on the possibility of truth-seeking in public discourse in that area.</li></ul></ul></ul><li>On the other hand, such public discussion is very valuable <em>to altruists </em>(and to people who are sufficiently close to being altruists). Altruists aren't perfect, or even necessarily very good, at reasoning through these things themselves. So it is great if there can be publically available clarity about what altruists should do (""should"" in the sense of rationality, not in the sense of blameworthiness).</li><ul><li>Furthermore, it's <em>pretty bad</em> for altruists if the public discourse manages to coordinate on misleading arguments which preserve the status quo by successfully equating selfish incentive gradients with altruistic incentive gradients and concluding that the best you can do is make small improvements while mostly following the norms of the bad equilibrium.</li><ul><li>This is really easy to do, because almost everyone learns by imitating others; you might explicitly question a few specifically fishy things, but the vast majority of the time, you imitate standard operating procedure.</li><ul><li>By ""question"", I mean ""question whether it's really worth it, in a way which may lead to change"". Everyone gripes about some things which seem like coordination failures (especially acts committed by <em>other</em> people). But, in the end, we follow a lot of norms by imitation, in part because it is difficult to evaluate everything ourselves.</li></ul><li>It's only when there are a bunch of smart people trying to figure out better ways of doing things that there's a significant threat of anything other than this happening; so, it would make sense for there to be a lot of pressure on such a discourse to avoid forming a true consensus that [bad thing] is bad.</li><ul><li>Again, this pressure <em>can come from altruists.</em> Maybe no one who has thought about it a bunch thinks [bad thing] should be punished. Even the altruists who recognize that [bad thing] is bad. So, even altruists who are savvy to this whole situation could engage in behavior to prevent and stifle conversation about whether [bad thing] is bad.</li><li>The avoidable tragedy here is the less savvy altruists who might trust the public discourse. We want to either find a way to discuss whether [bad thing] is bad (<em>without</em> being threatening; <em>without</em> being likely to slide into punishing people currently doing [bad thing]; <em>without</em> earning the ire and outrage of people whose livelihood currently depends on [bad thing]), <em>or, at least, </em>we want to avoid contaminating the public discourse of those people who are interested in figuring out what an altruistic person can do to forward their altruistic goals. </li><li>This could mean shutting down the conversation in a clearly-marked way, which avoids spreading any misleading justifications about incentives. This could mean propagating a meme which says ""we can't realistically talk about this without going crazy, because there are too many weird things going on here"" (similar to the don't-talk-politics meme). I don't know.</li><li>(certainly my wish would be to find a safe way to talk about things like this)</li></ul></ul></ul><li>This cuts both ways: because almost no one wants to be obviously acting from selfish motives, public clarity about what is altruistically worth it may be a good way, in itself, of achieving better equilibria. People are willing to coordinate around clear pictures of what the altruistic person would do, because it's good signalling, or perhaps because they're too afraid of getting called out to <em>not</em> coordinate around such things.</li><ul><li>But note that this fact <em>in itself</em> is part of the problem -- part of what will make some people upset about attempts to achieve such clarity.</li><li>(Even if we manage to successfully avoid the slippery slope from consensus-about-altruism to consensus-to-punish, a consensus-on-altruism may be enough to disrupt the current equilibrium simply due to the number of people who switch to the altruistic action. Even with no new norm and no enforcement, the disruption to the equilibrium may itself be costly for some individuals, whole current strategy depends on the status quo.)</li><ul><li>You can think of this as ""an equilibrium prefers homeostasis"". The economy/ecology has vested interest in maintaining the current equilibrium because that equilibrium has sprouted a number of niches, and people depend on those niches.</li><li>There will likely be more dependence on the consensus wrong answer that you realize, because people hide the ways which they depend on coordinating on the wrong answer to questions. (Instinctively -- they don't usually realize they're doing this.)</li></ul></ul><li>But [bad thing] really is blameworthy!</li><ul><li>Yeah, I mean obviously, the <em>ideal</em> situation would be to coordinate against [bad thing]. It's better for everyone, even the ""selfish"" people, in the end.</li><li><em>no! bad dog! get down! off the table!...</em></li></ul><li>One possible solution to this problem is to make a very clear ""no retroactive law"" kind of policy.</li><ul><li>If successfully established, this would stop people from feeling threatened about being punished for their current behavior. They'd trust that a social consensus against [bad thing] would leave them time to stop doing [bad thing] before any enforcement came into effect.</li><li>One problem with this is that there will still be people whose livelihood depends on [bad thing] -- often to a much greater degree than you might suspect (since people will tend to hide this, and hide it well). So, some people (again, including some altruists, especially if they don't understand the full argument about [bad thing]) will still become very defensive and try to muddy the waters whenever they notice people discussing whether [bad thing] is bad.</li><ul><li>(Again, they don't necessarily realize it's what they're doing -- it's natural to get defensive about things connected to your livelihood, and motivated cognition kicks in.)</li></ul><li>Another problem is that it would be hard to get that much legibility around norm enforcement. There might be a slowly building consensus around [bad thing]. Over time, more and more people think it is bad, and more and more people take it upon themselves to punish bad-doers. Punishment can take small and illegible forms. People might associate with you less, because they don't want to associate with people seen as bad. You might be blindsided -- maybe you weren't paying attention as consensus shifted over the course of a year, and now suddenly everyone is out to get you.</li><li>Still, a strong ""no retroactive law"" norm makes a lot of sense to me.</li><ul><li>In the conversation-norms example from <a href=""https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag"">the rabbit-hunt post</a>, it is obviously better to publicly discuss and confirm a norm before enforcing it. Not only, or even primarily, because this provides a clean starting point for a norm at which enforcement begins.</li></ul><li>More generally, a policy of carefully discussing what new equilibria should be and trying to address concerns from as many parties as possible by engaging in positive sum trades, before instituting new norms.</li><ul><li>You want people to <em>expect</em> this. You want people to be <em>happy</em> that you're discussing the possibility of a better equilibrium, <em>even though</em> the equilibrium shift might threaten them, <em>because</em> they expect compensation.</li><li>This might mean throwing people a bone even when you see them as ""bad actors"". Even extremely bad actors.</li><ul><li>From the inside, it might feel like <em>giving them reparations for the sad fact that they won't be able to be evil anymore.</em> It's very counterintuitive.</li><li>It's also sketchy. We generally don't want to incentivise people to find ways to selfishly extract value from a system at others' expense, on the expectation that we'll later reward them for that when we figure out what's going on.</li><li>It's kind of like the argument trolls and black-hat hackers sometimes make -- ""I'm teaching them a lesson; they shouldn't be so easy to exploit"". Except you're backing that argument up with cash payments for their service, or special treatment, or honors and acolade -- a Best Villain prize.</li></ul></ul></ul></ul><li>So, there's a sense in which punishment norms are the problem.</li><ul><li>Like, <em>because there's already a fairly broad norm against being overtly selfish </em>-- a norm which seems very reasonable and altruistically positive on the face of it -- we end up stuck in an equilibrium where it is difficult to get public clarity about what an altruist can do to best forward altruistic values.</li></ul><li>""Is the problem really so bad? It seems like altruists can just talk about altruism.""</li><ul><li>This part is difficult to get into without pointing at specific examples, but, I don't really expect pointing at examples to work. Examples tend to be really slippery, because everything is already covered in a blanket of plausible deniability.</li><ul><li>That's the severity of the issue. Those naive to the dynamic I'm pointing at will not be able to see that this is going on in a conversation, <em>because that's the point of the strategy -- to avoid creating common knowledge</em>. On the other hand, people who understand what's going on will tend to be in on it, so, will continue to throw up smokescreens to maintain plausible deniability around any example which is pointed out.</li></ul><li>The hope is that talking about these dynamics at the meta level helps to make people who would be naive more aware about what might be going on in a conversation. Talking about these issues a bunch on the meta-level makes plausible deniability harder to maintain in specific situations later on, without specifically threatening existing plausible deniability.</li><li>There's an obvious relationship to <a href=""http://www.paulgraham.com/say.html"">what you can't say</a>, and also an analogy to <a href=""http://www.paulgraham.com/lies.html"">lies we tell kids</a>.</li></ul><li>It might be that I'm just reinventing the book Moral Mazes. I haven't read it yet.</li></ul>",abramdemski,abramdemski,abramdemski,
36fxiKdEqswkedHyG,The Hacker Learns to Trust,the-hacker-learns-to-trust,https://www.lesswrong.com/posts/36fxiKdEqswkedHyG/the-hacker-learns-to-trust,2019-06-22T00:27:55.298Z,80,30,18,False,False,https://medium.com/@NPCollapse/the-hacker-learns-to-trust-62f3c1490f51,"<p><em>This is a linkpost for some interesting discussions of info security norms in AI. I threw the post below together in 2 hours, just to have a bunch of quotes and links for people, and to have the context in one place for a discussion here on LW (makes it easier for <a href=""https://www.lesswrong.com/posts/9QxnfMYccz9QRgZ5z/the-costly-coordination-mechanism-of-common-knowledge"">common knowledge</a> of what the commenters have and haven&#x27;t seen). I didn&#x27;t want to assume people follow any news on LW, so for folks who&#x27;ve read a lot about GPT-2 much of the post is skimmable.</em></p><h2>Background on GPT-2</h2><p>In February, OpenAI wrote <a href=""https://openai.com/blog/better-language-models/"">a blogpost announcing GPT 2</a>:</p><blockquote>We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.</blockquote><p>This has been a very important release, not least due to it allowing fans to try (and fail) to <a href=""https://towardsdatascience.com/openai-gpt-2-writes-alternate-endings-for-game-of-thrones-c9be75cd2425"">write better endings to Game of Thrones</a>. Gwern used GPT-2 to write <a href=""https://www.gwern.net/GPT-2"">poetry</a> and <a href=""https://www.thiswaifudoesnotexist.net/index.html"">anime</a>. There have been many Medium posts on GPT-2, <a href=""https://towardsdatascience.com/openais-gpt-2-the-model-the-hype-and-the-controversy-1109f4bfd5e8"">some very popular</a>, and at least one Medium post <a href=""https://medium.com/comet-ml/this-machine-learning-medium-post-does-not-exist-c4705215b4a0"">on GPT-2 written by GPT-2</a>. There is a subreddit where all users are copies of GPT-2, and they imitate other subreddits. It got too meta when the subreddit imitated <a href=""https://slatestarcodex.com/2019/06/20/if-only-turing-was-alive-to-see-this/"">another subreddit about people play-acting robots-pretending-to-be-humans</a>. <a href=""https://medium.com/@ysaw"">Stephen Woods</a> has lots of examples including food recipes. </p><p>Here in our rationality community, we created <a href=""https://www.lesswrong.com/users/gpt2"">user GPT-2</a> trained on the entire corpus of LessWrong comments and posts and released it onto the comment section on April 1st (a user who we <a href=""https://www.lesswrong.com/posts/cxpZYswq5QvfhTEzF/user-gpt2-has-a-warning-for-violating-frontpage-commenting"">warned</a> and then <a href=""https://www.lesswrong.com/posts/7xJiotzeonZaAbgSp/user-gpt2-is-banned"">banned</a>). And Nostalgebraist <a href=""https://uploadedyudkowsky.tumblr.com/"">created a tumblr</a> trained on the entire writings of Eliezer Yudkowsky (sequences+HPMOR), where Nostalgebraist picked their favourites to include on the Tumblr.</p><p>There was also very interesting analysis on LessWrong and throughout the community. The post that made me think most on this subject is Sarah Constantin&#x27;s <a href=""https://www.lesswrong.com/posts/4AHXDwcGab5PhKhHT/humans-who-are-not-concentrating-are-not-general"">Human&#x27;s Who Are Not Concentrating Are Not General Intelligences</a>. Also see SlateStarCodex&#x27;s <a href=""https://slatestarcodex.com/2019/02/18/do-neural-nets-dream-of-electric-hobbits/"">Do Neural Nets Dream of Electric Hobbits?</a> and <a href=""https://slatestarcodex.com/2019/02/19/gpt-2-as-step-toward-general-intelligence/"">GPT-2 As Step Toward General Intelligence</a>, plus my teammate jimrandomh&#x27;s <a href=""https://www.lesswrong.com/posts/s9sDyZ9AA3jKbM7DY/two-small-experiments-on-gpt-2"">Two Small Experiments on GPT-2</a>.</p><p>However, these were all using a nerfed version of GPT-2, which only had 175 million parameters, rather than the fully trained model with 1.5 billion parameters. (If you want to see examples of the full model, see the initial announcement posts for <a href=""https://openai.com/blog/better-language-models/"">examples with unicorns and more</a>.) </p><p><strong>Reasoning for only releasing a nerfed GPT-2 and response</strong></p><p>OpenAI writes:</p><blockquote>Due to our concerns about malicious applications of the technology, we are not releasing the trained model.</blockquote><p>While the post includes some discussion of how specifically GPT-2 could be used maliciously (e.g. automating false clickbait news, automated spam, fake accounts) the key line is here.</p><blockquote>This decision, as well as our discussion of it, is an experiment: while we are not sure that it is the right decision today, we believe that the AI community will eventually need to tackle the issue of publication norms in a thoughtful way in certain research areas. </blockquote><p>Is this out of character for OpenAI - a surprise decision? Not really.</p><blockquote>Nearly a year ago we wrote in the <a href=""https://blog.openai.com/openai-charter/"">OpenAI Charter</a>: “we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research,” and we see this current work as potentially representing the early beginnings of such concerns, which we expect may grow over time. </blockquote><blockquote>Other disciplines such as biotechnology and cybersecurity have long had active debates about responsible publication in cases with clear misuse potential, and we hope that our experiment will serve as a case study for more nuanced discussions of model and code release decisions in the AI community.</blockquote><p><strong>Public response to decision</strong></p><p>There has been discussion in news+Twitter, see <a href=""https://www.skynettoday.com/briefs/gpt2"">here</a> for an overview of what some people in the field/industry have said, and what the news media has written. The main response that&#x27;s been selected for by news+twitter is that OpenAI did this primarily as a publicity stunt.</p><p>For a source with a different bias than the news and Twitter (which selects heavily for anger and calling out of norm violation), I&#x27;ve searched through all Medium articles on GPT-2 and copied here any &#x27;most highlighted comments&#x27;. Most posts actually didn&#x27;t have any, which I think means they haven&#x27;t had many viewers. Here are the three I found, in chronological order.</p><p><a href=""https://towardsdatascience.com/openais-gpt-2-the-model-the-hype-and-the-controversy-1109f4bfd5e8"">OpenAIs GPT-2: The Model, The Hype, and the Controvery</a></p><blockquote>As ML researchers, we are building things that affect people. Sooner or later, we’ll cross a line where our research can be used maliciously to do bad things. Should we just wait until that happens to decide how we handle research that can have negative side effects?</blockquote><p><a href=""https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8"">OpenAI GPT-2: Understanding Language Generation through Visualization</a></p><blockquote>Soon, these <a href=""https://en.wikipedia.org/wiki/Deepfake"">deepfakes</a> will become personal. So when your mom calls and says she needs $500 wired to the Cayman Islands, ask yourself: Is this really my mom, or is it a language-generating AI that acquired a <a href=""https://www.americaninno.com/boston/funding-boston/cambridge-voice-skins-startup-modulate-raises-2m-in-seed-funding/"">voice skin</a> of my mother from that Facebook video she posted 5 years ago?</blockquote><p><a href=""https://medium.com/@NPCollapse/gpt2-counting-consciousness-and-the-curious-hacker-323c6639a3a8"">GPT-2, Counting Consciousness and the Curious Hacker</a></p><blockquote><em>If we have a system charged with detecting what we can and can’t trust, we aren’t removing our need to invest trust, we are only moving our trust from our own faculties to those of the machine.</em></blockquote><p>I wrote this linkpost to discuss the last one. See below.</p><h2>Can someone else just build another GPT-2 and release the full 1.5B parameter model?</h2><p>From the initial OpenAI announcement:</p><blockquote>We are aware that some researchers have the technical capacity to reproduce and open source our results. We believe our release strategy limits the initial set of organizations who may choose to do this, and gives the AI community more time to have a discussion about the implications of such systems.</blockquote><p>Since the release, one researcher has tried to reproduce and publish OpenAI&#x27;s result. Google has a program called TensorFlow Research Cloud that gives loads of free compute to researchers affiliated with various universities, which let someone train an attempted copy of GPT-2 with 1.5 billion parameters. They <a href=""https://medium.com/@NPCollapse/replicating-gpt2-1-5b-86454a7f26af"">say</a>:</p><blockquote>I really can’t express how grateful I am to Google and the TFRC team for their support in enabling this. They were incredibly gracious and open to allowing me access, without requiring any kind of rigorous, formal qualifications, applications or similar. I can really only hope they are happy with what I’ve made of what they gave me.</blockquote><blockquote>...I estimate I spent around 200 hours working on this project.... I ended up spending around 600–800€ on cloud resources for creating the dataset, testing the code and running the experiments</blockquote><p>That said, it turned out that the copy <a href=""https://medium.com/@NPCollapse/addendum-evaluation-of-my-model-e6734b51a830?postPublishedType=initial"">did not match up in skill level</a>, and is weaker even than nerfed model OpenAI released. The person who built it says (1) they think they know how to fix it and (2) releasing it as-is may still be a helpful &quot;shortcut&quot; for others interested in building a GPT-2-level system; I don&#x27;t have the technical knowledge to assess these claims, and am interested to hear from others who do.</p><p>During the period where people didn&#x27;t know that the attempted copy was not successful, the person who made the copy wrote a long and interesting post <a href=""https://medium.com/@NPCollapse/gpt2-counting-consciousness-and-the-curious-hacker-323c6639a3a8"">explaining their decision to release the copy</a> (with multiple links to LW posts). It discussed reasons why this specific technology may cause us to better grapple with misinformation on the internet that we hear. The author is someone who had a strong object level disagreement with the policy people at OpenAI, and had thought pretty carefully about it. However, it opened thus:</p><blockquote><em>Disclaimer: I would like it to be made very clear that I am absolutely 100% open to the idea that I am wrong about anything in this post. I don’t only accept but explicitly request arguments that could convince me I am wrong on any of these issues. If you think I am wrong about anything here, and have an argument that might convince me,</em> <em><strong>please</strong></em> <em>get in touch and present your argument. I am happy to say “oops” and retract any opinions presented here and change my course of action.</em></blockquote><blockquote><em>As the saying goes: “When the facts change, I change my mind. What do you do?”</em></blockquote><blockquote><em>TL;DR: I’m a student that replicated OpenAI’s GPT2–1.5B. I plan on releasing it on the 1st of July. Before criticizing my decision to do so, please read my arguments below. If you still think I’m wrong, contact me on Twitter @NPCollapse or by email (thecurioushacker@outlook.com) and convince me. For code and technical details, see</em> <em><a href=""https://medium.com/@NPCollapse/replicating-gpt2-1-5b-86454a7f26af"">this post</a>.</em></blockquote><p>And they later said</p><blockquote>[B]e assured, I read every single comment, email and message I received, even if I wasn’t able to respond to all of them.</blockquote><p>On reading the initial I was genuinely delighted to see such pro-social and cooperative behaviour from the person who believed OpenAI was wrong. They considered  unilaterally overturning OpenAI&#x27;s decision but instead chose to spend 11,000 words explaining their views and a month reading others&#x27; comments and talking to people. This, I thought, is how one avoids falling prey to Bostrom&#x27;s <a href=""https://nickbostrom.com/papers/unilateralist.pdf"">unilateralist curse</a>.</p><p>Their next post <a href=""https://medium.com/@NPCollapse/the-hacker-learns-to-trust-62f3c1490f51"">The Hacker Learns to Trust</a> was released 6 days later, where they decided not to release the model. Note that they did not substantially change their opinions on the object level decision.</p><blockquote>I was presented with many arguments that have made me reevaluate and weaken my beliefs in some of the arguments I presented in my last essay. There were also many, maybe even a majority of, people in full support of me. Overall I still stand by most of what I said.</blockquote><blockquote>...I got to talk to Jack Clark, Alec Radford and Jeff Wu from OpenAI. We had a nice hour long discussion, where I explained where I was coming from, and they helped me to refine my beliefs. They didn’t come in accusing me in any way, they were very clear in saying they wanted to help me gain more important insight into the wider situation. For this open and respectful attitude I will always be grateful. Large entities like OpenAI often seem like behemoths to outsiders, but it was during this chat that it really hit me that they were people just like me, and curious hackers to boot as well.</blockquote><blockquote>I quickly began to understand nuances of the situation I wasn’t aware of. OpenAI had a lot more internal discussion than their <a href=""https://openai.com/blog/better-language-models/"">blog post</a> made it seem. And I found this reassuring. Jack in particular also gave me a lot of valuable information about the possible dangers of the model, and a bit of insight into the workings of governments and intelligence agencies.</blockquote><blockquote>After our discussion, I had a lot to think about. But I still wasn’t really convinced to not release.</blockquote><p>They then talked with Buck from MIRI (author of <a href=""https://www.lesswrong.com/posts/4QemtxDFaGXyGSrGD/other-people-are-wrong-vs-i-am-right"">this</a> great post). Talking with Buck lead them to their new view.</p><blockquote>[T]his isn’t just about GPT2. What matters is that at some point in the future, someone <em>will</em> create something truly dangerous and there need to be commonly accepted safety norms <em>before</em> that happens.</blockquote><br/><blockquote>We tend to live in an ever accelerating world. Both the industrial and academic R&amp;D cycles have grown only faster over the decades. Everyone wants “the next big thing” as fast as possible. And with the way our culture is now, it can be hard to resist the pressures to adapt to this accelerating pace. Your career can depend on being the first to publish a result, as can your market share.</blockquote><blockquote>We as a community and society need to combat this trend, and create a healthy cultural environment that allows researchers to <em>take their time</em>. They shouldn’t have to fear repercussions or ridicule for delaying release. Postponing a release because of added evaluation should be the norm rather than the exception. We need to make it commonly accepted that we as a community respect others’ safety concerns and don’t penalize them for having such concerns, <em>even if they ultimately turn out to be wrong</em>. If we don’t do this, it will be a race to the bottom in terms of safety precautions.</blockquote><br/><blockquote>We as a community of researchers and humans need to trust one another and respect when one of us has safety concerns. We need to extend understanding and offer help, rather than get caught in a race to the bottom. And this isn’t easy, because we’re curious hackers. Doing cool things fast is what we do.</blockquote><p>The person also came to believe that the AI (and AI safety) community was much more helpful and cooperative than they&#x27;d expected.</p><blockquote>The people at OpenAI and the wider AI community have been incredibly helpful, open and thoughtful in their responses to me. I owe to them everything I have learned. OpenAI reached out to me almost immediately to talk and they were nothing but respectful and understanding. The same applies to Buck Shlegeris from MIRI and many other thoughtful and open people, and I am truly thankful for their help.</blockquote><blockquote>I expected a hostile world of skepticism and competition, and there was some of that to be sure. But overall, the AI community was open in ways I did not anticipate. In my mind, I couldn’t imagine people from OpenAI, or MIRI, or anywhere else actually wanting to talk to me. But I found that was wrong.</blockquote><blockquote>So this is the first lesson: The world of AI is full of smart, good natured and open people that I shouldn’t be afraid of, and neither should you.</blockquote><p>Overall, the copy turned out not to be strong enough to change the ability for malicious actors to automate spam/clickbait, but I am pretty happy with the public dialogue and process that occurred. It was a process whereby, in a genuinely dangerous situation, the AI world would not fall prey to Bostrom&#x27;s <a href=""https://nickbostrom.com/papers/unilateralist.pdf"">unilateralist&#x27;s curse</a>. It&#x27;s encouraging to see that process starting to happen in the field of ML.</p><p>I&#x27;m interested to know if anyone has any different takes, info to add, or broader thoughts on information-security norms.</p><p><em>Edited: Thanks to 9eB1 for pointing out how nerfed the copy was, I&#x27;ve edited the post to reflect that.</em></p>",Benito,benito,Ben Pace,
x5BFqov8rt2duhMvH,"""The Bitter Lesson"", an article about compute vs human knowledge in AI",the-bitter-lesson-an-article-about-compute-vs-human,https://www.lesswrong.com/posts/x5BFqov8rt2duhMvH/the-bitter-lesson-an-article-about-compute-vs-human,2019-06-21T17:24:50.825Z,52,23,14,False,False,http://www.incompleteideas.net/IncIdeas/BitterLesson.html,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p>The Bitter Lesson
Rich Sutton</p>
<p>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. The ultimate reason for this is Moore's law, or rather its generalization of continued exponentially falling cost per unit of computation. Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available. Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. These two need not run counter to each other, but in practice they tend to. Time spent on one is time not spent on the other. There are psychological commitments to investment in one approach or the other. And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation.  There were many examples of AI researchers' belated learning of this bitter lesson, and it is instructive to review some of the most prominent.</p>
<p>In computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer-chess researchers who had pursued methods that leveraged human understanding of the special structure of chess. When a simpler, search-based approach with special hardware and software proved vastly more effective, these human-knowledge-based chess researchers were not good losers. They said that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mstyle""><span class=""mjx-mrow""><span class=""mjx-mo""></span></span></span></span></span></span></span>brute force"" search may have won this time, but it was not a general strategy, and anyway it was not how people played chess. These researchers wanted methods based on human input to win and were disappointed when they did not.</p>
<p>A similar pattern of research progress was seen in computer Go, only delayed by a further 20 years. Enormous initial efforts went into avoiding search by taking advantage of human knowledge, or of the special features of the game, but all those efforts proved irrelevant, or worse, once search was applied effectively at scale. Also important was the use of learning by self play to learn a value function (as it was in many other games and even in chess, although learning did not play a big role in the 1997 program that first beat a world champion). Learning by self play, and learning in general, is like search in that it enables massive computation to be brought to bear. Search and learning are the two most important classes of techniques for utilizing massive amounts of computation in AI research. In computer Go, as in computer chess, researchers' initial effort was directed towards utilizing human understanding (so that less search was needed) and only much later was much greater success had by embracing search and learning.</p>
<p>In speech recognition, there was an early competition, sponsored by DARPA, in the 1970s. Entrants included a host of special methods that took advantage of human knowledge---knowledge of words, of phonemes, of the human vocal tract, etc. On the other side were newer methods that were more statistical in nature and did much more computation, based on hidden Markov models (HMMs). Again, the statistical methods won out over the human-knowledge-based methods. This led to a major change in all of natural language processing, gradually over decades, where statistics and computation came to dominate the field. The recent rise of deep learning in speech recognition is the most recent step in this consistent direction. Deep learning methods rely even less on human knowledge, and use even more computation, together with learning on huge training sets, to produce dramatically better speech recognition systems. As in the games, researchers always tried to make systems that worked the way the researchers thought their own minds worked---they tried to put that knowledge in their systems---but it proved ultimately counterproductive, and a colossal waste of researcher's time, when, through Moore's law, massive computation became available and a means was found to put it to good use.</p>
<p>In computer vision, there has been a similar pattern. Early methods conceived of vision as searching for edges, or generalized cylinders, or in terms of SIFT features. But today all this is discarded. Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better.</p>
<p>This is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.</p>
<p>One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.</p>
<p>The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done.</p>
</body></html>",lahwran,the-gears-to-ascension,the gears to ascension,
aLNrRLRsQz64oBdmF,"Decisions are hard, words feel easier",decisions-are-hard-words-feel-easier-1,https://www.lesswrong.com/posts/aLNrRLRsQz64oBdmF/decisions-are-hard-words-feel-easier-1,2019-06-21T16:17:22.366Z,9,6,4,False,False,,"<html><head></head><body><p>Categorization often serves the purpose of making decisions. Recall, making decisions is hard. Sometimes it's easier (but not necessarily more effective) to think about if a word applies, rather than to think directly about how a decision should be made. This happens on the in individuals and in groups.</p>
<h3>Groups</h3>
<p>Explicit note: When a word appears in quotation marks, I'm referring to the English word itself. When it appears unquoted, I'm referring to the concept it invokes.</p>
<p>Read this carefully. This post makes claims about ""racism"", not many about racism. If you disagree, please use your best inference powers to engage with what I mean, lest I become a ""racist"".</p>
<p>""Racism"". Not racism, but ""racism"".</p>
<p>What do I know about someone who is racist? Depends on what I meant. What do I know about someone who is ""racist""? I know that:</p>
<ul>
<li>If I associate with them I'm in danger of being attacked.</li>
<li>Other people consider them unsavory.</li>
</ul>
<p>The move to notice is that regardless of whatever racism actually is, there's a decently stable way that the word ""racism"" works in America. This is the lawyer. If the word sticks, game moves are allowed to be made. I'm allowed to dox someone on the internet for being ""racist"". I'm not allowed to dox someone on the internet who isn't ""racist"". But am I allowed to dox someone o the internet who is racist? Hard to say, unless I know they're ""racist"". Until that battle has been fought, I don't know what's going to happen.</p>
<p>My argument is not that you shouldn't use the word racism like this, but to make clear the actual cognition and social inferences that are happening. Defining racism is not a ""just for fun"" intellectual exercise. Through inter-subjective magic, the word ""racism"" has power. Maybe it should, maybe it shouldn't. Either way, it does.</p>
<p>Oops, we've got a situation on our hands that incentives being a lawyer. All parties know that if there is loud consensus on X being ""racist"", a certain outcome will happen. All parties want certain outcomes. Arguing about if the word ""racist"" applies becomes such a direct path to getting what one wants, why go a different route?</p>
<p>Being ""racist"" means that a lot of people agreed to use the word ""racist"" to talk about you. Being racist means [something].</p>
<p>Now, the ""if... then..."" decision rules are no longer applied to the neural categories. They are applied to things that the social fabric allows to be called a WORD.</p>
<p>You can argue if the CONCEPT of racism applies (what's active in your mind when you think of that? What decision rules? What memories?) Separately, you can argue if the WORD ""racism"" applies.</p>
<h3>Individual</h3>
<p>In some fields, one draws boundaries to convey a lot of meaning quickly and more easily make intellectual process. But remember, it's natural for neural categories to have corresponding ""if then"" decision rules. In my pursuit to figure <em>out once and for what a friend <strong>really</strong> means</em>, I might be a nerd who hates putting effort into figuring out social stuff and wants a straightforward decision procedure that will always lead me to outcomes I like.</p>
<p>Hah.</p>
<p>That is the goal. But also, it <em>feels</em> like friendship is just a thing that's out there and it must mean something. This is the <strong>student</strong> again. The student wants to always get the test right. It's easier to talk about what the right word is rather than to talk about what gets me what I want.</p>
</body></html>",Hazard,hazard,Hazard,
7Ze7YaFNfNw8T2hmW,Splitting Concepts,splitting-concepts,https://www.lesswrong.com/posts/7Ze7YaFNfNw8T2hmW/splitting-concepts,2019-06-21T16:03:11.177Z,8,3,0,False,False,,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p>For anyone who's gone through some form of primary education, I'd expect numbers to be a tight neural category. Numbers feel like things, they are pretty much the same, and there's little ambiguity over if something is a number or not.</p>
<p>So it might come to one's surprise when they learn about the ordinal number omega which has the interesting property:
<span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display""><span class=""mjx-math"" style=""width: 100%;"" aria-label=""
\omega < \omega + 1\\
|\omega| = |\omega + 1|
""><span class=""mjx-mrow"" style=""width: 100%;"" aria-hidden=""true""><span class=""mjx-stack"" style=""width: 100%; vertical-align: -1.324em;""><span class=""mjx-block"" style=""text-align: center;""><span class=""mjx-box""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">ω</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.372em;"">&lt;</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">ω</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span><span class=""mjx-block"" style=""text-align: center; padding-top: 0.442em;""><span class=""mjx-box""><span class=""mjx-mspace"" style=""width: 0px; height: 0px;""></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">ω</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-texatom MJXc-space3""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">ω</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span></span></span></span></span></span></span></span>
Where ""&lt;"" means ""less than"" and ""|x|"" means ""the size of the number x""</p>
<p>You're telling me there's a number that's less than another number, but still the same size as another number? Bullshit, quit pulling my leg.</p>
<p>A mathematician will quickly tell you that I got the meaning the above symbols wrong. ""&lt;"" talks about <em>order</em> and ""|x|"" talks about <em>cardinality</em>. Ordinality refers to somethings position in a list relative to other elements.  You can come before another element or after another element.</p>
<p>Cardinality refers to being ""bigger"" or ""smaller"".</p>
<p>Here's another fun fact:
<span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""
\omega + 1 \neq 1 + \omega
""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">ω</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">≠</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">ω</span></span></span></span></span></span>
Okay, now you're just shitting me. This might as well not be a number!</p>
<p>This annoy someone for a few reasons:</p>
<ul>
<li>In the numbers you're used to, ""comes before"" and ""is smaller than"" always meant the same thing.</li>
<li>You don't even remember the phrase ""communative"" and the second piece of math just looks fake.</li>
<li>In general, though these squiggly things look like the numbers you're used to, they sure don't act like them.</li>
</ul>
<p>You are being told to split a concept. This is always a bit difficult, and a source of mental resistance. You have to literally rewire your brain to prevent certain inferences being made. Rewiring = work = experience of effort.</p>
<p>If numbers never felt like yours to begin with, this will be easier. If you don't care about numbers that much you may resent your teacher for ""adding"" another cognitive stumbling block, but you will proceed.</p>
<p>If you feel like you ""know what a number is"", this will be harder. Even if you learn the math of ordinals, you will wail and gnash your teeth every time someone refers to them as numbers. This is the mistake of the <strong>student</strong>. Fine, naturals and ordinals are different, but we can't use the word number for both of them, because what if I was asked this question on a test and wasn't able to give the <strong>right answer</strong>?!?</p>
<p>Mathematicians redefine words all the time. In this way, they are like <strong>poets</strong>. They can get away with this because this is what they do. In school you will be told that if you intuition on what a mathematical word means clashes with how it is defined, well then it sucks to be you. Yes, the word still triggers an old neural category and you make incorrect inferences. Learn to swim or drop out. This attitude may or may not be the most helpful.</p>
</body></html>",Hazard,hazard,Hazard,
HE5DL6XeomYxFab74,Modeling AGI Safety Frameworks with Causal Influence Diagrams,modeling-agi-safety-frameworks-with-causal-influence-1,https://www.lesswrong.com/posts/HE5DL6XeomYxFab74/modeling-agi-safety-frameworks-with-causal-influence-1,2019-06-21T12:50:08.233Z,43,13,6,False,False,https://arxiv.org/abs/1906.08663,"<p>We have written a paper that represents various frameworks for designing safe AGI  (e.g., RL with reward modeling, CIRL, debate, etc.) as Causal Influence Diagrams (CIDs), to help us compare frameworks and better understand the corresponding agent incentives.</p><p>We would love to get comments, especially on</p><ul><li>Are the depicted frameworks represented accurately?</li><li>Is the CID representation helpful?</li><li>Frameworks we did not include that would be useful to model this way?</li></ul><p>The paper&#x27;s abstract:</p><p>Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks.</p>",ramana-kumar,ramana-kumar,Ramana Kumar,
wiLzN4jdAsW2TQtfS,Is your uncertainty resolvable?,is-your-uncertainty-resolvable-1,https://www.lesswrong.com/posts/wiLzN4jdAsW2TQtfS/is-your-uncertainty-resolvable-1,2019-06-21T07:32:00.819Z,30,17,15,False,False,,"<p>I was chatting with Andrew Critch about the idea of <a href=""http://lesswrong.com/posts/SDELNzyboMpZpDwSg/fb-discord-style-reacts"">Reacts on LessWrong</a>. </p><p>Specifically, the part where I thought there are particular epistemic states that <em>don’t have words yet</em>, but should. And that a function of LessWrong might be to make various possible epistemic states more salient as options. You might have reacts for “approve/disapprove” and “agree/disagree”... but you might also want reactions that let you quickly and effortless express “this isn’t exactly false or bad but it’s subtly making this discussion worse.”</p><p>Fictionalized, Paraphrased Critch said “hmm, this reminds me of some particular epistemic states I recently noticed that don’t have names.”</p><p>“Go on”, said I.</p><p>“So, you know the feeling of being uncertain? And how it feels different to be 60% sure of something, vs 90%?”</p><p>“Sure.”</p><p>“Okay. So here’s two other states you might be in: </p><ul><li><strong>75% sure that you’ll eventually be 99% sure,</strong> </li><li><strong>80% sure that you’ll eventually be 90% sure.</strong></li></ul><p>He let me process those numbers for a moment.</p><p>...</p><p>Then he continued: &quot;Okay, now imagine you’re thinking about a particular AI system you’re designing, which might or might not be alignable.</p><p>“If you’re feeling 75% sure that you’ll eventually be 99% sure that that AI is safe, this means you think that <em>eventually </em>you’ll have a clear understanding of the AI, such that you feel confident turning it on without destroying humanity. Moreover you expect to be able to convince <em>other people</em> that it’s safe to turn it on without destroying humanity.</p><p>“Whereas if you’re 80% sure that eventually you’ll be 90% sure that it’ll be safe, <em>even in the future state</em> where you’re better informed and more optimistic, you might still not actually be confident enough to turn it on. And even if for some reason you are, other people might disagree about whether you should turn it on.</p><p>“I’ve noticed people tracking how certain they are of something, without paying attention to whether their uncertainty is <em>possible to resolve</em>. And this has important ramifications for what kind of plans they can make. Some plans <em>require near-certainty. </em>Especially many plans that require group coordination.</p><p>“Makes sense”, said I. &quot;Can I write this up as a blogpost?&quot;</p><hr class=""dividerBlock""/><p>I’m not quite sure about the best name here, but this seems like a useful concept to have a handle for. Something like “unresolvable uncertainty?”</p>",Raemon,raemon,Raemon,
g2fQuAD6qpBERucwu,Should rationality be a movement?,should-rationality-be-a-movement,https://www.lesswrong.com/posts/g2fQuAD6qpBERucwu/should-rationality-be-a-movement,2019-06-20T23:09:10.555Z,48,22,13,False,False,,"<p><em>This post is a quick write-up of a discussion that I recently had with two members of the rationality community. For reasons of simplicity I&#x27;ll present them as holding a single viewpoint that is a merger of both their arguments. All parties seemed to be in agreement about the long-term future being an overwhelming consideration, so apologies in advance to anyone with a different opinion.</em></p><p>In a recent discussion, I noted that the rationality community didn&#x27;t have an organisation like CEA engaging in movement building and suggested this might at least partially why EA seemed to be much more successful than the rationality community. While the rationality community has founded the MIRI and CFAR, I pointed out that there were now so many EA-aligned organisations it&#x27;s impossible to keep track. EA runs conferences where hundreds of people attend, with more on the waitlist, while LW doesn&#x27;t even have a conference in it&#x27;s hometown. EA has groups at the most prominent universities, while LW has almost none. Further, EA now has it&#x27;s own university department at Oxford and the support of OpenPhil, a multi-billion dollar organisation. Admittedly, Scott Alexander grew out of the rationality community, but EA has 80,000 hours. I also noted that EA had created a large number of people who wanted to become AI safety researchers; indeed at some EA conferences it felt like half the people there were interested in pursuing that path.</p><p>Based on this comparison, EA seems to have been far more successful. However, the other two suggested that appearances could be misleading and that it therefore wasn&#x27;t so obvious that rationality should be a movement at all. In particular, they argued that most of the progress made so far in terms of AI safety didn&#x27;t come from anything &quot;mass-movement-y&quot;.</p><p>For example, they claimed:</p><ul><li>Slatestarcodex has been given enthusiastic praise by many leading intellectuals who may go on to influence how others think. This is the work of just one man who has intentionally tried to limit the growth of the community around it</li><li>Eliezer Yudkowsky was more influential than EA on Nick Bostrom&#x27;s Superintelligence. This book seems to have played a large role in convincing more academic types to take this viewpoint more seriously. Neither Yudkowsky&#x27;s work on Less Wrong nor Superintelligence are designed for a casual audience.</li><li>They argued that CFAR played a crucial role in developing an individual who helped found the Future of Life Institute. This institute ran the Asilomar Conference which kicked off a wave of AI safety research.</li><li>They claimed that even though 80,000 Hours had access to a large pool of EAs, they hadn&#x27;t provided any researchers to OpenPhil, only people filling other roles like operations. In contrast, they argued that CFAR mentors and alumni were around 50% of OpenPhil&#x27;s recent hires and likely deserved some level of credit for this.</li></ul><p>Part of their argument was that quality is more important than quantity for research problems like safe AI. In particular, they asked whether a small team of the most elite researchers was more likely to succeed in revolutionising science or building a nuclear bomb than a much larger group of science enthusiasts.</p><p>My (partially articulated) position was that it was too early to expect too much. I argued that even though most EAs interested in AI were just enthusiasts, some percentage of this very large number of EAs would go on to become to be successful researchers. Further, I argued that we should expect this impact to be significantly positive unless there was a good reason to believe that a large proportion of EAs would act in strongly net-negative ways.</p><p>The counterargument given was that I had underestimated the difficulty of being able to usefully contribute to AI safety research and that the percentage who could usefully contribute would be much smaller than I anticipated. If this were the case, then engaging in more targeted outreach would be more useful than building up a mass movement.</p><p>I argued that more EAs had a chance of becoming highly skilled researchers than they thought. I said that this was not just because EAs tended to be reasonably intelligent; but also because they tended to be much better than average at engaging in good-faith discussion, be more exposed to content around strategy/prioritisation and also benefit from network effects. </p><p>The first part of their response was to argue that by being a movement EA had ended up compromising on their commitment to truth, as follows:</p><p>i) EA&#x27;s focus on having an impact entails growing the movement which entails protecting the reputation of EA and attempting to gain social status </p><p>ii) This causes EA to prioritise building relationships with high-status people, such as offering them major speaking slots at EA conferences, even when they aren&#x27;t particularly rigorous thinker. </p><p>iii) It also causes EA to want to dissociate from low-status people who produce ideas worth paying attention to. In particular, they argued that this had a chilling effect on EA and caused people to speak in a way that was much more guarded.</p><p>iv) By acquiring resources and status EA had drawn the attention of people who were interested in these resources, instead of the mission of EA. These people would damage the epistemic norms by attempting to shift the outcomes of truth-finding processes towards outcomes that would benefit them.</p><p>They then argued that despite the reasons I pointed out for believing that EAs could be successful AI safety researchers, that most were lacking a crucial component which was a deep commitment to attempting to fix the issue as opposed to merely seeming like they are attempting to fix the issue. They believed that EA wasn&#x27;t the right kind of environment for developing people like this and that without this attribute most work people engaged in would end up being essentially pointless.</p><p>Originally I listed another point here, but I&#x27;ve removed it since it wasn&#x27;t relevant to this particular debate, but instead a second simultaneous debate about whether CEA was an effective organisation. I believe that the discussion of this topic ended here. I hope that I have represented the position of the people I was talking to fairly and I apologise in advance if I&#x27;ve made any mistakes.</p>",Chris_Leong,chris_leong,Chris_Leong,
JYdPbGS9mpJn3SAyA,Let Values Drift,let-values-drift-1,https://www.lesswrong.com/posts/JYdPbGS9mpJn3SAyA/let-values-drift-1,2019-06-20T20:45:36.618Z,4,15,19,False,False,,"<p>I <a href=""https://www.lesswrong.com/posts/fkLYhTQteAu5SinAc/corrigibility"">occasionally</a> <a href=""https://www.lesswrong.com/posts/vgFvnr7FefZ3s3tHp/mahatma-armstrong-ceved-to-death"">run</a> <a href=""https://www.lesswrong.com/posts/S9B3t6G6xfKL4ER9e/schelling-fences-versus-marginal-thinking"">across</a> <a href=""https://forum.effectivealtruism.org/posts/mZWFEFpyDs3R6hD3r/empirical-data-on-value-drift"">lines</a> <a href=""http://effectivethesis.com/value-drift-effective-altruism/"">of</a> <a href=""https://forum.effectivealtruism.org/posts/eRo5A7scsxdArxMCt/concrete-ways-to-reduce-risks-of-value-drift"">reasoning</a> <a href=""https://forum.effectivealtruism.org/posts/sF7Loj5joocgykSZ7/giving-later-in-life-giving-more"">that</a> depend on or favor the position that <a href=""https://www.overcomingbias.com/2018/02/on-value-drift.html"">value drift should be avoided</a>.</p><p>I find odd the idea of value drift, let alone the idea that value drift is bad. My intuition is that value drift is good if anything since it represents an update of one&#x27;s values based on new evidence and greater time to compute reflective equilibrium. But rather than arguing intuition, let&#x27;s explore value drift a bit before we come to any stronger conclusions.</p><p><em>(Fair warning, this is going to get into some deep philosophical territory, be pretty unapologetic about it, and assume you are reading carefully enough to notice what I say and not what you think I said. I&#x27;m still working some of these ideas out myself, so I don&#x27;t have the fluency to provide a more accessible explanation right now. I also take some pretty big inferential jumps at times that you may not be on board with as of yet, so the later parts might feel like unjustified reasoning. I don&#x27;t think that&#x27;s the case, but you&#x27;ll have to poke at me to help me figure out how to fill in those gaps.</em></p><p><em>In spite of all those apologies, there are some key insights here, and I&#x27;m unlikely to get clearer unless I am first more opaque, so please bear with me if you please, especially if you are interested in value as it relates to AI alignment.)</em></p><h1>Whence drifting values?</h1><p>The metaphor of drifting values is that your values are initially one place and then gradually relocate to another, like flotsam. The waves of fortune, chance, and intention combine to determine where they end up on the seas of change. In this metaphor, values are discrete, identifiable things. Linguistically, they are nouns.</p><p>When we talk of values as nouns, we are talking about the values that people have, express, find, embrace, and so on. For example, a person might say that altruism is one of their values. But what would it mean to &quot;have&quot; altruism as a value or for it to be one of one&#x27;s values? What is the thing possessed or of one in this case? Can you grab altruism and hold onto it, or find it in the mind cleanly separated from other thoughts? As best I can tell, no, unless contrary to evidence and parsimony something like Platonic idealism proves consistent with reality, so it seems a <a href=""https://www.lesswrong.com/posts/YvWfbLunzhiFm77GG/words-aren-t-type-safe"">type error</a> to say you possess altruism or any other value since values are not things but habituations or patterns of action (more on this in the next section). It&#x27;s only because we use the <a href=""https://en.wikipedia.org/wiki/Metaphors_We_Live_By"">metaphor</a> of possession to mean something like habitual valuing that it can seem as if these patterns over our actions are things in their own right.</p><p>So what, you may think, it&#x27;s just a linguistic convention and doesn&#x27;t change what&#x27;s really going on. That&#x27;s both wrong and right. Yes, it&#x27;s a linguistic convention and yes you get on with valuing all the same no matter how you talk about it, but linguistic conventions <a href=""https://slatestarcodex.com/2014/11/21/the-categories-were-made-for-man-not-man-for-the-categories/"">shape our thoughts</a> and <a href=""https://mapandterritory.org/the-developmental-role-of-play-443549cfed3b"">limit our ability</a> to express ourselves with the frames they provide. In the worst case, as I suspect is often happening when people reckon about value drift, we can focus so much on the convention that we forget what&#x27;s really going on and reason only about the abstraction, viz. <a href=""https://www.lesswrong.com/posts/3up8XBeGGHf77sNR4/the-map-has-gears-they-don-t-always-turn"">mistake the map</a> for <a href=""https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation"">the territory</a>. And since we&#x27;ve just seen that the value-as-thing <a href=""https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/"">abstraction is leaky</a> because it implies the ability to possess that which cannot be, it can lead us astray by allowing us to operate from a false assumption about how the world works, expecting it to function one way when it actually operates another.</p><p>To my listening most talk about value drift is at least partially if not wholly confused by this mistaking of values for things, and mistaking them <a href=""https://www.lesswrong.com/posts/WTS4ZbEwvKrcrnaaN/your-evolved-intuitions"">specifically</a> for <a href=""https://plato.stanford.edu/entries/natural-kinds/#Ess"">essences</a>. But let&#x27;s suppose you don&#x27;t make this mistake; is value drift still sensible?</p><p>I think we can rehabilitate it, but to do that we&#x27;ll need a clearer understanding of &quot;habitual valuing&quot; and &quot;patterns of action&quot;.</p><h1>Valuing valuing</h1><p>If we tear away the idea that we might possess values, we are left with the act of valuing, and to value something is ultimately to judge it or assess its worth. While I can&#x27;t hope to fit all <a href=""https://mapandterritory.org/introduction-to-noematology-fac7ae7d805d"">my philosophy</a> into this paragraph, I consider valuing, judging, or assessing to be one of the fundamental operations of &quot;conscious&quot; things, it being the <a href=""https://mapandterritory.org/form-and-feedback-in-phenomenology-d44f4e5c72b3"">key input that powers the feedback loops</a> that differentiate the &quot;living&quot; from the &quot;dead&quot;. For <a href=""https://en.wikipedia.org/wiki/Prat%C4%ABtyasamutp%C4%81da"">historical reasons</a> we might call this feeling or sensation, and if you like <a href=""https://en.wikipedia.org/wiki/Control_theory"">control theory</a> &quot;sensing&quot; seems appropriate since in a control system it is the sensor that determines and sends the signal to the controller after it senses the system. <a href=""https://slatestarcodex.com/2017/03/06/book-review-behavior-the-control-of-perception/"">Promising</a> <a href=""https://slatestarcodex.com/2018/03/04/god-help-us-lets-try-to-understand-friston-on-free-energy/"">modern</a> <a href=""https://slatestarcodex.com/2019/03/20/translating-predictive-coding-into-perceptual-control/"">theories</a> suggest control theory is useful for modeling the human mind as a hierarchy of control systems that minimize prediction error while also maintaining homeostasis, and this <a href=""https://www.mindandlife.org/remembrance-things-come-predictive-nature-mind-contemplative-practices/"">matches</a> with one of the most detailed and longest used theories of human psychology, so I feel justified in saying that the key, primitive action happening when we value something is that we sense or judge it to be good, neutral, or bad (or, if you prefer, more, same, or less).</p><p>We could get hung up on good, neutral, and bad, but let&#x27;s just understand them for now as relative terms in the sense of the brain as control system, where &quot;good&quot; signals better prediction or otherwise moving towards a set point and &quot;bad&quot; signals worse prediction or moving away from a set point. Then in this model we could say that <strong>to value something is to sense it and send a signal out to the rest of the brain that it is good</strong>. Thus to &quot;have a value&quot; is to observe a pattern of action that senses that pattern to be good. To return to the example of valuing altruism, when a person who values altruism acts in a way that pattern matches to altruism (maybe &quot;benefits others&quot; or something similar), the brain senses this pattern to be good and feeds that signal back into itself further habituating actions that match the altruism pattern. It is this habituation that we are pointing to when we say we &quot;have&quot; a value.</p><p><em>Aside: How any individual comes to sense any particular pattern, like altruism, to be good, neutral, or bad is an interesting topic in and of itself, but we don&#x27;t need that particular <a href=""https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding"">gear</a> to continue discussing value drift, so this is where the model bottoms out for this post.</em></p><p>We can now understand value drift to mean changes in habituations or patterns of action over time. I realize some of my readers will throw their hands up at this point and say &quot;why did we have to go through all that just to get back to where we started?!?&quot;, but the point was to unpack value drift so we can understand it as it is, not as we think it is. And as will become clear in the following analysis, that unpacking is key to understanding why value drift seems an odd thing to worry about to me.</p><h1>Values adrift</h1><p>My explanation of valuing implies that values-as-things are after-the-fact reifications drawn from the observation of accumulated effects of individual actions, and as such values cannot themselves directly drift because they are downstream of where change happens. The changes that will befall these reifications that we call &quot;values&quot; happen moment to moment, action to action, where each particular action taken will only later be aggregated to form a pattern that can be expressed as a value, and even then that value exists only by virtue of ontology because it is an inference from observation. Thus when values &quot;drift&quot; it&#x27;s about as meaningful as saying the drawings of continents &quot;drift&quot; over geological time: it&#x27;s sort of true, but only meaningful so long as understanding remains firmly grounded in the phenomena being pointed to, and unlike with maps of geography maps of mind are more easily confused for mind itself.</p><p>What instead drifts or changes are actions, although saying they drift or change is wrought because it supposes some stable viewpoint from which to observe the change, yet actions, via the preferences that cause us to choose any particular action over all others, are <a href=""https://www.lesswrong.com/posts/Bxw4iMS33Dc2iETpe/conditional-revealed-preference"">continuously</a> <a href=""https://www.lesswrong.com/posts/uHb2LDW3LGhBMyq74/preference-conditional-on-circumstances-and-past-preference"">dependent</a> on the <a href=""https://www.lesswrong.com/posts/ic8yoGBMYLtaJkbxZ/conditional-meta-preferences"">conditions</a> in which they arise because what we sense (value, judge, assess) is conditional on the entire context in which we do the sensing. So it is only outside the moment, whether before or after, that we judge change, and so change is also ontologically bound such that we can find no change if we look without ontology. In this sense change and drift in actions and patterns of action exist but are not real: they are in the map, but not the base territory.</p><p>Does that matter? I think it does, because we can be confused about ontology, confusion can only arise via ontology, and sensing/valuing is very near the root of ontology generation, so our understanding of what it means to value is mostly contaminated by valuing itself! Certainly by the time we put words to our thoughts we have already sensed and passed judgement on many phenomena, and that means that when we talk about value drift we&#x27;re talking from a motivated stance where valuation heavily shaped our perspectives, so I find it not at all odd that valuing would find a way to make itself and its products stable points within concept space such that it would feel natural to worry that they might drift, and that drifting and change in values would evaporate without sensing feedback loops to prop them up!</p><p>This is not to anthropomorphize valuing, but to point out the way it is prior to and self-incentivized to magnify its existence; it&#x27;s like a subagent carrying out its own goals regardless of yours, and it&#x27;s so good at it that it&#x27;s shaped your goals before you even knew you had them. And when we strip away everything posterior to valuing we find no mechanism by which value can change because we can&#x27;t even conceptualize change at that point, so we are left with valuing as a pure, momentary act that cannot drift or change because it has no frame to drift or change within. So when I say value drift is odd to me this is what I mean: it&#x27;s exists as a function of valuing, not of valuing itself, and we can find no place where value change occurs that is not tainted by the evaluations of sensing.</p><p><em>(Careful readers will note this is analogous to the <a href=""https://www.lesswrong.com/posts/5LP6Jc8ztwcyb296X/outline-of-metarationality-or-much-less-than-you-wanted-to"">epistemological problem</a> that necessitates a <a href=""https://www.iep.utm.edu/criterio/"">leap of faith</a> when knowledge is understood <a href=""https://en.wikipedia.org/wiki/Ontic"">ontologically</a>.)</em></p><p>Yikes! So what do we do?</p><h1>Steady on</h1><p>The questions that motivate this investigation are ones like &quot;how do we protect <a href=""https://www.effectivealtruism.org/articles/introduction-to-effective-altruism/"">effective altruists</a> (EAs) from value drift so that they remain altruistic later in life and don&#x27;t revert to the mean?&quot; and &quot;how do we <a href=""https://mapandterritory.org/formally-stating-the-ai-alignment-problem-fe7a6e3e5991"">align superintelligent AI with human values</a> such that they stay aligned with human values even as they think longer and more deeply than any human could?&quot;. Even if I lost you in the previous section—and I&#x27;m a little bit lost in my own reasoning if I&#x27;m totally honest—how can we cash out all this philosophy into information relevant to these questions?</p><p>In the case of drifting EAs, I say let them drift. They value EA because conditions in their lives caused them to value it, and if those conditions change so be it. Most people lack the agency to stay firm in the face of changing conditions, I think this is mostly a safety mechanism to protect them from overcommitting when they aren&#x27;t epistemically mature enough to know what they&#x27;re doing, and for every EA lost to this there will likely be another EA gained, so we don&#x27;t have to worry about it much other than to deal with churn effects on the least committed members of the movement. To do otherwise is to be inconsistent on respecting meta-preferences, assuming you think we should respect people&#x27;s meta-preferences, in this case specifically the meta-preference for autonomy of beliefs and actions. Just like you would probably find it troubling to find racists or fascists or some other <a href=""https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/"">outgroup</a> working on incentives to keep people racist or fascist in the face of evidence that they should change, you should find it troubling that we would seek to manipulate incentives such that people are more likely to continue to hold EA beliefs in the face of contrary evidence.</p><p>Most of this argument is aside my main point that value drift is a subtly motivated framing to keep values stable propagated by the very feedback processes that use sense signals as input with no prior manifestation to fall back on, but you might be able to see the deep veins of it running through. More relevant to this question directly are probably things like &quot;<a href=""https://www.lesswrong.com/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no"">Yes Requires the Possibility of No</a>&quot;, &quot;<a href=""https://slatestarcodex.com/2018/07/18/fundamental-value-differences-are-not-that-fundamental/"">Fundamental Value Differences are not that Fundamental</a>&quot;, &quot;<a href=""https://slatestarcodex.com/2014/06/07/archipelago-and-atomic-communitarianism/"">Archipelago</a>&quot;, and much about meta-consistency in ethics that&#x27;s not salient to me at this time.</p><p>On the question of AI alignment, this suggests concerns about value drift are at least partially about confusion on values and partially fear born of a desire for value self-preservation. That is, a preference to avoid value drift in superintelligent AIs may not be a principled stance, or may be principled but grounded in fear of change and nothing more. This is not to say we humans would be happy with any sense experiences, only that we are biased and anchored on our current sensing (valuing) when we think about how we might sense things other than we do now under other conditions. I realize this makes the alignment problem harder if you were hoping to train against current human values and then stick near them, and maybe that&#x27;s still a good plan because although it&#x27;s conservative and risks astronomical waste by denying us access to full optimization of valuing, that&#x27;s probably better than attempting and failing at a more direct approach that is less wasteful but maybe also ends up <a href=""https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/"">tiling the universe with smiley faces</a>. My concern is that if we take the more conservative approach, we might fail anyway because the value abstraction is leaky and we end up building agents that optimize for the wrong things, leaving gaps through which <a href=""https://nickbostrom.com/existential/risks.html"">x-risks</a> develop anyway.</p><p>(Unless it wasn&#x27;t clear, AI alignment is hard.)</p><p>If any of that left you more confused than when you started reading this, then good, mission accomplished. I continue to be confused about values myself, and this is part of a program of trying to see through them and become <a href=""https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/"">deconfused</a> on them, similar to the way I had to deconfuse myself on morality many years ago. Unfortunately not many people are deconfused on values (relatively more are deconfused on morals) so not much is written to guide me along. Look for the next post whenever I get more deconfused enough to have more to say.</p><p><em><a href=""https://mapandterritory.org/let-values-drift-7f8aa61d13ee"">Cross-posted to Map and Territory on Medium</a></em></p>",gworley,gordon-seidoh-worley,Gordon Seidoh Worley,
pE5LgmF9mJptvund9,"A case for strategy research: 
what it is and why we need more of it",a-case-for-strategy-research-what-it-is-and-why-we-need-more,https://www.lesswrong.com/posts/pE5LgmF9mJptvund9/a-case-for-strategy-research-what-it-is-and-why-we-need-more,2019-06-20T20:22:14.478Z,24,17,19,False,False,,"<p>Authors: Siebe Rozendal, Justin Shovelain, David Kristoffersson</p>
<p><em><a href=""https://forum.effectivealtruism.org/posts/oovy5XXdCL3TPwgLE/a-case-for-strategy-research-what-it-is-and-why-we-need-more"">Crossposted</a> to the Effective Altruism Forum</em></p>
<h1>Overview</h1>
<p>To achieve any ambitious goal, some strategic analysis is necessary. Effective altruism has ambitious goals and focuses heavily on doing research. To understand how to best allocate our time and resources, we need to clarify what our options in research are. In this article, we describe <em>strategy research</em> and relate it to values research, tactics research, informing research, and improvement research. We then apply the lens of strategy research to existential risk reduction, a major cause area of effective altruism. We propose a model in which the marginal value of a research type depends strongly on the maturity of the research field. Finally, we argue that strategy research should currently be given higher priority than other research in existential risk reduction because of the significant amount of strategic uncertainty, and we provide specific recommendations for different actors.</p>
<h1>Introduction</h1>
<p>Effective altruism is regularly framed as “figuring out how to do the most good, and then doing it.” However, figuring out how to do the most good is not easy. Different groups reach different conclusions. So how <em>do</em> we figure out how to do the most good?</p>
<p>Quite obviously, the first step is to figure out our values. We need to know what we roughly mean by ‘the most good.’ However, once our moral uncertainty is significantly diminished, what is the next step in figuring out how to do the most good? We believe the next step should be <em>strategy research</em>: high-level research on how to best achieve a high-level goal. A brief case was made for <em>strategic analysis</em> by Nick Bostrom in <em>Superintelligence</em> (p. 317):</p>
<blockquote>
<p>""Against a backdrop of perplexity and uncertainty, [strategic] analysis stands out as being of particularly high expected value. Illumination of our strategic situation would help us target subsequent interventions more effectively. Strategic analysis is especially needful when we are radically uncertain not just about some detail of some peripheral matter but about the cardinal qualities of the central things. For many key parameters, we are radically uncertain even about their sign…”</p>
</blockquote>
<p>This was written in the context of existential risk from AI. We believe it applies to existential risks in general, and that strategy research should be a core part of other effective altruism areas as well. However, this leaves many open questions. What is strategy research? How does strategy research relate to other types of research? When should strategy research be prioritized and when should it not be? These questions are relevant to potential and current researchers, research managers, and funders. The answers are necessary to allocate resources effectively. This article also provides motivation for the founding of the existential risk strategy research organization <a href=""https://www.convergenceanalysis.org/"">Convergence</a>. Convergence will be publishing more strategic analyses going forward. This article represents our current best (and somewhat simplified) understanding of the concepts outlined. Because we strive to clarify basic concepts and arguments, we have left out some of the finer details and complexities. We intend to further disentangle, clarify, and develop the ideas in the future. Furthermore, the underlying ideas presented here generalize to other fields, but some fields are in a different stage than existential risk reduction is and therefore need different research priorities.</p>
<p>To clarify what we are arguing for, we first describe strategy research and relate it to other types of research. We then argue that strategy research is important for reducing existential risk. We propose that the marginal value of strategy research depends on the maturity of the research field. We conclude that the current immaturity of the existential risk research field makes further strategy research highly valuable.</p>
<h1>What is strategy research?</h1>
<p>Strategy research seems intuitively valuable. But what is it about more precisely? Understanding this and the different options in research will help us make good decisions about how to allocate our resources and how to direct our research efforts. In this section, we position strategy research within a framework of different research types in effective altruism, we give an explicit definition, and we distinguish our terms from other commonly used terms.</p>
<h2>Five classes of effective altruism research</h2>
<p>To put strategy research in context to other types of research, we have developed a classification of different research types. Naturally, the classification is a simplification and research will often not fit neatly into a single category.</p>
<h3>The research spine of effective altruism: three levels</h3>
<p>We can approach ‘figuring out what to do’ at three different levels of directness (which are inspired by the same kind of goal hierarchy as the <a href=""https://forum.effectivealtruism.org/posts/Ekzvat8FbHRiPLn9Z/the-values-to-actions-decision-chain-a-lens-for-improving"">Values-to-Actions Chain</a>). Most indirectly, we can ask ‘what should we value?’ We call that <em>values research,</em> which is roughly the same as ethics. From our values, we can derive a high-level goal to strive for. For longtermism values, such a goal could be minimize existential risk.<sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-1"" id=""fnref-S2f9DLPkbyGebcqET-1"">[1]</a></sup> For another set of values , such as animal-inclusive neartermism, the high-level goal could be to minimize the aggregate suffering of farm animals.<sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-2"" id=""fnref-S2f9DLPkbyGebcqET-2"">[2]</a></sup></p>
<p><img src=""https://lh5.googleusercontent.com/NNLIie719m4laBS6QYcy42QLKYRlTgCau15UJ9hEH7lyFoJ2Cy1JJ5lyL4lHwu2501kWTbt1UCI15pWrVY9kP1FvhUp5zzZT5ogEFWyz5IMVWVehXZHXwEfrE9Z-54LNnawvMtX5"" alt=""""></p>
<p>More directly, we can ask ‘given our goal, how can we best achieve it?’ We call the research to answer that question <em>strategy research</em>. The result of strategy research is a number of strategic goals embedded in a strategic plan. For example, in existential risk reduction, strategy research could determine how to best allocate resources between reducing various existential risks based on their relative risk levels and timelines.</p>
<p>Most directly, we can ask ‘given our strategic plan, how should we execute it?’ We call the research to answer that question <em>tactics research</em>. Tactics research is similar to strategy research, but is at a more direct level. This makes tactics more specific. For example, in existential risk reduction, tactics research could be taking one of the sub goals from a strategic plan, say ‘reduce the competitive dynamics surrounding human-level AI’, and ask a specific question that deals with part of the issue: ‘How can we foster trust and cooperation between the US and Chinese governments on AI development?’ In general, less direct questions have more widely relevant answers, but they also provide less specific recommendations for actions to take.</p>
<p>Finally, the plans can be <em>implemented</em> based on the insights from the three research levels.</p>
<p>Each level of research requires some inputs, which it then processes to produce some outputs for the more direct level of research. For example, strategy research requires a goal or value to strive for, and this needs to be informed by moral philosophy.<sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-3"" id=""fnref-S2f9DLPkbyGebcqET-3"">[3]</a></sup> When strategy research is skipped, tactics research and implementation are only driven by implicit models. For example, a naive and implicit model is ‘when something seems important, try to persuade influential people of that.’ Acting on such a model can do harm. In emerging research fields, implicit models are often wrong because they have received less thought and have not been exposed to feedback. For tactics research and implementation to be effective, they should often be driven by a well-informed and thoughtfully crafted strategy.</p>
<p>The boundary between strategy and tactics is gradual rather than sharp. Thus, some research questions fall somewhere in between. Note as well that implementation is very simplified here; it refers to a host of actions. Implementation can be ‘doing more research’, but it can also be ‘trying to change opinions of key stakeholders’ or ‘building up research capacity.’</p>
<h3>A spine is not sufficient: informing and improvement research</h3>
<p>You could say that these levels form a <em>spine</em>: they create a central structure that supports and structures the rest of the necessary building blocks. For instance, strategic clarity makes information more useful by giving it a structure to fit into. To illustrate this, imagine learning a piece of information about an improved method of gene writing. Without any strategic understanding, it would just be an amorphous piece of information; it would not be clear how learning it should affect your actions. However, with more strategic clarity it would be more clear how this new method could affect important parameters, the possible consequences of that, and how one should best react to it.</p>
<p>Still, a spine is not a complete body; it needs additional building blocks. Strategic clarity can not be achieved without being sufficiently informed about the state of the world, or without understanding how to effectively conduct research in a domain.</p>
<p>Therefore, in addition to the research levels, we also identify two additional research classes:<sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-4"" id=""fnref-S2f9DLPkbyGebcqET-4"">[4]</a></sup> <em>informing research</em> and <em>improvement research</em>. Informing research mostly concerns questions about what the world is like. They can be very important questions, and science has built an enormous trove of such knowledge that effective altruism can draw from. Improvement research helps to improve other types of research by identifying important considerations, by improving existing research methods, and by identifying useful models from other fields. Philosophy of science, epistemology, mathematics, economics, and computer science can all be used for improvement research. For example, improvement research focused on ethics could discuss the role that intuitions should have in the methodology of moral philosophy.</p>
<p><img src=""https://lh4.googleusercontent.com/4EntOI41HIX-jsR3PxplC8Sbwm_5XwIgbKd8WPqqvkqfihB7P0pbThHPk2Ci2pdRJjyGF3-APM4pPwCGq8VfpIluoXmHaRsD0_bUAZt-PztQW9bL_D0orVhh0r1D5LfdpQJcnxxY"" alt=""""></p>
<h2>A definition of strategy research</h2>
<p>Based on the model of the research classes above, we will formulate a definition of strategy research. We want a definition that is simple and captures the core purpose of strategy research. Strategy research is an imprecise concept, so the definition should reflect that. We also want the term to correspond to how people have used it in the past. For these reasons, we propose the following definition for strategy research:</p>
<p>   <em>High-level research on how to best achieve a high-level goal.</em></p>
<p>Thus, the central strategy question is “how to best achieve our high-level goal?” And to achieve a goal, you implicitly or explicitly need to form and act on <em>plans</em>. The challenge of strategy research is to figure out the best plans: those that best achieve a particular high-level goal given the existing constraints. To figure out the best plans, a lot of different activities are necessary. It requires, among others, understanding which parts of the world are relevant for making plans, what actions lead to what consequences, how to compose actions into plans, and how to prioritize between plans.</p>
<p>This means that, in order to figure out the best plans, strategy research will involve a substantial amount of informing research, as well as improvement research. For example, if you have a model of how different risk levels and timelines should affect resource allocation, you also need to know what the different risk levels and timelines are (i.e. <em>informing research</em>) in order to form a comprehensive strategic plan. This research is <em>high-level</em> because it is focused on plans to achieve a high-level goal. In contrast, research on figuring out one’s values is top-level, and research on how to best achieve a tactical goal is low-level.<sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-5"" id=""fnref-S2f9DLPkbyGebcqET-5"">[5]</a></sup></p>
<h2>How do other research terms in effective altruism relate to this framework?</h2>
<p>In effective altruism, there have been many terms used for different types of research. Each organization uses a term slightly differently, and it is difficult to find precise definitions of these terms. Let’s briefly consider some research terms in effective altruism that relate to strategy research.</p>
<p><strong><em>Cause prioritization, <a href=""https://concepts.effectivealtruism.org/concepts/prioritization-research/"">prioritization research</a>, <a href=""https://80000hours.org/problem-profiles/global-priorities-research/"">global priorities research</a></em></strong></p>
<p>These three terms have been used interchangeably to describe roughly similar types of research: prioritization between and within cause areas.<sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-6"" id=""fnref-S2f9DLPkbyGebcqET-6"">[6]</a></sup> Prioritization <em>between</em> cause areas overlaps significantly with values research, although in practice it often does not deal with the more fundamental issues in ethics. Prioritization <em>within</em> cause areas overlaps significantly with strategy research.</p>
<p><strong><em><a href=""https://www.fhi.ox.ac.uk/research/research-areas/"">Macro-strategy research</a></em></strong></p>
<p>This term is mostly used by FHI, and seems to refer to uncovering <em><a href=""https://concepts.effectivealtruism.org/concepts/the-importance-of-crucial-considerations/"">crucial considerations</a></em> with regard to improving the long-term future. Crucial considerations can “radically change the expected value of pursuing some high-level sub goal.”<sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-7"" id=""fnref-S2f9DLPkbyGebcqET-7"">[7]</a></sup> A high-level sub goal refers here to things like “increase economic progress” or “decrease funding into AGI research”. The intention appears to focus on the higher-level questions of strategy research (hence “macro”) although FHI also classifies their paper on the <em><a href=""https://nickbostrom.com/papers/unilateralist.pdf"">unilateralist’s curse</a></em> as macro-strategy. That concept does not seem to be a crucial consideration, but a strategic consideration for multiple existential risks.</p>
<p><strong><em>AI strategy</em></strong></p>
<p>As the term has been used in effective altruism, AI strategy research is simply strategy research focused on reducing existential risk from AI specifically.<sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-8"" id=""fnref-S2f9DLPkbyGebcqET-8"">[8]</a></sup></p>
<p><strong><em>Charity evaluation</em></strong></p>
<p>A number of organizations evaluate interventions and charities, or select charities to donate to (e.g. GiveWell, Animal Charity Evaluators, Open Philanthropy Project, Founders Pledge, Rethink Priorities). Although we would not classify charity evaluation itself as strategy research, it heavily relies on strategic views and many of the mentioned organizations perform some kind of strategy research. For an example for neartermism human-centric values, we would call GiveWell’s research to identify their priority programs strategy research, and would call their evaluation of charities tactics or tactics-informing research.</p>
<h1>Why strategy research is important to reduce existential risk</h1>
<p>Because of strategic uncertainty, we believe that more strategy research is currently particularly important for reducing existential risk. In this section, we give our main reasons and support them with a model in which the value of a research class depends on the maturity of the field. We then note some other considerations that affect the importance of strategy research and discuss how strategy research could do harm.</p>
<h2>The current stage of existential risk research makes strategy research valuable</h2>
<p>Strategy research makes the most sense when (1) a community knows roughly what it wants (e.g. reduce existential risk), when (2) it is unlikely that this goal will undergo substantial changes from further research or reflection on values, and (3) when the field has not yet reached strategic clarity. Strategic uncertainty is the stage where the expected value of strategy research is the highest. It is in between the stages of <em>value uncertainty</em> and <em>strategic clarity</em>.</p>
<p>Here we argue that doing strategy research should be a high priority because it is currently unclear how to best achieve existential risk reduction. Strategic uncertainty means that we are uncertain which actions are (in expectation) valuable, which are insignificant, and which are harmful. This implies that there is valuable information to be gained.</p>
<p><strong><em>We are currently strategically uncertain</em></strong></p>
<p>To see whether we are actually strategically uncertain, we can ask what strategic clarity would look like. The further we are away from that ideal, the more strategically uncertain we are. With strategic clarity we would know what to do. Specifically, we would know...</p>
<ul>
<li>who the relevant actors are</li>
<li>what actions are available to use</li>
<li>how the future might develop from those actions</li>
<li>what good sequences of actions (plans) are</li>
<li>how to best prioritize plans</li>
<li>that we have not missed any important considerations</li>
</ul>
<p>We currently have only a basic understanding of each of these in existential risk reduction. The claim that we are strategically uncertain in the field of existential risk seems widely shared. For example, it is echoed in <a href=""https://forum.effectivealtruism.org/posts/RCvetzfDnBNFX7pLH/personal-thoughts-on-careers-in-ai-policy-and-strategy"">this post</a> by Carrick Flynn, and again in <em>Superintelligence</em> (p. 317).</p>
<p><strong><em>Strategic uncertainty implies there is information to be gained</em></strong></p>
<p>The cost of strategy research is only worth it if it significantly improves our understanding of which actions are (in expectation) valuable, which are insignificant, and which are harmful. Useful information has been gained in the past by uncovering crucial considerations that had a massive influence on our current priorities and plans. These include the separate realizations that AI and synthetic biology might be existential risks. More crucial considerations could be uncovered by strategy research. In addition, there are many current open questions to which different answers would imply substantially different priorities. Examples include ‘how widely is existential risk distributed over different possible causes?’, ‘when would an AI takeoff happen?’, and ‘how likely is human civilization to recover after collapse?’. There is still substantial disagreement on these questions, and progress on these questions would reduce our strategic uncertainty.</p>
<p>In addition, the information needs to be acquirable for a reasonable amount of effort. Strategy research would not be valuable if it was completely intractable. We believe some actors and attempts at strategy research can succeed, but it is hard to predict success beforehand.</p>
<p><strong><em>Strategic uncertainty implies that interacting with the ‘environment’ has a reduced net value of information</em></strong></p>
<p>Interacting with one’s environment can be highly informative. Interacting with a complex system often yields a substantial amount of information that cannot be obtained by outside observation. For example, it is hard to assess how receptive policy makers are towards existential risk reduction without engaging with them. Interacting with them would allow efficient learning about the domain.</p>
<p>However, this information comes with a risk. Strategic uncertainty also implies that tactical recommendations and direct implementations can be <em>harmful</em>. For example, approaching the wrong policy makers or approaching them in the wrong way can reduce the chance for existential risk to be taken seriously by governments. Taking uninformed action to reduce existential risk may backfire catastrophically in hard-to-reverse and hard-to-predict ways. This reduces the <em>net</em> value of that action.<sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-9"" id=""fnref-S2f9DLPkbyGebcqET-9"">[9]</a></sup></p>
<p>In contrast, strategy research is less likely to directly cause harm because it gives general and imprecise recommendations. This means they are less likely to be wrong and that they are further away from implementation, which allows for more opportunities to correct mistakes. Strategy research is also <em>self-correcting</em>: it can change its focus and method based on its own generated insights; part of strategy research is to analyze whether we should continue doing strategy research.</p>
<h2>A model of research value as a function of a field’s maturity</h2>
<p>We have argued that we are currently strategically uncertain with respect to existential risk reduction and that this implies that strategy research is high priority. However, we can make a more complex model than “first solve values, then solve strategy, then solve tactics, then implement plans”. In practice, resources (e.g. capital and labour) are spread over multiple levels of research and resources become specialized. The optimal allocation of marginal resources depends on the current state of knowledge.</p>
<p>We propose a model in which the cumulative value of research levels (i.e. values, strategy, and tactics research) follows s-curves. S-curves <a href=""https://www.lesswrong.com/posts/oaqKjHbgsoqEXBMZ2/s-curves-for-trend-forecasting"">are described as</a> “fundamental patterns that exist in many systems that have positive feedback loops and constraints. The curve speeds up due to the positive feedback loop, then slows down due to the constraints.” In this section, we describe the different constraints and the positive feedback loop that creates the s-shaped curve we expect the value of a research level to exhibit.</p>
<p><img src=""https://lh6.googleusercontent.com/BlqMTPI6WITwDq-cuWF2KJMgTmurLQB7pNkL2ecbI5zp_89Ya_jEnf7V3yOV18Mzwb-i8c_oVMH9uthM84IPicz0MHh-1MuLh31sGz5xdZrer4YvQSAZmLZlSBZlcmsxAcYUB8yN"" alt=""""></p>
<p><strong><em>Early phase: constraints need to be addressed</em></strong></p>
<p>When research on a particular level (e.g. strategy research) in a particular field (e.g. x-risk reduction) is just getting started, we expect progress to be slowed down by two constraints. The first constraint is a lack of clarity on the higher level. For instance, it is not valuable to try to figure out a good strategy when you are uncertain about your values, because you are much more likely to work on questions that turn out to be not very relevant to your values. The first constraint should be addressed at the higher level.</p>
<p>The second constraint is that doing early research in a field is hard. There is not yet an established paradigm; the problems are messy, entangled, and vague, rather than structured, independent, and clear. What is needed in an early stage is <a href=""https://forum.effectivealtruism.org/posts/RCvetzfDnBNFX7pLH/personal-thoughts-on-careers-in-ai-policy-and-strategy"">disentanglement</a> - structuring the research field, identifying the central questions, and clarifying concepts. This constraint cannot be addressed by research at a higher level (resolving moral uncertainty does not help us any further in our strategic uncertainty). Consequently, it needs to be addressed head-on, which means that progress will be slow at first.</p>
<p><strong><em>Middle phase: positive feedback loops create exponential growth</em></strong></p>
<p>The middle phase starts when the constraints become weaker. Answers to higher-level questions narrow down the range of relevant questions at the lower level. Generally, we expect that a higher proportion of research projects produce value, because irrelevant questions can be better identified beforehand. Furthermore, as the field becomes more structured, each successful piece of research tends to identify multiple new and compelling research questions. This is a period of exponential growth.</p>
<p><strong><em>Late phase: new constraints arise</em></strong></p>
<p>The late phase starts when new constraints arise. One constraint is that the big questions have either been solved or have been found intractable. The remaining questions will be either conceptually hard, will require information that is not (yet) available, or will be lower-level questions. At this point, the lower research level has progressed through its own early phase, and the marginal value of doing research at a lower research level surpasses the value of doing research at the current level.</p>
<p>In summary, as our insight progresses, the marginal value of research shifts towards lower-level questions. A good heuristic in this model is to ‘do research at the highest level that is most sensitive to new information’.</p>
<h2>Implications of the model</h2>
<p>First, this model does not imply that, at any point in time, we should invest all resources into a single level of research. Rather, it suggests where to spend our <em>marginal</em> effort, which depends on the stage we are in. It is often useful to keep some resources in an earlier type, because those resources have become specialized and may be in their best position. For example, moral philosophers who believe in <em>longtermism</em> and existential risk reduction may want to keep working on moral philosophy to improve the rigour of the arguments and potentially uncover new (though most likely more minor) considerations. Furthermore, insights down the line might give rise to new questions higher up, so we should maintain some capacity to answer these questions.</p>
<p>Second, even if most of the marginal expected value today lies within strategy research, it would be useful to invest some marginal resources into tactics research and even some into implementation. There might be some easy-to-uncover tactical insights applicable to a wide range of strategic plans, trying out some tactics research might illuminate some strategic uncertainties, and building the capacity to do tactics research allows for a faster response to strategic insight.</p>
<p>Third, the model assumes that research at each level also involves improvement and informing research. However, this does not mean that improvement, strategy, and strategy-informing research are equally represented in each phase. It is possible that early research involves more improvement than informing research or vice versa, but it is unclear what is more likely.</p>
<p>This model also addresses a common criticism that the effective altruism community frequently receives, namely that the community spends so much time thinking, discussing, and doing research, and so little about taking action. (This criticism is not completely off-mark: there is productive discussion and unproductive discussion.) It is tempting to reply by pointing out all the things the effective altruism community has achieved: moved money to effective charities, set up new organisations, et cetera. However, we can also give another answer based on this model: ""Yes, currently we are still focusing on research. But we are progressing at what seems to be the appropriate speed and we will increase the amount of implementation we do as we gain more clarity.""</p>
<h2>Other considerations that affect the value of strategy research</h2>
<p>We believe the reasons in the previous section provide enough support for the claim that strategy research should be highly prioritized. However, there are additional important considerations that affect the strength of our claim. We believe they pose important questions, but have significant uncertainty about them. Analyzing these considerations and providing evidence for them is beyond the scope of this article. We welcome further discussion on these points.</p>
<p><strong><em>How much time is there for strategic insights to compound or mature into implementation?</em></strong></p>
<p>Before a robustly good strategy can be implemented, models need to be created and refined and crucial considerations need to be uncovered. This means that strategy research needs enough time to pay off.</p>
<p>The higher one’s credence is that we will encounter an existentially risky event soon - such as the invention of human-level AI - the more likely it is that acting on our current best guess for handling existential risk is better than systematically creating a top-down strategy.</p>
<p>However, we (the authors) are significantly uncertain about the timelines of various existential risks, especially of AI. Therefore we are reluctant to act as if timelines are short. Such short-term actions (e.g. raising the alarm without nuance, or trying to build a capable and reputable research field rapidly) often seem costly or are harmful in the long-term. In addition, many promising strategies can only affect existential risk on a medium or long timeframe. Even discounted by the probability that there is not enough time for them to be impactful, strategies with medium to long timeframe probably have a high expected value.</p>
<p><strong><em>How likely are the strategic insights to affect concrete actions and the environment?</em></strong></p>
<p>Information is only valuable if it eventually affects the world. It is possible that there is already enough actionable strategic knowledge available, but that only a few people are willing and able to act on it. In such a case, resources would be better spent on lobbying influential people so they make better decisions for the future of humanity, or on increasing the influence of people who are expected to make good decisions for the future of humanity.</p>
<p>We believe it is hard to assess how likely insights are to affect other actors. Lobbying influential people and coalition building could be the best action for some people. In addition, influence and coalition building may <a href=""https://www.effectivealtruism.org/articles/ea-neoliberal/"">take decades</a>, which would imply that early action on this front is valuable. Nonetheless, some strategy research also takes a long time to fruition.</p>
<p><strong><em>How likely is it that there are hard-to-reverse developments that require immediate action?</em></strong></p>
<p>Sometimes it is necessary to act on insufficient information, even if we would prefer to do more strategic analysis. Our hands may be forced by other actors that are about to take hard-to-reverse actions, such as implementing premature national AI policies. New policies by major actors could significantly limit the range of possible and desirable strategies in the future if these policies are implemented prematurely or with a lack of nuance. In cases where key decision makers cannot be persuaded to exercise ‘strategic restraint’, it may be beneficial to step in and do ‘damage control’ even if everything would have been better if no one had moved early.</p>
<p>We believe that some hard-to-reverse actions are in fact being taken, but only some actors could find good opportunities to effectively advocate strategic restraint or do ‘damage control’. Some could even create good conditions for further (strategic) action.</p>
<h2>How could strategy research do harm?</h2>
<p>Just like for every other project, it’s important to consider the <a href=""https://80000hours.org/articles/accidental-harm/"">possibility of doing harm</a>. We identify the following three important ways strategy research might do harm.</p>
<p><strong><em>Strategy research may carry <a href=""https://nickbostrom.com/information-hazards.pdf"">information hazards</a>.</em></strong> Some knowledge may be dangerous to discover, and some knowledge may be dangerous if it spreads to the wrong people. In mapping possible existential risks, strategy research may uncover new ways for humans to risk existential catastrophe. Sharing those possible risks could make them more likely to occur by inspiring malicious or careless actors. Another information hazard is when plans become known to actors with conflicting (instrumental) goals, which allows them to frustrate those plans. Some goals are more likely to conflict with other agents’ goals than others. We generally recommend against publicly identifying these conflicts, unless the other party is definitely already aware of you and your plans.</p>
<p><strong><em>Strategy research may create strategic confusion</em>.</strong> Badly executed or communicated research could confuse, rather than illuminate, important actors. Creating bad research makes it more difficult to find good research. Furthermore, strategy research could overstate the amount of strategic uncertainty and thereby excessively limit the behavior of careful actors while less careful actors could take the lead.</p>
<p><strong><em>Strategy research may waste resources.</em></strong> It is hard to assess the expected value of specific strategy research projects, even after they have been completed, because it is difficult to trace consequences back to specific research projects. Even if strategy research is not worse than inaction, resources like money and talent still carry opportunity costs: they might have been used better elsewhere. We believe it is very likely that a number of projects are a waste of resources in this sense. This waste can be reduced by effective feedback loops, such as the evaluation of research organizations (like <a href=""https://forum.effectivealtruism.org/posts/BznrRBgiDdcTwWWsB/2018-ai-alignment-literature-review-and-charity-comparison"">this one</a>).</p>
<h1>Discussion</h1>
<p>The goal of this article was to describe strategy research more clearly and to argue that it should currently be given a high priority in the field of existential risk reduction. This article has introduced some terms and models that can increase our collective understanding of different research classes, as well as provide input for fruitful discussion. Based on our model, we proposed the heuristic to ‘do research at the highest level that is most sensitive to new information’. We believe that strategy research is currently this highest level in the field of existential risk reduction.</p>
<h2>Recommendations</h2>
<p>Our main recommendation is to expand the existential risk strategy field. We would like to see more strategy research from both existing and new actors in the field. What follows are some recommendations for particular groups. We encourage readers to come up with other implications.</p>
<p><strong><em>Researchers: explore the big picture and share strategic considerations</em></strong><sup class=""footnote-ref""><a href=""#fn-S2f9DLPkbyGebcqET-10"" id=""fnref-S2f9DLPkbyGebcqET-10"">[10]</a></sup></p>
<p>We recommend current existential risk researchers to grapple with the questions of how their research focus fits within the larger picture. We especially encourage researchers to share their strategic insights and considerations in write ups and blog posts, unless they pose information hazards. We believe most researchers have some implicit models which, when written up, would not meet the standards for academic publication. However, sharing them will allow these models to be built upon and improved by the community. This will also make it easier for outsiders, such as donors and aspiring researchers, to understand the crucial considerations within the field.</p>
<p><strong><em>Research organizations: incentivize researchers</em></strong></p>
<p>Research organizations should incentivize researchers to explore doing strategy research and to write their ideas and findings up in public venues, even if those are provisional ideas and therefore do not meet the standards for academic publication.</p>
<p><strong><em>Donors: increase funding for existential risk strategy</em></strong></p>
<p>We encourage donors to explore opportunities to fund new existential risk strategy organizations, as well as opportunities within existing organizations to do more strategy research. Given the newness of the research field and given that there are few established researchers, we believe this is currently a space to apply <a href=""https://www.openphilanthropy.org/blog/hits-based-giving"">hits-based giving</a>. Not all projects will pay off, but those that do will make a big difference. As funders learn and the field matures, we expect strategy research to become ‘safer bets’.</p>
<p><strong><em>Effective altruists: learn, support, start</em></strong></p>
<p>For those that aspire to move into existential risk strategy research, we recommend exploring one’s fit by doing an internship with a strategy organization or writing and sharing a simple model of a strategy-related topic. People with operations skills can make a large impact by supporting existing strategy research, or even starting up a new organization, since we believe there is enough room for more existential risk strategy organizations.</p>
<h2>Limitations &amp; further research</h2>
<p>We have simplified a number of points in this article, and it contains a number of gaps that should be addressed in further research.</p>
<p><strong><em>Focused on basics → elaborate on the details of strategy research</em></strong></p>
<p>We have strived to make the basics of strategy research clear, but many details have been left out. Further research could delve deeper into the different parts of strategy research to assess what they are, which parts are most valuable, and to examine how to do effective strategy research. This research could also disentangle the difference between ‘narrow’ and ‘broad” strategy research we allude to in footnote 4.</p>
<p><strong><em>Focused on x-risk → assess the need for strategy research in other areas</em></strong></p>
<p>This article, because it is written by Convergence, focuses on existential risk strategy. However, we could also have chosen to focus on effective altruism strategy, longtermism strategy, or AI strategy. Further research could approach the strategic question for a wider, narrower, or otherwise different high-level goal. For example, it appears that both community building and animal welfare would benefit greatly from more strategy research.</p>
<p><strong><em>Incomplete risk analysis → research how strategy research can do harm</em></strong></p>
<p>We have only briefly discussed how strategy research can do harm, and have argued that it is less likely to do harm because it is more indirect. Further research could investigate this claim further and draft guidelines to reduce the risk of harmful strategy research.</p>
<h1>Conclusion</h1>
<p>This article has explained, in part, why we believe strategy research is important and neglected. We hope it contributes towards strategic clarity for important goals such as reducing existential risk. Finally, we hope this article motivates other research groups, as well as donors and other effective altruists, to focus more on strategy research.</p>
<h1>Acknowledgements</h1>
<p><em>This post was written by Siebe Rozendal as a Research Assistant for <a href=""http://convergenceanalysis.org/"">Convergence</a> in collaboration with Justin Shovelain, who provided many of the ideas, and David Kristoffersson, who did a lot of editorial work. We are especially grateful for the thorough feedback from Ben Harack, and also want to thank Tam Borine and Remmelt Ellen for their useful input.</em></p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-S2f9DLPkbyGebcqET-1"" class=""footnote-item""><p>Other high-level goals for longtermism have also been suggested, such as <a href=""https://www.effectivealtruism.org/articles/a-proposed-adjustment-to-the-astronomical-waste-argument-nick-beckstead/"">Beckstead’s</a> “make path-dependent aspects of the far future go as well as possible.” <a href=""#fnref-S2f9DLPkbyGebcqET-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S2f9DLPkbyGebcqET-2"" class=""footnote-item""><p>Interestingly, animal-inclusive neartermism values do not have a clear analogue goal to ‘minimize x-risk’. We understand the focus on farm animals might not be the optimal goal, because it excludes suffering of non-farm animals. <a href=""#fnref-S2f9DLPkbyGebcqET-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S2f9DLPkbyGebcqET-3"" class=""footnote-item""><p>Actors do not necessarily need to share the same values to have the same high-level goals. For example, many cause areas would benefit from an effective altruism community that is healthy, full of capable people, and strategically wise. <a href=""#fnref-S2f9DLPkbyGebcqET-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S2f9DLPkbyGebcqET-4"" class=""footnote-item""><p>Research often falls under multiple of these classes at the same time. For instance, research into how to build prudent national AI policies may be highly informing to strategy research (important to high-level strategy) and tactical (important to tactical questions of policy making) at the same time.
Further, if a researcher is figuring out important improvement and informing issues for strategy, isn't that strategy research? We believe it is; we prefer a “broad” definition of strategy research. In contrast, a “narrow” definition of strategy research would refer only to pure questions of strategy construction. We think there are some important distinctions and tradeoffs here that we hope to illuminate in further work. <a href=""#fnref-S2f9DLPkbyGebcqET-4"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S2f9DLPkbyGebcqET-5"" class=""footnote-item""><p>That something is low-level does not mean it is not high quality, or not important. The level refers to the level of directness: how closely it informs action. <a href=""#fnref-S2f9DLPkbyGebcqET-5"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S2f9DLPkbyGebcqET-6"" class=""footnote-item""><p>Whether some research is between or within a cause area depends on how a ‘cause area’ is defined. However, just like the term ‘prioritization research’, different people use the term ‘cause area’ differently. In this article, we regard ‘existential risk reduction’ as a single cause area. <a href=""#fnref-S2f9DLPkbyGebcqET-6"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S2f9DLPkbyGebcqET-7"" class=""footnote-item""><p><a href=""https://www.effectivealtruism.org/articles/crucial-considerations-and-wise-philanthropy-nick-bostrom/"">Bostrom (2014). ‘Crucial Considerations and Wise Philanthropy.’</a> <a href=""#fnref-S2f9DLPkbyGebcqET-7"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S2f9DLPkbyGebcqET-8"" class=""footnote-item""><p>AI governance and AI policy are two related terms. Possibly, AI policy maps to AI-risk specific tactics research and AI governance maps to the combination of AI strategy and AI policy, but we are uncertain about this classification.
We also advise against the use of the term ‘AI tactics research’ as it may sound adversarial/military-like. <a href=""#fnref-S2f9DLPkbyGebcqET-8"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S2f9DLPkbyGebcqET-9"" class=""footnote-item""><p>Actions during strategic uncertainty can be harmful, but trying to take action could still provide useful information. This is a good reason to focus current AI policy on the near- and medium-term; those policies will still yield a good (though smaller) amount of information while carrying significantly lower risk of doing long-term harm. <a href=""#fnref-S2f9DLPkbyGebcqET-9"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S2f9DLPkbyGebcqET-10"" class=""footnote-item""><p>Allan Dafoe, director of the Centre for the Governance of AI, <a href=""https://80000hours.org/podcast/episodes/allan-dafoe-politics-of-ai/"">has a different take</a>: “Some problems are more important than others. However, we are sufficiently uncertain about what are the core problems that need to be solved that are precise enough and modular enough that they can be really focused on that I would recommend a different approach. Rather than try to find really the highest-leverage, most-neglected problem, I would advise people interested in working in this space to get a feel for the research landscape.” <a href=""#fnref-S2f9DLPkbyGebcqET-10"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",Siebe,siebe,Siebe,
N3TtJX5Hv495K76yQ,LW Dev question: FB-style tagging?,lw-dev-question-fb-style-tagging,https://www.lesswrong.com/posts/N3TtJX5Hv495K76yQ/lw-dev-question-fb-style-tagging,2019-06-20T19:19:45.807Z,11,3,1,True,True,,"<p>As LW is an intellectual **community** where people get to know each other and know &quot;who knows what, who&#x27;s working on what&quot; it seems like something like tagging people FB-style in specific posts/comments is a very low-hanging fruit. (Of course you should be able to silence people from tagging you or sort taggers by some score). Is this in the pipeline, or has been ruled out for some reason? </p>",Dr_Manhattan,dr_manhattan,Dr_Manhattan,
wSbox2KCoHgfXQ6F7,STRUCTURE: A Hazardous Guide to Words,structure-a-hazardous-guide-to-words-1,https://www.lesswrong.com/posts/wSbox2KCoHgfXQ6F7/structure-a-hazardous-guide-to-words-1,2019-06-20T15:27:45.276Z,8,3,7,False,False,,"<html><head></head><body><p><em>This post is part of my Hazardous Guide To Rationality. I don't expect this to be new or exciting to frequent LW people, and I would super appreciate comments and feedback in light of intents for the sequence, as outlined in the above link. Also, note this is a STRUCTURE post, again see the above link for what that means.</em></p>
<h2>Intro</h2>
<p>Communication can be hard. Thinking can be hard. Trying to get another person to understand what you mean can be hard. Trying to explain to yourself what you mean can be hard. This sequence draws out in detail some of the things that happen in human brains that leads to these hardships, and proposes a few tactics to make it less hard.</p>
<h2>Some Problems</h2>
<p>When a tool becomes familiar enough, it can become forgotten. You forget a black box is a black box, and see it as a lever of reality.</p>
<p>What you meant is not what you said, but also, what you meant is not what you said.</p>
<p>I was expressing two distinct thoughts there. It wasn't very obvious. The first sentence meant, ""You don't always say words that make other people think of the same things you're thinking about"". The second meant, ""What you mean is a different type of thing from what words you produce, and they should not be mistaken for each other"".</p>
<p>What you meant is not what you said; the language you use is not the same thing as the cognition that produced the language. Apologies if you're getting bored.</p>
<p>In this sequence I'm going to make some claims about how we think and how language works, and then point to a bunch of pit-falls that can lead to confusion, never ending argument, and bad times. I believe these pitfalls have to do with <strong>inaccurate models of how language works</strong> and <strong>subtly forgetting that cognition and language are different</strong>.</p>
<p>(aside:  what if after reading this sequence you think ""Well, it's not really inaccurate models and subtly forgetting that is the cause of these problems, it's XYZ"". Oh really? Why would it matter if it was the case?)</p>
<h2>Cognition, Classification, and Inference</h2>
<p>Your brain's job is to keep you alive. This takes a lot of guess work. Is this edible or poisons? Is that blur a tiger on its way to kill me? The usual. It sure would be useful if the brain could turn sensory input into the exact set of motor movements needed to not die. This is what the brain does. You take the outside world and make categories. Once categorized, you are use various ""if... then..."" rules that you previously associated with the category. If all goes well, you hear a rustle, categorize as predator, and activate ""run!"" This is not the only thing the brain does, but it is <em>something</em> the brain does.</p>
<p>These neural categories predates language, though they can be effected by language.</p>
<p>Some neural categories are loose, some are tight. Some categories immediately spawn action. The tightness, triggers, and consequences of a category are all learned through [opaque process]. Humans are basically the same considering how from more than nothing we share. Humans are nothing alike considering how far from exactly alike we are. No one will ever know your inner soul, yet you can still drive to work without being killed 100 times over in a car crash.</p>
<p>Remember, up to now, we have not talked about language, just the cognition of categorization. Later we talk about how language can effect how you categorize.</p>
<p>Sometimes, you goof. I hear a dog barking, think there's a dog around the corner, but it turns out it was just someone's ring tone. I think my new manager is going to be an ass-hole because of the face they didn't smile at me, turns out they just literally never smile and are super nice.</p>
<h2>Pre Language seed of error [under construction]</h2>
<p>I want to make a point about what an error in cognition feels like versus an error in modeling language.</p>
<blockquote>
<p>*Ugna is relaxing under a tree when Targno appears over the hill being chased by *</p>
</blockquote>
<p>[some dialoge about a caveman bringing up a shrieking pet with big teeth, Ugna say's it's ""scary"" Targno says it isn't]</p>
<p>Ugna is probably going to feel uneasy about not running. But something Ugna won't do is say, ""Targno, scary by definition means something that makes me feel fear and activates my parasympathetic nervous system, this is absolutely scary!""</p>
<p>Ugna's mind has a quick categorization where loud shrieks get put in the ""shit I should run away from"". Ugna might aslo use large fangs, being huge, or moving really fast as triggers to put things in the ""shit I should run away from"" category. In their primitive cave language, ""scary"" is used to communicate ""this is in the shit you should run away from category!""</p>
<p>For the very first time, Ugna has encountered something that shrieks loudly, has fangs, and yet isn't going to try and kill her.</p>
<p>It takes something special for Targno to think that Ugna is thinking about if the pet moves fast, as opposed to whether or not it can kill her.</p>
<p>They might still disagree, but they are disagreeing where it matters. They both really care about how to not die. There are a lot of arguments about what is scary that don't look like this.</p>
<h2>Language</h2>
<p>Language is more broad than spoken and written languages like English. Take sign language as an example. Despite this, I'll mostly talk about language in terms of words, which will be me thinking of things like ""catch"" and ""derivative"".</p>
<p>Words are not just a 1 to 1 mapping from sounds to categories in our heads. ""the"" and ""is"" are not categories. Language has it's own logic and structure to it. Verbs and nouns, subject agreement, something something I never payed attention in grade school nor took a linguistics class.</p>
<p>The point: cognition has it's own internal structure. Language has it's own internal structure. Language is used to try to poke in another persons brain and produce similar patterns of cognition as you did. Telepathy exists, it's just not what you expected.</p>
<p>This works well, a lot of the time. This fucks up a lot of the time. A deep see marine biologist has trouble talking to a mathematician because there are some concepts that just aren't in the other persons brain (how you build new concepts in a mind from old ones?). The French don't understand the Japanese because the symbols used are completely different (there are likely <em>also</em> conceptual differences between the cultures). A teacher has a hard time understanding kids, because even though they use the same symbol set, kids quickly make slang. Kids grew up differently, and thus a lot of words might relate to dif cognition in subtle or gross ways.</p>
<p>We are all so different. We are all so similar.</p>
<h2>Bad Theories and How they got there</h2>
<p>Nows the fun part. How do things break? Having a bad model of what language is what leads to arguing about the definition of ""scary"" instead of figuring out if this thing is going to kill us.</p>
<p>The <strong>child</strong> thinks that words have power. When I shout ""hungry"" the humans bring me food. You get confused when your dog doesn't stop when you tell it to. You lack theory of mind. Words not only mean things, words have power. There is no reflection.</p>
<p>The <strong>student</strong> thinks that words mean something. It may be the case that you don't know what they mean, but there is a right answer to this question. You know because your teacher gives you a bad score if you can't answer it ""correctly"".</p>
<p>The <a href=""https://www.readthesequences.com/The-Parable-Of-Hemlock""><strong>Greek</strong></a> thinks that words have meaning, and are the same thing as thoughts. There is reflection. All words and all thoughts correspond to perfectly cut logical categories. The world is syllogisms.</p>
<p>The <strong>poet</strong> learns that meaning is in people, not the words. A word means whatever you want it to mean, as it gets its meaning from how it's used. You see through the illusion.</p>
<p>The <strong>engineer</strong> learns that <em>some</em> words have real meanings. If they learn these meanings deeply enough, they can remake the world in their image. It is forbidden to use the words that do not mean something, and only talk about the ones that do.</p>
<p>The <strong>lawyer</strong> learns that once again, words have power. Not from magic, but from social contract. If you can get the right people to accept the right words, you can force other things that you want. This is all a game. Be careful not to say the wrong words when someone is listening, they might force you to make a move you don't like.</p>
<p>These names themselves do not form strict boundaries. In the following examples, I'll often refer to a mistake as ""poet behavior"" or ""student behavior"". That's mostly to help you have a richer web of mnemonic connections and hopefully remember this stuff more. This post is the framework, the rest will be worked examples and more subtle points. Examples of conversation, diagnostics of what went wrong, and lessons learned. Discrete skills will be pointed out, and there will be links to practice exercises to hammer in those skills. ""I don't need to practice remembering that other people have different minds"". Hmmm, maybe. Hard to say.</p>
<p>(aside: did you disagree with the names of the groups? Do you think I think something wrong? Do you not like that I unwittingly made you think of a concept you don't actually think I'm talking about? Do you think it was irresponsible to name the groups this way given common usage? The difference is interesting.)</p>
</body></html>",Hazard,hazard,Hazard,
JiXsGyoWH9oHr6Lqq,Should there be a header feature?,should-there-be-a-header-feature-1,https://www.lesswrong.com/posts/JiXsGyoWH9oHr6Lqq/should-there-be-a-header-feature-1,2019-06-20T06:45:38.654Z,2,1,2,False,False,,"<p>Headers are very common on Less Wrong, unfortunately, they interfere with the summary by taking up most of the space. I&#x27;d suggest the creation of a feature to mark certain content as a header which would then exclude it from this summary.</p>",Chris_Leong,chris_leong,Chris_Leong,
eCJJyHLGxdQMYEa8a,Defending points you don't care about,defending-points-you-don-t-care-about,https://www.lesswrong.com/posts/eCJJyHLGxdQMYEa8a/defending-points-you-don-t-care-about,2019-06-19T20:40:05.152Z,41,18,6,False,False,,"<p><em>This post is part of my</em> <em><a href=""https://www.lesswrong.com/s/pKxyvLyhEFxHgogdD"">Hazardous Guide To Rationality.</a></em> <em>I don&#x27;t expect this to be new or exciting to frequent LW people, and I would appreciate comments and feedback in light of intents for the sequence, as outlined in the above link.</em></p><p>A dialogue:</p><blockquote>Nicky: I&#x27;ve been wondering, do you think math was invented or discovered?<br/>Dee: Seems like it must have been discovered. I read about how circles are everywhere in nature, and that you can even find the fibonacci sequence in plants!<br/>Nicky: Yeah, but there aren&#x27;t actually any numbers in nature. Numbers are just something we made up to describe and talk about these patterns that we see in nature. Numbers themselves don&#x27;t really exist out in the world.<br/>Dee: Of course numbers are real! Made up constructs don&#x27;t have the predictive power that math does. They totally exists.<br/>Nicky: Well if numbers exist, where are they? You can&#x27;t show me where a number is. You can&#x27;t empirically test for numbers. You can&#x27;t find them anywhere in the physical world. They&#x27;re just constructs!<br/>Dee: Sure, they don&#x27;t a physical location in the world. That&#x27;s silly. There&#x27;s no circles floating around out behind the moon. What I&#x27;m saying is that they exist outside of space and time. Mathematical existence is it&#x27;s own sort of domain, separate from the domain of physical existence.<br/>Nicky: Bleh, next you&#x27;re going to tell me that you believe in cartesian dualism.<br/>Dee: Bleh, next you&#x27;re going to tell me that math is arbitrary and people can build rockets that work however they feel like.<br/><em>The two never talked again. Dee, remembering this conversation in great detail, went on the become a commited Platonist and write many articles trying to defend this complex philosophical view</em></blockquote><p>To help draw out the point I&#x27;m trying to make, here&#x27;s an alternative history of this conversation.</p><blockquote>Nicky: Hey Dee, you got any views on mathematical platonism?<br/>Dee: What&#x27;s that?<br/>Nicky: It&#x27;s the idea that mathematical objects exist in reality, but seperate from physical reality. Physical reality defines what&#x27;s true about the world we live in, and mathematical reality defines what&#x27;s true about math objects.<br/>Dee: Hmmmm, I&#x27;m not sure. I mean, that seems like it would explain why math is so certain and precise, but it also feels weird to posit a whole new fundamental element of reality, and I&#x27;m partial to materialism. I&#x27;ll have to think about this.</blockquote><p>Dee didn&#x27;t really care about or have well formed beliefs about whether or not mathematical Platonism is true. Yet a conversation happened in such a way that left Dee defending Platonism. That seems a little weird, let&#x27;s look at what happened.</p><ol><li>When Dee hear&#x27;s &quot;social construct&quot; she thinks about things being arbitrary and not having to do with reality. When she things of things that are &quot;real&quot; she thinkgs of useful and true things. </li><li>Dee thinks math is useful and true and says it&#x27;s &quot;real&quot;.</li><li>Nicky here&#x27;s &quot;real&quot; and things about things that can be located in time and space. When she thinks of &quot;social construct&quot; she thinks of things that are in people&#x27;s heads.</li><li>Nicky says math isn&#x27;t real and brings up the point about location</li><li>Dee thinks that if she can&#x27;t call math &quot;real&quot;, she doesn&#x27;t get to consider it useful and true.</li><li>Dee agrees that math doesn&#x27;t have a location in time in space, and that this notion is relevant to calling something &quot;real&quot;.</li><li>Dee extends the shared definition of &quot;real&quot; to include &quot;existing in physical reality or some other kind of reality&quot;</li><li>Dee claims mathematical Platonism.</li><li>Nicky implicitly accepts the extension of the definition of &quot;real&quot;</li><li>Nicky explicitly argues against the claim of math platonism</li></ol><p>5 is the crux of the issue.</p><p>Dee felt like she weren&#x27;t allowed to consider math to be useful and certain unless they were able to say it was &quot;real&quot;. If you&#x27;re thinking about the mind map model of meaning, this is trivially wrong. The word &quot;real&quot; can be linked to all sorts of concepts and criterion that don&#x27;t have to always come in a package. Math can be useful and certain, even if it doesn&#x27;t have a physical location in space and time. No problem.</p><p>But if you aren&#x27;t thinking about the mind map model, are are just inside the algorithm, the word &quot;real&quot; does not feel like a pointer connected to other concepts. It &quot;feels&quot; like those concepts. And not getting to use the word &quot;real&quot; feels like not getting to use those concepts.</p><p>The second really interesting part of this conversation is points 6-9. </p><p>In an effort to get to use the word real, despite Nicky. Dee implicitly claimed, &quot;Being real doesn&#x27;t have to mean existing somehwere in physical reality. It can also mean existing somewhere in another kind of reality&quot;.</p><p>Nicky implicitly accepted, &quot;If there was another kind of reality, yes, I would consider something that existed in it to be real. But I will now argue that I don&#x27;t think there is this other kind of reality.&quot;</p><p>So Dee goes to alter the shared meaning of the word &quot;real&quot; and also make another claim about that extended definition applying to math. This extension was implicitly accepted, and the argument turned to being about that claim.</p><p>in our first dialogue, ALbert and Barry were paying a lot of attention to how they were defining words, and even explicitly argued about it. Nicky and Dee also talked and moved around definitions, but this all happened IMPLICITLY.</p><p>When you are stuck, it&#x27;s common to go &quot;I&#x27;m both making an extension to the definition, and making a claim about X that allows the think I want to be in definition.&quot;</p><p>&quot;I agree that if X, then Y is word, but I don&#x27;t agree with X&quot;</p><p>But from the outside, it&#x27;s a seamless switch where the original content was lost and now we are arguing about X. You&#x27;ve lost track of what you cared about when you started the talking.</p><p>Once you are in a mental state of &quot;What can I do to make sure that I get to apply this word to this concept&quot; (&quot;real&quot; to math), weird shit can ensue. In our case, Nicky settles into taking a Platonist view of mathematics. That itself is not a bad thing. You can be a Platonist if you want. Even if Platonism wasn&#x27;t true, it&#x27;s not a given that thinking it&#x27;s true is a mistake. The problem is now Nicky has tied the claim of mathematical Platonism to the claim of maths usefulness and certainty. It&#x27;s unlikely that Nicky will ever come to believe math is not useful or certain, so she has accidentally come to hold a belief about Platonism that won&#x27;t be changed, and might not be warranted.</p><p>I think a lot of beliefs get formed like this over time.</p><p>I have one friend that I argue with a lot, and I&#x27;m constantly accidentally forming and defending stances I don&#x27;t actually care about because in the moment I think it is necessary to prove a point i actually do care about. We&#x27;re getting better though.</p>",Hazard,hazard,Hazard,
YvWfbLunzhiFm77GG,Words Aren't Type Safe,words-aren-t-type-safe,https://www.lesswrong.com/posts/YvWfbLunzhiFm77GG/words-aren-t-type-safe,2019-06-19T20:34:23.699Z,22,11,12,False,False,,"<p><em>This post is part of my</em> <em><a href=""https://www.lesswrong.com/s/pKxyvLyhEFxHgogdD"">Hazardous Guide To Rationality.</a></em> <em>I don&#x27;t expect this to be new or exciting to frequent LW people, and I would appreciate comments and feedback in light of intents for the sequence, as outlined in the above link.</em></p><p> <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\int x^2(a^2-x^2)^{1/2}dx""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo"" style=""padding-right: 0.138em;""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">∫</span></span><span class=""mjx-msubsup MJXc-space1""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> </p><p>Student: Hmmm, let&#x27;s see if I can remember how to integrate. So I know the x is a variable that I&#x27;m trying to integrate over. What&#x27;s that &quot;a&quot; though? I remember it being called a constant. But what&#x27;s that?</p><p>Tutor: A constant is just some number. It could be anything.</p><p>Student: Cool! </p><p><em>student disappears for a while and comes back with the following</em></p><p> <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\int x^2(a^2-x^2)^{1/2}dx = a""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo"" style=""padding-right: 0.138em;""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.593em; padding-bottom: 0.593em;"">∫</span></span><span class=""mjx-msubsup MJXc-space1""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span></span></span></span></span> </p><p>Student: I did it!</p><p>Tutor: The fuc... um, how about you walk me through how you got that?</p><p>Curious. Here&#x27;s another tale of confusion:</p><blockquote>Once upon a time — specifically, 1976 — there was an AI named TALE-SPIN. This AI told <a href=""https://en.wikipedia.org/wiki/Interactive_storytelling"">stories</a> by inferring how characters would respond to problems from background knowledge about the characters&#x27; traits. One day, TALE-SPIN constructed a most peculiar tale.</blockquote><blockquote><br/> Henry Ant was thirsty. He walked over to the river bank where his good friend Bill Bird was sitting.  Henry slipped and fell in the river. Gravity drowned.</blockquote><blockquote><br/>Since Henry fell in the river near his friend Bill, TALE-SPIN concluded that Bill rescued Henry. But for Henry to fall in the river, gravity must have pulled Henry. Which means gravity must have been in the river. TALE-SPIN had never been told that gravity knows how to swim; and TALE-SPIN had never been told that gravity has any friends. So gravity drowned.</blockquote><blockquote><br/>TALE-SPIN had previously been programmed to understand involuntary motion in the case of characters being pulled or carried by other characters — like Bill rescuing Henry. So it was programmed to understand &#x27;character X fell to place Y&#x27; as &#x27;gravity moves X to Y&#x27;, as though gravity were a character in the story.<a href=""https://www.lesswrong.com/posts/3WuAjWMtxQwTxr2Qn/bridge-collapse-reductionism-as-engineering-problem#footnote1"">1</a></blockquote><blockquote><br/>For us, the hypothesis &#x27;gravity drowned&#x27; has low prior probability because we know gravity isn&#x27;t the <em>type</em> of thing that swims or breathes or makes friends. We want agents to seriously consider whether the law of gravity pulls down rocks; we don&#x27;t want agents to seriously consider whether the law of gravity pulls down the law of electromagnetism. We <a href=""https://www.lesswrong.com/lw/jr/how_to_convince_me_that_2_2_3/"">may not want</a> an AI to assign <a href=""https://www.lesswrong.com/lw/mp/0_and_1_are_not_probabilities/""><em>zero</em> probability</a> to &#x27;gravity drowned&#x27;, but we at least want it to neglect the possibility as Ridiculous-By-Default.</blockquote><p>Computer Science has a notion of &quot;type safety&quot;. In a given language, there are different &quot;types&quot; of things. Any operation you can do also specifies what types it&#x27;s allowed to act on. In python <code>1+1</code> is allowed, but <code>1 + &quot;hello&quot;</code> isn&#x27;t. If you try to execute the second chunk you get a &quot;type error&quot; because &quot;+&quot; is the type of thing that expects two integers, and &quot;hello&quot; is a string, not an integer. A language is type safe to the degree that it catches and warns you of type errors.</p><p>Human language is <strong>not</strong> type safe. Saying that gravity drowned is completely valid English, and also isn&#x27;t how reality works. The students math derivation was also completely valid manipulation of English sentences, but invalid calculus. Human language is not completely detached from reality; it wouldn&#x27;t be useful if it wasn&#x27;t. And still, it is not a given that valid English sentences form valid conclusions about the world.</p><p>Here&#x27;s a look at how the student did their calculus problem:</p><ol><li>&quot;a&quot; is a constant</li><li>A constant is something that could be anything.</li><li>If something could be anything, it could be &quot;x^2&quot;</li><li><em>cancellation</em></li><li>Integral of 0 is C</li><li>C is a constant</li><li>&quot;a&quot; is a constant</li><li>C is &quot;a&quot;</li></ol><p>Almost none of us would make the mistake TALE-SPIN did about gravity. And if you know calculus, you&#x27;d probably never make the mistake the student did. But if you <em>don&#x27;t know calculus</em>, it&#x27;s not readily apparent that the derivation is false. </p><p>Human language is incredibly overloaded. Even in a domain like math, which does in fact invent a shit ton of it&#x27;s own words, most of the words on the Wikipedia page for &quot;category theory&quot; are common English words. </p><p>Any given words that you are likely to use to refer to a specific concept are words that also have all sorts of other meanings. If the person parsing a sentence that contains <strong>words they are familiar with</strong>, but <strong>concepts they aren&#x27;t familiar with</strong>, is inclined to try and understand the sentence in terms of the concepts they are familiar with. This can produce obvious nonsense that one immediately rejects and concludes they don&#x27;t know what the sentence means. But in the case of TALE-SPIN, there was no meta level sanity check, and it spits out &quot;Gravity drowned&quot;. In the case of the student, their false reasoning still lead to the type of thing that is allowed to be an answer (if they had concluded the integral equaled &quot;walrus&quot;, they might have been given pause). That plus their lack of calculus knowledge meant that they didn&#x27;t get any red flags while doing their operation</p><p>It is entirely possible to see a thing in the world, use a certain word for it, and hop around a path of twisted false syllogisms, and produce a conclusion that&#x27;s completely opposed to the reality of the original object.</p><p>Be careful, and remember TALE-SPIN</p>",Hazard,hazard,Hazard,
JqQq2HXFpjJRWub2o,Arguing Definitions,arguing-definitions,https://www.lesswrong.com/posts/JqQq2HXFpjJRWub2o/arguing-definitions,2019-06-19T20:29:44.323Z,12,6,1,False,False,,"<p><em>This post is part of my</em> <em><a href=""https://www.lesswrong.com/s/pKxyvLyhEFxHgogdD"">Hazardous Guide To Rationality.</a></em> <em>I don&#x27;t expect this to be new or exciting to frequent LW people, and I would appreciate comments and feedback in light of intents for the sequence, as outlined in the above link.</em></p><p>Here&#x27;s a <a href=""https://www.readthesequences.com/Disputing-Definitions"">dialogue </a>straight out of <a href=""https://www.readthesequences.com/A-Humans-Guide-To-Words-Sequence"">A Human&#x27;s Guide to Words</a> (highly recommended, but then if you&#x27;d just take my recommendation and read the sequences, I wouldn&#x27;t be writing this would I?)</p><blockquote><em>If a tree falls in the forest, and no one hears it, does it make a sound?</em><br/>Albert: Of course it does. What kind of silly question is that? Every time I’ve listened to a tree fall, it made a sound, so I’ll guess that other trees falling also make sounds. I don’t believe the world changes around when I’m not looking.<br/>Barry: Wait a minute. If no one hears it, how can it be a sound?<br/>Albert: What do you mean, there’s no sound? The tree’s roots snap, the trunk comes crashing down and hits the ground. This generates vibrations that travel through the ground and the air. That’s where the energy of the fall goes, into heat and sound. Are you saying that if people leave the forest, the tree violates Conservation of Energy?<br/>Barry: But no one hears anything. If there are no humans in the forest, or, for the sake of argument, anything else with a complex nervous system capable of ‘hearing,’ then no one hears a sound.<br/>Albert: This is the dumbest argument I’ve ever been in. You’re a niddlewicking fallumphing pickleplumber.<br/>Barry: Yeah? Well, you look like your face caught on fire and someone put it out with a shovel.<br/>Albert: The tree produces acoustic vibrations. By definition, that is a sound.<br/>Barry: No one hears anything. By definition, that is not a sound.<br/>Albert: My computer’s microphone can record a sound without anyone being around to hear it, store it as a file, and it’s called a ‘sound file.’ And what’s stored in the file is the pattern of vibrations in air, not the pattern of neural firings in anyone’s brain. ‘Sound’ means a pattern of vibrations.<br/>Barry: Oh, yeah? Let’s just see if the dictionary agrees with you.<br/>Albert: Hah! Definition 2c in Merriam-Webster: ‘Sound: Mechanical radiant energy that is transmitted by longitudinal pressure waves in a material medium (as air).’ <br/>Barry: Hah! Definition 2b in Merriam-Webster: ‘Sound: The sensation perceived by the sense of hearing.’ <br/>Albert and Barry, chorus: Consarned dictionary! This doesn’t help at all!<br/>Albert: Look, suppose that I left a microphone in the forest and recorded the pattern of the acoustic vibrations of the tree falling. If I played that back to someone, they’d call it a ‘sound’! That’s the common usage! Don’t go around making up your own wacky definitions!<br/>Barry: One, I can define a word any way I like so long as I use it consistently. Two, the meaning I gave was in the dictionary. Three, who gave you the right to decide what is or isn’t common usage?</blockquote><p>In this conversation, the only thing that Albert and Barry disagree about is how to use the word &quot;sound&quot;. It even seems likely that neither of them previously cared about what the meanings should be allowed to attach to the word &quot;sound&quot;.</p><ol><li>The word sound activates different concepts in Barry and Albert&#x27;s brains.</li><li>Barry and Albert don&#x27;t realize that they are thinking about different things when they say &quot;sound&quot; and thus think they have a disagreement.</li><li>They begin to argue and get tied to their positions.</li><li>By the time they figure out they meant different things by the word &quot;sound&quot;, insults have been exchanged, egos have been bruised, and they are both set on trying to win the argument.</li><li>They change tactics to arguing that the other was <em>wrong</em> to use a word a certain way.</li><li>No one is happy.</li></ol><p>2 and 4 are the key points of this dialogue. Let&#x27;s look at 2. I&#x27;ve seen some arguments where no one <em>ever even realized they weren&#x27;t talking about the same thing</em>. There is a whole art to figuring out if you and another person are thinking about the same think (or more accurately, thinking about similar enough things for the purpose of your conversation). I think I&#x27;ll have a post on that later, but hopefully just remembering that this sort of confusion is possible will cause a little thought in the back of your head next time you are in a disagreement with someone.</p><p>I picked this dialogue to because it is so clear that the characters don&#x27;t have any disagreement besides the one they faulted into about definitions.</p><p>On 4. There are times where it is very useful to argue about how to define a word, or how to carve a boundary in thing-space. You might think that using a certain word for a certain concept is likely to make a lot of people think of the wrong thing. You might be working in a very technical domain, and want to use a common word to point to a very specific concept you&#x27;ve created. This is another topic that should have it&#x27;s own post.</p><p>The biggest problem with the fact that this dialogue became an argument about definitions was that neither person really gave a shit about it. If you are arguing about what the definition of a word <em>is</em>, you&#x27;ve forgotten that meaning controls words, not words to meaning. See pre req skill. If you are arguing about what a definition <em>should be</em>/ how you and your convo partner <em>should</em> be using a word, by bet is that you are in the tail end of an argument that never existed or was already resolved, yet you still feel like you&#x27;re in an argument, and are grasping at straws.</p><p>If someone uses the &quot;wrong&quot; meaning of a word when making an argument, hitting them with a dictionary does not go back in time, alter what they were thinking when they made the argument, and cause them to be wrong about their argument.</p><h4><strong>Summary</strong></h4><ul><li>Arguing a definition has no power over reality.</li><li>Arguing about what a definition should be is rarely what you cared about when you started the conversation.</li></ul><p><strong>Don&#x27;t focus on the words, focus on the people</strong>.</p>",Hazard,hazard,Hazard,
ZvjYRmkTfWxhTXCaT,LW2.0: Technology Platform for Intellectual Progress,lw2-0-technology-platform-for-intellectual-progress-1,https://www.lesswrong.com/posts/ZvjYRmkTfWxhTXCaT/lw2-0-technology-platform-for-intellectual-progress-1,2019-06-19T20:25:20.228Z,31,8,5,True,False,,"<p>This post presents one lens I (Ruby) use to think about LessWrong 2.0 and what we’re trying to accomplish. While it does not capture everything important, it does capture much and explains a little how our disparate-seeming projects can combine into a single coherent vision.</p><p>I describe a complimentary lens in <a href=""https://www.lesswrong.com/posts/nBcvEFXw4Yoz7rSDk/lesswrong-2-0-community-culture-and-intellectual-progress"">LessWrong 2.0: Community, Culture, &amp; Intellectual Progress</a><em>.</em></p><p>(While the <a href=""https://www.lesswrong.com/about"">stated purpose</a> of LessWrong is to be a place to learn and apply rationality, as with any minimally specified goal, we could go about pursuing this goal in multiple ways. In practice, myself and other members of the LessWrong team care about <em>intellectual progress, truth, existential risk, and the far-future </em>and these broader goals drive our visions and choices for LessWrong.)</p><h1>Our greatest challenges are intellectual problems</h1><p>It is an understatement to say that we can aspire to the world being better than it is today. There are myriad forms of unnecessary suffering in the world, there are the utopias we could inhabit, and, directly, there is an all too real chance that our civilization might wipe itself out in the next few years. </p><p>Whether we do it in a state of terror, excitement, or both - there is work to be done. We <em>can</em> improve the odds of good outcomes. Yet the challenge which faces us isn’t rolling up our sleeves and “putting in hard work” - we’re motivated - it’s that we need to figure out what exactly it is we need to do. Our problems are not of <em>doing, </em>but <em>knowing. </em></p><p>Which interventions for global poverty are most cost-effective? What is the likelihood of a deadly pandemic or nuclear war? How does one influence government? What policies should we want governments to adopt? Is it better for me to earn-to-give or do direct work? How do we have a healthy, functioning community? How do we cooperate from groups small to large? How does one build a safe AGI? Will AGI takeoffs be fast or slow? How do we think and reason better? Which questions are the most important to answer? And so on, and so on, and so on.</p><p>One of our greatest challenge is answering the questions before us. One of our greatest needs is to make more intellectual progress: to understand the world, to understand ourselves, to know how to think, and to figure out what is true.</p><h1>Technologies for intellectual progress</h1><p>While humans have been improving our understanding the world for hundreds of thousands of years, our rate of progress has increased each time we evented new technologies which facilitate even more intellectual progress.</p><p>Such technologies for intellectual progress include:<em> speech, writing, libraries, encyclopedias, libraries, microscopes, lectures, conferences, schools, universities, the scientific method, statistics, peer review, Double Crux, the invention of logic, the identification of logical fallacies, whiteboards and blackboards, flow charts, researching funding structures, spreadsheets, typewriters, the Internet, search engines, blogging, Wikipedia, StackExcange and Quora, collaborative editing such as Google Docs, the field of heuristics and biases, epistemology, rationality, and so on. </em></p><p>I am using the term <em>technology </em>broadly to include all things which did not exist naturally which we humans designed and implement to serve a function, including ideas, techniques, and social structures. What unifies the above list is that each item helps us to organize or share our knowledge. By building on the ideas of others and thinking collectively, we accomplish far more than we ever could alone. Each of the above, perhaps among other things, has been a technology which increased humanity’s rate of intellectual progress.</p><h1>LessWrong as a technology platform for intellectual progress</h1><p>I see no absolutely no reason to think that the above list of technologies for intellectual progress is anywhere near complete. There may even be relatively low-hanging fruit lying around that hasn’t been picked up since the invention of the Internet a mere thirty years ago. For example, the academic journal system, while now online, is mostly a digitized form of the pre-Internet systems not taking advantage of all the new properties of the internet such as effectively free and instantaneous distribution of material.</p><p>My understanding of the vision for LessWrong 2.0 is that we are a team who builds new technologies for intellectual progress and that LessWrong 2.0 is the technology platform upon which we can build these technologies. </p><h1>Which technologies might LessWrong 2.0 build?</h1><p>Having stated the vision for LessWrong 2.0 is to be a technology platform, it’s worth listing <strong>examples</strong> of thing we might build (or already are).</p><h2>Open Questions Research Platform</h2><p><em>In December 2018, we launched a beta of our Open Questions platform. Click to see <a href=""https://www.lesswrong.com/questions"">current questions</a>.</em></p><ul><li>Make the goal of answering important questions explicit on LessWrong.</li><li>Provide affordance for asking, for knowing, and for providing.</li><li>Provide infrastructure to coordinate on what the most important problems are.</li><li>Provide incentives to spend days, weeks, or months researching answers to hard questions.</li><li>Lowering the barriers to contributing to the community’s research output, e.g. you don’t have to be hired to an organization to contribute.</li><li>Building a communal repository of knowledge upon which everyone can build.</li><li>Applying our community’s interests, techniques, culture, and truth-seeking commitment to uniquely high-quality research on super tricky problems.</li></ul><p>In the opening of this document, I asserted that humanity’s greatest challenges are intellectual problems, that is, knowledge we need to build and questions we need to answer. It makes sense that we should make explicit that we want to ask, prioritize, and answer important questions on the site. And to further make it explicit that we are aiming to build explicit community of people who work to answer these important questions.</p><p>The core functionality of LessWrong to date has been people making posts and commenting on them. Authors write posts which are the intersection of their knowledge and community’s overall interests, perhaps there will be something of a theme at times. We haven’t had an obvious affordance that you can specifically request someone else generate or share content about a question you have. We haven’t had a way that people can easily see which questions other people have and which they could help with. And we overall haven’t had a way for the community to coordinate on which questions are most important.</p><p>As part of the platform, we can build new incentive systems to make it worth people’s time to spend days or weeks researching answers to hard questions.</p><p>The platform could provide an accessible way for new people to starting contributing to the community’s research agenda. Getting hired at a research org is very difficult, the platform could provide a pathway for far more people to contribute, especially if we combine it with a research talent training pipeline.</p><p>By the platform being online and public, it would begin to build a repository of shared knowledge upon which others can continue to build. Humanity’s knowledge comes from our sharing knowledge and building upon each other’s work. The more we can do that, the better.</p><p>Last, Open Questions is a way to turn our community’s specialized interests (AI, AI safety, rationality, self-improvement, existential risk, etc.), our culture, techniques and tools (Bayesian epistemology, quantitative mindset, statistical literacy, etc.), and truth-seeking commitment to uniquely high-quality research on super tricky problems.</p><p>See <em><a href=""https://www.lesswrong.com/posts/rFcbBbpK9yBSEFzZo/list-of-q-and-a-assumptions-and-uncertainties-lw2-0-internal#rWJHRhExJLXHuJgYK"">this comment thread</a></em> for a detailed list of reasons why it’s worth creating a new questions platform when others already exist.</p><h2>Marketplace for Intellectual Labor</h2><ul><li>Standard benefits of a market: matches up people want to hire work with people who want to perform work, therefore causing more valuable work to happen.</li></ul><p>Possible advantages over Open Question:</p><ul><li><em>Marketplace</em> is a standard thing people are used to, easier to drive adoption than a very novel questions platform.</li><li>Potentially reduces uncertainty around payments/incentives.</li><li>Can help with trust.</li><li>Can provide more privacy (possibly a good thing, possibly not).</li><li>Less of a two-sided marketplace challenge.</li><li>Diversifies the range of work which can be traded, e.g. hiring, proofreading, lit-reviews, writing code.</li></ul><p>A related idea to Open Questions (especially if people are paid for answers) is that of a general marketplace place where people can sell and buy intellectual labor including tasks like tutoring, proofreading essays, literature reviews, writing code, or full-blown research. </p><p>It might look like TaskRabbit or Craigslist except specialized for intellectual labor. The idea being that this would cause more valuable work to happen than otherwise would, and progress on important things to made.</p><p>A more detailed description of the Marketplace idea can be found in my document, <em><a href=""https://www.lesswrong.com/posts/LEAwacyRr3CZjdoyW/review-of-q-and-a-lw2-0-internal-document"">Review of Q&amp;A</a>.</em></p><h2>Talent Pipeline for Research</h2><ul><li>A requirement for intellectual research is people capable of do it.</li><li>An adequate supply of skill researchers is especially required for an Open Questions research platform.</li><li>LessWrong could potentially build expertise in doing good research and training others to do it.</li><li>We could integrate our trainees into the Open Questions platform.</li></ul><p>A requirement for intellectual progress is that there are people capable of doing it, so generally we want more people capable of doing good research. </p><p>It may especially be a requirement for the Open Questions Platform to succeed. One of my primary uncertainties about whether Open Questions can work is whether we will have enough people willing and able to conduct research to answer questions. This leads to the idea that LessWrong might want to set up a training pipeline that helps people who want to become good researchers train up. We could build up expertise in both good research process and in teaching that process to people. We can offer to integrate our trainees into the LessWrong Open Question platform.</p><p>A research training pipeline might be a non-standard definition of “technology”, but I think it still counts, and entirely fits within the overall frame of LessWrong. In <em>LW2.0: Culture, Community, and Intellectual Progress, </em>I list <em>training</em> as one of LessWrong’s core activities.</p><h2>Collaborative Documents a la Google Docs</h2><ul><li>Communications technologies are powerful and important causing more and different work to accomplished than would be otherwise.</li><li>Google Docs represents a powerful new technology, and we could improve on it even further with our own version of collaborative documents..</li><li>I expect this to result in significant gains to research productivity.</li></ul><p>There have been successive generations of technology which make the generation and communication of ideas easier. One lineage might be: writing, the typewriter, Microsoft Word, email, Google Docs. Each has made communication easier and more efficient. Speed, legibility, ease of distribution, and ease of editing have made each successive technology more powerful.</p><p>I would argue that sometimes the efficiency gains with these technologies are so significant that they enable qualitatively new ways to communicate. </p><p>With Google Docs, multiple collaborators can access the same document (synchronously or asynchronously), this document is kept in sync, collaborators can make or suggest edits, and they comment directly on specific text. Consider how this was not really possible at all with Microsoft Word plus email attachments. You might at most send a document for feedback from one person, if you’d made edits in the meantime, you’d have merge them with their revisions. If you sent the document to two people via email attachment, they wouldn’t see each others feedback. And so on.</p><p>Google Documents, though we might take it for granted by now, was a significant improvement in how we can collaborate. It is generally useful, but also especially useful to do those doing generative intellectual work together who can share, collaborate, and get feedback in a way not possible with previous technologies.</p><p>Yet as good as Google Docs is, it could be better. The small things add up. You can comment on any text, but it is difficult to have any substantial discussion in comment chain due to length restrictions and how the comment chains are displayed. There isn’t a built in comment section for the whole document rather than specific text. Support for footnotes is limited. There isn’t support for Latex. This is only a starting list of optimizations which could make Google Docs an even better tool for research and general intellectual progress.</p><p>There could be further benefits from having this tool integrated with LessWrong. Easy and immediate publishing of documents to posts, access to a community of collaborators and feedback givers. Through the tool encouraging people to do their work on LessWrong, we could become the archive and repository for the community’s intellectual output.</p><h2>Prediction Markets</h2><ul><li>Making predictions is a core rationality skill.</li><li>Prediction markets aggregate individual opinions to get an even better overall prediction.</li><li>LessWrong could build or partner with an existing prediction market project.</li></ul><p>First, making good predictions is a core rationality skill, and one of the best ways to ensure you make good predictions is to have something riding on them, e.g. a bet. Second, aggregating the (financially-backed) predictions of multiple people is often an excellent generate overall predictions that are better than those of individuals.</p><p>Given the above, we could imagine that LessWrong, as a technology platform for intellectual progress, should be integrated with a prediction market and associated community of forecasters. There have been many past attempts at prediction markets, some existing ones, and few more nascent ones. I don’t know if LessWrong should set up its own new version, or perhaps seek to partner with an existing project.</p><p>I haven’t thought through this idea much, but it’s an idea the team has had.</p>",Ruby,ruby,Ruby,
nBcvEFXw4Yoz7rSDk,"LW2.0: Community, Culture, and Intellectual Progress",lw2-0-community-culture-and-intellectual-progress-1,https://www.lesswrong.com/posts/nBcvEFXw4Yoz7rSDk/lw2-0-community-culture-and-intellectual-progress-1,2019-06-19T20:25:08.682Z,27,6,2,True,False,,"<p>This post presents one lens I (Ruby) use to think about LessWrong 2.0 and what we’re trying to accomplish. While it does not capture everything important, it does capture much and explains a little how our disparate-seeming projects can combine into a single coherent vision.</p><p>I describe a complimentary lens in <a href=""https://www.lesswrong.com/posts/ZvjYRmkTfWxhTXCaT/lesswrong-2-0-technology-platform-for-intellectual-progress"">LessWrong 2.0: Technology Platform for Intellectual Progress</a>.</p><p>(While the <a href=""https://www.lesswrong.com/about"">stated purpose</a> of LessWrong is to be a place to learn and apply rationality, as with any minimally specified goal, we could go about pursuing this goal in multiple ways. In practice, myself and other members of the LessWrong team care about <em>intellectual progress, truth, existential risk, and the far-future</em> and these broader goals drive our visions and choices for LessWrong.)</p><h1>A Goal for LessWrong</h1><p>Here is one goal that it might make sense to me for LessWrong to have:</p><p> <strong><em>A goal of LessWrong is to grow and sustain a community of aligned* members who are well-trained and well-equipped with the right tools and community infrastructure to make progress on the biggest problems facing humanity, with a special focus on the intellectual problems.</em></strong></p><p><em>*</em>sharing our broad values of improving the world, ensuring the long-term future is good, etc.</p><p>Things not well-expressed in this goal: </p><ul><li>LessWrong’s core focus on rationality and believing true things.</li><li>What LessWrong aims to provide users.</li></ul><p>(These are better expressed in the <a href=""https://www.lesswrong.com/about"">About/Welcome page</a>.)</p><p>I want to point out that while the above might be a goal of the LessWrong team, that doesn’t mean it has to be a goal of our users. I wholeheartedly welcome users who come to LessWrong for their own purposes such as improving their personal rationality, learning interesting things, getting feedback on their ideas, being entertained by stimulating ideas, or participating socially in a community they like. </p><h1>My RTC-P Framework</h1><p>The reason I like the goal expressed above is that provides unity to a wide range of activities we devote resources to. I like to group those activities into four overarching categories.</p><p><strong>Recruitment:</strong> attracting new aligned and capable members to our community (and creating a funnel into EA orgs and projects)</p><p><strong>Training:</strong> providing means both new and existing members improve their skills, knowledge, and effectiveness.</p><p><strong>Community/Culture:</strong> we aim to improve community health and flourishing via means such encouraging good epistemic norms, affordance to interact, e.g. meetups and conferences.</p><p><strong>[Intellectual] Progress:</strong> we work to provide the LW platform and other tools that assist community members to contribute progress on the challenging intellectual problems we face.</p><h2>Recruitment</h2><p>LessWrong has a non-trivial presence on the Internet. In the last 12 months, LessWrong has seen on average over 100k visitors each month [1]. Admittedly, many of these arrive due to low-relevance Google searches, however over 25k each month are navigating directly to the lesswrong.com domain. Depending on the month, several hundred to several thousand arrive each from SlateStarCodex and Hacker News. There are several hundred to a thousand unique pageviews of the openings posts of Rationality: A-Z each month. Around 500 more views of the <a href=""https://www.lesswrong.com/hpmor/chapter-1-a-day-of-very-low-probability"">first chapter</a> of HPMOR.</p><p>The meaning of this is that a relatively large number of people are probably being exposed to the ideas of the LessWrong, Rationality, and Effective Altruism communities for the first time when they encounter LessWrong. LessWrong has the opportunity to spread its ideas, but more importantly, there is scope here for us to onboard new capable and aligned people in our community. </p><p>The team has recently been building things to help new visitors have an experience conducive to becoming a member of our community. The homepage has been redesigned to present <a href=""https://www.lesswrong.com/posts/PfceWjEqdRDvGkKZ8/recommendation-features-on-lesswrong-1"">recommendations</a> of our core readings and other top posts to new users. We’ve also written a new <a href=""https://www.lesswrong.com/about"">welcome post</a> and <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1"">FAQ covering</a> how the site works, what it’s about, and how to get up to speed.</p><p>Something else we might do is start posing content outside of LessWrong to make people aware of what is on the site. We could create a “newsletter” collection of content (a mix of the best recent and classic posts) and share this via a Facebook page, relevant places on Reddit, Twitter, etc. This might also help us draw back some past users who dropped off during the great decline of 2015-2016. </p><p>Of course, recruitment doesn’t consist solely of your first moments being exposed online. There is a “funnel” as you progress through getting up to speed on the community’s knowledge and culture, your first experiences engaging with people on the site (your first comments and posts), attending in-person meetups (these were significant for me), and so on. These are all steps by which someone becomes part of our band trying to do good things.</p><p>Indeed, if want to be a community of people trying to do good, important things, then it’s important we have an apparatus for having new people join. Recruitment. (I have the belief that if you are not growing, at least somewhat, then you are shrinking.) It’s not clear that we need to grow a lot, even 2x-5x might be sufficient. Certainly we should not grow at the expense of our culture and values. </p><p>Fortunately, LessWrong is a nonprofit which helps with the incentives.</p><h2>Training</h2><p>LessWrong was not build to be solely a site for entertainment, recreation, or passive reading. The goal was always to improve: to think better, know more, and accomplish more. <a href=""https://www.lesswrong.com/posts/DoLQN5ryZ9XkZjq5h/tsuyoku-naritai-i-want-to-become-stronger"">Tsuyoku Naritai</a> is an emblematic post. The concept of a <em><a href=""https://www.lesswrong.com/posts/teaxCFgtmCQ3E9fy8/the-martial-art-of-rationality"">rationality dojo</a></em> caught on. The goal is to do better individually and collectively.</p><p>LessWrong’s archive of rationality posts constitute considerable training material. LessWrong has over 23k posts with non-negative karma scores. Noteworthy authors include <a href=""https://www.lesswrong.com/users/eliezer_yudkowsky"">Eliezer_Yudkowsky</a> (1021 posts) , <a href=""https://www.lesswrong.com/users/yvain"">Scott Alexander</a> (230), <a href=""https://www.lesswrong.com/users/lukeprog"">Lukeprog</a> (416), <a href=""https://www.lesswrong.com/users/kaj_sotala"">Kaj_Sotala</a> (221), <a href=""https://www.lesswrong.com/users/annasalamon"">AnnaSalamon</a> (68), <a href=""https://www.lesswrong.com/users/so8res"">So8res</a> (51), <a href=""https://www.lesswrong.com/users/academian"">Academian</a> (37), and many others.</p><p>Effort has been invested to create easily accessible <a href=""https://www.lesswrong.com/library"">sequences of posts</a> with convenience features such as Previous/Next buttons and <a href=""https://www.lesswrong.com/posts/PfceWjEqdRDvGkKZ8/recommendation-features-on-lesswrong-1"">Continue Reading</a> suggestions on the homepage. The recently launched <a href=""https://www.lesswrong.com/posts/PfceWjEqdRDvGkKZ8/recommendation-features-on-lesswrong-1"">recommendation system</a> suggests to users which posts they might be interested in and benefit from. For example, here are some classics you might have missed:</p><ul><li><a href=""https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic"">Humans are not automatically strategic</a></li><li><a href=""https://www.lesswrong.com/posts/eRohP4gbxuBuhqTbe/attempted-telekinesis"">Attempted Telekinesis</a></li><li><a href=""https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields"">Ugh fields</a></li><li><a href=""https://www.lesswrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess"">Ask and Guess</a></li><li><a href=""https://www.lesswrong.com/posts/64FdKLwmea8MCLWkE/the-neglected-virtue-of-scholarship"">The Neglected Virtue of Scholarship</a></li><li><a href=""https://www.lesswrong.com/posts/ttGbpJQ8shBi8hDhh/checklist-of-rationality-habits"">Checklist of Rationality Habits</a></li></ul><p>You can see the <a href=""https://docs.google.com/spreadsheets/d/1pVquj1wGUYlcKYBTBO1-b29AE2CDUQA4Xh-5W3l7qq8/edit?usp=sharing"">list of the ten most upvoted LessWrong posts for each year 2010-2018</a>. We hope to have users not just reading recent content, but our most valuable content from all time.</p><p>Another idea, costly to implement, but of which the team is fond of is to building LessWrong into a “textbook” where there are exercises which increase comprehension and retention.</p><p>And, although it doesn’t happen on the site, I’d count all the rationality practice that LessWrong members get together and perform at their in-person meetups as part of the training caused by LessWrong. There are 101 posts on LessWrong with <em><a href=""https://www.lesswrong.com/posts/teaxCFgtmCQ3E9fy8/the-martial-art-of-rationality"">dojo</a></em> in the title, marking them as meetups where people intended to get together and practice. Sampling from those posts, these are meetups where people worked on <a href=""https://en.wikipedia.org/wiki/Calibrated_probability_assessment"">calibration</a>, urge propagation, non-violent communication, growth mindset, statistics, Bayesian Reasoning, <a href=""https://arbital.com/p/bayes_rule/?l=1zq"">Intuitive Bayes</a>,<a href=""https://praxtime.com/2014/05/27/ideological-turing-test/""> Ideological Turing Tests</a>, difficult conversations, Hamming prompts, stress, and memory.</p><h2>Community &amp; Culture</h2><p>As online forum, LessWrong naturally forms a community whose members share a culture and exchange ideas. Beyond the online, the LessWrong has caused many in-person, offline communities to exists. In the past year, there were LessWrong meetups in thirty one countries. There have been ~3,576 meetups made on LessWrong (238 of these were SSC or LW/SSC combined meetups). Notably, the Bay Area Rationalist community exists in large part due to LessWrong, even if it is now somewhat separate. Other communities which have historically gained notice are the New York, Seattle, and Melbourne groups. Large area “mega-meetups” have been held in the <a href=""https://www.lesswrong.com/posts/gahx6nY66N4P2kpbG/east-coast-rationalist-megameetup-2018"">US East Coast</a>, <a href=""https://www.lesswrong.com/posts/Jr2WwFWkn769KAGa2/meetup-upper-canada-lw-megameetup-ottawa-toronto-montreal"">Canada</a>, <a href=""https://www.lesswrong.com/posts/cjbWFdT2MXw5bYZ5P/australian-mega-meetup-2014-retrospective"">Australia</a>, and <a href=""https://www.lesswrong.com/posts/gcHXqDry66NphyAEY/european-community-weekend-2018-announcement"">Europe</a>. There is a thriving community rationalist/LessWrong community in <a href=""https://www.lesswrong.com/posts/WmfapdnpFfHWzkdXY/rationalist-community-hub-in-moscow-3-years-retrospective"">Moscow</a>.</p><p>Even with in-person communities already existing, I still see plenty of place for LessWrong to continue to bolster both online and offline community. For the offline world, we could provide support materials <a href=""https://www.lesswrong.com/posts/qMuAazqwJvkvo8teR/how-to-run-a-successful-less-wrong-meetup"">as in the past</a>, funding (as <a href=""https://app.effectivealtruism.org/groups/resources/resources-and-support"">CEA does to EA local groups</a>), further our <a href=""https://www.lesswrong.com/community"">meetup coordination infrastructure</a>, or host LessWrong conferences.</p><p>Culturally, LessWrong is defined by its epistemic norms, focus on truth, and openness to unconventional ideas. The community share a distinctive body of knowledge and set of tools for thinking clearly. The core culture was established by Eliezer’s Sequences shaping the <a href=""https://wiki.lesswrong.com/wiki/Map_and_Territory"">approach to belief, reason, explanation, truth and evidence</a>, the <a href=""https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb"">use of language</a>, and the <a href=""https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"">practice of changing one’s mind</a>. As far as I know, the commitment to clear thinking and good communication present thinking on LessWrong is unparalleled by that on any other public place on the Internet.</p><p>A goal of LessWrong is keeping this culture strong and being wary of any changes which could dilute this most valuable aspect of our community, e.g. promoting growth without ensuring new members are properly inculturated.</p><p>At present, standards are in part being kept high by active and careful moderation. Some may note that the discussion on LessWrong presently more constructive and civil in tone than at times in the past. This is evident when looking at the style of commenter <a href=""https://www.lesswrong.com/posts/7xJiotzeonZaAbgSp/user-gpt2-is-banned"">GPT2</a> compared to those it was conversing with. GPT2, trained on the entire historical comment corpus, has a noticeably more condescending and contrarian tone than the comments typical on modern LessWrong.</p><h2>Intellectual Progress</h2><p>This category is arguably too broad, but that perhaps captures the fact that LessWrong 2.0 is open to quite a wide range of projects in the pursuit of further intellectual progress, (or even just progress, intellectual or otherwise).</p><p>The LessWrong forum with posts, comments, and votes is already a <a href=""https://www.lesswrong.com/posts/ZvjYRmkTfWxhTXCaT/lesswrong-2-0-technology-platform-for-intellectual-progress"">technology for intellectual progress</a> which allows thinkers to share ideas, get feedback, and build upon each others work. The team spends a lot of time thinking about what we could build to help people be more intellectually generative. The team has ongoing debates about what the sections of the site should be (“bringing back something like Main vs Discussion?” “Ah, but the problems!”), whether and how to promote the sharing of unpolished ideas (shortform feeds? these have been gaining in popularity without explicit support), or can we set up our own peer review process. Hearteningly, the goal is always to generate more “good content”, not merely to drive-up content production and activity. Growth, if not exactly feared, is viewed with suspicion - perhaps it will dilute quality. Already some on the team fear that things trend too much towards <em><a href=""https://mindlevelup.wordpress.com/2016/10/28/insight-porn/"">insight porn</a></em> rather than substantive contributions to the intellectual commons.</p><p>It’s worth noting that the LessWrong 2.0 team invests efforts to promote intellectual progress outside of the <em>lesswrong.com </em>domain. The <a href=""https://forum.effectivealtruism.org/"">Effective Altruism Forum</a> runs on the LessWrong codebase (and receives some support from the team). And last year the LessWrong team launched the <a href=""https://www.alignmentforum.org/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq"">AI Alignment Forum</a>: a forum specifically for dedicated AI safety researchers. (It is no secret the LessWrong 2.0 team members especially wish to see progress made on the intellectual problems of AI safety.)</p><p>One of the ideas for increasing intellectual progress which the team has been especially occupied with recently is that of an <a href=""https://www.lesswrong.com/posts/rFcbBbpK9yBSEFzZo/list-of-q-and-a-assumptions-and-uncertainties-lw2-0-internal#kZoEWqYcdg38tMNJN"">Open Questions platform</a>. One of many functions, such a platform would be place where the community coordinates on which problems are most important and creates surface area so more researchers can contribute.</p><p>Other ideas for things the LessWrong 2.0 team could build to drive intellectual progress are: an optimized collaborative tool (like Google Docs, but better); a marketplace for intellectual labor (think Craigslist/TaskRabbit), a prediction market platform, and a researcher training program. I have written more about these ideas in <a href=""https://www.lesswrong.com/posts/ZvjYRmkTfWxhTXCaT/lesswrong-2-0-technology-platform-for-intellectual-progress"">LW2.0: Technology Platform for Intellectual Progress</a>.</p>",Ruby,ruby,Ruby,
DbZDdupuffc4Xgm7H,1hr talk: Intro to AGI safety,1hr-talk-intro-to-agi-safety,https://www.lesswrong.com/posts/DbZDdupuffc4Xgm7H/1hr-talk-intro-to-agi-safety,2019-06-18T21:41:29.371Z,36,14,4,False,False,,"<html><head></head><body><p>This is an hour-long talk (actually ~45 minutes plus questions) that I gave on AGI safety technical research, for an audience of people in STEM but with no prior knowledge of the field. My goal was to cover all the basic questions that someone might have when considering whether to work in the field: motivation, basic principles, a brief survey of the different kinds of things that people in the field are currently working on, and resources for people who want to learn more. I couldn't find an existing talk covering all that, so I wrote my own.</p>
<p>In the interest of having good educational and outreach material available to the community, I’m happy for anyone to copy any or all of this talk. The actual slides are in Powerpoint format <a href=""https://sjbyrnes.com/AIslides/IntroToAGISafetyResearch.pptx"">here</a>, and below you'll find screenshots of the slides along with the transcript.</p>
<p><strong>Abstract:</strong> Sooner or later—no one knows when—we'll be able to make Artificial General Intelligence (AGI) that dramatically outperforms humans at virtually any cognitive task. If, at that point, we are still training AIs the way we do today, our AGIs may accidentally spin out of control, causing catastrophe (even human extinction) for reasons explained by Stuart Russell, Nick Bostrom, and others. We need better paradigms, and a small but growing subfield of CS is working on them. This is an interdisciplinary field involving machine learning, math, cybersecurity, cognitive science, formal verification, logic, interface design, and more. I'll give a summary of motivation (why now?), foundational principles, ongoing work, and how to get involved.</p>
<h1>Opening</h1>
<p><img src=""https://sjbyrnes.com/AIslides/Slide1-2.png"" alt=""""></p>
<p>Thanks for inviting me! I had an easy and uneventful trip to get here, and we all know what that means ... Anyone? ... What it means is that evil robots from the future did <em>not</em> travel back in time to stop me from giving this talk. [Laughter.] And there’s an important lesson here! The lesson is: <em>The Terminator</em> movie is the wrong movie to think about when we talk about safety for advanced AI systems. The right movie is: <em>The Sorcerer’s Apprentice</em>!<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-1"" id=""fnref-MZs6yojF4FNt5T6gi-1"">[1]</a></sup> When the machines become more and more intelligent, they don’t spontaneously become self-aware and evil. Instead, the machines do the exact thing that we tell them to do … but the thing that we tell them to do is not what we actually <em>want</em> them to do! I’ll get back to Mickey Mouse later.</p>
<p><strong>Here’s the outline of this talk.</strong> I’m going to start with some motivation. Artificial General Intelligence (AGI) doesn’t exist yet. So why work on AGI safety today? Then I’ll move on to some foundational principles. These are principles that everyone in AGI safety research understands and agrees on, and they help us understand the nature of the problem, and why it’s a hard problem. Then I’ll go over the active research directions. What are people actually working on today? Finally, I’ll talk about what you can do to help with the problem, because we need all the help we can get.</p>
<h2>Definitions</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide3-4.png"" alt=""""></p>
<p>I’ll start with some <strong>definitions</strong>. <strong>Intelligence</strong> is (more or less) the ability to choose actions to bring about a desired goal, or to do similar sorts of search-and-optimization processes, like theorem-proving or model-building. <strong>Artificial General Intelligence (AGI)</strong> is an AI system that can display intelligent behavior in a wide range of domains and contexts, just as humans can. Humans didn’t evolve an instinct for space travel, but when we wanted to go to the moon, we were able to figure out how to do it. Here’s a little cartoon of an AGI.</p>
<p><img src=""https://sjbyrnes.com/AIslides/3-step-AI.png"" alt="""">
<em>from <a href=""https://intelligence.org/2015/07/27/miris-approach/"">https://intelligence.org/2015/07/27/miris-approach/</a></em></p>
<p>The AGI takes in perceptions of the world, fits it into its world model, then considers possible courses of action, predicts their consequences, and it takes the course of action with the best consequence according to its goal system. Finally, <strong>Superintelligent</strong> describes an AGI that is much better than humans at essentially all important cognitive tasks. Now, an AGI doesn't need to be superintelligent to be extremely powerful and dangerous—it needs to be <em>competent</em> in <em>some</em> cognitive domains, it doesn’t need to be <em>superhuman</em> in <em>all</em> cognitive domains. But still, I do expect that we’ll make superintelligent AGIs at some point in the future, for reasons I’ll talk about shortly.</p>
<p>""Thus the first superintelligent machine is the last invention that man need ever make,"" according to computer pioneer I.J. Good. I mean, everyone here appreciates the transformative impact of technology. Well, imagine how transformative it will be when we can automate the process of invention and technological development itself.</p>
<h1>Motivation</h1>
<p><img src=""https://sjbyrnes.com/AIslides/Slide5-6.png"" alt=""""></p>
<p>OK, AGI doesn’t exist yet. Why work on AGI safety? Why work on it now? The way I see it, there’s five steps. First, we’re going to have AGI sooner or later. Second, we need to prepare for it ahead of time. Third, as we’ll see later in the talk, we can make progress right now. Fourth, the work is time-sensitive and grossly under-invested. So, step five, more people should be working on it today, including us. Let’s go through this one at a time.</p>
<h2>Someday we'll have AGI</h2>
<p>The first step is, <strong>someday, we’ll have AGI</strong>. Well, it’s physically possible. Brains exist. And every time anyone makes even a tiny step towards AGI, they’re showered in money and status, fame and fortune. That includes both hardware and software. I mean, maybe society will collapse, with nuclear winter or whatever, but absent that, I don’t see how we’ll stop making AIs more and more intelligent, until we get all the way to superintelligent AGIs. Don’t bet against human ingenuity.</p>
<h2>We need to prepare ahead of time</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide7-8.png"" alt=""""></p>
<p>Next, <strong>we need to prepare for AGI ahead of time</strong>. The argument here is I think equally compelling, and it goes something like this. If the AGIs’ goals are not exactly the same as humanity’s goals—and you might notice that it’s hard to express The Meaning Of Life as machine code!—and if the AGIs bring to bear greater intelligence for effectively achieving goals, then the future will be steered by the AGIs’ preferences, not ours.</p>
<p>This kind of logic leads people to expect that unimaginably bad outcomes are possible from AGI—up to and including human extinction. Here’s Stuart Russell, professor of computer science at UC Berkeley and author of the leading textbook on AI: <a href=""https://www.technologyreview.com/s/602776/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/"">""Yes, we are worried about the existential risk of artificial intelligence"",</a> where ""existential risk"" means literally human extinction. At the same time, I don’t want you to get the impression that this is an anti-technology or Luddite position here. It’s equally true that unimaginably <em>good</em> outcomes are possible from AGI, up to and including utopia, whatever <em>that</em> is. These are not opposites—in fact, they are two sides of the same coin, and that coin is ""AGI is a powerful technology"". By the same token, a nuclear engineer can say ""It will be catastrophic if we build a nuclear power plant without any idea how to control the chain reaction"", and then in the next breath they can say, ""It will be awesome if we build a nuclear power plant and we <em>do</em> know how to control the chain reaction"".<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-2"" id=""fnref-MZs6yojF4FNt5T6gi-2"">[2]</a></sup> These things are not opposites.</p>
<p><img src=""https://sjbyrnes.com/AIslides/Slide9.png"" alt=""""></p>
<p>As a matter of fact, it seems pretty clear at this point that running AGI code without causing catastrophe is a very hard problem. It’s a hard problem in the way that space probe firmware is a hard problem, in that certain types of bugs are irreversibly catastrophic, and can’t necessarily be discovered by testing.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-3"" id=""fnref-MZs6yojF4FNt5T6gi-3"">[3]</a></sup> It’s a hard problem in the way that cybersecurity is a hard problem, in that the AGI is running superintelligent searches for weird edge cases in how it’s implemented and the environment it’s in. Of course, if we mess up, it could also be like cybersecurity in the more literal sense that, if the AGI doesn’t have exactly the same goals as us, the AGI will be running superintelligent <em>adversarial</em> searches for holes in whatever systems we build to control and constrain it.</p>
<p>Finally, it’s a hard problem in the way that climate change is a hard problem, in that there’s a global-scale coordination problem rife with externalities and moral hazards. So as we’re going towards AGI, researchers who skimp on safety will make progress faster, and they’re going to be the ones getting that money and status, fame and fortune. Also, as we’ll see later, a lot of the ways to make AGIs safer also make them less effective at doing things we want them to do, so everyone with such an AGI will have an incentive to lower the knob on safety bit by bit. Finally, even in the best case that people can ""use"" AGIs far more intelligent than themselves, well it’s a complicated world with many actors with different agendas, and we may wind up radically upending society in ways that nobody chose and nobody wants.</p>
<p>So there’s everything at stake, avoiding catastrophe is a very hard engineering problem, so we’d better prepare ahead of time instead of wandering aimlessly into the era of AGI.</p>
<p>And by the way, ""wandering aimlessly into the era of AGI"" is a very real possibility! After all, evolution wandered aimlessly into the era of generally-intelligent humans. It designed our brains by brute-force trial-and-error. It had no understanding of what it was doing or what the consequences would be. So, again, understanding AGI safety isn’t something that happens automatically on our way to developing AGI. It only happens if we make it happen.</p>
<h2>AGI safety work is time-sensitive</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide10.png"" alt=""""></p>
<p>Next step in the motivation: AGI safety work is time-sensitive. When we think about this, there are three relevant timescales. <strong>The first timescale is, how long do we have until we know how to make AGIs? I’m going to say: Somewhere between 3 and 300 years.</strong> I don’t know any more specifically than that, and neither do you, and neither does <em>anybody</em>, because technological forecasting is hard. Here’s Wilbur Wright: ""I confess that in 1901 I said to my brother Orville that man would not fly for fifty years. Two years later we ourselves made flights."" Remember, Wilbur Wright was at that point the #1 world expert on how to build an airplane, at he got the timeline wildly wrong; what chance does anyone else have?</p>
<p>The experts on television who say that ""We definitely won’t have AGI before 2100"" or whatever the date is—that kind of statement is not based on a validated quantitative scientific theory of AGI forecasting, because there <em>is</em> no validated quantitative scientific theory of AGI forecasting! It’s very tempting to think about what it’s like to use today’s tools, and today’s hardware, and today’s concepts, and to use your own ideas about the problem, and to think about how hard AGI would be for <em>you</em>, and make a prediction on that basis. It’s much more of a mental leap to think about tomorrow’s tools, tomorrow’s hardware, and tomorrow’s insights, and to think about combining the knowledge and insights of many thousands of researchers around the world, some of whom are far smarter than you. We don’t know how many revolutionary insights it will take to get from where we are to AGI, and we don’t know how frequently they will come. We just don’t know.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-4"" id=""fnref-MZs6yojF4FNt5T6gi-4"">[4]</a></sup></p>
<p>Some people try to do this type of forecast based on hardware. As far as I can tell, extremely short timelines, like even within a decade or two, are not definitively ruled out by hardware constraints. I’m not saying it’s guaranteed or even likely, just that it’s not obviously impossible. For one thing, today’s supercomputers are already arguably doing a similar amount of processing as a human brain. Admittedly, people disagree about this in both directions by multiple orders of magnitude,<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-5"" id=""fnref-MZs6yojF4FNt5T6gi-5"">[5]</a></sup> because it’s really comparing apples and oranges; the architectures are wildly different. For example, supercomputers are maybe 100 million times faster but 100 million times less parallel. Next, new chips tailored to neural nets are coming out even in the next few years⁠—for example there’s <a href=""https://www.lightelligence.ai/"">that MIT spin-off with an optical neural processing chip which is supposed to be orders of magnitude faster and lower-power than the status quo</a>. And finally, most importantly, human-brain-equivalent processing power may not be necessary anyway. For one thing, brains are restricted to massively-parallelizable algorithms, whereas supercomputers are not. We all know that a faster and more serial processor can emulate a slower and more parallel processor, but not vice-versa. For another thing, a processor much less powerful than an insect brain—<em>much</em> less powerful!—can play superhuman chess.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-6"" id=""fnref-MZs6yojF4FNt5T6gi-6"">[6]</a></sup> We know this because we built one! So how much processing power is necessary to do superhuman science and engineering? It’s undoubtedly a harder problem than chess. An insect brain is probably not enough. But do we need to get all the way up to human brain scale? Nobody knows.</p>
<p><img src=""https://sjbyrnes.com/AIslides/Slide11-12.png"" alt=""""></p>
<p><strong>The second timescale we care about is, how long does it take to fully understand the principles of AGI safety?</strong> I’m going to say again, some unknown amount of time between 3 and 300 years. Sometimes we have the Claude Shannon Information Theory situation, where one guy just sits down and writes a paper with everything that needs to be known about a field of knowledge. More often, it seems we need many generations to write and rewrite textbooks, and develop a deeper and broader understanding of a subject. From my perspective, the ongoing research programs, which I’ll talk about later, feel like they could fill many textbooks.</p>
<p>Finally, <strong>the third timescale is, How long do we have until the <em>start</em> of research towards AGI?</strong> The answer here is easy: None, it’s already started! For example, OpenAI and DeepMind are two organizations doing some of the best Machine Learning research on the planet, and each has a charter explicitly to make superintelligent AGI. Look it up on their websites. In fact just last month OpenAI announced their next big research effort, to develop deep networks that can reason. Will they succeed? Who knows, but they’re certainly trying.</p>
<p><img src=""https://sjbyrnes.com/AIslides/Slide13-14.png"" alt=""""></p>
<p>So those are the three timescales. How does that relate to whether or not AGI safety work is time-sensitive? Well there are a couple different scenarios. We can imagine that we’re doing safety work, and in parallel we’re doing capability work, and the hope is that we develop the principles of safe AGI by the time we’re capable of building AGI.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-7"" id=""fnref-MZs6yojF4FNt5T6gi-7"">[7]</a></sup> So then safety research is urgent. And that’s actually the <em>good</em> scenario! We could also imagine that there are different paths to make an AGI, and if we go down one development path, we need 100 brilliant insights, and then we know how to build AGI, but we’re building it in a way that’s impossible to control and use safely. Or, we can go down a different development path, and maybe now we need 300 brilliant insights, but we get an AGI that can be used safely. So in this scenario, we ideally want to have an excellent understanding of the principles of AGI safety before we even <em>start</em> research in the direction of AGIs. So in this case, we’re already behind schedule, and AGI safety research is <em>super</em>-urgent.</p>
<p>I’ll give a concrete example of this latter scenario. We can imagine a spectrum between two different AGI architectures. Here’s the cartoon AGI from before. In the model-free approach, we take the whole thing—the world-modeling, the prediction engine, the goal system, and everything else—and bundle it up in a giant complicated recurrent neural net. By contrast, in the model-based approach, we have lots of relatively narrow neural nets, maybe one for the goal system, and 16 neural nets encompassing the world model, and so on, and it’s all tied together with conventional computer code. You can call this the AlphaZero model; AlphaZero, the champion Go-playing program, has a big but simple feedforward neural net at its core⁠—basically a module that says which Go positions are good and bad. Then that neural net module is surrounded by a bunch of conventional, old-fashioned computer code running the tree search and the self-play and so on. Now, to me, it seems intuitively reasonable to expect that the model-free approach is a lot harder to control and use safely than the model-based approach. But that's just my guess. We don’t know that. If we <em>did</em> know that for sure, and we understood exactly why, then we could stop researching the model-free approach, and put all our effort into the model-based approach.</p>
<p>There are lots more examples like that. The field of AI is diverse, with lots of people pursuing very different research programs that are leading towards very different types of AGI.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-8"" id=""fnref-MZs6yojF4FNt5T6gi-8"">[8]</a></sup> We don't know which of those destinations is the type of AGI that we want, the AGI that's best for humanity. So we're pursuing all of them in parallel, and whatever program succeeds first, well that's the AGI we're gonna get, for better or worse, unless we figure out ASAP which are the most promising paths that we need to accelerate.</p>
<h2>AGI safety is grossly under-invested</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide15.png"" alt=""""></p>
<p>The final part of the motivation is that <strong>AGI safety is grossly under-invested</strong>. It’s a tiny subfield of CS. A <a href=""https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/"">recent figure</a> put it at $10M/year of dedicated funding. That means, for every $100 spent on AGI safety, the world spends $1M on climate change. AGI safety goes out to dinner, and meanwhile climate change buys itself a giant house and puts its children through college. Not that we should be siphoning money out of climate change; quite the contrary, I think we should be spending <em>more</em> on climate change. Rather, climate change is a benchmark for the resources that society can put towards a looming global threat. And according to that benchmark, AGI safety research should be growing by four orders of magnitude. And it’s not just that the field needs more money⁠—probably even more importantly, the field needs more smart people with good ideas. Lots of essential problems are not being studied, because there’s simply not enough people.</p>
<p>By the way, I had another bullet point in here about how much we spend on AGI safety research, versus how much we spend on movies about evil robots, but it was just too depressing, I took it out. [Laughter]</p>
<p>So that’s the motivation. More people should be working on it today, and that means us.</p>
<h1>Foundations</h1>
<p><img src=""https://sjbyrnes.com/AIslides/Slide17.png"" alt=""""></p>
<p>Next we move on to some <strong>foundational principles</strong>. These help explain the nature of the problem and why it’s hard. The main reference here is this 2014 book <em>Superintelligence</em>, by Oxford professor Nick Bostrom⁠—which you might have heard of because Elon Musk and Bill Gates and Stephen Hawking and others endorsed it, so it was in the news.</p>
<h2>Orthogonality thesis</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide18-19.png"" alt=""""></p>
<p>We’ll start with the <strong>orthogonality thesis</strong>. When we think about highly advanced AGI systems, some people have this optimistic intuition: ""If the AGI is overwhelmingly smarter than a human, its goals will be correspondingly wise and wonderful and glorious!"" The reality, unfortunately, is that a superintelligent AGI can in principle have <em>any goal whatsoever</em>. It’s called the orthogonality thesis, because we draw this graph with two orthogonal axes—a mind can have any capability and any goal, leaving aside some fine print.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-9"" id=""fnref-MZs6yojF4FNt5T6gi-9"">[9]</a></sup></p>
<p>To make sure everyone understands this point, one AGI safety researcher<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-10"" id=""fnref-MZs6yojF4FNt5T6gi-10"">[10]</a></sup> came up with a thought experiment with the <em>most inane goal you could possibly imagine</em>: A ""Paperclip Maximizer"". It’s a superintelligent AGI, as intelligent as you like, and its one and only goal is to maximize the number of paperclips in the universe’s far future. Of course someone made <a href=""http://www.decisionproblem.com/paperclips/"">a computer game</a> out of this, so you can explore different strategies, like, I presume, making synthetic bacteria that build nano-paperclips with their ribosomes, or developing space travel technology to go make paperclips on other planets, and so on.</p>
<p>Why do we believe the orthogonality thesis? There are a lot of arguments, but I think the most convincing is a ""proof by construction"". We just take that AGI cartoon from earlier, and rank possible courses of action according to their expected number of paperclips. There’s nothing impossible or self-contradictory about this, and as it gets more and more intelligent, there is nothing in this system that would cause it to have an existential crisis and start doubting whether paperclips are worth making. It’s just going to come up with better strategies for making paperclips.</p>
<h2>Goodhart's law</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide20-21.png"" alt=""""></p>
<p>So, that’s the orthogonality thesis, and it says that good goals don’t write themselves. <strong>Goodhart’s law</strong> is a step further, and it says that good goals are in fact very very hard to write. Goodhart’s law actually comes from economics. If you quantify one aspect of your goals, then optimize for that, you’ll get it—and you’ll get it in abundance—but you’re going to get it <em>at the expense of everything else you value</em>! So here’s Mickey Mouse in <em>The Sorcerer’s Apprentice</em>. He wanted the cauldron to be filled with water, so he programs his broomstick to do that, and indeed the cauldron does get filled with water! But he also gets his shop flooded, he almost drowns, and worst of all—<em>worst</em> of all—he gets in trouble with the Dean of Students!<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-11"" id=""fnref-MZs6yojF4FNt5T6gi-11"">[11]</a></sup> [Laughter.] We can go through different goals all day. Nobody ever dies of cancer? You can kill all humans! Prevent humans from coming to harm? (I put in this one for you Isaac Asimov fans.) Of course if you read the Asimov books, you’ll know that a good strategy here is imprisoning everyone in underground bunkers. [Laughter.] Again the theme is, ""Software does what you tell it to do, not what you want it to do.""</p>
<p>Goodhart’s law is absolutely rampant in modern machine learning, as anyone would expect who thinks about it for five minutes. Someone set up an evolutionary search for image classification algorithms, and it turned up a timing-attack algorithm, which inferred the image labels based on where they were stored on the hard drive. Someone trained an AI algorithm to play Tetris, and it learned to survive forever by pausing the game. [Laughter.] You can find <a href=""https://www.gwern.net/Tanks#alternative-examples"">lists of dozens of examples like these</a>.</p>
<p><img src=""https://sjbyrnes.com/AIslides/Slide22-23.png"" alt=""""></p>
<p>Now, some people have a hope that maybe the dumb AI systems of today are subject to Goodhart’s Law, but the superintelligent AGIs of tomorrow are not. Unfortunately—you guessed it—it doesn’t work that way. Yes, a superintelligent system with a good understanding of psychology will <em>understand</em> that we didn’t want it to cure cancer by killing everyone. But <em>understanding</em> our goals is a different thing than <em>adopting</em> our goals. That doesn’t happen automatically, unless we program it to do so—more on which later in the talk. There’s a beautiful example of this principle, because it actually happened! Evolution designed us for inclusive genetic fitness. We understand this, but we still use birth control. Evolution’s goal for us is not our goal for ourselves.</p>
<p>One way of thinking about Goodhart’s Law is that AI to date, just like economics, control theory, operations research, and other fields, have all been about optimizing a goal which is exogenous to the system. You read the homework assignment, and it says right there what the goal is, right? Here we broaden the scope: What should the goal be? Or should the system even have a goal?</p>
<h2>Instrumental convergence</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide24.png"" alt=""""></p>
<p>OK, so far we have that first, good goals don’t write themselves, and second, good goals are extremely hard to write. In a cruel twist of irony, our third principle, <strong>instrumental convergence</strong>, says that <em>bad and dangerous goals</em> more-or-less <em>do</em> write themselves by default! [Laughter.]</p>
<p>Let’s say an AGI has a real-world goal like ""Cure cancer"". Good strategies towards this goal tend to converge on certain sub-goals. First, stay alive, and prevent any change to your goal system. You can imagine the AGI saying to itself, ""If I’m dead, or if I’m reprogrammed to do something else, then I can’t cure cancer, and then it’s less likely that cancer will get cured."" Remember, an AGI is by definition an agent that makes good decisions in pursuit of its goals, in this case, the goal that cancer gets cured. So the AGI is likely to make decisions that result in it staying alive and keeping its goal system intact. In particular, if you’re an AGI, and there are humans who can reprogram you, you should lead them to believe that you are well-behaved, while scheming to eliminate that threat, for example by creating backup copies of yourself in case of shutdown. Second, increase your knowledge &amp; intelligence. ""If I become smarter, then I’ll be better able to cure cancer."" Gain money &amp; influence. Build more AGIs with the same goal, including copies of yourself. And so on.</p>
<p>I should say here that the instrumental convergence problem seems to have an easy solution: Don’t build an AGI with long-term real-world goals! And that may well be part of the solution. But it’s trickier than it sounds. Number 1, we could try not to build AGIs with long-term real-world goals, but build one by accident—all you need is one little bug that lets long-term real-world consequences somehow leak into its goal system. Number 2, we humans actually <em>have</em> long-term real-world goals, like curing Alzheimer's and solving climate change, and generally the best way to achieve goals is to have an agent seeking them. Number 3, related to that, even if all of us here in this room agree, ""Don’t make an AGI with long-term real-world goals,"" we need to also convince everybody for eternity to do the same thing. Solving problems like these is part of the ongoing research in the field. So that brings us to …</p>
<h1>Active research directions</h1>
<p><img src=""https://sjbyrnes.com/AIslides/Slide25-26.png"" alt=""""></p>
<p>...Active research directions in AGI safety.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-12"" id=""fnref-MZs6yojF4FNt5T6gi-12"">[12]</a></sup> What are people doing today? There are a lot of research directions, because it’s a multi-faceted problem, and because it’s a young field that hasn’t really converged on a settled paradigm. By the way, pretty much all new work gets posted at <a href=""https://alignmentforum.org"">alignmentforum.org</a>, and most of this technical work is happening at the <a href=""https://openai.com/"">OpenAI</a> safety team, the <a href=""https://medium.com/@deepmindsafetyresearch"">DeepMind safety team</a>, the nonprofit <a href=""https://intelligence.org/"">MIRI</a>, <a href=""https://humancompatible.ai/"">a center affiliated with UC Berkeley</a>, and the <a href=""https://www.fhi.ox.ac.uk/"">Future of Humanity Institute</a> at Oxford, among other places. I should also mention the <a href=""https://futureoflife.org/"">Future of Life Institute</a> which is doing great work especially on the non-technical aspects of the problem, for example organizing conferences, and trying to establish professional codes of ethics among AI researchers, and so on.</p>
<h2>Foundational principles</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide27-28.png"" alt=""""></p>
<p>We’ll start with <strong>foundational principles of AGI safety</strong>, an area where there remain lots and lots of important open questions. First, how do we make a safe AGI if we have infinite computer power? We know how to play perfect chess if we have infinite computing power, but we don’t know how to make safe AGI. Second, if we go with the model-based AGI approach that I discussed earlier, then what are these narrow neural nets, how are they interconnected, and does it help with safety?</p>
<p>So for example, people have worked on the <a href=""https://intelligence.org/2018/10/31/embedded-decisions/"">open problems in decision theory</a>, which is related to how to properly build a goal system and action-picker. There’s been work on <a href=""https://intelligence.org/2016/09/12/new-paper-logical-induction/"">how to formally represent knowledge and uncertainty</a>, including weird cases like uncertainty about your own capacity to reason correctly, and that’s part of how to properly build a world model. <a href=""https://www.fhi.ox.ac.uk/reframing/"">Comprehensive AI Services</a> is a proposal by Eric Drexler related to tying together lots of narrow neural nets, and putting humans in the loop in certain positions.</p>
<p>Here’s another fundamental question. An AGI will change over time. Can we verify that good behavior will persist? There’s a few reasons an AGI will change over time. It’s certainly going to be learning, and reasoning, and forming new memories. Those are all changes to the system. More dramatically, it might reprogram itself, unless of course we somehow forbid it from doing so. Heck, a cosmic ray could flip a bit! And now instead of <em>maximizing</em> human flourishing... [laughter]. ...Right, it’s <em>minimizing</em> it! I’m joking. Well, I think I’m joking. I don’t know. This is absolutely the kind of thing we should be thinking about.</p>
<p>Here’s an example of how learning and reasoning could cause problematic behavior to appear. It’s called an <a href=""https://wiki.lesswrong.com/wiki/Ontological_crisis"">""ontological crisis""</a>. For example, say I build an AGI with the goal ""Do what I want you to do"". Maybe the AGI starts with a primitive understanding of human psychology, and thinks of me as a monolithic rational agent. So then ""Do what I want you to do"" is a nice, well-defined goal. But then later on, the AGI develops a more sophisticated understanding of human psychology, and it realizes that I have contradictory goals, and context-dependent goals, and I have a brain made of neurons and so on. Maybe its goal is still ""Do what I want you to do"", but now it’s not so clear what exactly that refers to, in its updated world model. The concern is that the goal system could end up optimizing for something weird and unexpected, with bad consequences.</p>
<p>Another foundational question is: how do you put in a goal anyway? Remember the evolution example, where the external reward function, inclusive genetic fitness, doesn't match the internal goal.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-13"" id=""fnref-MZs6yojF4FNt5T6gi-13"">[13]</a></sup> And relatedly, how do you ensure that intelligent subsystems do not get misaligned with the top-level goal?</p>
<h2>Value learning</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide29-30.png"" alt=""""></p>
<p>Next, we have <strong>value learning</strong>.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-14"" id=""fnref-MZs6yojF4FNt5T6gi-14"">[14]</a></sup> The basic idea of value learning is: We don’t know how to write machine code for the Meaning of Life. We can’t even write machine code for Fill A Cauldron With Water, if we need to add in all the side goals like Don't flood the shop. But we’ve seen problems like that before! We also don’t know how to write down code that says whether an image is of a cat. But we can train a neural net, and with enough examples it can <em>learn</em> to recognize cats. By the same token, maybe we can build a machine that will <em>learn</em> our goals and values. Now, again, human goals and values are inconsistent, and they’re unstable, and maybe there are cases where it’s unclear what exactly we want the AGI to be learning. But I think everyone in the field agrees that value learning is going to be part of the solution.</p>
<p>Now, in a sense, people have been doing value learning from the get-go. There’s a traditional way to do it. You write down an objective function. You run the AI. If it doesn’t do what you wanted it to do, you try again. This won’t work for AGI. We have no hope of writing down an objective function, and mistakes can be catastrophic. Luckily, there are a lot of alternatives being explored. (This area is generally called Cooperative Inverse Reinforcement Learning.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-15"" id=""fnref-MZs6yojF4FNt5T6gi-15"">[15]</a></sup>) You can demonstrate the desired behavior while the AI watches. You can give real-time feedback to the AI. The AI could ask you questions. Heck, the AI could even propose its own objective function and ask for feedback. The DeepMind safety team has a <a href=""https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"">recent paper on Recursive Reward Modeling</a>, where you train AIs to help train more complicated AIs. And so on. Do we need brain-computer interfaces? Anyway, this area is the frontier of human-computer interface design, and it’s only just beginning to be explored.</p>
<h2>Limiting</h2>
<p>Next is <strong>Limiting</strong>. Here the goal is not for the AGI to do the right thing, the goal is for the AGI to <em>not</em> do the catastrophically <em>wrong</em> thing. There are four main subcategories here. First, ""Boxing"", or AI-in-a-box. The simplest aspect of this is not giving the AGI internet access. Second, ""Impact measures"". Imagine that Mickey can program his broomstick with a module that says that flooding the lab is high-impact, and therefore don’t do it without permission. Defining high-impact is trickier than it sounds. In fact, <em>all</em> of these are trickier than they sound. Mild optimization and Task-Based AGI are kind of like Mickey telling the broomstick, ""Fill the cauldron but don’t try too hard, just get it mostly full in the most obvious way, and then shut yourself down."" Finally, in ""Norm Learning"", we want the broomstick to have some knowledge that flooding labs isn’t something that people do, so don’t do it, or at least don’t do it without double-checking with Mickey.</p>
<p>All these are bad permanent solutions, because at the same time that we’re making the AGI safer, we’re also making it less able to get things done effectively, like curing Alzheimer's and solving climate change, so people are going to be tempted to ramp down the protections. For example, my AI-in-a-box is going to give me much better advice if I give it just a <em>little bit</em> of internet access. I'll closely supervise it. What could go wrong? And then later, maybe a little bit <em>more</em> internet access, and so on. But temporary solutions are still valuable⁠—for example, they can be a second line of defense as we initially deploy these systems. It’s an open problem how to implement any of these, but lots of interesting progress on all these fronts, and and as always you can email me for a bibliography.</p>
<h2>Interpretability, Bootstrapping, Robustness &amp; verification</h2>
<p><img src=""https://sjbyrnes.com/AIslides/Slide31-32.png"" alt=""""></p>
<p>Some other categories, in brief. <strong>Interpretability</strong>: Can we look inside an AGI’s mind as it performs calculations, and figure out what its goals are? <strong>Robustness &amp; verification</strong>: How do we gain confidence that there won’t be catastrophic behavior, without testing? For example, it would be awesome if we could do formal verification, but no one has a good idea of how. <strong>Bootstrapping</strong>: Can we use advanced AI systems to help ensure the safety of advanced AI systems? I already mentioned recursive reward modeling, and there are a lot of other bootstrapping-related ideas for all these categories. Sorry, I know I'm going pretty quick here; there's more on these in the <a href=""https://sjbyrnes.com/AIslides/IntroToAGISafetyResearch.pptx"">backup slides</a> if it comes up in Q&amp;A.</p>
<h2>Non-technical work</h2>
<p>Finally, non-technical work—and it’s <em>completely</em> unfair for me to put this as a single slide, because it’s like half of the field, but here goes anyway. How do we avoid arms races or other competitive situations where researchers don’t have time for safety? There’s outreach and advocacy work. And how about job losses? What do we do when all intellectual labor can be automated, and then shortly thereafter we’ll have better robots and all labor <em>period</em> can be automated? How do we redistribute the trillions of dollars of wealth that might be accumulated by whoever invents AGI? Who is going to use AGI technology? What are they going to do with it? How do we keep it out of the hands of bad actors? When will AGI arrive, and where, and how? And on and on. By the way, maybe ""non-technical"" is the wrong term; things like strategy and forecasting obviously require the best possible technical understanding of AGI. But you know what I mean: There's the project of figuring out how to write AGI code, and then there's everything else.</p>
<p>We need the non-technical and we need the technical. We need the technical work in order to do <em>anything whatsoever</em> with an AGI, without risking catastrophe. We need the non-technical work to know what to do with an AGI, and to be in a position to actually do it. As I mentioned before, <a href=""https://futureoflife.org/"">the Future of Life Institute website</a> is a good place to start.<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-16"" id=""fnref-MZs6yojF4FNt5T6gi-16"">[16]</a></sup></p>
<h1>What can we do today?</h1>
<p><img src=""https://sjbyrnes.com/AIslides/Slide33-34.png"" alt=""""></p>
<p>Finally, what can we do today? Well, certain parts of AGI safety have immediate practical applications. In those cases, there’s already ongoing work in those directions in the ML community, and these areas are growing, and they need to grow more. For example, as we build ML systems that need to interact with humans and navigate complex environments, we start running into the situation I was talking about before, where we can’t write down the goals, and therefore we need value learning. Things like robustness and interpretability are growing in importance as ML systems go out of the lab and start to get deployed in the real world. I think this is great, and I encourage anyone to get involved in these areas. I <em>also</em> think everyone involved in these areas should study up on long-term AGI Safety, so that we can come up with solutions that are not just narrowly tailored to the systems of today, but will also scale up to the more advanced systems of tomorrow.</p>
<p><img src=""https://sjbyrnes.com/AIslides/Slide35-36.png"" alt=""""></p>
<p>Other parts of AGI safety don’t seem to have any immediate applications. For example, take AI Limiting. With today's technology, it's <em>all we can do</em> to make AI systems accomplish <em>anything at all</em>! AI Limiting means deliberately handicapping your AI⁠—there's no market demand for that! Or take formal verification:<sup class=""footnote-ref""><a href=""#fn-MZs6yojF4FNt5T6gi-17"" id=""fnref-MZs6yojF4FNt5T6gi-17"">[17]</a></sup> There’s been some work on formal verification of machine learning systems, but it relies on having simple enough goals that you know what you’re trying to prove. What’s the formal specification for ""don’t cause a catastrophe""? There are lots more examples. These types of problems are still relatively underdeveloped niche topics, even though I think they’re likely to grow dramatically in the near future. And that’s a good thing for you in the audience, because you can catch up with the state-of-the-art in many of these areas without years of dedicated coursework.</p>
<p>Last but not least, go to the <a href=""https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/"">80,000 hours problem profile and career guide</a>, which summarizes the problem and what steps you can take to get involved. In fact, if there’s one thing you get out of this talk, it’s the link <a href=""https://80000hours.org/"">80000hours.org</a>. When you choose a career, you’re thinking about how to spend 80,000 hours of your life, and it’s well worth spending at least many hundreds of hours thinking it through. I mean, life is uncertain, plans change, but as they say, ""Plans are useless but planning is essential"". The mission of <a href=""http://80000hours.org"">80000hours.org</a> is to provide resources for making good decisions related to your life and career—assuming that one of your goals is to make a better future for the world. This link on the slide is for AI, and they also have pages on climate change, pandemics, preventing war, improving public policy, and many other vitally important problems, and they also more generic resources about things like choosing majors and finding jobs.</p>
<p>You should also subscribe to <a href=""https://alignmentforum.org/"">alignmentforum.org</a>. All the papers on AGI safety get posted there, and you can see the top researchers commenting on each other’s work in real time.</p>
<p>In conclusion, this is one of the grand challenges of our age. The field is poised to grow by orders of magnitude. It offers an endless stream of hard and fascinating problems to solve. So get involved! <a href=""http://mindingourway.com/dive-in/"">Dive in</a>! Thank you very much, and I’m happy to take any questions.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-MZs6yojF4FNt5T6gi-1"" class=""footnote-item""><p>The <em>Sorcerer’s Apprentice</em> analogy—like much else in this talk—comes from <a href=""https://intelligence.org/2017/04/12/ensuring/"">Nate Soares's google talk</a>. I believe that <em>The Sorcerer's Apprentice</em> analogy originates with <a href=""https://intelligence.org/stanford-talk/"">this earlier talk by Eliezer Yudkowsky</a>. <a href=""#fnref-MZs6yojF4FNt5T6gi-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-2"" class=""footnote-item""><p>This analogy—like much else in the talk—comes from Stuart Russell; in this case <a href=""https://www.amazon.com/Possible-Minds-Twenty-Five-Ways-Looking/dp/0525557997"">his chapter in a recent popular book</a> <a href=""#fnref-MZs6yojF4FNt5T6gi-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-3"" class=""footnote-item""><p>There is controversy in the community over the extent to which AGI safety problems will manifest themselves as small problems before they manifest themselves as irreversibly catastrophic problems. I would argue that many possible problems probably would, but at least some possible problems probably wouldn't. For example, here's a possible problem: As an AGI learns more, contemplates more deeply, and gains more freedom of action, maybe at some point its goals or behavior will shift. (See, for example, the discussion of ""ontological crisis"" below.) No matter how much testing we do, it's always possible that this particular problem will arise, but that it hasn't yet! Maybe it never will in the testing environment, but will arise a few years after we start deploying AGI in ways where we can't reliably switch it off. A different way that problems might manifest themselves as irreversibly catastrophic without first manifesting themselves as small problems is if AGI happens kinda suddenly, and catch people off-guard, with their systems being more capable than expected. Also, it's possible that something like goal instability will be a known problem, but without a clear solution, and people will have ideas about how to solve the problem that don't really work—they just delay the onset of the problem without eliminating it. Then some overconfident actor may incorrectly think that they've solved the problem, and will thus deploy a catastrophically unsafe system. <a href=""#fnref-MZs6yojF4FNt5T6gi-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-4"" class=""footnote-item""><p>For more on timelines, see <a href=""https://intelligence.org/2017/10/13/fire-alarm/"">https://intelligence.org/2017/10/13/fire-alarm/</a> <a href=""#fnref-MZs6yojF4FNt5T6gi-4"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-5"" class=""footnote-item""><p>For more on human-brain-equivalent computer hardware, and other aspects of AGI forecasting, see <a href=""https://aiimpacts.org/human-level-hardware-timeline/"">this</a> and other articles at <a href=""https://aiimpacts.org/"">AI Impacts</a> <a href=""#fnref-MZs6yojF4FNt5T6gi-5"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-6"" class=""footnote-item""><p><a href=""https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/#transcript"">Paul Christiano, 80,000 hours interview</a> <a href=""#fnref-MZs6yojF4FNt5T6gi-6"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-7"" class=""footnote-item""><p>Of course, the paths are not really parallel; lots of safety work will only become possible as we get closer to building an AGI and learn more about the architecture. The ideal would be to develop as much understanding of AGI safety as possible <em>at any given time</em>. As discussed below, we are far from that: there is a massive backlog of safety work that we can do right now but that no one has gotten around to. Beyond that, we want to get to a world where ""AI safety"" is like ""bridge-building safety"": nobody talks about ""AI safety"" <em>per se</em> because <em>all</em> AI researchers recognize the importance of safety, where it would be unthinkable to design a system without working exhaustively to anticipate possible problems before they happen. <a href=""#fnref-MZs6yojF4FNt5T6gi-7"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-8"" class=""footnote-item""><p>More examples: We could train AGIs by reinforcement learning, or by <a href=""https://www.lesswrong.com/posts/yAiqLmLFxvyANSfs2/counterfactual-oracles-online-supervised-learning-with"">supervised learning</a>, or by <a href=""https://www.lesswrong.com/posts/EMZeJ7vpfeF4GrWwm/self-supervised-learning-and-agi-safety"">self-supervised (predictive) learning</a>. Vicarious and Numenta are trying to understand and copy brain algorithms, but leaving out emotions. (Or should we figure those out and put them in too?) We could figure out how to help the system understand humans, or <a href=""https://www.lesswrong.com/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models"">design it to be forbidden from modeling humans</a>! David Ferrucci at Elemental Cognition thinks we need an AGI that talks to humans and helps them reason better. Ray Kurzweil and Elon Musk argue that brain-computer interfaces are key to AGI safety. <a href=""https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety"">Chris Olah thinks that interpretability tools are the path to safe AGI.</a>. Eric Drexler thinks the path to AGI will look <a href=""https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as"">like a collection of narrow programs</a>. Anyway, all these things and more seem to be fundamentally different research programs, going down different paths towards different AGI destinations. In a perfect world, we would collectively pause here and try to figure out which destination is best. <a href=""#fnref-MZs6yojF4FNt5T6gi-8"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-9"" class=""footnote-item""><p>One example of fine print on the Orthogonality Thesis would be the fact that if a system has very low capability, such that it's not able to even understand a certain goal, then it probably can't optimize for that goal. <a href=""#fnref-MZs6yojF4FNt5T6gi-9"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-10"" class=""footnote-item""><p>Eliezer Yudkowsky <a href=""https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/"">says</a> that the Paperclip Maximizer thought experiment probably originated with him, but that it also might have been Nick Bostrom. <a href=""#fnref-MZs6yojF4FNt5T6gi-10"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-11"" class=""footnote-item""><p>To make the joke work, substitute the appropriate scary authority figure for your audience, e.g. ""The head of Corporate Compliance"", ""The Head of EH&amp;S"", ""The Head of the IRB"", ""The CEO"", etc. <a href=""#fnref-MZs6yojF4FNt5T6gi-11"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-12"" class=""footnote-item""><p>This section loosely follows Rohin Shah's summaries of the field <a href=""https://futureoflife.org/2019/04/11/an-overview-of-technical-ai-alignment-with-rohin-shah-part-1/"">here</a> and <a href=""https://futureoflife.org/2019/04/25/an-overview-of-technical-ai-alignment-with-rohin-shah-part-2/"">here</a> and <a href=""https://www.youtube.com/watch?v=AMSKIDEbjLY"">here</a>. <a href=""#fnref-MZs6yojF4FNt5T6gi-12"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-13"" class=""footnote-item""><p>A <a href=""https://intelligence.org/2019/06/07/new-paper-learned-optimization/"">recent paper</a> calls the challenge of installing a known goal the ""Inner Optimization Problem"". <a href=""#fnref-MZs6yojF4FNt5T6gi-13"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-14"" class=""footnote-item""><p>To learn more, a good place to start is the <a href=""https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc"">Value Learning Sequence</a>. <a href=""#fnref-MZs6yojF4FNt5T6gi-14"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-15"" class=""footnote-item""><p>The perspective wherein Cooperative Inverse Reinforcement Learning is an all-encompassing framework for human-computer interactions is advocated by Dylan Hadfield-Menell <a href=""https://futureoflife.org/2019/01/17/cooperative-inverse-reinforcement-learning-with-dylan-hadfield-menell/"">here</a>. The original paper on CIRL is <a href=""https://arxiv.org/abs/1606.03137"">here</a>. <a href=""#fnref-MZs6yojF4FNt5T6gi-15"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-16"" class=""footnote-item""><p>See also the <a href=""https://80000hours.org/articles/ai-policy-guide/"">80,000 hours guide to working in AI policy</a>. And the <a href=""https://jack-clark.net/"">Import AI newsletter</a> is a nice way to keep up to date with AI news relevant to policy and strategy. <a href=""#fnref-MZs6yojF4FNt5T6gi-16"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-MZs6yojF4FNt5T6gi-17"" class=""footnote-item""><p>For more on formal verification in the context of AGI safety, a good place to start is <a href=""https://medium.com/@deepmindsafetyresearch/towards-robust-and-verified-ai-specification-testing-robust-training-and-formal-verification-69bd1bc48bda"">this recent review from the DeepMind safety team</a> <a href=""#fnref-MZs6yojF4FNt5T6gi-17"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
</body></html>",steve2152,steve2152,Steven Byrnes,
tAzdgzTtCG5vHwTt6,Reneging prosocially by Duncan Sabien,reneging-prosocially-by-duncan-sabien,https://www.lesswrong.com/posts/tAzdgzTtCG5vHwTt6/reneging-prosocially-by-duncan-sabien,2019-06-18T18:52:46.501Z,70,22,5,False,False,https://medium.com/@ThingMaker/reneging-prosocially-5b44bdec3bb9,"<html><head></head><body><p>A good post about reneging on agreements, acting when the other person reneges on you, and making agreements.</p>
</body></html>",crabman,philip_b,philip_b,
mLzA746mERFsZEq8j,Magic Arena Bot Drafting,magic-arena-bot-drafting,https://www.lesswrong.com/posts/mLzA746mERFsZEq8j/magic-arena-bot-drafting,2019-06-18T16:00:00.402Z,18,7,1,False,False,,"<p>Epistemic Status: Attempting to be useful and to learn via exploration. Real proposals. Content assumes familiarity with Magic: The Gathering booster drafting, but does not much depend on a knowledge of deep Magic strategy, so it holds potential interest for those interested in game narrow AI and machine learning.</p>
<p>Reflections Partially Brought on By (Eric Moyer @ Channel-Fireball): <a href=""https://www.channelfireball.com/articles/drafting-like-a-computer-part-1/"">Drafting Like a Computer Part 1</a>, <a href=""https://www.channelfireball.com/articles/drafting-like-a-computer-part-2/?_ga=2.44349479.227490316.1560197122-1048591906.1533907990"">Part 2</a></p>
<h3>Part 1 – The Problem is Hard</h3>
<p>I appreciated Eric Moyer’s articles because they show that building a good bot is <em>hard. </em>They do so by <em>failing to design good bots. </em>Eric’s proposed bots are overly formulaic and predictable, with obvious holes in their systems. If you are feeding one of these bots and know their rankings, you can narrow down their whole path based on possible first pick colors and strengths in highly exploitable ways. The proposed bots will reliably fail to take into account holistic information, they won’t draft playable decks or try to shore up their weaknesses or strengths, and so on, and so on.</p>
<p>That’s not because Eric isn’t making a reasonable first attempt here. It’s because the problem is hard! Programming a bot to draft reasonably is hard.</p>
<p></p>
<p>What we want above all is a bot experience that <em>feels human, </em>where you’re trying to figure out what is going on, cannot ever fully rely on your hunches, and need to consider all the information at your disposal. Where you get to make hero reads, cram decks down the throats of other drafters, sweat our attempts to wheel key cards, and otherwise have a fresh, exciting and new experience more often.</p>
<p>Making this even more difficult is that bots that we know are bots will inherently feel like bots, and that bots will get blamed even more than humans for mistakes. When bots mimic human patterns of doing things that look crazy, the bot often looks crazy rather than realistic or human.</p>
<p>As an extreme example. if a person passes us an Ugin, we know that a human made a horrible mistake. Maybe someone didn’t see it, or misclicked, or is trying something weird. And we can be happy for our good fortune. If a bot passed it, as happened to Gerry Thompson on the first day of War of the Spark drafts, we get deeply confused and frustrated.</p>
<p>Same goes for everything else the bots do. Suddenly you have <em>someone who interacted with the problem, </em>and is therefore responsible for it, as per <a href=""https://blog.jaibot.com/the-copenhagen-interpretation-of-ethics/"">Copenhagen Interpretation of Ethics</a>. Whereas if you leave the decisions to humans, they are free to be idiots or have their own quirky reasons. That’s a big reason people don’t like the bots. Another big one is consistency. The bots reliably do similar things in similar spots. You can predict easily which cards will wheel, and which cards one can pick up cheap, and those answers rarely change. It’s easy to construct decks that rely on the cards the bots don’t want, and reliably draft those decks.</p>
<p>Always drafting three of the same set makes this that much more reliable – I know what the market research says, but I miss two-set and three-set drafting. Alas.</p>
<p>This leads to three broad conclusions.</p>
<p>First, humans <em>really are better </em>than bots even if the bots were as good or better at drafting real decks and/or strong cards. So we should use human drafters wherever possible. Humans inherently have understandable logic but also variation and unpredictability, and are viewed as their own agents rather than part of a blameworthy system. We can cry over how this is unfair, or we can accept that bots will be held to a higher and more complex standard than we would like. This means, among other things, making sure that certain crazy things do not happen with bots, even if they happen in real life.</p>
<p>Second, bots need to appear <em>unpredictable </em>and <em>inconsistent </em>to give a good experience, which requires their decision process to be <em>complex</em>. But without looking too much like idiots. Each bot should come with its own set of preferences and places of emphasis, over cards and over colors, and ideally other things as well such as how much it cares about what it is forcing, what signals it gets, its mana curve, and so on. We need to mix up who and what you are dealing with, so you can’t play them like a fiddle, or rely on them to act the way you expect. And so part of a draft if figuring out <em>which types of bots </em>you are dealing with, in addition to what might be open.</p>
<p>Third, bots do not need to be <em>geniuses </em>that go super deep with logic to figure out their pods and situations, or otherwise do something amazing. It would be cool if they were, <em>but the players won’t notice, </em>so there’s no point. Who cares if the bots improve their average deck strength in this way? It might even be bad, because <em>very few real players are geniuses. </em>We can’t all be Ben Stark.</p>
<h3>Part 2 – What a Problem Solution Might Look Like</h3>
<p>Saying a problem is hard is important, describing how it is hard is a valuable unhint – my high school classmate Daniel Strong coined that term to describe information that got you closer to finding the solution, but made your estimate of your distance from the solution go up. I am sad it never caught on.</p>
<p>Ideally, it will be a solution good enough that we can offer something deeply cool: To show, after the draft is completed, what the bots did. You should be able to see what the bot passing to you was up to, and how the bot you passed to handled the signals you sent its way. Why not? It would be super cool, and only begin to scratch the surface of improving the drafting experience. When I make my own game with its own limited formats, the sky’s the limit on the awesome.</p>
<p>With or without such lofty goals, a first sketch of a solution is almost always better than mumbling about how it would be great to have a solution at all.</p>
<p>So here’s my proposal for how to build better draft bots.</p>
<p>The basic idea is that each bot has a set of attributes that they use to guide how they draft. As a first proposal, a bot will care about the following things:</p>
<ol>
<li>Color, color combination and archetype preference.</li>
<li>Individual card rankings, in general and/or within color combinations and archetypes.</li>
<li>Speculativeness, which is how much they discount a card when they aren’t sure they can play it.</li>
<li>Preferred number of creatures, removal and tricks, and how much they care.</li>
<li>Mana curve.</li>
<li>Signal received, and what constitutes a signal.</li>
<li>Signal given, and what constitutes a signal (that second part will match in both directions).</li>
<li>Stubbornness (how much they attach themselves to early picks).</li>
<li>Cost of commitment (how much they want to commit or stay open).</li>
<li>Rare drafting.</li>
<li>Mana fixing, and willingness to go 3+ colors.</li>
</ol>
<p>That seems like a lot, and it is. You can certainly implement a version that ignores some or most of this list, and still does a reasonable drafter impression. Some of these might even end up being counter-productive from the point of view of the average user, who never sees the intricacies.</p>
<p>The basic idea is that for every pick, all of these will function as weights. When a draft is created, each of the seven computer drafters is instantiated with a randomized set of preferences over everything on the list, with choices correlating somewhat if they are observed to be correlated among human drafters.</p>
<p>The computer will add up all considerations for each card, then pick the card with the highest combined weight for all of one’s picks after making that pick, plus an adjustment for how open one is and thus the expected value of remaining packs. Thus, if a pick increases the computer’s estimate of your chance of playing red from 75% to 90%, that increases the value of all chosen red cards, not only the card you’re picking, and reduces the value of any chosen other cards. Getting these estimates right will be the trickiest part of the whole equation.</p>
<p>Consider the pack shown in the Drafting Like a Computer article:</p>
<p><img src=""https://227rsi2stdr53e3wto2skssd7xe-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/EM1.png"" /></p>
<p>The numbers on the right are LSV’s set review’s ratings for each card. In his model, the reasoning would go something like “There’s a 1-color 3.5, which wins over a 2-color 3.5 and a 1-color 3.0.”</p>
<p>In our version, the numbers would be less even. We’d effectively see the bot saying something like “If we take the red 3.5, we’re passing a primary black and green pack to the left (+0.3), and we have only a small red commitment (-0.1) and can likely run it (85% of value). Whereas if we take the rare, we get a rare (+0.3), which is good, but our color commitments on both sides are high (-0.5) and two of the three best remaining cards are in those two colors (-0.1).” All of these magnitudes would be different for different bots, to simulate different drafters.</p>
<p>Thus it would sometimes say “But I <em>really like </em>black/green in this format (+0.3) and dislike red (-0.2), and a rare is a rare (extra +0.3 for +0.6 total), and besides I don’t think the red card is all that (-0.2)” and take the rare, whereas other times it will say “I highly value staying open (+0.3) and don’t care about rares at all, red is pretty good (+0.1) and I just want to win (+0 rather than +0.3), and take a red common instead.</p>
<p>Then in pick 2:</p>
<p><img src=""https://227rsi2stdr53e3wto2skssd7xe-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Capture-2.png"" /></p>
<p>It would now consider signal passed, signal received, and so on. Since each bot would have a different card value for each card, each bot would care differently about what it passed to the left, and care read differently what was passed to it from the right. This would make bots that much more divergent, which is very good. Already here at pick two, there will be important uncertainty in what happens, with a few extreme drafters choosing to further commit to red, and some disliking Ugin’s Construct, despite a seemingly clear choice. Most drafters will care more about staying open and reliably playing cards, occasionally you’ll see one that prefers to commit more to improve the signal and wants to take a second red card here. Ideally that would be rare due to the backup red cards available, which would typically reduce the measured impact of cutting each card. And so on.</p>
<p>Later in the draft, the bots will deal more and more with their own unique problems and circumstances.</p>
<p>Player rank should impact the quality of the opposition if and only if players are paired via ranking. Otherwise, bot quality should be maximized for high-stakes events, and perhaps lowered somewhat for low-stakes events. We don’t want to punish drafters in unranked matches for having a high ranking.</p>
<h3>Part 3 – Getting There From Here</h3>
<p>It’s all well and good to lay out a grand vision for how a bot should act. It is quite another to explain how to program the damn thing. A lot of the things I suggest in Part 2 won’t be easy to get right. An initial version will doubtless be stripped down dramatically from this final vision, with far less considerations and a much simpler method of color selection.</p>
<p>Thus, this third section won’t aim to get us all of the way there. This is my vision of how to get a minimum viable product, that will fix the bulk of the obvious issues that players observe, and get the bots drafting reasonable decks.</p>
<p>That means solving a few problems. Here’s my take on how to do that.</p>
<p>Whenever possible, throughout this process, we’re going to use the simplest and most reliable indicator of all, and rely on the players themselves and their draft choices.</p>
<p><strong>Problem 1: Baseline Card Rankings</strong></p>
<p>Before one can want to randomize card values, one must first get initial card values right. This seems to be an ongoing issue, which is surprising. We can divide the problem in two: The problem <em>before </em>we have a substantial number of human drafts to reference, and the problem after that.</p>
<p>Once we have enough human drafts, the baseline ranking of a card is how likely drafters of appropriate rank and results are to draft a given card. So take all Arena accounts that have at least ten (ranked plus traditional) matches with a 60%+ win rate, say, and look at their average pick number for each card, then adjust for rarity (since rare cards don’t have to compete with other rare cards), such that the bots would draft the cards with the same average pick number as the humans, and you’re good.</p>
<p>Magic Online can be used to jump-start this process, using all Magic Online drafters as the initial sample. One can then update in real time, by having new pods read a file containing current rankings. If we’re not comfortable using Magic Online to jump start, or we want Arena to go first, we can use an R&amp;D survey, a player survey or the average of set reviews for the first hour, then use the human drafters from there.</p>
<p>Later, when we add more considerations, we’ll find that those other considerations alter how willing bots are to take various cards early. To avoid factoring these things twice (once for what humans do, another time for what bots do), we’ll then want to adjust our baseline ratings until simulations of all-bot drafts have the right average pick number for all cards. Basic search algorithms should be sufficient for this, as would be basic machine learning.</p>
<p>Thus, I will assume that we will always calibrate our other considerations using current baseline card ratings, then adjust baseline card ratings to reflect the added new consideration, and continuing to iterate as we advance. For any given set of other considerations, there should always exist (unless I’m missing something) a set of card rankings that causes average pick number to be correct for all cards. This then introduces some other issues in secondary ways, because the bots will incorrectly overvalue or undervalue cards in secondary ways, but I’m willing to worry about those problems later.</p>
<p>I will proceed to the next section as if this means that we’ve solved the individual card valuation problem, and it will stay solved while introducing additional complexity.</p>
<h4>Problem 2: The Signal</h4>
<p>We can mimic human behavior via optimizing over plausible things to consider, either on Arena, Magic Online or both. Thus, look at what pack contents being passed to you cause players to alter their picks, or what cards or patterns of cards one passes that cause players to alter their picks, and mimic those choices. We can (at least for now) abstract cards away as “Passed a red 3.2 and a red 2.9 and a blue 2.7.” So we use gradient descent or other machine learning techniques to figure out what weights to put on such matters to best mimic human choices. Again, if we want ‘advanced’ versus ‘basic’ bots, we can train on different sets of human data for both. Once we have done this for past sets, we can use the past version as the baseline early in the next set, until we have new human data.</p>
<p>Ideally, rather than use fully flexible machine learning techniques that will be impossible to interpret or customize, we’d fix the computer to only considering a few things and putting weights on those things. This then allows us to randomly alter those weights to create different personalities, or to let us experiment with what happens with bots that creatively use different weights. I’d rather have something we can understand, even if it performs a little worse, for many reasons. At least, until we have the bots playing out their matches, and we can evaluate them on that basis.</p>
<p>The big problem with this approach is that it would miss important subtlety that humans would pick up upon, such as a red card that wants one to pair it with black versus another that wants to be paired with white, and a third that wants to be a splash card. These problems are hard, hard to solve all at once, and there’s a lot of them. So in the long run, we’d want to move to a system that was smart enough to pick up on what each card was signaling, in terms of colors, color commitments and color combinations and even archetypes, rather than assume the card’s colors said it all, slash allow the system to learn arbitrary things about drafting via machine learning.</p>
<p>A second approach would be to not worry at first about what humans do, and instead run simulations of bots drafting against each other, trying to maximize the quality of their decks, and learn via self play. The problem with this is that it would only be as good as our evaluation of what a good deck is, which is also a problem for making good later picks, which we’ll talk about solving in earnest in problem four.</p>
<p>In the meantime slash as a preview, you can certainly improvise a not-too-terrible version that gives every card a value, then discounts that value based on difficulty of casting that card based on color and total mana requirements (so a splashed card would both hurt the value of the rest of the deck, and be discounted itself, and so on). That would let you get started so you could bootstrap.</p>
<h4>Problem 3: Choosing Colors and Strategies</h4>
<p>The trickiest and most impactful decision of a draft is what colors and strategy to pursue, and when to commit to it to varying extents. Should one stay open in one or both colors, or rush in to send a stronger signal and get to assembling the pieces? Good players have widely varying philosophies here. I would want the bots to also have widely varying philosophies, but more importantly I’d want them to have reasonable ones.</p>
<p>Note that the computer would be wise not to consider what it was thinking in the previous pick, when deciding on the current pick. Saying one is fully committed is an observation about future decisions rather than an alteration of them. If the facts change, I change my opinion.</p>
<p>The simplest and most extreme version of color selection is to make a pick in each of two colors, then only draft cards in those two colors for the rest of the draft whenever possible. That’s your deck. There are some humans who <em>effectively </em>follow this strategy, and there’s reason to suspect current bots are not too far from it. But its full form is rare, and it is not very effective.</p>
<p>A more reasonable version of this would be to value your current picks by valuing cards in your top two colors only, up to a maximum of 22-24, modified by the signals sent and received and the resulting likelihood of future value in those colors. Thus, if I pick a green 4.3 and a red 3.2, then am choosing between a red 2.1 and a blue 3.7, I would compare (4.3+3.2+2.1) to (4.3+3.7). We’d want to calibrate the point scale such that such additions were reasonable, and use what pick of the draft we are on to determine what placeholder to use for unfilled slots, which is also how we’d account for signals. So if we are ‘ahead of schedule’ we would assume the marginal card we end up with is likely pretty good, whereas if we’re ‘behind schedule’ and it’s pick 9 and we’re considering a color pair where we’d only have 2 cards so far, we’d assign a low value to (at least the last few) remaining future cards, since we’ll often be scrounging for value. We could calibrate and train such systems via all-bot drafts, once we had a reasonable first guess to work from, or from seeing what works and doesn’t work in drafts against humans.</p>
<p>An improved version of that would be to value all colors/pairs/strategies somewhat, but discount them by the probability you end up playing them, which is a function of how good they look in expectation right now. So sometimes, you’d take an amazing off-color card even if it likely wasn’t going to cause you to switch, because even a small chance of playing it was enough value to justify that, and then see what happens. Or, if we anticipate a potential splash. The bot could also simulate what the mana might look like, and see whether splashing is likely to make the deck better.</p>
<p>That’s also a good way to value drafting lands or other mana fixing. Mana fixing reduces the discount you apply on the other cards in your deck, including future picks, but in most cases does not otherwise provide value. So if you pick a dual land, this causes your expected mana base to improve, which makes all cards in the improved colors more valuable, which is exactly what is actually happening, and then you apply a <em>penalty </em>based on how much downside the card has to use – so if you draft a Sacred Foundry, it’s actually a negative-value card but it increases your colored land count by one, so it sometimes is worth it. And like everything else, there’s variance in how much each bot cares about its mana, and how much it discounts cards on that basis.</p>
<p>Getting the formula for ‘how often am I going to be drafting this color going forward’ correct will be tricky, and again something we’ll want to tune. But it’s not terrible to start by assuming something similar to, being the total worth of one first-pick quality card ahead of alternatives in the middle of pack one, considering all factors, puts you at something like 75%, two such picks at 95%, three at 99%, and that getting more extreme as the draft goes on.</p>
<p>As the end of the draft approaches, the bots should naturally start to get worried about finishing their decks in reasonable fashion, if they’re still caught between three or more colors, and start to put higher value on depth and on leaning on small advantages, and less weight on signals because there’s less time left to use them. So we should see reasonable play emerge here.</p>
<p>Archetypes are trickier. We all know that some color combinations are better than others, some cards are better in some color combinations, and so on, but that can be hard to quantify. What to do?</p>
<p>Look at human drafts in the later packs. Look at deck frequency and match results.</p>
<p>As a first step level-one attempt, one can define each two-color pair as having different card values assuming that theme. We can get those values implicitly from the decisions players make in the third pack of the draft, for those drafts that are clearly in a given two-color pair (e.g. they have number of picks and assessed total value that is heavily concentrated in exactly two colors). We could add picks made earlier for drafts that have clearly committed, and which then stayed committed, and still likely have good data. Thus, if given a commitment to white/blue, a given white card has an average pick number of 4.6, as opposed to 6.3 for white/black decks, that tells us it’s a better card in white/blue. So we set the conditional numbers accordingly. Ideally, we’d then figure out the value we have in each color pair, times the estimated probability we end up in that pair from this point in the draft. The resulting behavior would then tell us what adjustments we’d need to keep average pick numbers in line with the humans.</p>
<p>For the quality of color pairs, we can look at frequency of them being drafted by humans that have strong overall records, and we can look at the records of drafts containing those colors. Results have statistical issues, since the decision to draft an unusual deck could tend to happen when conditions are unusually good (e.g. I only draft this color when I open a bomb) or unusually bad (nothing else was available), so we should expect some distortions that will be hard to properly correct for. Given that the system is trying to correct card valuations on a card by card basis, the system could also get caught in a loop where it can’t satisfy all its masters at once, because humans are not telling a consistent story. We’re like that. Thus, I’d be inclined to do some sanity checking of the computer’s tenancies here and do manual adjustments where needed.</p>
<p>That brings another important point. Magic Online and Magic Arena draft rates for different color combinations should ideally look similar at similar drafter skill levels, and have similar win rates. This not being true would be a sign that the bots are doing something wrong, and this being close enough to true is a sign we’re doing at least some things right.</p>
<p>There are lots of checks like this, and lots of ways to forcibly adjust the system. We should focus on the ones that are primary to player experience, and ensure those work even if the bots have to do things in quirky ways in other places to make that happen. In particular, players should be drafting about the same color combinations with about the same win rates for them, and with about the same concentration of duplicate cards, and about the same frequency of being passed the best cards in the set. And the bots should pass rares about as often as humans do on Arena.</p>
<p>That doesn’t solve the problem of what happens if a color pair has two or more distinct strategies that it supports. If you <em>knew </em>this was the case, and could make a rule for recognizing which was which, you could have the computer treat them as distinct color pair options, and everything would work properly with enough work. But asking the computer to figure this out on its own seems like quite the rough ask, especially if some of them are rare, such as relying on getting particular uncommon cards like High Alert or Time of Heroes.</p>
<p>We could try to apply machine learning to the contents of successful decks to see if such patterns emerge on their own. And if we have enough resources to try, it’s worth trying. But I’m guessing that for practical purposes, when there are multiple major distinct things going on, a human is going to have to prompt the system to know this. The whole proposal here is about making something human-readable so it can be adjusted to serve our needs and create meaningful but realistic feeling variations and bot personalities, as opposed to a machine learning mess that might create better draft decks but which wouldn’t (I believe) serve our needs better.</p>
<p><strong>Problem 4: Drafting a Cohesive Deck</strong></p>
<p>Magic has many levels. If you draft the good cards in two colors, you’ll have a deck of playable cards most of the time. If you then take the best 23 cards and pair them with 17 lands(adjusting those counts a little when mana costs are especially low or high) with the lands chosen weighted by maximizing the total value of your spells after discounting, rather than current land defaults, those decks would likely play fine at Friday Night Magic.</p>
<p>We still have the capability to do a lot better. By doing so, we let subtle things in the draft matter in ways that will make drafting a better and richer experience, even if no one ever looks at the bot decks, or uses the bot decks for anything. We’d like to know that if we cut someone off of creatures, they’ll need to prioritize them later in the draft, or if they are glutting on four drops they’ll start passing us good ones, or if we give them a bunch of tribal lords they’ll focus more on tribe members.</p>
<p>What are some low hanging fruit we can pick to 80/20 these improvements, and get the bots to where their decisions are at least reasonable?</p>
<p>I think we can safely think like humans, and create a few weighting categories to implement, and that will get us to the 80/20 spot. Beyond that, this is a spot where machine learning would be at its best, and be the least disruptive to making the overall package human readable, so I think that’s what you’d look to if you wanted to go beyond that.</p>
<p>We start with a model that takes the quality of each deck by adding up the quality of its cards discounted by how easy they are to cast, and adds that up, comparing that to the results. That’s our baseline, and we look for how much factors cause humans to underperform or overperform, to figure out what weights to attach.</p>
<p>First, I’d assume pairwise synergies were real. So if there was an Elf lord, you’d expect players who had him to do better with more elves than you’d expect from their decks otherwise. Sometimes these would be subtle, including things humans might miss.</p>
<p>Second, I’d look at mana curve. We would assume that having a smooth curve has some value, both for creature-like things and for things in general. We would assume that one did not want to generally be too slow, although being super fast can go either way (sometimes it’s just you running out of power, sometimes you win a lot of your games on turn five).</p>
<p>Third, I’d look at counts for various types of effects, especially creatures and removal, but also card draw slash filtering, counters, and mana acceleration.</p>
<p>In some cases where the synergies were obvious (e.g. proliferate in War of the Spark) we’d check to see if the first check was good enough to cover this, or if we need to do something manually. My guess is that pairwise is <em>mostly </em>good enough to 80/20 this, but with exceptions, and with benefits for doing something more complex.</p>
<h4>Problem 5: Mimic versus Optimize</h4>
<p>There are two things one can seek to centrally optimize for at almost every step of this process.</p>
<p>One option is to mimic human behavior. We’d seek to make the same decisions and have the same statistical properties, on average, as humans of the right reference class making those decisions. We learn from humans what their revealed preferences are, and match them.</p>
<p>The other option is to maximize results (or some combination of results and rarity/value). We form a system for predicting how decks will do based on the cards they contain, and we maximize the quality of the drafted deck as much as possible. We could look at overall draft deck results, or even drill in deep (in a way that I don’t suggest doing here, it seems far too complex and hard for a first step) and ask whether drawing a particular card in games is helpful. If humans fail to realize that a card is awesome, the bots should pick up on this and prioritize it. If humans consistently first pick a card, but the card is bad and doesn’t help you win, the bots should let the humans have it.</p>
<p>Balancing these two approaches will be one of the keys to success. If we focus too much on mimicry without a good reason why, the resulting decks won’t be good and we’ll be duplicating the biases and errors we see, often compounding them in order to make things come out even. If we focus too much on optimization, it risks being invisible to players, looking like it’s reaching dumb conclusions and making terrible decisions, or reaching actually dumb conclusions and making actually terrible decisions. It could backfire quite badly. Thus, in most places, I emphasize mimicry and trusting the humans, and only don’t do this in the synergy department because I couldn’t find a clean way to do so there that seemed like it would work and be reasonable to implement.</p>
<h3>Conclusion</h3>
<p>The problem of how to draft as a human is deep enough to keep us engaged for a long time. So is the problem of programming a bot to do so, no matter which goals one prioritizes. I hope this points us in the direction of how to get a reasonable first attempt, so we can start incorporating some of the most important principles of good drafting, allowing reasonable self-play and a better drafting experience for humans. Doubtless better coders than I would be required to make it happen, and would point to many of these things and say ‘that’s way harder than you think it is’ or ‘that won’t work.’ Others will prove difficult or unworkable in practice. But the first step is making a real effort, iterating until we can get something that is at least trying, and which is designed to allow growth as it learns more.</p>
<p>From there, the sky’s the limit.</p>
<p> </p>
<p> </p>
<p> </p>",Zvi,zvi,Zvi,
n6FTwZRki8po2gBvw,"Is the ""business cycle"" an actual economic principle?",is-the-business-cycle-an-actual-economic-principle-1,https://www.lesswrong.com/posts/n6FTwZRki8po2gBvw/is-the-business-cycle-an-actual-economic-principle-1,2019-06-18T14:52:00.348Z,43,14,15,False,True,,"<p>E.g. <a href=""https://www.investopedia.com/terms/b/businesscycle.asp"">Investopedia: Business Cycle</a> has a section &quot;Stages of the Business Cycle&quot;:</p><blockquote>All business cycles are characterized by several different stages, as seen below.</blockquote><blockquote><strong>1. Expansion.</strong> This is the first stage. When expansion occurs, there is an increase in employment, incomes, production, and sales. People generally pay their debts on time. The economy has a steady flow in the money supply and investment is booming.</blockquote><blockquote><strong>2. Peak.</strong> The second stage is a peak when the <u><a href=""https://www.investopedia.com/articles/investing/042915/5-industries-driving-us-economy.asp"">economy</a></u> hits a snag, having reached the maximum level of growth. Prices hit their highest level, and economic indicators stop growing. Many people start to restructure as the economy&#x27;s growth starts to reverse.</blockquote><blockquote><strong>3. Recession:</strong> These are periods of contraction. During a recession, unemployment rises, production slows down, sales start to drop because of a decline in demand, and incomes become stagnant or decline.</blockquote><blockquote><strong>4. Depression:</strong> <u><a href=""https://www.investopedia.com/articles/economics/12/okuns-law.asp"">Economic growth</a></u> continues to drop while unemployment rises and production plummets. Consumers and businesses find it hard to secure credit, trade is reduced, and bankruptcies start to increase. Consumer confidence and investment levels also drop.</blockquote><blockquote><strong>5. Trough:</strong> This period marks the end of the depression, leading an economy into the next step: recovery.</blockquote><blockquote><strong>6. Recovery:</strong> In this stage, the economy starts to turn around. Low prices spur an increase in demand, employment and production start to rise, and lenders start to open up their credit coffers. This stage marks the end of one business cycle.</blockquote><p>If all you know is that <a href=""https://www.lesswrong.com/posts/h24JGbmweNpWZfBkM/markets-are-anti-inductive"">markets are anti-inductive</a>, and therefore anticipate a random walk with an average of 10%/year growth, don&#x27;t you automatically expect the stock market fluctuations to sometimes look like bull markets and recessions?</p><p>So does the &quot;business cycle&quot; add any explanatory power, or is it merely the type of retroactive non-explanation that you get when you read a news article about the 1-day movement of the stock market?</p><p>Maybe you can argue that the business cycle is self-reinforcing due to psychological reactions.</p><p>But to the degree that the business cycle is a separate understandable phenomenon, can&#x27;t investors use that understanding to place bets which make them money while dampening the effect?</p>",Liron,liron,Liron,
TMFNQoRZxM4CuRCY6,Reason isn't magic,reason-isn-t-magic,https://www.lesswrong.com/posts/TMFNQoRZxM4CuRCY6/reason-isn-t-magic,2019-06-18T04:04:58.390Z,164,86,19,False,False,http://benjaminrosshoffman.com/reason-isnt-magic/,"<p>Here&#x27;s a story some people <a href=""https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/"">like to tell</a> about the limits of reason. There&#x27;s this plant, manioc, that grows easily in some places and has a lot of calories in it, so it was a staple for some indigenous South Americans since before the Europeans showed up. Traditional handling of the manioc involved some elaborate time-consuming steps that had no apparent purpose, so when the Portuguese introduced it to Africa, they didn&#x27;t bother with those steps - just, grow it, cook it, eat it.</p><p>The problem is that manioc&#x27;s got cyanide in it, so if you eat too much too often over a lifetime, you get sick, in a way that&#x27;s not easily traceable to the plant. Somehow, over probably hundreds of years, the people living in manioc&#x27;s original range figured out a way to leach out the poison, without understanding the underlying chemistry - so if you asked them why they did it that way, they wouldn&#x27;t necessarily have a good answer.</p><p>Now a bunch of Africans growing and eating manioc as a staple regularly get cyanide poisoning.</p><p>This is offered as a cautionary tale against innovating through reason, since there&#x27;s a lot of information embedded in your culture (via hundreds of years of selection), even if people can&#x27;t explain why. The problem with this argument is that it&#x27;s a nonsense comparison. </p><p>First of all, it&#x27;s not clear things got worse on net, just that a tradeoff was made. How many person-days per year were freed up by less labor-intensive manioc handling? Has anyone bothered to count the hours lost to laborious traditional manioc-processing, to compare them with the burden of consuming too much cyanide? How many of us, knowing that convenience foods probably lower our lifespans relative to slow foods, still eat them because they&#x27;re ... more convenient?</p><p>How many people <strong><em>didn&#x27;t starve</em></strong> because manioc was available and would grow where and when other things wouldn&#x27;t?</p><p>If this is the best we can do for how poorly reason can perform, reason seems pretty great.</p><p>Second, we&#x27;re not actually comparing reason to tradition - we&#x27;re comparing <em>changing things</em> to <em>not changing things</em>. Change, as we know, <a href=""https://thezvi.wordpress.com/2017/07/20/change-is-bad/"">is bad</a>. Sometimes we change things anyway - when we think it&#x27;s worth the price, or the risk. Sometimes, we&#x27;re wrong.</p><p>Third, the actually existing Portuguese and Africans involved in this experiment weren&#x27;t committed rationalists - they were just people trying to get by. It probably doesn&#x27;t take more than a day&#x27;s reasoning to figure out which steps in growing manioc are really necessary to get the calories palatably. Are we imagining that someone making a concerted effort to improve their life through reason would just stop there?</p><p>This is being compared with <em>many generations</em> of trial and error. Is that the standard we want to use? Reasoning isn&#x27;t worth it unless a day of untrained thinking can outperform hundreds of years of accumulated tradition?</p><p>It gets worse. This isn&#x27;t a randomly selected example - it&#x27;s specifically selected as a case where reason would have a hard time noticing when and how it&#x27;s making things worse. In this particular case, reason introduced an important problem. But life is full of risks, sometimes in ways that are worse for traditional cultures. Do we really want to say that reasoning isn&#x27;t the better bet unless it outperforms literally every time, without ever making things locally worse? Even theoretically perfect Bayesian rationality will sometimes recommend changes that have an expected benefit, but turn out to be harmful. Not even tradition meets this standard! Only logical certainties do - provided, that is, we haven&#x27;t made an error in one of our proofs.</p><p>We also have to count all the deaths and other problems averted by reasoning about a problem. Reasoning introduces risks - but also, risks come up even when we&#x27;re not reasoning about them, just from people doing things that affect their environments. There&#x27;s absolutely no reason to think that the sort of gradual iteration that accretes into tradition never enters a bad positive feedback loop. Even if you think modernity is an exceptional case of that kind of bad feedback loop, we had to have gotten there via the accretion of premodern tradition and iteration!</p><p>The only way out is through. But why did we have this exaggerated idea of what reason could do, in the first place?</p>",Benquo,benquo,Benquo,
EYKBME45wWwRW3MLh,Discussion Thread: The AI Does Not Hate You by Tom Chivers,discussion-thread-the-ai-does-not-hate-you-by-tom-chivers,https://www.lesswrong.com/posts/EYKBME45wWwRW3MLh/discussion-thread-the-ai-does-not-hate-you-by-tom-chivers,2019-06-17T23:43:00.297Z,26,10,10,False,False,,"<p>This post is provided as convenient place to discussion of the new book, <a href=""https://www.amazon.com/Does-Not-Hate-You-Superintelligence-ebook/dp/B07K258VCV"">The AI Does Not Hate You</a> by Tom Chivers, which covers LessWrong and rationalist community.</p><blockquote><strong>The AI Does Not Hate You:</strong> <strong>Superintelligence, Rationality and the Race to Save the World</strong></blockquote><blockquote>This is a book about AI and AI risk. But it&#x27;s also more importantly about a community of people who are trying to think rationally about intelligence, and the places that these thoughts are taking them, and what insight they can and can&#x27;t give us about the future of the human race over the next few years. It explains why these people are worried, why they might be right, and why they might be wrong. It is a book about the cutting edge of our thinking on intelligence and rationality <em>right now</em> by the people who stay up all night worrying about it.</blockquote><p>Note that the book is available on Kindle only to people with UK/European Amazon accounts. I was able to order it in physical copy in the US, but I haven&#x27;t received a shipping notification yet.</p>",Ruby,ruby,Ruby,
CSEdLLEkap2pubjof,Research Agenda v0.9: Synthesising a human's preferences into a utility function,research-agenda-v0-9-synthesising-a-human-s-preferences-into,https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into,2019-06-17T17:46:39.317Z,74,25,26,False,False,,"<p>I'm now in a position where I can see a possible route to a safe/survivable/friendly Artificial Intelligence being developed. I'd give a 10+% chance of it being possible this way, and a 95% chance that some of these ideas will be very useful for other methods of alignment. So I thought I'd encode the route I'm seeing as research agenda; this is the first public draft of it.</p>
<p>Clarity, rigour, and practicality: that's what this agenda needs. Writing this agenda has clarified a lot of points for me, to the extent that some of it now seems, in retrospect, just obvious and somewhat trivial - ""of course that's the way you have to do X"". But more clarification is needed in the areas that remain vague. And, once these are clarified enough for humans to understand, they need to be made mathematically and logically rigorous - and ultimately, cashed out into code, and tested and experimented with.</p>
<p>So I'd appreciate any comments that could help with these three goals, and welcome anyone interested in pursuing research along these lines over the long-term.</p>
<p><strong>Note</strong>: I periodically edit this document, to link it to more recent research ideas/discoveries.</p>
<h1>0 The fundamental idea</h1>
<p>This agenda fits itself into the broad family of <a href=""https://ai.stanford.edu/~ang/papers/icml00-irl.pdf"">Inverse</a> <a href=""https://arxiv.org/abs/1606.03137"">Reinforcement</a> <a href=""https://www.youtube.com/watch?v=Ts-nTIYDXok"">Learning</a>: delegating most of the task of inferring human preferences to the AI itself. <strong>Most</strong> of the task, since it's been shown that humans need to build the right assumptions into the AI, or else <a href=""https://arxiv.org/abs/1712.05812"">the preference learning will fail</a>.</p>
<p>To get these ""right assumptions"", this agenda will look into what preferences actually are, and how they may be combined together. There are hence four parts to the research agenda:</p>
<ol>
<li>A way of identifying the (partial<sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-1"" id=""fnref-P3TWjrKkBbha8q8zh-1"">[1]</a></sup>) preferences of a given human <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span>.</li>
<li>A way for ultimately synthesising a utility function <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> that is an adequate encoding of the partial preferences of a human <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>.</li>
<li>Practical methods for estimating this <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, and how one could use the definition of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> to improve other suggested methods for value-alignment.</li>
<li>Limitations and lacunas of the agenda: what is not covered. These may be avenues of future research, or issues that cannot fit into the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> paradigm.</li>
</ol>
<p>There has been a <a href=""https://www.lesswrong.com/users/stuart_armstrong"">myriad of small posts</a> on this topic, and most will be referenced here. Most of these posts are stubs that hint to a solution, rather than spelling it out fully and rigorously.</p>
<p>The reason for that is to check for impossibility results ahead of time. The construction of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is deliberately designed to be <strong>adequate</strong>, rather than elegant (indeed, the search for an elegant <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> might be counterproductive and even dangerous, if genuine human preferences get sacrificed for elegance). If this approach is to work, then the safety of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> has to be robust to different decisions in the synthesis process (see Section 2.8, on avoiding disasters). Thus, initially, it seems more important to find approximate ideas that cover all possibilities, rather than having a few fully detailed sub-possibilities and several gaps.</p>
<p>Finally, it seems that if a sub-problem is not formally solved, we stand a much better chance of getting a good result from ""hit it with lots of machine learning and hope for the best"", than we would if there were huge <em>conceptual holes</em> in the method - a conceptual hole meaning that the relevant solution is broken in an unfixable way. Thus, I'm publishing this agenda now, where I see many implementation holes, but no large conceptual holes.</p>
<p>A word of warning here, though: <a href=""https://www.lesswrong.com/posts/fHSf8ACvTCvH9fFyd/ai-prediction-case-study-1-the-original-dartmouth-conference"">with some justification</a>, the original Dartmouth AI conference could also have claimed to be confident that there were no large conceptual holes in their plan of developing AI over a summer - and we know how <a href=""https://www.tandfonline.com/doi/pdf/10.1080/0952813X.2014.895105"">wrong they turned out to be</a>. With that thought in mind, onwards with the research agenda.</p>
<h2>0.1 Executive summary: synthesis process</h2>
<p>The first idea of the project is to identify partial preferences as residing within human mental models. This requires identifying the actual and hypothetical internal variables of a human, and thus solving the ""symbol grounding problem"" for humans; ways of doing that are proposed.</p>
<p>The project then sorts the partial preferences into various categories of interest (basic preferences about the world, identity preferences, meta-preferences about basic preferences, global meta-preferences about the whole synthesis project, etc...). The aim is then to synthesise these into a single utility function <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, representing the preference of the human <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span> (at a given time or short interval of time). Different preference categories play different roles in this synthesis (eg object-level preferences get aggregated, meta-preferences can modify the weights of object-level preferences, global meta-preferences are used at the design stage, and so on).</p>
<p>The aims are to:</p>
<ol>
<li>Ensure the synthesis <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> has good properties and reflects <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s actual preferences, <strong>and not any of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s erroneous factual beliefs</strong>.</li>
<li>Ensure that highly valued preferences weight more than lightly held ones, even if the lightly held one is more ""meta"" that the other.</li>
<li>Respect meta-preferences about the synthesis as much as possible, but...</li>
<li>...always ensure that the synthesis actually reaches an actual non-contradictory <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>.</li>
</ol>
<p>To ensure point 4. and 2., there will always be an initial way of synthesising preferences, which certain meta-preferences can then modify in specific ways. This is designed to resolve contradictions (when ""I want a simple moral system"" and ""value is fragile and needs to be preserved"" are both comparably weighted meta-preferences) and remove preference loops (""I want a simple moral system"" is itself simple and could reinforce itself; ""I want complexity in my values"" is also simple and could undermine itself).</p>
<p>The ""good properties"" of 1. are established, in large part, by the global meta-preferences that don't comfortably sit within the synthesis framework. As for erroneous beliefs, if <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span> wants to date <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H'""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msup""><span class=""mjx-base"" style=""margin-right: -0.057em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.176em; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span></span></span></span></span></span> because they think that would make them happy and respected, then an AI will synthesise ""being happy"" and ""being respected"" as preferences, and would push <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span> away from <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H'""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msup""><span class=""mjx-base"" style=""margin-right: -0.057em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.176em; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span></span></span></span></span></span> if <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span> were actually deluded about what dating them would accomplish.</p>
<p>That is the main theoretical contribution of the research agenda. It then examines what could be done with such a theory in practice, and whether the theory can be usefully approximated for constructing an actual utility function for an AI.</p>
<h2>0.2 Executive summary: agenda difficulty and value</h2>
<p>One early commentator on this agenda remarked:</p>
<blockquote>
<p>[...] it seems like this agenda is trying to solve at least 5 major open problems in philosophy, to a level rigorous enough that we can specify them in code:</p>
<ol>
<li>The symbol grounding problem.</li>
<li>Identifying what humans really care about (not just what they say they care about, or what they act like they care about) and what preferences and meta-preferences even are.</li>
<li>Finding an acceptable way of making incomplete and inconsistent (meta-)preferences complete and consistent.</li>
<li>Finding an acceptable way of aggregating many people's preferences into a single function<sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-2"" id=""fnref-P3TWjrKkBbha8q8zh-2"">[2]</a></sup>.</li>
<li>The nature of personal identity.</li>
</ol>
<p>I agree that AI safety researchers should be more ambitious than most researchers, but this seems extremely ambitious, and I haven't seen you acknowledge the severe outside-view difficulty of this agenda.</p>
</blockquote>
<p>This is indeed an extremely ambitious project. But, in a sense, a successful aligned AI project will <em>ultimately</em> have to solve all of these problems. Any situation in which most of the future trajectory of humanity is determined by AI, is a situation where there are solutions to all of these problems.</p>
<p>Now, these solutions may be implicit rather than explicit; equivalently, we might be able to delay solving them via AI, for a while. For example, a <a href=""https://www.lesswrong.com/posts/nAwTGhgrdxE85Bjmg/tools-versus-agents"">tool AI</a> solves these issues by being contained in such a way that human judgement is capable of ensuring good outcomes. Thus <em>humans</em> solve the grounding problem, and we design our questions to the AI to ensure compatibility with our preferences, and so on.</p>
<p>But as the power of AIs increase, humans will become confronted by situations they have never been in before, and our ability to solve these issues diminish (and the probabilities increase that we might be manipulated or fall into a bad attractor). This transition may sneak up on us, so it is useful to start thinking of how to a) start solving these problems, and b) start identifying these problems crisply so we can know when and whether they need to be solved, and when we are moving out of the range of validity of the ""trust humans"" solution. For both these reasons, all the issues will be listed explicitly in the research agenda.</p>
<p>A third reason to include them is so that we know what we need to solve those issues <em>for</em>. For example, it is easier to assess the quality of any solution to symbol grounding, if we know what we're going to do with that solution. We don't need a full solution, just one good enough to define human partial preferences.</p>
<p>And, of course, we need to also consider scenarios where partial approaches like tool AI just don't work, or only work if we solve all the relevant issues anyway.</p>
<p>Finally, there is a converse: partial solutions to problems in this research agenda can contribute to improving other methods of AI safety alignment. Section 3 will look into this in more detail. The basic idea is that, to improve an algorithm or an approach, it is very useful to know what we are ultimately trying to do (eg compute partial preferences, or synthesise a utility function with certain acceptable properties). If we rely only on making local improvements, guided by intuition, we may ultimately get stuck when intuition runs out; and the improvements are more likely to be ad-hoc patches than consistent, generalisable rules.</p>
<h2>0.3 Executive aside: the value of approximating the theory</h2>
<p>The theoretical construction of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> in Sections 1 and 2 is a highly complicated object, involving millions of unobserved counterfactual partial preferences and a synthesis process involving higher-order meta-preferences. Section 3 touches on how <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> could be approximated, but, given its complexity, it would seem that the answer would be ""only very badly"".</p>
<p>And there is a certain sense in which this is correct. If <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_V""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span></span></span></span></span></span> is the actual idealised utility defined by the process, and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""V_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.186em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is the approximated utility that a real-world AI could compute, then it is likely<sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-3"" id=""fnref-P3TWjrKkBbha8q8zh-3"">[3]</a></sup> that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""V_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.186em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> will be quite different in many formal senses.</p>
<p>But there is a certain sense in which this is incorrect. Consider many of the AI failure scenarios. Imagine that the AI, for example, extinguished all meaningful human interactions because these can sometimes be painful and the AI knows that we prefer to avoid pain. But it's clear to us that most people's partial preferences will not endorse total loneliness as good outcome; if it's clear to us, then it's <em>a fortiori</em> clear to a very intelligent AI; hence the AI will avoid that failure scenario.</p>
<p>One should be careful with using arguments of this type, but it is hard to see how there could be a failure mode that a) we would clearly understand is incompatible with proper synthesis of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, but b) a smart AI would not. And it seems that any failure mode <a href=""https://www.lesswrong.com/posts/rArsypGqq49bk4iRr/can-there-be-an-indescribable-hellworld"">should be understandable to us</a>, as a failure mode, especially given some of the innate conservatism of the construction of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>.</p>
<p>Hence, even if <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""V_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.186em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is a poor approximation of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> in a certain sense, it is likely an excellent approximation of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""V_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.186em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> in the sense of avoiding terrible outcomes. So, though <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""d(U_H,V_H)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-msubsup MJXc-space1""><span class=""mjx-base"" style=""margin-right: -0.186em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> might be large for some formal measure of distance <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""d""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span></span></span></span></span>, a world where the AI maximises <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""V_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.186em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> will be highly ranked according to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>.</p>
<h2>0.4 An inspiring just-so story</h2>
<p>This is the story of how evolution created humans with preferences, and what the nature of these preferences are. The story is not true, in the sense of accurate; instead, it is intended to provide some inspiration as to the direction of this research agenda. This section can be skipped.</p>
<p>In the beginning, evolution created instinct driven agents. These agents had no preferences or goals, nor did they need any. They were like <a href=""https://en.wikipedia.org/wiki/Q-learning"">Q-learning agents</a>: they knew the correct action to take in different circumstances, but that was it. Consider baby turtles that walk towards the light upon birth, because, traditionally, the sea was lighter than the land - of course, this behaviour fails them in the era of artificial lighting.</p>
<p>But evolution has a tiny bandwidth, acting once per generation. So it created agents capable of planning, of figuring out different approaches, rather than having to follow instincts. This was useful, especially in varying environments, and so evolution offloaded a lot of its ""job"" onto the planning agents.</p>
<p>Of course, to be of any use, the planning agents need to be able to model their environment to some extent (or else their plans can't work) and had to have preferences (or else every plan was as good as another). So, in creating the first planning agents, evolution created the first agents with preferences.</p>
<p>Of course, evolution is a messy, undirected process, so the process wasn't clean. Planning agents are still riven with instincts, and the modelling of the environment is situational, used for when it was needed, rather than some consistent whole. Thus the ""preferences"" of these agents were underdefined and sometimes contradictory.</p>
<p>Finally, evolution created agents capable of self-modelling and of modelling other agents in their species. This might have been because of <a href=""https://en.wikipedia.org/wiki/Red_Queen_hypothesis"">competitive social pressures</a> as agents <a href=""https://en.wikipedia.org/wiki/Machiavellian_intelligence"">learn to lie and detect lying</a>. Of course, this being evolution, this self-and-other-modelling took the form of kludges built upon <a href=""https://en.wikipedia.org/wiki/Spandrel_(biology)"">spandrels</a> built upon kludges.</p>
<p>And then arrived humans, who developed <a href=""https://80000hours.org/podcast/episodes/robin-hanson-on-lying-to-ourselves/"">norms and norm-violations</a>. As a side effect of this, we started having higher-order preferences as to what norms and preferences <em>should</em> be. But instincts and contradictions remained - this is evolution, after all.</p>
<p>And evolution looked upon this hideous mess, and <a href=""http://worldpopulationreview.com/continents/world-population/"">saw that it was good</a>. Good for evolution, that is. But if we want it to be good for us, we're going to need to straighten out this mess somewhat.</p>
<h1>1 The partial preferences of a human</h1>
<p>The main aim of this research agenda is to start with a human <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span> at or around a given moment <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span> and produces a utility function <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_{H_t}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.057em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span><span class=""mjx-sub"" style=""font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span></span></span></span></span> which is an adequate synthesis of the human's preferences at the time <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span>. Unless the dependence on <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span> needs to be made explicit, this will simply be designated as <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>.</p>
<p>Later sections will focus on what can be done with <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> or the methods used for its construction; this section and the next will focus solely on that construction. It is mainly based on <a href=""https://www.lesswrong.com/posts/Y2LhX3925RodndwpC/resolving-human-values-completely-and-adequately"">these</a> <a href=""https://www.lesswrong.com/posts/qezBTig6p6p5xtL6G/a-theory-of-human-values"">posts</a>, with some commentary and improvements.</p>
<p>Essentially the process is to identify human preferences and meta-preferences within human (partial) mental model (Section 1), and find some good way of synthesising these into a whole <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> (Section 2).</p>
<p>Partial preferences (see Section 1.1) will be decomposed into:</p>
<ol>
<li>Partial preferences about the world.</li>
<li>Partial preferences about our own identity.</li>
<li>Partial meta-preferences about our preferences.</li>
<li>Partial meta-preferences about the synthesis process.</li>
<li>Self-referential contradictory partial meta-preferences.</li>
<li>Global meta-preferences about the outcome of the synthesis process.</li>
</ol>
<p>This section and the next will lay out how preferences of types 1, 2, 3, and 4 can be used to synthesise the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>. Section 2 will conclude by looking what role preferences of type 6 can play. Preferences of type 5 are not dealt with in this agenda, and remain a perennial problem (see Section 4.5).</p>
<h2>1.1 Partial models, partial preferences</h2>
<p>As was shown in the paper ""<a href=""https://arxiv.org/abs/1712.05812"">Occam's razor is insufficient to infer the preferences of irrational agents</a>"", an agent's behaviour is never enough to establish their preferences - even with simplicity priors or regularisation (see also <a href=""https://www.lesswrong.com/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever"">this post</a> and <a href=""https://www.lesswrong.com/posts/YfQGZderiaGv3kBJ8/figuring-out-what-alice-wants-non-human-alice"">this one</a>).</p>
<p>Therefore a definition of preference needs to be grounded in something other than behaviour. There are further arguments, <a href=""https://www.lesswrong.com/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values"">presented here</a>, as to why a theoretical grounding is needed even when practical methods are seemingly adequate; this point will be returned to later.</p>
<p>The first step is to define a partial preference (and a partial model for these to exist in). A partial preference is a preference that exists <a href=""https://www.lesswrong.com/posts/rcXaY3FgoobMkH2jc/figuring-out-what-alice-wants-part-ii"">within a human being's internal mental model</a>, and which contrasts two<sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-4"" id=""fnref-P3TWjrKkBbha8q8zh-4"">[4]</a></sup> situations along a single axis of variation, keeping other aspects constant. For example, ""I wish I was rich (rather than poor)"", ""I don't want to go down that alley, lest I get mugged"", and ""this is much worse if there are witnesses around"" are all partial preferences. A more formal definition of partial preferences, and the partial mental model in which they exist, is <a href=""https://www.lesswrong.com/posts/CiB3myyeEhFRgmKPL/partial-preferences-and-models"">presented here</a>.</p>
<p><strong>Note that this is one of the fundamental theoretical underpinnings of the method. It identifies human (partial) preferences as existing within human mental models.</strong> This is a ""normative assumption"": we <em>choose</em> to define these features as (partial) human preferences, the universe does not compel us to do so.</p>
<p>This definition gets around the ""Occam's razor"" impossibility result, since these mental models are features of the human brain's internal process, not of human behaviour. Conversely, this also violates certain versions of <a href=""https://plato.stanford.edu/entries/functionalism/"">functionalism</a>, precisely because the internal mental states are relevant.</p>
<p>A key important feature is to extract not only the partial preferences itself, but the intensity of the preferences, referred to as its <em>weight</em>. This will be key in combining the preferences together (technically, we only need the weight relative to other partial preferences).</p>
<h2>1.2 Symbol grounding</h2>
<p>In order to interpret what a partial model means, we need to solve the old problem of <a href=""https://en.wikipedia.org/wiki/Symbol_grounding_problem"">symbol grounding</a>. ""I wish I was rich"" was presented as an example of a partial preference; but how can we identify ""I"", ""rich"" and the counterfactual ""I wish"", all within the mess of the neural net that is the human brain?</p>
<p>To ground these symbols, we should approach the issue of symbol grounding empirically, by aiming to <a href=""https://www.lesswrong.com/posts/EEPdbtvW8ei9Yi2e8/bridging-syntax-and-semantics-empirically"">predict the values of real world-variables</a> through knowledge of internal mental variables (see also the example <a href=""https://www.lesswrong.com/posts/bbw6c9as5STvWXAgB/syntax-vs-semantics-alarm-better-example-than-thermostat"">presented here</a>). This empirical approach can provide sufficient grounding for the purposes of partial models, even if <a href=""https://www.lesswrong.com/posts/XApNuXPckPxwp5ZcW/bridging-syntax-and-semantics-with-quine-s-gavagai"">symbol grounding is not solved in the traditional linguistic sense of the problem</a>.</p>
<p>This is because each symbol has a <a href=""https://www.lesswrong.com/posts/ix3KdfJxjo9GQFkCo/web-of-connotations-bleggs-rubes-thermostats-and-beliefs"">web of connotations</a>, a collection of other symbols and concepts that co-vary with it, in normal human experience. Since the partial models are generally defined to be within normal human experiences, there is little difference between any symbols that are strongly correlated.</p>
<p>To formalise and improve this definition, we'll have to be careful about <a href=""https://www.lesswrong.com/posts/pHHhyZX5zwvwNqDXm/finding-the-variables"">how we define the internal variables</a> in the first place - overly complicated or specific internal variables can be chosen to correlate artificially well with external variables. This is, essentially, ""symbol grounding overfitting"".</p>
<p>Another consideration is the extent to which the model is conscious or subconscious; <a href=""https://en.wikipedia.org/wiki/Alief_(mental_state)"">aliefs</a>, for example, could be modelled as subconscious partial preferences. For consciously endorsed aliefs, this is not much of a problem - we instinctively fear touching fires, and don't desire to lose that fear. But if we don't endorse that alief - for example, we might fear flying and not want to fear it - this becomes more tricky. Things get confusing with partially endorsed aliefs: amusement park rides are extremely safe, and we wouldn't want to be crippled with fear at the thought of going on one. But neither would we want the experience to feel perfectly bland and safe.</p>
<h2>1.3 Which (real and hypothetical) partial models?</h2>
<p>Another important consideration is that humans do not have, at the moment <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span>, a complete set of partial models and partial preferences. They may have a single partial model in mind, with maybe a few others in the background - or they might not be thinking about anything like this at all. We could extend the parameters to some short period <em>around</em> the time <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span> (reasoning that people's preferences rarely change in such a short time), but though that gives us more data, it doesn't give us nearly enough.</p>
<p>The most obvious way to get a human to produce an internal model is to ask them a relevant question. But we have to be careful about this - since human values are <a href=""https://www.lesswrong.com/posts/KCg7NeKQ7MycXWpYd/our-values-are-underdefined-changeable-and-manipulable"">changeable and manipulable</a>, the very act of asking a question can cause humans to think in certain directions, and even create partial preferences where none existed. The more interaction between the questioner and the human, the more extreme preferences can be created. If the questioner is motivated to maximise the utility function that it is also computing (i.e. if the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is an <em>online</em> learning process), then the questioner can <a href=""https://www.lesswrong.com/posts/upLot6eG8cbXdKiFS/reward-function-learning-the-learning-process"">rig or influence</a> the learning process.</p>
<p>Fortunately, there are ways of <a href=""https://agentfoundations.org/item?id=1294"">removing the questioner's incentives to rig or influence</a> the learning process.</p>
<p>Thus the basic human preferences at time <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span> are defined to be those partial models produced by ""<a href=""https://www.lesswrong.com/posts/i6hWWcKyxBPj7ELT6/one-step-hypotheticals"">one-step</a> <a href=""https://www.lesswrong.com/posts/zo5K8QeDZiLicSCe6/a-small-example-of-one-step-hypotheticals"">hypotheticals</a>""<sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-5"" id=""fnref-P3TWjrKkBbha8q8zh-5"">[5]</a></sup>. These are questions that do not cause the human to be put in unusual mental situations, and try and minimise any departure from the human's base-state. We need to distinguish between <a href=""https://www.lesswrong.com/posts/eptGDwahJPvhqDgvF/simple-and-composite-partial-preferences"">simple and composite</a> partial preferences: the latter happen when a hypothetical question elicits a long chain of reasoning, covering multiple partial preferences, rather than a single clear answer based on a single internal model.</p>
<p>Some <a href=""https://www.lesswrong.com/posts/uHb2LDW3LGhBMyq74/preference-conditional-on-circumstances-and-past-preference"">preferences are conditional</a> (eg ""I want to eat something different from what I've eat so far this week""), as are some <a href=""https://www.lesswrong.com/posts/ic8yoGBMYLtaJkbxZ/conditional-meta-preferences"">meta-preferences</a> (eg ""If I hear a convincing argument about X being good, I want to prefer X""), which could violate the point of the one-step hypothetical. Thus conditional (meta-)preferences are only acceptable if their conditions are achieved by short streams of data, unlikely to manipulate the human. They also should be weighted more if they fit a consistent narrative of what the human is/wants to be, rather than being ad hoc (this will be assessed by machine learning, see Section 2.4).</p>
<p>Note that among the one-step hypotheticals, are included questions about rather extreme situations - heaven and hell, what to do if plants were conscious, and so on. In general, we should reduce the weight<sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-6"" id=""fnref-P3TWjrKkBbha8q8zh-6"">[6]</a></sup> of <a href=""https://www.lesswrong.com/posts/WAmpz8Z4FZ4FbCNtR/models-of-preferences-in-distant-situations"">partial preferences in extreme situations</a><sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-7"" id=""fnref-P3TWjrKkBbha8q8zh-7"">[7]</a></sup>. This is because of the unfamiliarity of these situations, and because the usual human <a href=""https://www.lesswrong.com/posts/ix3KdfJxjo9GQFkCo/web-of-connotations-bleggs-rubes-thermostats-and-beliefs"">web of connotations</a> between concepts may have broken down (if a plant was conscious, would it be a plant in the sense we understand that?). Sometimes the breakdown is so extreme that we can say that the partial preference <a href=""https://www.lesswrong.com/posts/am5ubSoSe6Hf5tnTF/being-wrong-in-ethics"">is factually wrong</a>. This includes effects like the <a href=""https://en.wikipedia.org/wiki/Hedonic_treadmill"">hedonic treadmill</a>: our partial models of achieving certain goals often include an imagined long-term satisfaction that we would not actually feel. Indeed, it might be good to <a href=""https://www.lesswrong.com/posts/PX8BB7Rqw7HedrSJd/by-default-avoid-ambiguous-distant-situations"">specifically avoid these extreme situations</a>, rather than having to make a moral compromise that might lose part of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s values due to uncertainty. In that case, ambiguous extreme situations get a slight intrinsic negative - that might be overcome by other considerations, but is there nonetheless.</p>
<p>A final consideration is that some concepts just disintegrate in general environments - for example, consider a preference for ""natural"" or ""hand-made"" products. In those cases, the web of connotations can be used <a href=""https://www.lesswrong.com/posts/am5ubSoSe6Hf5tnTF/being-wrong-in-ethics"">to extract some preferences</a> in general - for example, ""natural"", used in this way has connotations<sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-8"" id=""fnref-P3TWjrKkBbha8q8zh-8"">[8]</a></sup> of ""healthy"", ""traditional"", and ""non-polluting"", all of which extend better to general environments than ""natural"" does. Sometimes, the preference can be preserved but <a href=""https://www.lesswrong.com/posts/FgRRY3vSwDLx2Hk46/the-low-cost-of-human-preference-incoherence"">routed around</a>: some versions of ""no artificial genetic modifications"" could be satisfied by selective breeding that achieved the same result. And some versions couldn't; it's all a function of what powers the underlying preference: specific techniques, or a general wariness of these types of optimisation. Meta-preferences might be very relevant here.</p>
<h1>2 Synthesising the preference utility function</h1>
<p>Here we will sketch out the construction of the human utility function <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, from the data that is the partial preferences and their (relative) weights.</p>
<p><strong>This is not, by any means, the only way of constructing <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span></strong>. But it is illustrative of how the utility could be constructed, and can be more usefully critiqued and analysed than a vaguer description.</p>
<h2>2.1 What sort of utility function?</h2>
<p>Partial preferences are defined over states of the world or states of the human <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>. The later included both things like ""being satisfied with life"" (purely internal) and ""being an honourable friend"" (mostly about <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s behaviour).</p>
<p>Consequently, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> must also be defined over such things, so <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is dependent on states of the world and states of the human <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>. Unlike standard <a href=""https://en.wikipedia.org/wiki/Markov_decision_process"">MDP</a>-like situations, these states can include the history of the world or of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span> up to that point - preferences like ""don't speak ill of the dead"" abound in humans.</p>
<h2>2.2 Why a utility function?</h2>
<p>Why should we aim to synthesise a utility function, when human preferences are <a href=""https://www.lesswrong.com/posts/KCg7NeKQ7MycXWpYd/our-values-are-underdefined-changeable-and-manipulable"">very far from being utility functions</a>?</p>
<p>It's not of an innate admiration for utility functions, or a desire for mathematical elegance. It's because they <a href=""https://www.alignmentforum.org/posts/nsbKeodxHJFKX2yYp/probabilistic-tiling-preliminary-attempt"">tend</a> <a href=""https://www.alignmentforum.org/posts/5bd75cc58225bf067037556d/logical-inductor-tiling-and-why-it-s-hard"">to be</a> <a href=""https://www.lesswrong.com/posts/ZTN6bLWqpwWn2i4qZ/money-pumping-the-axiomatic-approach"">stable</a> <a href=""https://www.lesswrong.com/posts/tGhz4aKyNzXjvnWhX/expected-utility-without-the-independence-axiom"">under</a> <a href=""https://www.lesswrong.com/posts/gc7EiRpkgGSEZDSve/expected-utility-unlosing-agents-and-pascal-s-mugging"">self</a>-<a href=""https://www.lesswrong.com/posts/Gksm692agaGdNHShw/model-of-unlosing-agents"">modification</a>. Or, to be more accurate, they seem to be much more stable than preferences that are not utility functions.</p>
<p>In the imminent future, human preferences <a href=""https://www.lesswrong.com/posts/4H8N3fEfXQmzxSaRo/upcoming-stability-of-values"">are likely to become stable and unchanging</a>. Therefore it makes more sense to create a preference synthesis that is already stable, that create a potentially unstable one and let it <a href=""https://www.lesswrong.com/posts/WeAt5TeS8aYc4Cpms/values-determined-by-stopping-properties"">randomly walk itself to stability</a> (though see Section 4.6).</p>
<p>Also, and this is one of the motivations behind classical <a href=""http://ai.stanford.edu/~ang/papers/icml00-irl.pdf"">inverse reinforcement learning</a>, reward/utility functions tend to be quite portable, and can be moved from one agent to another or from one situation to another, with greater ease than other goal structures.</p>
<h2>2.3 Extending and normalising partial preferences</h2>
<p>Human values are changeable, manipulable, underdefined, and contradictory. By focusing around time <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span>, we have removed the changeable problem for partial preferences (see <a href=""https://www.lesswrong.com/posts/i6hWWcKyxBPj7ELT6/one-step-hypothetical-preferences"">this post</a> for thoughts on how long a period around <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span> should be allowed); manipulable has been dealt with by removing the possibility of the AI influencing the learning process.</p>
<p>Being underdefined remains a problem, though. It would be possible to overfit absurdly specifically to the human's partial models, and generate a <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> that is in full agreement with our partial preferences and utterly useless. So the first thing to do is to group the partial preferences together according to similarity (for example, preferences for concepts closely related in terms of webs of connotations should generally be grouped together), and generalise them in some regularised way. Generalise means, here, that they are transformed into full preferences, comparing all possible universes. Though this would only be comparing on the narrow criteria that were used for the partial preference: a partial preference fear of being mugged could generalise to a fear of pain/violence/violation/theft across all universes, but would not include other aspects of our preferences. So they are full preferences, in terms of applying to all situations, but not the full set of our preferences, in terms of taking into account all our partial preferences.</p>
<p>It seems that standard machine learning techniques should already be up to the task of making full preferences from collections of partial preferences (with all the usual <a href=""https://openai.com/blog/adversarial-example-research/"">current</a> <a href=""https://arxiv.org/abs/1607.02533"">problems</a>). For example, <a href=""https://en.wikipedia.org/wiki/Cluster_analysis"">clustering</a> of similar preferences would be necessary. There are unsupervised ML algorithms that can do that; but even supervised ML algorithms end up grouping labelled data together in ways that define <a href=""https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"">extensions of the labels into higher dimensional space</a>. Where could these labels come from? Well, they could come from grounded symbols within meta-preferences. A meta-preference of the form ""I would like to be free of bias"" contains some model of what ""bias"" is; if that meta-preference is particularly weighty, then clustering preferences by whether or not they are biases could be a good thing to do.</p>
<p>Once the partial preferences are generalised in this way, remains the problem of them being contradictory. This is not as big a problem as it may seem. First of all, it is very rare for preferences to be utterly opposed: there is almost always some compromise available. So an altruist with murderous tendencies could combine charity work with aggressive online gaming; indeed some whole communities (such as BDSM) are designed to balance ""opposing"" desires for risk and safety.</p>
<p>So in general, the way to deal with contradictory preferences is to weight them appropriately, then add them together; any compromise will then appear naturally from the weighted sum<sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-9"" id=""fnref-P3TWjrKkBbha8q8zh-9"">[9]</a></sup>.</p>
<p>To do that, we need to normalise the preferences in some way. We might seek to do this in an a priori, <a href=""https://www.lesswrong.com/posts/hBJCMWELaW6MxinYW/intertheoretic-utility-comparison"">principled way</a>, or through partial models that include the <a href=""https://www.lesswrong.com/posts/NS3Kk9WJBnEauzgCr/relative-exchange-rate-between-preferences"">tradeoffs between different preferences</a>. Preferences that pertain to <a href=""https://www.lesswrong.com/posts/WAmpz8Z4FZ4FbCNtR/models-of-preferences-in-distant-situations"">extreme situations</a>, far removed from everyday human situations, could also be penalised in this weighting process (as the human should be less certain about these).</p>
<p>Now that the partial preferences have been identified and weighted, the challenge is to synthesise them into a single <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>.</p>
<h2>2.4 Synthesising the preference function: first step</h2>
<p>So this is how one could do the first step of preference synthesis:</p>
<ol>
<li>Group similar partial preferences together, generalise them to full preferences without overfitting.</li>
<li>Use partial models to compute the relative weight between different partial preferences.</li>
<li>Using those relative weights, and again without overfitting, synthesise those preferences into a single utility function <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H^0""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.327em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.225em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span></span>.</li>
</ol>
<p>This all seems doable in theory within standard machine learning. See Section 2.3 and the discussion of clustering for point 1. Point 2. comes from the definition of partial preferences. And point 3. is just an issue of fitting a good regularised approximation to noisy data.</p>
<p>In certain sense, this process is the partial opposite how Jacob Falkovich <a href=""https://putanumonit.com/2017/03/12/goddess-spreadsheet/"">used a spreadsheet to find a life partner</a>. In that process, he started by <a href=""https://www.lesswrong.com/posts/Cu5C5KhkoXhrPMLFN/goal-factoring"">factoring his goal</a> of having a life-partner in many different subgoals. He then ranked the putative partners on each of the subgoals by comparing two options at a time, and building a (cardinal) <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.363.3057&amp;rep=rep1&amp;type=pdf"">ranking from these comparisons</a>. The process here also aims to assign cardinal values from comparisons of two options, but the construction of the ""subgoals"" (full preferences) is handled by machine learning from the sets of weighted comparisons.</p>
<h2>2.5 Identity preferences</h2>
<p>Some preferences are best understood as pertaining to <a href=""https://www.lesswrong.com/posts/kSiHWuuhfe7rB4wAG/mysteries-identity-and-preferences-over-non-rewards"">our own identity</a>. For example, <em>I</em> want to understand how black holes work; this is separate from my other preference that <em>some</em> humans understand black holes (and separate again from an instrumental preference that, had we a convenient black hole close to hand, that we could use it to get energy out of).</p>
<p>Identity preferences seem to be different from preferences about the world; they seem more <a href=""https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile"">fragile</a> than other preferences. We could combine identity preference differently from standard preferences, for example using <a href=""https://www.lesswrong.com/posts/MxLK2fvEuijAYgsc2/smoothmin-and-personal-identity"">smoothmin</a> rather than summation. <a href=""https://www.lesswrong.com/posts/GnPSQAi3QzHjK8ZQR/gratification-a-useful-concept-maybe-new"">Gratifications</a> seem to be particular types of identity preferences: these are preferences about how we achieved something, rather than what we achieved (eg achieving a particularly clever or impressive victory in a game, rather than just achieving a victory).</p>
<p>Ultimately, the human's <a href=""https://www.lesswrong.com/posts/NS3Kk9WJBnEauzgCr/relative-exchange-rate-between-preferences"">mental exchange rate between preferences</a> should determine how preferences are combined. This should allow us to treat identity and world-preferences in the same way. There are two reasons to still distinguish between world-preferences and identity preferences:</p>
<ol>
<li>For preferences where relative weights are unknown or ill-defined, linear combinations and smooth-min serve as a good default for world-preferences and identity preferences respectively.</li>
<li>It's not <a href=""https://www.lesswrong.com/posts/kgQCpiuHwDWNgz7E4/preferences-over-non-rewards"">certain that identity can be fully captured</a> by partial preferences; in that case, identity preferences could serve as a starting point from which to build a concept of human identity.</li>
</ol>
<h2>2.6 Synthesising the preference function: meta-preferences</h2>
<p>Humans generally have meta-preferences: preferences over the kind of preferences they should have (often phrased as preferences over their identity, eg ""I want to be more generous"", or ""I want to have consistent preferences"").</p>
<p>This is such an important feature of humans, that it needs its own treatment; <a href=""https://www.lesswrong.com/posts/Y2LhX3925RodndwpC/resolving-human-values-completely-and-adequately"">this post</a> first looked into that.</p>
<p>The standard meta-preferences endorse or unendorse lower lever preferences. First one can combine them as in the method above, and get a synthesised meta-preference. Then this increases or decreases the weights of the lower level preferences, to reach a <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H^n""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-stack"" style=""vertical-align: -0.327em;""><span class=""mjx-sup"" style=""font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.225em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span></span> with preference weights adjusted by the synthesised meta-preferences.</p>
<p>Note that this requires some ordering of the meta-preferences: each meta-preference refers only to meta-preferences ""below"" itself. Self-referential meta-preferences (or, equivalently, meta-preferences referring to each other in a cycle) are more subtle to deal with, see Section 4.5.</p>
<p>Note that an ordering does not mean that the higher meta-preferences must dominate the lower ones; a weakly held meta-preference (eg a vague desire to fit in with some formal standard of behaviour) need not overrule a strongly held object level preference (eg a strong love for a particular person, or empathy for an enemy).</p>
<h2>2.7 Synthesising the preference function: meta-preference about synthesis</h2>
<p>In a special category are the meta-preference about the synthesis process itself. For example, philosophers might want to give greater weight to higher order meta-preferences, or might value the simplicity of the whole <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>.</p>
<p>One can deal with that by using the standard synthesis (of Section 2.4) to combine the method meta-preferences, then use this combination to change how standard preferences are synthesised. <a href=""https://www.lesswrong.com/posts/Y2LhX3925RodndwpC/resolving-human-values-completely-and-adequately"">This old post</a> has some examples of how this could be achieved. Note that these meta-preferences include <a href=""https://www.lesswrong.com/posts/CG7oi95zG3knJ74s5/toy-model-6-rationality-and-partial-preferences"">preferences over using rationality to decide between lower-level preference</a>.</p>
<p>As long as there is an ordering of meta-preferences about synthesis, one can use the standard method to synthesise the highest level of meta-preferences, which then tells us how to synthesise the lower-level meta-preferences about synthesis, and so on.</p>
<p>Why use the standard synthesis method for these meta-preferences - especially if they contradict this synthesis method explicitly? There are three reasons for this:</p>
<ol>
<li>These meta-preferences may be weakly weighted (hence weakly held), so they should not automatically overwhelm the standard synthesis process when applied to themselves (think of continuity as the weight of the meta-preference fades to zero).</li>
<li>Letting meta-preferences about synthesis determine how they themselves get synthesised leads to circular meta-preferences, which may cause problems (see Section 4.5).</li>
<li>The standard method is more predictable, which makes the whole process more predictable; self-reference, even if resolved, could lead to outcomes <a href=""https://www.lesswrong.com/posts/WeAt5TeS8aYc4Cpms/values-determined-by-stopping-properties"">randomly far away from the intended one</a>. Predictability could be especially important for ""meta-preferences over outcomes"" of the next section.</li>
</ol>
<p>Note that these synthesis meta-preferences should be of a type that affects the synthesis of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, not its final form. So, for example, ""simple (meta-)preferences should be given extra weight in <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>"" is valid, while ""<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> should be simple"" is not.</p>
<p>Thus, finally, we can combine everything (except for some self-referencing contradictory preferences) into one <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>.</p>
<p>Note there are many degrees of freedom in how the synthesis could be carried out; it's hoped that they don't matter much, and that each of them will reach a <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> that avoids disasters<sup class=""footnote-ref""><a href=""#fn-P3TWjrKkBbha8q8zh-10"" id=""fnref-P3TWjrKkBbha8q8zh-10"">[10]</a></sup> (see Section 2.8).</p>
<h2>2.8 Avoiding disasters, and global meta-preferences</h2>
<p>It is important that we don't end up in some disastrous outcome; the very <a href=""https://www.lesswrong.com/posts/qezBTig6p6p5xtL6G/a-theory-of-human-values"">definition of a good human value theory</a> requires this.</p>
<p>The approach has some in-built protection against many types of disasters. Part of that is that it can include very general and universal partial preferences, so any combination of ""local"" partial preferences must be compatible with these. For example, we might have a collection of preferences about autonomy, pain, and personal growth. It's possible that, when synthesising these preferences together, we could end up with some ""<a href=""https://www.lesswrong.com/posts/vgFvnr7FefZ3s3tHp/mahatma-armstrong-ceved-to-death"">kill everyone</a>"" preference, due to bad extrapolation. However, if we have a strong ""don't kill everyone"" preference, this will push the synthesis process away from that outcome.</p>
<p>So some disastrous outcomes of the synthesis should be avoided, precisely because <em>all</em> of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s preferences are used, including those that would specifically label that outcome a disaster.</p>
<p>But, even if we included all of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s preferences in the synthesis, we'd still want to be sure we'd avoided disasters.</p>
<p>In one sense, this requirement is trivially true and useful. But in another, it seems perverse and worrying - the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is supposed to be a synthesis of true human preferences. By definition. So how could this <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> be, in any sense, a disaster? Or a failure? What criteria - apart from our own preferences - could we use? And shouldn't we be using these preferences in the synthesis itself?</p>
<p>The reason that we can talk about <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> not being a disaster, is that not all our preferences can best be captured in the partial model formalism above. Suppose one fears a <a href=""https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search"">siren world</a> or reassures oneself that we can never encounter <a href=""https://www.lesswrong.com/posts/rArsypGqq49bk4iRr/can-there-be-an-indescribable-hellworld"">an indescribable hellworld</a>. Both of these could be clunkily transformed into standard meta-preferences (maybe about what some <a href=""https://www.lesswrong.com/posts/5bd75cc58225bf0670375454/like-this-world-but"">devil's advocate AI</a> could tell us?). But that somewhat misses the point. These top-meta-level considerations live most naturally at the top-meta-level: reducing them to the standard format of other preferences and meta-preferences risks losing the point. Especially when we only partially understand these issues, translating them to standard meta-preferences risks losing the understanding we do have.</p>
<p>So, it remains possible to say that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is ""good"" or ""bad"", using higher level considerations that are difficult to capture entirely within <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>.</p>
<p>For example, there is an argument that <a href=""https://www.lesswrong.com/posts/FgRRY3vSwDLx2Hk46/the-low-cost-of-human-preference-incoherence"">human preference incoherence</a> should not cost us much. If true, this argument suggests that overfitting to the details of human preferences is not as bad as we might fear. One could phrase this as a synthesis meta-preference allowing more over-fitting, but this doesn't capture a coherent meaning of ""not as bad"" - which precludes the real point of this argument, which is ""allow more overfitting if the argument holds"". To use that, we need some criteria for establishing ""the argument holds"". This seems very hard to do within the synthesis process, but could be attempted as top-level meta-preferences.</p>
<p>We should be cautious and selective when using these top-level preferences in this way. This is not generally the point at which we should be adding preferences to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>; that should be done when constructing <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>. Still, if we have a small selection of criteria, we could formalise these and check ourselves whether <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> satisfies them, or have an AI do so while synthesising <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>. A <a href=""https://intelligence.org/files/CEV.pdf"">Last Judge</a> can be a sensible precaution (especially if there are more downsides to error than upsides to perfection).</p>
<p>Note that we need to distinguish between the global meta-preferences of the designers (us) and those of the subject <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>. So, when designing the synthesis process, we should either allow options to be automatically changed by <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s global preferences, or be aware that we are overriding them with our own judgement (which may be inevitable, as most <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s have not thought deeply about preference synthesis; still, it is good to be aware of this issue).</p>
<p>This is also the level at which experimental testing of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> synthesis is likely to be useful - keeping in mind what we expect from <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> synthesis, and running the synthesis in some complicated toy environments, we can see whether our expectations are correct. We may even discover extra top-level desiderata this way.</p>
<h2>2.9 How much to delegate to the process</h2>
<p>The method has two types of basic preferences (world-preferences and identity preferences). This is a somewhat useful division; but there are others that could have been used. Altruistic versus selfish versus anti-altruistic preferences is a division that was not used (though see Section 4.3). Moral preferences were not directly distinguished from non-moral preferences (though some human meta-preferences <a href=""https://www.lesswrong.com/posts/BQzPyBjahycCN24dR/moral-as-a-preference-label"">might make the distinction</a>).</p>
<p>So, why divide preferences this way, rather than in some other way? The aim is to allow the process itself to take into account most of the divisions that we might care about; things that go into the model explicitly are structural assumptions that are of vital importance. So the division between world- and identity preferences was chosen because it seemed absolutely crucial to get that right (and to err on the side of caution in distinguishing the two, even if our own preferences don't distinguish them as much). Similarly, the whole idea of meta-preferences seems a crucial feature of humans, which might not be relevant for general agents, so it was important to capture it. Note that meta-preferences are treated as a different type to standard preferences, with different rules; most distinctions built into the synthesis method should similarly be between objects of a different type.</p>
<p>But this is not set in stone; global meta-preferences (see Section 2.8) could be used to justify a different division of preference types (and different methods of synthesis). But it's important to keep in mind what assumptions are being imposed from outside the process, and what the method is allowed to learn during the process.</p>
<h1>3 <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> in practice</h1>
<h2>3.1 Synthesis of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> in practice</h2>
<p>If the definition of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> of the previous section could be made fully rigorous, and if the AI has a perfect model of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s brain, knowledge of the universe, and unlimited computing power, it could construct <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> perfectly and directly. This will almost certainly not be the case; so, do all these definitions give us something useful to work with?</p>
<p>It seems they do. Even extreme definitions can be approximated, hopefully to some good extent (and the theory allows us to assess the quality of the approximation, as opposed to another method without theory, where there is no meaningful measure of approximation ability). See Section 0.3 for an argument as to why even very approximate versions of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> could result in very positive outcomes: even approximated <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> rule out most bad AI failure scenarios.</p>
<p>In practical terms, the synthesis of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> from partial preferences seems quite robust and doable; it's the definition of these partial preferences that seems tricky. One might be able to directly see the internal symbols in the human brain, with some future super-version of fMRI. Even without that direct input, having a theory of what we are looking for - partial preference in partial models with human symbols grounded - allows us to use results from standard and moral psychology. These results are insights into behaviour, but they are often also, at least in part, insights into how the human brain processes information. In Section 3.3, we'll see how the definition of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> allows us to ""patch"" other, more classical methods of value alignment. But the converse is also true: with a good theory, we can use more classical methods to figure out <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>. For example, if we see <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span> as being in a situation where they are likely to tell the truth about their internal model, then their stated preferences become good proxies for their internal partial preferences.</p>
<p>If we have a good theory for how human preferences change over time, then we can use preferences at time <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t'""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.298em;"">′</span></span></span></span></span></span></span></span> as evidence for the hypothetical preferences at time <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span>. In general, more practical knowledge and understanding would lead to a better understanding of the partial preferences and how they change over time.</p>
<p>This could become an area of interesting research; once we have a good theory, it seems there are many different practical methods that suddenly become usable.</p>
<p>For example, it seems that humans model themselves and each other using <a href=""https://www.lesswrong.com/posts/eXiuk8Yjd7FsunhxM/humans-interpreting-humans"">very similar methods</a>. This allows us to use our own judgement of irrationality and intentionality, to some extent, and in a principled way, to assess the internal models of other humans. As we shall see in Section 3.3, an awareness of what we are doing - using the similarity between our internal models and those of others - also allows us to assess when this method stops working, and patch it in a principled way.</p>
<p>In general, this sort of research would give results of the type ""assuming this connection between empirical facts and internal models (an assumption with some evidence behind it), we can use this data to estimate internal models"".</p>
<h2>3.2 (Avoiding) uncertainty and manipulative learning</h2>
<p>There are arguments that, as long as we account properly for our uncertainty and <a href=""https://www.lesswrong.com/posts/QJwnPRBBvgaeFeiLR/uncertainty-versus-fuzziness-versus-extrapolation-desiderata"">fuzziness</a>, <a href=""https://www.lesswrong.com/posts/urZzJPwHtjewdKKHc/using-expected-utility-for-good-hart"">there are no</a> <a href=""https://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy"">Goodhart-style problems</a> in maximising an approximation to <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>. This argument has been <a href=""https://www.lesswrong.com/posts/YJq6R9Wgk5Atjx54D/does-bayes-beat-goodhart"">disputed</a>, and there are ongoing <a href=""https://www.lesswrong.com/posts/YJq6R9Wgk5Atjx54D/does-bayes-beat-goodhart#5xrCxRPCaiCnbppFt"">debates</a> about it.</p>
<p>With a good definition of what it means for the AI to <a href=""https://www.lesswrong.com/posts/upLot6eG8cbXdKiFS/reward-function-learning-the-learning-process"">influence the learning process</a>, online learning of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> becomes possible, even for powerful AIs learning over long periods of time in which the human changes their views (either naturally or as a consequence of the AI's actions).</p>
<p>Thus, we could construct an online version of inverse reinforcement learning without assuming rationality, where the AI learns about partial models and human behaviour simultaneously, constructing the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> from observations given the right data and the right assumptions.</p>
<h2>3.3 Principled patching of other methods</h2>
<p>Some of the theoretical ideas presented here can be used to improve other AI alignment ideas. <a href=""https://www.lesswrong.com/posts/qezBTig6p6p5xtL6G/a-theory-of-human-values"">This post</a> explains one of the ways this can happen.</p>
<p>The basic idea is that there exist methods - stated preferences, <a href=""https://en.wikipedia.org/wiki/Revealed_preference"">revealed preferences</a>, an idealised human reflecting for a very long time - that are often correlated with <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> and with each other. However, all of the methods fail - stated preferences are often dishonest (the <a href=""https://en.wikipedia.org/wiki/Revelation_principle"">revelation principle</a> doesn't apply in the social world), revealed preferences assume a rationality that is often absent in humans (and some models of revealed preferences <a href=""https://www.lesswrong.com/posts/DuPjCTeW9oRZzi27M/bounded-rationality-abounds-in-models-not-explicitly-defined"">obscure how unrealistic this rationality assumption is</a>), humans that think for a long time have the possibility of value drift or <a href=""https://www.lesswrong.com/posts/WeAt5TeS8aYc4Cpms/values-determined-by-stopping-properties"">random walks to convergence</a>.</p>
<p>Given these flaws, it is always tempting to patch the method: add caveats to get around the specific problem encountered. However, if we patch and patch until we can no longer think of any further problems, that <a href=""https://arbital.com/p/patch_resistant/"">doesn't mean</a> there are no further problems: simply that they are likely beyond our capacity to predict ahead of time. And, if all that it has is a list of patches, the AI is unlikely to be able to deal with these new problems.</p>
<p>However, if we keep the definition of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> in mind, we can come up with principled reasons to patch a method. For example, lying on stated preferences means a divergence between stated preferences and internal model; revealed preferences only reveal within the parameters of the partial model that is being used; and value drift is a failure of preference synthesis.</p>
<p>Therefore, each patch can have an explanation for the divergence between method and desired outcome. So, when the AI develops the method further, it can itself patch the method, when it enters a situation where a similar type of divergence. It has a reason for <em>why</em> these patches exist, and hence the ability to generate new patches efficiently.</p>
<h2>3.4 Simplified <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> sufficient for many methods</h2>
<p>It's been argued that many different methods rely upon, if not a complete synthesis <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, at least some simplified version of it. <a href=""https://intelligence.org/files/Corrigibility.pdf"">Corrigibility</a>, <a href=""https://arxiv.org/abs/1705.10720"">low impact</a>, and <a href=""https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616"">distillation/amplification</a> all seem to be methods that <a href=""https://www.lesswrong.com/posts/sEqu6jMgnHG2fvaoQ/partial-preferences-needed-partial-preferences-sufficient"">require some simplified version of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span></a>.</p>
<p>Similarly, some concepts that we might want to use or avoid - such as ""manipulation"" or ""understanding the answer"" - also may <a href=""https://www.lesswrong.com/posts/Q7WiHdSSShkNsgDpa/how-much-can-value-learning-be-disentangled"">require a simplified utility function</a>. If these concepts can be defined, then one can disentangle them from the rest of the alignment problem, allowing us to instructively consider situations where the concept makes sense.</p>
<p>In that case, a simplified or incomplete construction of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, using some simplification of the synthesis process, might be sufficient for one of the methods or definitions just listed.</p>
<h2>3.5 Applying the intuitions behind <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> to analysing other situations</h2>
<p>Finally, one could use the definition of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> as inspiration when analysing other methods, which could lead to interesting insights. See for example <a href=""https://www.lesswrong.com/posts/iutXWSDd56ieAiyTi/hierarchical-system-preferences-and-subagent-preferences"">these</a> <a href=""https://www.lesswrong.com/posts/8cedA5q2cBSZiYSPo/preferences-in-subpieces-of-hierarchical-systems"">posts</a> on figuring out the goals of a hierarchical system.</p>
<h1>4 Limits of the method</h1>
<p>This section will look at some of the limitations and lacuna of the method described above. For some limitations, it will suggest possible ways of dealing with them; but these are, deliberately, chosen to be extras beyond the scope of the method, where synthesising <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is the whole goal.</p>
<h2>4.1 Utility at one point in time</h2>
<p>The <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is meant to be a synthesis of the <em>current</em> preferences and meta-preferences of the human <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>, using one-step hypotheticals to fill out the definition. Human preferences are changeable on a short time scale, without us feeling that we become a different person. Hence it may make sense to replace <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_{H_t}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.057em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span><span class=""mjx-sub"" style=""font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span></span></span></span></span></span></span> with some average <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, averaged over a short (or longer) period of time. Shorter period lead to more ""overfitting"" to momentary urges; longer period allow more manipulation or drift.</p>
<h2>4.2 Not a philosophical ideal</h2>
<p>The <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is also not a <a href=""https://en.wikipedia.org/wiki/Reflective_equilibrium"">reflective equilibrium</a> or other idealised distillation of what preferences <em>should be</em>. Philosophers will tend to have a more idealised <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, as will those who have reflected a lot and are more willing to be <a href=""https://www.scottaaronson.com/blog/?p=326"">bullet swallowers</a>/<a href=""http://www.overcomingbias.com/2008/05/biting-evolutio.html"">bullet bitters</a>. But that is because these people have strong meta-preferences that push in those idealised directions, so any honest synthesis of their preferences must reflect these.</p>
<p>Similarly, this <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is defined to be the preferences of some human <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>. If that human is bigoted or selfish, their <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> will be bigoted or selfish. In contrast, moral preferences that can be considered <a href=""https://www.lesswrong.com/posts/am5ubSoSe6Hf5tnTF/being-wrong-in-ethics"">factually wrong</a> will be filtered out by this construction. Similarly, preferences based on erroneous factual beliefs (""trees can think, so..."") will be removed or qualified (""if trees could think, then..."").</p>
<p>Thus if <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span> is <strong>wrong</strong>, the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> will not reflect that wrongness; but if <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span> is <strong>evil</strong>, then <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> will reflect that evilness.</p>
<p>Also, the procedure will not distinguish between moral preferences and other types of preferences, <a href=""https://www.lesswrong.com/posts/BQzPyBjahycCN24dR/moral-as-a-preference-label"">unless the human themselves does</a>.</p>
<h2>4.3 Individual utility versus common utility</h2>
<p>This research agenda will not look into how to combine the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> of different humans. One could simply <a href=""https://www.lesswrong.com/posts/hBJCMWELaW6MxinYW/intertheoretic-utility-comparison"">weight the utilities according to some semi-plausible scale</a> and <a href=""https://www.lesswrong.com/posts/W2ufY8ihDDWWqJA7h/if-you-don-t-know-the-name-of-the-game-just-tell-me-what-i"">add them together</a>.</p>
<p>But we could do many other things as well. I've suggested <a href=""https://www.lesswrong.com/posts/Abws49L8CNFEorpXg/combining-individual-preference-utility-functions"">removing anti-altruistic preferences</a> before combining the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>'s into some global utility function <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mathbb{U}_{\mathbb{H}}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">U</span></span></span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">H</span></span></span></span></span></span></span></span></span></span></span></span> for all of humanity - or for all future and current sentient beings, or for all beings that could suffer, or for <a href=""https://blog.oup.com/2017/08/electrons-consciousness-philosophy/"">all physical entities</a>.</p>
<p>There are strong game-theoretical reasons to remove anti-altruistic preferences. We might also add philosophical considerations (eg moral realism) or deontological rules (eg human rights, restrictions on copying themselves, extra weighting to certain types of preferences), either to the individual <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> or when combining them, or prioritise moral preferences over other types. We might want to preserve the capacity for moral growth, somehow (see Section 4.6).</p>
<p>That can all be done, but is not part of this research agenda, whose sole purpose is to synthesise the individual <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>'s, which can then be used for other purposes.</p>
<h2>4.4 Synthesising <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> rather than discovering it (moral anti-realism)</h2>
<p>The utility <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> will be constructed, rather than deduced or discovered. Some moral theories (such as some versions of moral realism) posit that there is a (generally unique) <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> waiting to be discovered. But none of these theories give effective methods for doing so.</p>
<p>In the absence of such a definition of how to discover an ideal <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, it would be <a href=""https://www.lesswrong.com/posts/TrudRcZxEjA2RNCxh/learning-known-information-when-the-information-is-not"">highly dangerous</a> to assume that finding <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is a process of discovery. Thus the whole method is constructive from the very beginning (and based on a small number of arbitrary choices).</p>
<p>Some versions of moral realism could make use of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> as a starting point of their own definition. Indeed, in practice, moral realism and moral anti-realism seem to be <a href=""https://www.lesswrong.com/posts/FQKjY563bJqDeaEDr/to-first-order-moral-realism-and-moral-anti-realism-are-the"">initially almost identical</a> when meta-preferences are taken into account. Moral realists often have mental examples of what counts as ""moral realism doesn't work"", while moral anti-realists still want to simplify and organise moral intuitions. To a first approximation, these approaches can be very similar in practice.</p>
<h2>4.5 Self-referential contradictory preferences</h2>
<p>There remain problems with self-referential preferences - preferences that claim they should be given more (or less) weight than otherwise (eg ""all simple meta-preferences should be penalised""). This was already observed in a <a href=""https://www.lesswrong.com/posts/Y2LhX3925RodndwpC/resolving-human-values-completely-and-adequately"">previous post</a>.</p>
<p>This includes formal Gödel-style problems, with preferences explicitly contradicting themselves, but those seem solvable - with one or another version of <a href=""https://intelligence.org/2016/09/12/new-paper-logical-induction/"">logical uncertainty</a>.</p>
<p>More worrying, from the practical standpoint, is the human tendency to reject values imposed upon them, just because they are imposed upon them. This resembles a preference of the type ""reject any <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> computed by any synthesis process"". This preference is weakly existent in almost all of us, and a variety of our other preferences should prevent the AI from forcibly re-writing us to become <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>-desiring agents.</p>
<p>So it remains not at all clear what happens when the AI says ""this is what you really prefer"" and we almost inevitably answer ""no!"". This concept can be seen, in a sense, as a <a href=""https://www.lesswrong.com/posts/GnPSQAi3QzHjK8ZQR/gratification-a-useful-concept-maybe-new"">gratification</a>: we're not objecting to the outcome of the synthesis, per se, but to the way that the outcome was imposed on us.</p>
<p>Of course, since the <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is constructed rather than real, there is some latitude. It might be possible to involve the human in the construction process, in a way that increases their buy-in (thanks to Tim Genewein for the suggestion). Maybe the AI could construct the first <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, and refine it with further interactions with the human. And maybe, in that situation, if we are confident that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is pretty safe, we'd want the AI to subtly manipulate the human's preferences towards it.</p>
<h2>4.6 The question of identity and change</h2>
<p>It's not certain that <a href=""https://www.lesswrong.com/posts/kSiHWuuhfe7rB4wAG/mysteries-identity-and-preferences-over-non-rewards"">human concepts of identity</a> can be fully captured by identity preferences and meta-preferences. In that case, it is important that human identity be figured out somehow, lest humanity itself vanish even as our preferences are satisfied. Nick Bostrom sketched how this might happen: in the <a href=""https://nickbostrom.com/fut/evolution.html"">mindless outsourcers</a> scenario, human outsource more and more of their key cognitive features to automated algorithms, until nothing remains of ""them"" any more.</p>
<p>Somewhat related is the fact that many humans see <a href=""https://www.youtube.com/watch?v=n-kxy4D9HGg"">change and personal or moral growth</a> as a key part of their identity. Can such a desire be accommodated, despite a <a href=""https://www.lesswrong.com/posts/4H8N3fEfXQmzxSaRo/upcoming-stability-of-values"">likely stabilisation of values</a>, without just becoming a <a href=""https://www.lesswrong.com/posts/WeAt5TeS8aYc4Cpms/values-determined-by-stopping-properties"">random walk</a> across preference space?</p>
<p>Some aspects of growth and change can be accommodated. Humans can certainly become more skilled, more powerful, and more knowledgeable. Since humans don't distinguish well between terminal and instrumental goals, some forms of factual learning resemble moral learning (""if it turns out that anarchism results in the greatest flourishing of humanity, then I wish to be a anarchist; if not, then not""). If we take into account the preferences of all humans in some roughly equal way (see Section 4.3), then we can <a href=""https://www.lesswrong.com/posts/SDr45pcgJJyvTqmZa/for-the-past-in-some-ways-we-are-moral-degenerates"">get ""moral progress""</a> without needing to change anyone's individual preferences.  Finally, professional roles, contracts, and alliances allow for behavioural changes (and sometimes values changes), in ways that maximise the initial values. Sort of like ""if I do PR work for the Anarchist party, I will spout anarchist values"" and ""I accept to make my values more anarchist, in exchange for the Anarchist party shifting their values more towards mine"".</p>
<p>Beyond these examples, it gets trickier to preserve moral change. We might put a slider that makes our own values less instrumental or less selfish over time, but that feels like a cheat: we already know what we will be, we're just taking the long route to get there. Otherwise, we might allow our values to change within certain defined areas. This would have to be carefully defined to prevent random change, but the main challenge is efficiency: changing values have an inevitable efficiency cost, so there needs to be strong positive pressure to preserve the changes - and not just preserve an unused ""possibility for change"", but actual, efficiency-losing, changes. This ""possibility for change"" can be seen as a <a href=""https://www.lesswrong.com/posts/GnPSQAi3QzHjK8ZQR/gratification-a-useful-concept-maybe-new"">gratification</a>: a cost we are willing to pay in terms of perfect efficiency, in order to have a process (continued moral learning) that we prefer.</p>
<p>This should be worth investigating more; it feels like these considerations need to be built into the synthesis process for this to work, rather than the synthesis project making them work itself (thus this kind of preferences is one of the ""Global meta-preferences about the outcome of the synthesis process"").</p>
<h2>4.7 Other Issues not addressed</h2>
<p>These are other important issues that need to be solved to get a fully friendly AI, even if the research agenda works perfectly. They are, however, beyond the scope of this agenda; a partial list of these is:</p>
<ol>
<li>Actually building the AI itself (left as an exercise to the reader).</li>
<li>Population ethics (though some sort of average of individual human population ethics might be doable with these methods).</li>
<li>Taking into account other factors than individual preferences.</li>
<li>Issues of ontology and ontology changes.</li>
<li><a href=""https://nickbostrom.com/papers/aipolicy.pdf"">Mind crime</a> (conscious suffering beings simulated within an AI system), though some of the work on identity preferences may help in identifying conscious minds.</li>
<li><a href=""https://www.lesswrong.com/posts/5bd75cc58225bf067037540c/infinite-ethics-comparisons"">Infinite ethics</a>.</li>
<li>Definitions of <a href=""https://plato.stanford.edu/entries/counterfactuals/"">counterfactuals</a> or which <a href=""https://wiki.lesswrong.com/wiki/Updateless_decision_theory"">decision</a> <a href=""https://arxiv.org/abs/1710.05060"">theory</a> to use.</li>
<li><a href=""https://intelligence.org/files/TechnicalAgenda.pdf"">Agent foundations</a>, <a href=""https://intelligence.org/files/LogicalInduction.pdf"">logical uncertainty</a>, how to keep a utility stable.</li>
<li>Acausal trade.</li>
<li><a href=""https://arbital.com/p/daemons/"">Optimisation daemons</a>/inner optimisers/emergent optimisation.</li>
</ol>
<p>Note that the <a href=""https://intelligence.org/research/"">Machine Intelligence Research Institute</a> is working heavily on issues 7, 8, and 9.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-P3TWjrKkBbha8q8zh-1"" class=""footnote-item""><p>A partial preference being a preference where the human considers only a small part of the variables describing the universe; see Section 1.1. <a href=""#fnref-P3TWjrKkBbha8q8zh-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-P3TWjrKkBbha8q8zh-2"" class=""footnote-item""><p>Actually, <em>this</em> specific problem is not included directly in the research agenda, though see Section 4.3. <a href=""#fnref-P3TWjrKkBbha8q8zh-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-P3TWjrKkBbha8q8zh-3"" class=""footnote-item""><p>Likely but not certain: we don't know how effective AIs might become at computing counterfactuals or modelling humans. <a href=""#fnref-P3TWjrKkBbha8q8zh-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-P3TWjrKkBbha8q8zh-4"" class=""footnote-item""><p>It makes sense to allow partial preferences to contrast a small number of situations, rather than just two. So ""when it comes to watching superhero movies, I'd prefer to watch them with Alan, but Beth will do, and definitely not with Carol"". Since partial preferences with <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""n""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span></span></span></span></span> situations can be built out of smaller number of partial preferences with two situations, allowing more situations is a useful practical move, but doesn't change the theory. <a href=""#fnref-P3TWjrKkBbha8q8zh-4"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-P3TWjrKkBbha8q8zh-5"" class=""footnote-item""><p>""One-step"" refers to hypotheticals that can be removed from the human's immediate experience (""Imagine that you and your family are in space..."") but not very far removed (so no need for lengthy descriptions that could sway the human's opinions by hearing them). <a href=""#fnref-P3TWjrKkBbha8q8zh-5"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-P3TWjrKkBbha8q8zh-6"" class=""footnote-item""><p>Equivalently to reducing the weight, we could increase uncertainty about the partial preference, given the unfamiliarity. There are many options for formalisms that lead to the same outcome. Though note that here, we are imposing a penalty (low weight/high uncertainty) for unfamiliarity, whereas the actual human might have incredibly strong internal certainty in their preferences. It's important to distinguish assumptions that the synthesis process makes, from assumptions that the human might make. <a href=""#fnref-P3TWjrKkBbha8q8zh-6"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-P3TWjrKkBbha8q8zh-7"" class=""footnote-item""><p>Extreme situations are also situations where we have to be very careful to ensure the AI has the right model of all preference possibilities. The flaws of incorrect model <a href=""https://www.lesswrong.com/posts/xEdJfK8dYRm6YSE8w/jfk-was-not-assassinated-prior-probability-zero-events"">can be corrected by enough data</a>, but when data is sparse and unreliable, then model assumptions - including prior - tend to dominate the result. <a href=""#fnref-P3TWjrKkBbha8q8zh-7"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-P3TWjrKkBbha8q8zh-8"" class=""footnote-item""><p>""Natural"" does not, of course, mean any of ""healthy"", ""traditional"", or ""non-polluting"". However those using the term ""natural"" are often assuming all of those. <a href=""#fnref-P3TWjrKkBbha8q8zh-8"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-P3TWjrKkBbha8q8zh-9"" class=""footnote-item""><p>The human's meta-preferences are also relevant to this it. It might be that, whenever asked about this particular contradiction, the human would answer one way. Therefore <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span>'s <a href=""https://www.lesswrong.com/posts/ic8yoGBMYLtaJkbxZ/conditional-meta-preferences"">conditional meta-preferences</a> may contain ways of resolving these contradictions, at least if the meta-preferences have high weight and the preferences have low weight.</p>
<p>Conditional meta-preferences can be tricky, though, as we don't want them to allow the synthesis to get around the <a href=""https://www.lesswrong.com/posts/i6hWWcKyxBPj7ELT6/one-step-hypotheticals"">one-step hypotheticals</a> restriction. A ""if a long theory sounds convincing to me, I want to believe it"" meta-preference in practice do away with these restrictions. That particular meta-preference might be cancelled out by the ability of <a href=""https://agentfoundations.org/item?id=1636"">many</a> <a href=""https://www.lesswrong.com/posts/6mtoy9gh32rQzfT4r/on-self-deception"">different theories</a> to sound convincing. <a href=""#fnref-P3TWjrKkBbha8q8zh-9"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-P3TWjrKkBbha8q8zh-10"" class=""footnote-item""><p>We <em>can</em> allow meta-preferences to determine a lot more of their own synthesis if we find an appropriate method that a) always reaches a synthesis, and b) doesn't artificially boost some preferences through a feedback effect. <a href=""#fnref-P3TWjrKkBbha8q8zh-10"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
uHb2LDW3LGhBMyq74,Preference conditional on circumstances and past preference satisfaction,preference-conditional-on-circumstances-and-past-preference,https://www.lesswrong.com/posts/uHb2LDW3LGhBMyq74/preference-conditional-on-circumstances-and-past-preference,2019-06-17T15:30:32.580Z,11,2,1,False,False,,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p>I've <a href=""https://www.lesswrong.com/posts/ic8yoGBMYLtaJkbxZ/conditional-meta-preferences"">mentioned conditional preferences before</a>. These are preferences that are dependent on facts about the world, for example ""I'd want to believe X if there are strong argument for X"".</p>
<p>But there is another type of preference that is conditional: my tastes can vary depending on circumstances and on my past experience. <a href=""https://www.lesswrong.com/posts/i6hWWcKyxBPj7ELT6/one-step-hypothetical-preferences#sqekDsnDs3D9Bpm5n"">For example</a>, I might prefer to eat apples during the week and oranges on weekends. Or, <a href=""https://www.lesswrong.com/posts/WMDy4GxbyYkNrbmrs/in-praise-of-boredom"">because of the miracle of boredom</a>, I might prefer oranges if (but only if) I've been eating apples all week so far.</p>
<p>What if I currently want apples, would want oranges tomorrow, but falsely believe (today) that I would want apples tomorrow? This is a known problem with ""<a href=""https://www.lesswrong.com/posts/i6hWWcKyxBPj7ELT6/one-step-hypothetical-preferences"">one-step hypotheticals</a>"", and a strong argument <strong>in practice</strong> for assessing preferences over time rather than at a single moment <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span>.</p>
<p>In theory, there are meta-preferences that allow one to get this even at a single moment <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""t""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span></span></span></span></span>, such as ""I want to be able to follow my different tastes at different times"" or a more formalised desire for variety and exploration.</p>
</body></html>",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
bYPuXrqwmRNRNHH4w,"In physical eschatology, is Aestivation a sound strategy?",in-physical-eschatology-is-aestivation-a-sound-strategy-1,https://www.lesswrong.com/posts/bYPuXrqwmRNRNHH4w/in-physical-eschatology-is-aestivation-a-sound-strategy-1,2019-06-17T07:27:31.527Z,17,5,11,False,True,,"<p>In <a href=""https://arxiv.org/abs/1705.03394"">this paper</a>, Anders Sandberg, Stuart Armstrong and Milan M. Cirkovic argue that </p><blockquote>If a civilization wants to maximize computation it appears rational to aestivate until the far future in order to exploit the low temperature environment: this can produce a 1030 multiplier of achievable computation. </blockquote><p>Later Charles H. Bennett, Robin Hanson, C. Jess Riedel <a href=""https://arxiv.org/abs/1902.06730"">disagree</a>, claiming</p><blockquote>In fact, while this assumption may apply in the distant future, our universe today contains vast reservoirs and other physical systems in non-maximal entropy states, and computer-generated entropy can be transferred to them at the adiabatic conversion rate of one bit of negentropy to erase one bit of error. This can be done at any time, and is not improved by waiting for a low cosmic background temperature. Thus aliens need not wait to be active. As Sandberg et al. do not provide a concrete model of the effect they assert, we construct one and show where their informal argument goes wrong.</blockquote><p>Who was right?</p>",MakoYass,mako-yass,mako yass,
Yfg2AmSmkvQ4AEomf,Is there a guide to 'Problems that are too fast to Google'?,is-there-a-guide-to-problems-that-are-too-fast-to-google,https://www.lesswrong.com/posts/Yfg2AmSmkvQ4AEomf/is-there-a-guide-to-problems-that-are-too-fast-to-google,2019-06-17T05:04:39.613Z,48,16,13,False,True,,"<p>It seems to me like problems come in a variety of required response speeds, but there&#x27;s a natural threshold to distinguish fast and slow: whether or not you can Google it. The slow ones, like getting an eviction notice from your landlord or a cancer diagnosis from your doctor, can&#x27;t be <em>ignored</em> but you have time to figure out best practices before you act. Fast ones, like getting bit by a rattlesnake or falling from a high place, generally require that you already know best practices in order to properly implement them.</p><p>Also useful would be the meta-guide, which just separates out which problems are fast and slow (or how fast they are). Getting bit by a tick, for example, seems like it might be quite urgent when you discover one biting you, but isn&#x27;t; you have about 24 hours from when it first attaches to remove it, which is plenty of time to research proper removal technique. Getting a bruise might seem like you have time, but actually applying cold immediately does more to prevent swelling than applying cold later does to reduce it. </p><p>Of course, this is going to vary by region, profession, age, sex, habits, and so on. I&#x27;m sort of pessimistic about this existing at all, and so am interested in whatever narrow versions exist (even if it&#x27;s just &quot;here&#x27;s what you need to know about treating common injuries to humans&quot;). Basic guides also seem useful from a &#x27;preventing illusion of transparency&#x27; perspective.</p>",Vaniver,vaniver,Vaniver,
TeBDgvmvafo2GQkcn,ISO: Automated P-Hacking Detection,iso-automated-p-hacking-detection,https://www.lesswrong.com/posts/TeBDgvmvafo2GQkcn/iso-automated-p-hacking-detection,2019-06-16T21:15:52.837Z,7,2,3,False,False,,"<p>I&#x27;m sure there&#x27;s some ML students/researchers on Lesswrong in search of new projects, so here&#x27;s one I&#x27;d love to see and probably won&#x27;t build myself: an automated method for predicting which papers are unlikely to replicate, given the text of the paper. Ideally, I&#x27;d like to be able to use it to filter and/or rank results from Google scholar.</p><p>Getting a good data set would probably be the main bottleneck for such a project. Various replication-crisis papers which review replication success/failure for tens or hundreds of other studies seem like a natural starting point. Presumably some amount of feature engineering would be needed; I doubt anyone has a large enough dataset of labelled papers to just throw raw or lightly-processed text into a black box.</p><p>Also, if anyone knows of previous attempts to do this, I&#x27;d be interested to hear about it.</p>",johnswentworth,johnswentworth,johnswentworth,
DhNQsjEmSqG4QWhtm,Does scientific productivity correlate with IQ?,does-scientific-productivity-correlate-with-iq,https://www.lesswrong.com/posts/DhNQsjEmSqG4QWhtm/does-scientific-productivity-correlate-with-iq,2019-06-16T19:42:29.980Z,27,9,12,False,True,,"<p>Anders Ericsson, in his popular book <em>Peak: Secrets from the New Science of Expertise</em>, claims that though one may need a reasonably high IQ to even be a scientist, after that level, IQ is irrelevant to scientific success. </p><blockquote>The average IQ of scientists is certainly higher than the IQ of the general population, but among scientists, there is no correlation between IQ and scientific productivity. Indeed, a number of noble prize winning scientists have had IQs that would not even qualify them for Mensa, an organization who&#x27;s members must have a measured IQ of at least 132, a number that put&#x27;s you in the upper 2 percentile of the population.</blockquote><p>(From chapter 8: &quot;But What About Natural Talent&quot;, of <em>Peak: Secrets from the New Science of Expertise</em>) </p><p>My understanding is that this is completely wrong, and the best scientists tend to have higher IQs, at all levels of performance. But this is a background belief that I haven&#x27;t concretely verified. </p><p>Is there a canonical reference regarding the impact of IQ on scientific contribution?</p><br/>",elityre,elityre,Eli Tyre,
CajbkL342tgxDuRLF,"Does the _timing_ of practice, relative to sleep, make a difference for skill consolidation?",does-the-_timing_-of-practice-relative-to-sleep-make-a,https://www.lesswrong.com/posts/CajbkL342tgxDuRLF/does-the-_timing_-of-practice-relative-to-sleep-make-a,2019-06-16T19:12:48.358Z,30,11,7,False,True,,"<p>It is well known that sleep (both mid-day naps and nighttime sleep) has a large effect on the efficacy of motor skill acquisition. Performance on a newly learned task improves, often markedly, following a period of sleep. </p><p>A few citations (you can find many more by searching &quot;motor skill acquisition sleep&quot; or similar in google scholar) :</p><ul><li><a href=""https://www.nature.com/articles/nn1959"">https://www.nature.com/articles/nn1959</a></li><li><u><a href=""https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0000341"">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0000341</a></u></li><li><u><a href=""https://link.springer.com/article/10.1111/j.1479-8425.2012.00576.x"">https://link.springer.com/article/10.1111/j.1479-8425.2012.00576.x</a></u></li></ul><p>I want to know if the _timing_ of practice, relative to sleep, makes a difference for skill acquisition. </p><p>For instance, if you practice a skill at 7:00 PM, shortly before a night of sleep, will your performance be better in the morning than if you had practiced at 7:00 AM had a full day of wakefulness, and _then_ gone to sleep? If so, what is the effect size?</p><p>Josh Kaufman makes a claim to this effect in his book, <em>The First 20 Hours</em>. I have no particular reason to doubt him, 40 minutes of searching on google scholar did not turn up any papers about the importance of sleep and practice timing.</p><p>Can you point me at a relevant citation?</p>",elityre,elityre,Eli Tyre,
4NuRCvo23HTYJEepB,"Is ""physical nondeterminism"" a meaningful concept?",is-physical-nondeterminism-a-meaningful-concept,https://www.lesswrong.com/posts/4NuRCvo23HTYJEepB/is-physical-nondeterminism-a-meaningful-concept,2019-06-16T15:55:58.198Z,23,6,15,False,True,,"<p>Background: Our mental models of the universe can contain uncertainty or probability-links, as in a causal network. One may have a deterministic understanding of a phenomenon if the probability-values are all 0 and 1.</p><p>Question: Beyond that, is it meaningful to distinguish whether or not the *universe itself* is deterministic or nondeterministic?</p><p>For example, is it meaningful to say that the Copenhagen interpretation of QM implies a &quot;nondeterministic universe&quot;, while Many Worlds implies a &quot;deterministic universe&quot;?</p>",Liron,liron,Liron,
3pCLT5CrxJb6cR6sz,SSC-Madison: Andrew Yang,ssc-madison-andrew-yang,https://www.lesswrong.com/events/3pCLT5CrxJb6cR6sz/ssc-madison-andrew-yang,2019-06-16T14:09:26.690Z,3,2,1,False,False,,,marywang,marywang,marywang,
SDQuYNC9hRsud4ksE,Reasonable Explanations,reasonable-explanations,https://www.lesswrong.com/posts/SDQuYNC9hRsud4ksE/reasonable-explanations,2019-06-16T05:29:09.873Z,78,25,7,False,False,,"<p>Today I watched a friend do calibration practice and was reminded of how wide you have to cast your net to get well-calibrated 90% confidence.  This is true even when the questions aren&#x27;t gotchas, just because you won&#x27;t think of all the ways something could be wildly unlike your quick estimate&#x27;s model.  Being well-calibrated for 90% confidence intervals (even though this doesn&#x27;t sound all <em>that</em> confident!) requires giving lots of room even in questions you really do know pretty well, because you will <em>feel</em> like you really do know pretty well when in fact you&#x27;re missing something that wrecks you by an order of magnitude.</p><p>Being miscalibrated can <em>feel like</em> &quot;if it were outside of this range, I have just... no explanation for that&quot; - and then it turns out there&#x27;s a completely reasonable explanation.</p><p>Anyway, I thought a fun exercise would be describing weird situations we&#x27;ve encountered that turned out to have reasonable explanations.  In initial descriptions, present only the information you (and whoever was thinking it over with you) <em>remembered to consider at the time</em>, then follow up in <a href=""https://rot13.com"">ROT-13</a> with what made the actual sequence of events come clear.</p>",Alicorn,alicorn,Alicorn,
oYm9L95LQXrEwNEZY,Accelerate without humanity: Summary of Nick Land's philosophy,accelerate-without-humanity-summary-of-nick-land-s,https://www.lesswrong.com/posts/oYm9L95LQXrEwNEZY/accelerate-without-humanity-summary-of-nick-land-s,2019-06-16T03:22:04.474Z,38,24,24,False,False,,"<html><head></head><body><p>I took note of the philosopher Nick Land from reading about <a href=""https://en.wikipedia.org/wiki/Posthumanism"">posthumanism on Wikipedia</a>.</p>
<blockquote>
<p>A more pessimistic alternative to transhumanism in which humans will not be enhanced, but rather eventually replaced by artificial intelligences. Some philosophers, including Nick Land, promote the view that humans should embrace and accept their eventual demise. This is related to the view of ""cosmism"", which supports the building of strong artificial intelligence even if it may entail the end of humanity, as in their view it ""would be a cosmic tragedy if humanity freezes evolution at the puny human level"".</p>
</blockquote>
<p>I was intrigued by such boldness, so I read more. And turns out Nick Land's writing is sometimes easy to read but most of the times extremely hard to read, and probably garbage. I wrote this post so that you don't have to waste time wading through the garbage, looking for fragments of good poetry.</p>
<blockquote>
<p>Nothing human makes it out of the near-future. -- Nick Land</p>
</blockquote>
<p>First of all, Nick Land was obsessed with hating Kant, loving <a href=""https://en.wikipedia.org/wiki/Gilles_Deleuze"">Gilles Deleuze</a> and <a href=""https://en.wikipedia.org/wiki/F%C3%A9lix_Guattari"">Félix Guattari</a> and their philosophical style (called <a href=""https://en.wikipedia.org/wiki/Schizoanalysis"">schizoanalysis</a>, related to schizophrenia). He likes to think about the world from very nonhuman viewpoints, such as other animals, robots, computers, machines that humans made, earth, the universe, etc. He likes capitalism and technological revolution, as fast as possible, without regard for its goodness.</p>
<p>Recently there's some mainstream reports on his philosophy of Neoreactionism (""<a href=""https://en.wikipedia.org/wiki/Dark_Enlightenment"">Dark Enlightenment</a>""), the idea that democracy sucks and monarchy/CEO-president works better. This philosophy has gained a bit of following, but uninteresting to me, so we won't review that. I'd simply note that the phrase ""Dark Enlightenment"" really should be ""Delightenment"". Really missing out such a pun.</p>
<h2>Schizoanalysis</h2>
<p>The idea of schizoanalysis just means that there's a lot of ways to make a theory about the world, and make philosophies, and there's no one way to do it, and further, there could be genuine conflicts that cannot be resolved by appealing to a higher standard.</p>
<p>In mathematics, there's some fringe movement of this style. Most mathematicians are in favor of logical consistency, but some are okay with controlled inconsistency (<a href=""https://plato.stanford.edu/entries/logic-paraconsistent/"">paraconsistency</a>). Most mathematicians are in favor of using infinities, but some are finitists who think that infinities don't exist, and a few are <a href=""https://mathoverflow.net/questions/44208/is-there-any-formal-foundation-to-ultrafinitism"">ultrafinitists who think that there are finite large numbers (such as e^{e^{10}}) that can be assumed to not exist</a>.</p>
<p>Coincidentally, these fringe mathematicians tend to be obnoxious and argumentative (Doron Zeilberger is a prominent example). Maybe there's such a thing as an ""obnoxious fringe personality""...</p>
<p>Schizoanalysis uses an analogy for how to think about theories: the <em>rhizome</em>. A rhizome is a bunch of underground roots, touching each other in a messy network. This is in contrast to a tree, from a big trunk going up to little branches.</p>
<p><img src=""https://upload.wikimedia.org/wikipedia/commons/b/b1/Euphorbia_rhizophora2_ies.jpg"" alt=""rhizome""></p>
<p>Traditionally, stories about the world are told like a tree: there's a great principle of the world: be it God, Existentialism, or Absurdism, and the story gets more and more details as it explains the smaller things like how to treat other people.</p>
<p>But maybe there are many stories just messed up and knotted, without any way to unify them in a single principle. I make stories about Infinities and you about Ultrafinitism and there's no way to unify us. Two powerful countries with incompatible philosophies go to war, unable to unify their stories.</p>
<h2>Really obscure style</h2>
<p>Kant is hard enough, Deleuze and Guattari's books are unreadable (I tried). Nick Land, being immersed in such kinds of books, often wrote in the same extreme obscure style. For example, <a href=""https://web.archive.org/web/20190304091609/http://xenopraxis.net/readings/land_machinicdesire.pdf""><em>Machinic Desire</em> (1992)</a>:</p>
<blockquote>
<p>The transcendental unconscious is the auto-construction of the real, the production of production, so that for schizoanalysis there is the real exactly in so far as it is built. Production is production of the real, not merely of representation, and unlike Kantian production, the desiring production of Deleuze/Guattari is not qualified by humanity (it is not a matter of what things are like for us)...</p>
</blockquote>
<p>Don't bother trying to understand that. A big part of reading philosophy is to ignore real nonsense while still spending time on apparent nonsense that is actually sensible.</p>
<h2>Non-human viewpoints</h2>
<h4>Rats</h4>
<p>Nick Land uses schizoanalysis by considering very non-human viewpoints. For example, he once gave a talk about <a href=""https://www.prospectmagazine.co.uk/philosophy/nick-land-the-alt-writer"">studying the Black Death from the perspective of rats</a>:</p>
<blockquote>
<p>""Putting the Rat back Into Rationality"", in which he argued that, rather than seeing death as an event that happened at a particular time to an individual, we should look at it from the perspectives of the rats carrying the Black Death into Europe; that is, as a world-encircling swarm... An older professor tried to get his head round this idea: “How might we locate this description within human experience?” he asked. Nick told him that human experience was, of course, worthy of study, but only as much as, say, the experience of sea slugs: “I don’t see why it should receive any special priority.”</p>
</blockquote>
<h4>Earth</h4>
<p>Another paper/fiction, <a href=""https://web.archive.org/web/20190616030845/http://the-eye.eu/public/Books/cnqzu.com/Philosophy/neoreaction/Nick%20Land/LAND%20--%20Barker%20Speaks.pdf""><em>Barker Speaks</em></a>, develops the theory of ""geotrauma"", a story about how the Earth feels, and it feels endless PAIN. This is my most favorite story so far, just because it's easy to picture (especially if you know <a href=""https://en.wikipedia.org/wiki/Gaia_hypothesis"">Gaia theory</a>).</p>
<blockquote>
<p>Deleuze and Guattari ask: Who does the Earth think it is?... during the Hadean epoch, the earth was kept in a state of superheated molten slag [from asteroid impacts]... the terrestrial surface cooled, due to the radiation of heat into space... During the ensuing – Archaen – epoch the molten core was buried within a crustal shell, producing an insulated reservoir of primal exogeneous trauma, the geocosmic motor of terrestrial transmutation... It’s all there: anorganic memory, plutonic looping of external collisions into interior content, impersonal trauma as drive-mechanism.</p>
</blockquote>
<p>Basically, do psychoanalysis on geology. The center of the earth is full of heat, and tension, leftovers from its early pains of being hit by asteroids. This trauma is being expressed in geological phenomena like earthquakes, volcanoes, and continental drifts.</p>
<blockquote>
<p>Fast forward seismology and you hear the earth scream.</p>
</blockquote>
<p>Further, even biological creatures should be thought of as one kind of geological phenomenon. This isn't complete nonsense, considering that we have possible clay-life earlier on Earth, and the fact that biological lifeforms have shaped geological strata.</p>
<blockquote>
<p>Geotrauma is an ongoing process, whose tension is continually expressed – partially frozen – in biological organization.</p>
</blockquote>
<p>In this story, biological creatures are just one way for Earth to express its trauma. We are the skin-crawls, manifestations of Earth's inner suffering.</p>
<h4>Machinic desires</h4>
<p>Nick Land talks a lot about cyborgs, AI, and machinic desire/desiring machines. The idea is that humans, animals, anything that has desires, are machines behaving as if they have true desires. It's not necessary for there to be deep reasons behind wanting to do something. A creature desires something (like sugar) because it's constructed to seek it.</p>
<p>Humans, animals, computers, cyborgs, they are all desiring machines. Some are better at achieving their desires, but there's no way to judge who has a superior/inferior desire.</p>
<h2>Acceleration and capitalism</h2>
<p>Nick Land is obsessed with progress and capitalism. Progress here seems to be defined by increasing complexity, increased number of machines, and numbers going up. I have some sympathies with this idea, but at the same time is also very uncomfortable with it.</p>
<h4>Idle game of the whole universe</h4>
<p>The easiest way to summarize accelerationism seems to be: The universe should be consumed into an idle game.</p>
<p>Think of Cookie Clicker. You click to make a number go up and enslave grandmas and build factories to make more cookies, with which to buy more cookie makers. It's the purest form of capitalism: You never get to consume any cookies, and all that's produced is reinvested to produce more. You don't have any friends, you consume the whole universe to make cookies, and that's all there is. The number of cookies accelerates exponentially, and you feel happy and empty and can't stop going anyway.</p>
<p>Accelerationism sees an idle game universe as good, or the least bad of all choices.</p>
<h4>Previous work</h4>
<p>The idea that capitalism is a great innovative, destructive force is nothing new. <a href=""https://www.marxists.org/archive/marx/works/1848/communist-manifesto/ch01.htm"">Karl Marx's <em>Communist Manifesto</em> (1848) already states</a>:</p>
<blockquote>
<p>Constant revolutionising of production, uninterrupted disturbance of all social conditions, everlasting uncertainty and agitation distinguish the bourgeois epoch from all earlier ones. All fixed, fast-frozen relations, with their train of ancient and venerable prejudices and opinions, are swept away, all new-formed ones become antiquated before they can ossify. All that is solid melts into air, all that is holy is profaned, and man is at last compelled to face with sober senses his real conditions of life, and his relations with his kind.</p>
</blockquote>
<blockquote>
<p>The need of a constantly expanding market for its products chases the bourgeoisie over the entire surface of the globe. It must nestle everywhere, settle everywhere, establish connexions everywhere.</p>
</blockquote>
<p>The idea that things are changing way too fast is not new either, <a href=""https://en.wikipedia.org/wiki/Future_Shock""><em>Future Shock</em> (1970)</a>, by Alvin and Heidi Toffler is one famous book that argues that the modern area is changing so fast that it's causing many kinds of psychological stress on people. The book is quite accurate in its diagnosis, and its list of features of modern society are so common sense as to be banal (I yawned).</p>
<h4>Accelerationism</h4>
<p>Capitalism has many criticisms, such as turning people into products, giving prices to things that shouldn't have a price, etc. The idea of accelerationism is that we should keep capitalism going, keep technology going, go with the flow of technology even if it destroys humanity and everything we love. After all, there's no alternative. And why resist? It's glorious to burn up like a shooting star, like the fuel of a rocket that accelerates into empty space.</p>
<blockquote>
<p>Capitalism’s destructive force is picked up in the 1990s by the British philosopher Nick Land. In a series of incendiary essays, Land celebrates absolute deterritorialization as liberation—even (or above all) to the point of total disintegration and death... He sees its absolute, violently destructive speed as an alien force that should be welcomed and celebrated.</p>
</blockquote>
<p>Or just like Facebook said:</p>
<blockquote>
<p>Move fast and break things.</p>
</blockquote>
<p>Note: some people use the word ""accelerationalism"" in a different sense, that capitalism is bad, but the only way to escape capitalism is to make it go faster until it arrives at its bitter end, then we can escape. Kind of like diving into the center of a black hole and hoping that we'll escape into a better universe. I'm not interested in this sense of accelerationalism.</p>
<p>This is similar in spirit to cosmism, as a philosophy against humanism, detailed in <em>The Artilect War</em> (2005), by Hugo de Garis. The <a href=""https://web.archive.org/web/20090626023410/https://www.forbes.com/2009/06/18/cosmist-terran-cyborgist-opinions-contributors-artificial-intelligence-09-hugo-de-garis.html"">basic idea</a> is simple though. There are the Terrans, or the humanists, who prefer to keep humans in control, and there are the Cosmists, who wants to keep the progress of intelligence expansion going, and fulfill a kind of cosmic destiny.</p>
<blockquote>
<p>I think humanity should build these godlike supercreatures with intellectual capacities trillion of trillions of trillions times above our levels. I think it would be a cosmic tragedy if humanity freezes evolution at the puny human level.</p>
</blockquote>
<h2>Technological determinism</h2>
<p>Technology is not neutral. It's a mere ""tool"", but even tools have desires and tendencies, controlling the very users who controls the tools. This is an ancient idea, going way back to Socrates's criticism of writing as affecting the memories of its users. Kevin Kelly is a modern thinker who wrote a book <a href=""https://en.wikipedia.org/wiki/What_Technology_Wants""><em>What Technology Wants</em> (2010)</a>, and his idea is that the technologies are very much not neutral, and can even be <a href=""https://www.edge.org/conversation/the-technium-and-the-7th-kingdom-of-life"">thought of as something alive</a>, with its own goals. The future of earth is very much determined by how this ecosystem of technologies evolves.</p>
<p>The cars are mechanical horses that wants you to build more roads so that it can go to more places. In order to encourage you to make more roads, it allows you to sit in them and take you everywhere. Thus proven itself useful, the cars entice you to build more roads. And that's how in just 100 years, there are suddenly these thin, gray, flat concrete things called ""roads"" everywhere on earth. The Internet want to expand, enticing you to join by providing so much stuff there. Junk food wants to be eaten, and diet books want you to get fat. Books want you to make more printing machines, and printing machines want you to read more books.</p>
<blockquote>
<p>This Texan hotel, for instance, was an entirely virtual construction, ones and zeros embedded in a set of chips. And yet, the hotel direly wanted to exist. It would become very beautiful, and it was already very smart. It could sweet-talk itself into physical existence from random piles of raw materials.</p>
</blockquote>
<blockquote>
<p>Oscar lugged the self-declared cornerstone to the corner of the southern wall. ""I belong here,"" the cornerstone declared. ""Put mortar on me.""</p>
</blockquote>
<blockquote>
<p>Oscar picked up a trowel. ""I'm the tool for the mortar,"" the little trowel squeaked cheerfully.</p>
</blockquote>
<blockquote>
<p><em>Distraction</em> (1998), by Bruce Sterling.</p>
</blockquote>
<p>Nick Land takes this to an extreme.</p>
<blockquote>
<p>Machinic desire can seem a little inhuman, as it rips up political cultures, deletes traditions, dissolves subjectivities, and hacks through security apparatuses, tracking a soulless tropism to zero control. This is because what appears to humanity as the history of capitalism is an invasion from the future by an artificial intelligent space that must assemble itself entirely from its enemy's resources.</p>
</blockquote>
<p>It means something like this: our world, with its cars, finances, AI, and other industrial technologies, has a clear goal of its own: a future dominated by more of upgraded versions of these technologies, with humans becoming extinct or irrelevant. An inevitable AI apocalypse. It's called an invasion from the future, because this inhuman future is not yet here, but we already feel like we are being pulled towards it, as if someone has sent agents back in time to ensure humans do not mess up this plan. It's like the plot of Terminator.</p>
<h2>Materialistic nihilism</h2>
<blockquote>
<p>No one could ever ‘be’ a libidinal materialist. This is a ‘doctrine’ that can only be suffered as an abomination, a jangling of the nerves, a combustion of articulate reason, and a nauseating rage of thought. It is a hyperlepsy of the central nervous-system, ruining the body’s adaptive regimes, and consuming its reserves in rhythmic convulsions that are not only futile, but devastating... An aged philosopher is either a monster of stamina or a charlatan.</p>
</blockquote>
<blockquote>
<p>What matters is the violent impulse to escape that gives this book its title. The thirst for annihilation.</p>
</blockquote>
<p>This section is based on his book <a href=""https://www.amazon.com/Thirst-Annihilation-Bataille-Virulent-Nihilism/dp/041505608X""><em>The Thirst for Annihilation</em> (1992)</a> that I have been reading on and off sometimes. This book is a collection of essays on <a href=""https://en.wikipedia.org/wiki/Georges_Bataille"">George Bataille</a>, a very weird writer that I encountered twice. The first time, I encountered him during my research on <a href=""https://en.wikipedia.org/wiki/Lingchi""><em>lingchi</em></a>, as he wrote about it in a really hard to read book (<em>Tears of Eros</em>) that sexualizes violence.</p>
<p>The second time, it was in this book by Nick Land.</p>
<p>Basically, George Bataille wrote a lot, and his writing about materialistic nihilism, death, shit, vomit, garbage, and all that's ugly about life. (He also wrote a lot of sexual fetishes, but it's not very interesting.)</p>
<p>He wrote about them repetitively, not because he wanted to repeat himself a lot, but because to write was to howl in pain. We scream when we are burnt, no matter how many times it happens. Bataille wrote ugly despair whenever ugly despair hit his brain like a tsunami.</p>
<h4>The meaning of life is to waste energy</h4>
<p>Bataille thought Life is evil and ugly and meaningless. Life doesn't try to conserve energy, instead, life is about wasting energy. The Sun is a giant source of energy, and all the excess energy has to be used up somehow... hence life! Life appears when the blind materials of earth become overheated by all the energy of the sun, and shaken into more and more complicated shapes, in order to consume all the excess energy.</p>
<blockquote>
<p>All energy must ultimately be spent pointlessly and unreservedly, the only questions being where, when, and in whose name... Bataille interprets all natural and cultural development upon the earth to be side-effects of the evolution of death, because it is only in death that life becomes an echo of the sun, realizing its inevitable destiny, which is pure loss.</p>
</blockquote>
<p>The Sun is the source of energy. All the energy ends up being wasted, turned to ""zero"", nothing. Life is a thin, fragile, and very complex middle-layer between the Sun and the zero.</p>
<blockquote>
<p>Life is ejected from the energy-blank and smeared as a crust upon chaotic zero, a mold upon death. This crust is also a maze - a complex exit back to the energy base-line - and the complexity of the maze is life trying to escape from out of itself... life is itself the maze of its route to death...</p>
</blockquote>
<p>Nick Land's ""maze"" means something like this: Life is really simple: it's about wasting energy. But life is anything but simple, since life has developed more and more complicated ways to waste energy. In order to waste the maximal amount of energy, it's necessary that life doesn't start wasting energy immaturely (by, for example, committing suicide), but accumulate and grow, before it starts to massively waste energy (by, for example, making babies, then dying and turning into a warm pool of rotten wasted energy).</p>
<p>Paradoxically, in order to waste a lot of energy, life must not waste energy immediately, and so it has to stay alive for quite a while. Thus, life has become more and more complicated, like a maze that keeps growing, apparently wandering further and further away from death, even though it is simply preparing for even more massive wasting of energy later.</p>
<p>This is probably the reason why people fear death, and also fear immortality. They fear death, because they need to accumulate energy. They fear immortality, because they need to waste all the energy at the end of life. An endless life defeats the purpose of life: to waste a lot of energy.</p>
<p>Perhaps a good illustration of this idea is <a href=""https://www.youtube.com/watch?v=GY_uMH8Xpy0"">a time-lapse video of slime molds</a>. They even look like mazes!</p>
<h4>My comments on the theory of life as energy-waster</h4>
<p>Scientifically, I think this is stupid. But it's a good story, and has some kernels of truth.</p>
<p>When Darwinism first became famous, many people thought it was nonsense, because it's just so unlikely that life could emerge in the first place. Sure, once simple life emerges, evolution can start and allow more complicated lifeforms to appear, but why did simple life appear from purely lifeless matter?</p>
<p>This thinking has changed among some scientists. There are theories that say that life, far from being a lucky accident, is in fact inevitable by the laws of physics. Life is in fact ""meant"" to waste solar energy. This was explicitly proposed in <a href=""https://www.sciencedirect.com/science/article/pii/0895717794901880""><em>Life as a manifestation of the second law of thermodynamics</em></a> (1994), by E.D.Schneider, J.J.Kay:</p>
<blockquote>
<p>We argue that as ecosystems grow and develop, they should increase their total dissipation, develop more complex structures with more energy flow, increase their cycling activity, develop greater diversity and generate more hierarchical levels, all to abet energy degradation. Species which survive in ecosystems are those that funnel energy into their own production and reproduction and contribute to autocatalytic processes which increase the total dissipation of the ecosystem.</p>
</blockquote>
<blockquote>
<p>In short, ecosystems develop in ways which systematically increase their ability to degrade the incoming solar energy.</p>
</blockquote>
<p>This theory was given a more mathematical treatment in <a href=""https://arxiv.org/pdf/1209.1179.pdf""><em>Statistical Physics of Self-Replication</em> (2012)</a> by Jeremy England, where it's proposed that self-replication, which is the fundamental part of life, is fueled by entropy. This paper has generated a lot of publicity, for it makes the ideas sketched above mathematically precise. As reported in <a href=""https://www.quantamagazine.org/first-support-for-a-physics-theory-of-life-20170726""><em>First Support for a Physics Theory of Life</em> (2017)</a>:</p>
<blockquote>
<p>It’s not easy for a group of atoms to unlock and burn chemical energy. To perform this function, the atoms must be arranged in a highly unusual form. According to England, the very existence of a form-function relationship “implies that there’s a challenge presented by the environment that we see the structure of the system as meeting.”</p>
</blockquote>
<p>Think of what the humans are doing as they dig up coals and oils and burn them? They are having fun, sure, but from a thermodynamic point of view, they are turning high-quality, useful chemical energy into low-quality, useless heat. If oil and coal are left buried, they would stay undisturbed for millions of years. With human intervention, all the useful energy is turned into useless energy in a hundred years.</p>
<p>Humans perhaps are the solution to the problem of consuming fossil energy. And by analogy, perhaps life is the solution to the problem of consuming solar energy.</p>
<p>As another analogy, think of a bottle of water with its cap unscrewed and turned upside down. Water flows out, to turn its gravitational energy into kinetic energy, and then turn into the useless energy of heat after it splashes into the ground. A big vortex forms in the bottle, and with that vortex, water flows out that much faster.</p>
<p>The beautiful vortexes of steam rising from a cup of hot coffee are similar: ordered structures arising to turn the useful temperature difference (you can run a heat engine with that!) between the coffee and the air, into a useless temperature equality, as fast as possible.</p>
<p>A bacterium is a little vortex for turning sugar into heat.</p>
<blockquote>
<p>But how and why do atoms acquire the particular form and function of a bacterium, with its optimal configuration for consuming chemical energy? England hypothesizes that it’s a natural outcome of thermodynamics in far-from-equilibrium systems.</p>
</blockquote>
<blockquote>
<p>Coffee cools down because nothing is heating it up, but England’s calculations suggested that groups of atoms that are driven by external energy sources can behave differently: They tend to start tapping into those energy sources, aligning and rearranging so as to better absorb the energy and dissipate it as heat. He further showed that this statistical tendency to dissipate energy might foster self-replication. (As he explained it in 2014, “A great way of dissipating more is to make more copies of yourself.”) England sees life, and its extraordinary confluence of form and function, as the ultimate outcome of dissipation-driven adaptation and self-replication.</p>
</blockquote>
<p>Perhaps pockets of low-entropy life emerged only to increase the entropy of the universe at the fastest possible rate.</p>
<p>Perhaps Bataille's desperate theory of life isn't that insane, after all.</p>
</body></html>",Yuxi_Liu,yuxi_liu,Yuxi_Liu,
tvHJ2Zkkn3TBWqcNK,"28 social psychology studies from *Experiments With People* (Frey & Gregg, 2017)",28-social-psychology-studies-from-experiments-with-people,https://www.lesswrong.com/posts/tvHJ2Zkkn3TBWqcNK/28-social-psychology-studies-from-experiments-with-people,2019-06-16T02:23:08.500Z,12,5,1,False,False,,"<html><head></head><body><p>I'm reading a very informative <em>and</em> fun book about human social psychology, <a href=""https://www.amazon.com/Experiments-People-Kurt-P-Frey/dp/1138282111""><em>Experiments With People</em></a> (2nd ed, 2018).</p>
<blockquote>
<p>... 28 social psychological experiments that have significantly advanced our understanding of human social thinking and behavior. Each chapter focuses on the details and implications of a single study, while citing related research and real-life examples along the way.</p>
</blockquote>
<p>Here I summarize each chapter so that you can save time. Some results are old news to me, but some were quite surprising. I often skip over the experimental details, such as how the psychologists used ingenious tricks to make sure the participants don't guess the true purposes of the experiments. Refer to originals for details.</p>
<p>The experiments start in the 1950s and get up to 2010s, and occasionally literatures from before 1900s are quoted.</p>
<p>Chapters I find especially interesting are:</p>
<ul>
<li>Chap 14. It lists the many failures of introspection, and raises question as to what consciousness can do.</li>
<li>Chap 16. It has significant similarity with superrationality and acausal trade.</li>
<li>Chap 20. It warns about how credulous humans are.</li>
<li>Chap 27. It is about the human fear of death and the psychological defenses against it.</li>
<li>Chap 28. It shows how belief in free will can be motivated by a desire to punish immoral behaviors. Understanding why people believe in free will is necessary for a theory of what is the use of the belief in free will.</li>
</ul>
<h2>Chap 1. Conforming to group norms</h2>
<p><a href=""https://en.wikipedia.org/wiki/Asch_conformity_experiments"">Asch conformity experiment</a>, from <a href=""https://www.jstor.org/stable/pdf/24943779.pdf""><em>Opinions and Social Pressure</em> (Asch, 1955)</a></p>
<p><a href=""https://www.youtube.com/watch?v=qA-gbpt7Ts8"">Video demonstration.</a></p>
<blockquote>
<p>Groups of eight participated in a simple ""perceptual"" task. In reality, all but one of the participants were actors, and the true focus of the study was about how the remaining participant would react.</p>
</blockquote>
<blockquote>
<p>Each student viewed a card with a line on it, followed by another with three lines labeled A, B, and C (see accompanying figure). One of these lines was the same as that on the first card, and the other two lines were clearly longer or shorter. Each participant was then asked to say aloud which line matched the length of that on the first card... The actors would always unanimously nominate one comparator, but on certain trials they would give the correct response and on others, an incorrect response. The group was seated such that the real participant always responded last.
<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Asch_experiment.svg/585px-Asch_experiment.svg.png"" alt=""""></p>
</blockquote>
<p>It was found that</p>
<ul>
<li>When there are over 3 actors giving unanimously the wrong answer, the participant went along 1/3 of time.</li>
<li>Increasing the number of actors above 3 did not increase compliance.</li>
<li>Even when the difference between the lines was 7 inches, there were still some who complied.</li>
<li>If there is at least one actor disagreeing with the majority, the participant decreased compliance.</li>
<li>If the fellow dissenter joins the majority, the participant increased compliance to the same level of 1/3.</li>
<li>If the fellow dissenter leaves, the participant increased compliance only slightly.</li>
</ul>
<p>There are two reasons for this compliance. One is heuristic about knowledge: the majority is usually more correct. Another is normative: social acceptance matters more than being correct.</p>
<p>The effect of a dissenting minority is notable.</p>
<blockquote>
<p>Research finds that, whereas majorities inspire heuristic judgments and often compliance, minorities provoke a more systematic consideration of arguments, and possibly, an internal acceptance of their position.<a href=""https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1559-1816.1987.tb00339.x"">(Nemeth, 1987)</a> Majorities tend to have a greater impact on public conformity, whereas minorities tend to have more effect on private conformity. <a href=""https://www.annualreviews.org/doi/pdf/10.1146/annurev.ps.38.020187.003043"">(Chaiten &amp; Stangor, 1987)</a></p>
</blockquote>
<h2>Chap 2. <a href=""https://en.wikipedia.org/wiki/Forced_compliance_theory"">Forced compliance theory</a> and <a href=""https://en.wikipedia.org/wiki/Cognitive_dissonance"">cognitive dissonance</a></h2>
<p>In <a href=""https://en.wikipedia.org/wiki/When_Prophecy_Fails#Conditions""><em>When Prophecy Fails</em></a>, the story of a UFO cult was detailed. When the doomsday prophecy failed, most people left, but some became even firmer believers.</p>
<p>(My own example, not appearing in the book.) In Borges's story <a href=""https://web.archive.org/save/https://biblioklept.org/2019/06/13/a-problem-a-three-paragraph-story-by-jorge-luis-borges/"">A Problem</a>, Borges asks, how would Don Quixote react if he kill a man?</p>
<blockquote>
<p>Having killed the man, don Quixote cannot allow himself to think that the terrible act is the work of a delirium; the reality of the effect makes him assume a like reality of cause, and don Quixote never emerges from his madness.</p>
</blockquote>
<p>This chapter reviews of <a href=""https://www.uni-muenster.de/imperia/md/content/psyifp/aeechterhoff/sommersemester2012/schluesselstudiendersozialpsychologiejens/06_festinger_carlsmith_cognconsequcompliance_jabnormsocpsy1959.pdf""><em>Cognitive consequences of forced compliance</em> (Festinger &amp; Carlsmith, 1959)</a></p>
<ol>
<li>Participants were asked to do an extremely boring task.</li>
<li>Then the experimenter asked the participant to deceive the next participant that the experiment was fun. Half were paid $1, another half paid $20.</li>
<li>A control group was not asked to lie.</li>
<li>Then they were nudged to take a survey about how they felt about the experiment.</li>
</ol>
<p>The result is that, those paid $1 thought the experiment was fun, and those paid $20 thought it was boring, and those that didn't get asked to lie thought it was very boring.</p>
<p>Festinger explains this by the theory of cognitive dissonance:</p>
<ol>
<li>An attitude (thinking the experiment was boring) and a behavior (saying it was fun) clashes, creating an uncomfortable feeling.</li>
<li>The participant then is motivated to remove the discomfort by changing the attitude by <em>rationalization</em> (thinking the experiment was actually fun).</li>
<li>If the participant was paid $20, then there was no dissonance, as there was a ready explanation of the dissonant behavior.</li>
<li>If the participant was paid $1, then there was dissonance, because the participant regarded the lying behavior as mostly <em>voluntary</em>.</li>
</ol>
<p>An alternative explanation from <a href=""https://psycnet.apa.org/record/1967-13584-001""><em>Self-perception: An alternative interpretation of cognitive dissonance phenomena</em> (Bem, 1967)</a>:</p>
<ol>
<li>We don't form beliefs about ourselves by direct introspection, instead, we infer it through</li>
<li>When we behave against previously self-beliefs, this creates an update on our self-beliefs.</li>
</ol>
<p>See also Chap 14 for more on the lack of introspection.</p>
<p>Current consensus is that both theories are correct, in different situations. The self-perception effect happens when the behavior is mildly different from self-beliefs, and the cognitive dissonance effect happens when the behavior is grossly different.</p>
<p>There are also many complications, such as in <a href=""https://www.tandfonline.com/doi/abs/10.1080/00224549709595481""><em>Double forced compliance and cognitive dissonance theory</em> (Girandola, 1997)</a>, which reported that even if participants performed a boring task, then told others about how boring it was, afterwards they still felt the task was more interesting afterwards.</p>
<p>There is a lot of ongoing research.</p>
<h2>Chap 3. Suffering can create liking</h2>
<p>Such curious phenomena as <a href=""https://en.wikipedia.org/wiki/Hazing""><em>hazing</em></a> has been studied since <a href=""https://psycnet.apa.org/record/1960-02853-001""><em>The effect of severity of initiation on liking for a group</em> (Aronson, 1959)</a></p>
<blockquote>
<p>An experiment was conducted to test the hypothesis that persons who undergo an unpleasant initiation to become members of a group increase their liking for the group; that is, they find the group more attractive than do persons who become members without going through a severe initiation.</p>
</blockquote>
<p>The group was a made-up thing by the experimenters. It purports to discuss interesting sexual things, but the participants, after finally ""joining"", would only hear a very boring group discussion about animal sex.</p>
<blockquote>
<p>This hypothesis was derived from Festinger's theory of cognitive dissonance."" 3 conditions were employed: reading of ""embarrassing material"" before a group, mildly embarrassing material to be read, no reading. The results clearly verified the hypothesis.</p>
</blockquote>
<p>The ""embarrassing material"" are lists of obscene words. The ""mildly embarrassing material"" are lists of mildly sexual words.</p>
<p>Result: the very embarrassing ritual increased liking for the group.</p>
<p>Explanation was by the theory of cognitive dissonance: ""I have already invested so much to join the group. I must be a fool if the group turned out to be bad! And I'm not a fool.""</p>
<p>Cognitive dissonance has been used for brainwashing, persuasion, education, and many other kinds of things.</p>
<blockquote>
<p>One of the authors learned from an investigative journalist about how a dodgy car company... had customers unnecessarily wait or hours while their finance deal was supposedly being negotiated upstairs.</p>
</blockquote>
<p>[<em>Commitment and community: Communes and utopias in sociological perspective</em> (Kanter, 1972)] noted that</p>
<blockquote>
<p>19th-century utopian cults requiring their member to make significant sacrifices were more successful. For example, cults that had their members surrender all their personal belongings lasted much longer than those that did not.</p>
</blockquote>
<p>Some bad investments are continued far after they had become clearly unprofitable, this is the <em>sunk cost fallacy</em>.</p>
<h2>Chap 4. Just following orders</h2>
<p>The <a href=""https://en.wikipedia.org/wiki/Eichmann_in_Jerusalem"">banality of evil</a> is the theory that everyday people can do great evils such as the Holocaust, by simply following orders.</p>
<p><a href=""https://psycnet.apa.org/record/1964-03472-001""><em>Behavioral study of obedience</em> (Milgram, 1965)</a> reported the famous <a href=""https://en.wikipedia.org/wiki/Milgram_experiment"">Milgram experiment</a>. A video recreation is <a href=""https://www.youtube.com/watch?v=y6GxIuljT3w"">here</a>.</p>
<p>This is a very famous experiment with many followups. There is sufficient material freely online, such as the Wikipedia page. So I won't recount it here.</p>
<p>I was most surprised to learn that personality had very little effect. That is, obedience exhibited by the participants in this experiment was mostly <em>situational</em>, instead of stemming from the <em>personality</em> of the participants.</p>
<h2>Chap 5. <a href=""https://en.wikipedia.org/wiki/Bystander_effect"">Bystander apathy effect</a></h2>
<blockquote>
<p>The murder of Kitty Genovese stimulated research into the ""bystander effect"". On March 13, 1964 Genovese was murdered... 38 witnesses watched the stabbings but did not intervene or even call the police until after the attacker fled and Genovese had died...</p>
</blockquote>
<p>In <a href=""https://web.archive.org/web/20180403133237/http://www.ivcc.edu/uploadedFiles/_faculty/_pommier/Example%20Article(2).pdf""><em>Bystander intervention in emergencies: Diffusion of responsibility</em> (Latané &amp; Darley, 1968)</a> attributed the lack of help by witnesses to <em>diffusion of responsibility</em>: because each saw others witnessing the same event, they assumed that the others would take responsibility.</p>
<p>This phenomenon has a big literature, and is very popularly known, possibly due to the dramatic stories.</p>
<p>Concerning the original experiment by Latane and Darley, I was again surprised that personality factors had little effect, except one: growing up in a big community is correlated with a <em>lower</em> probability of helping.</p>
<h2>Chap 6. The effect of an audience</h2>
<p>When people perform a task in the presence of others, they perform better if the task is easy, and worse if the task is hard. One theory is that presence of others increases <a href=""https://en.wikipedia.org/wiki/Physiological_arousal"">physiological arousal</a>, which then enhances performance of simple tasks and decreases performance of hard tasks. Other theories</p>
<p>In <a href=""https://psycnet.apa.org/record/1970-00434-001""><em>Social enhancement and impairment of performance in the cockroach</em> (Zajonc, 1969)</a>, it is found that this is true even for cockroaches. In the experiment, Zajonc gave cockroaches two possible tasks: going through a straight maze, or a more complex maze. They either did the task alone, or while being watched by others outside (the maze was transparent).</p>
<p>While being watched, cockroaches solved faster on the straight maze but slower on the complex maze. This demonstrates that the physiological arousal theory is correct in cockroaches: the effect of an audience can happen without any complex cognitive ability.</p>
<p>However, complex cognitive ability sometimes does occur in humans. As reported in <a href=""https://psycnet.apa.org/journals/psp/9/3/245/""><em>Social facilitation of dominant responses by the presence of an audience and the mere presence of others</em> (Cottrell et al, 1968)</a>, blindfolded audience does not exert an effect on the performer.</p>
<h2>Chap 7. <a href=""https://en.wikipedia.org/wiki/Group_conflict"">Group conflicts</a> from trivial groups</h2>
<p>This chapter begins with the <a href=""https://en.wikipedia.org/wiki/Realistic_conflict_theory#Robbers_cave_study"">Robbers Cave experiment</a>, which was a study that investigates the <em>realistic conflict theory</em>, which sounds very common-sense:</p>
<ol>
<li>group conflicts and feelings of resentment for other groups arise from conflicting goals and competition over limited resources</li>
<li>length and severity of the conflict is based upon the perceived value and shortage of the given resource</li>
<li>positive relations can only be restored with goals that require cooperation between groups</li>
</ol>
<p>Then it recounts the <a href=""https://en.wikipedia.org/wiki/Jane_Elliott#First_exercise_involving_eye_color_and_brown_collars"">blue eyes-brown eyes experiment</a>. The problem, then, is, what is the least amount of group-difference in order to make a difference? Enter the <a href=""https://en.wikipedia.org/wiki/Minimal_group_paradigm""><em>minimal group paradigm</em></a> of <a href=""https://www.jstor.org/stable/pdf/24927662.pdf""><em>Experiments in intergroup discrimination</em> (Tajfel, 1970)</a>. Participants first took a test on estimating dot numbers, then divided into ""overestimators"" and ""underestimators"", while in truth they were random. Then, they were given points (convertible to cash) to divide among the groups. Participants favored their own groups significantly more.</p>
<p>In fact, the most favored strategy was to maximize (own group) - (other group), even though it did not maximize (own group). Thus, even the most minimal social groups induced ingroup-outgroup conflict.</p>
<p>The minimal group paradigm has been studied in many ways. It was also found that <a href=""https://en.wikipedia.org/wiki/Out-group_homogeneity"">outgroup homogeneity effect</a>, that is, ""they are all the same; we are diverse"", could also arise from minimal groups.</p>
<p>One theoretical explanation is Tajfel and Turner's <a href=""https://en.wikipedia.org/wiki/Social_identity_theory"">social identity theory</a> <a href=""https://web.archive.org/web/20190510215219/http://ark143.org/wordpress2/wp-content/uploads/2013/05/Tajfel-Turner-1979-An-Integrative-Theory-of-Intergroup-Conflict.pdf"">(Tajfel &amp; Turner, 1979)</a>, which states that:
0. A person's self-esteem depends on having a good identity.</p>
<ol>
<li>A person's identity has two parts: personal and social.</li>
<li>Personal identity are about one's own traits and outcomes.</li>
<li>Social identity are derived from social groups and comparison between groups.</li>
<li>A person is motivated to improve self-esteem, and thus social identity.</li>
<li>Thus, one is motivated to improve the standings of one's ingroups and decrease the standings of one's outgroups.</li>
</ol>
<p>One supporting evidence is that when a person has more self-esteem, they are less discriminating against outgroups <a href=""https://psycnet.apa.org/journals/psp/52/5/907.html?uid=1987-25067-001"">(Crocker et al, 1987)</a>.</p>
<h2>Chap 8. The Good Samaritan Experiment</h2>
<p>In the <a href=""https://en.wikipedia.org/wiki/Parable_of_the_Good_Samaritan"">parable of the Good Samaritan</a>,</p>
<blockquote>
<p>a traveller is stripped of clothing, beaten, and left half dead alongside the road. First a priest and then a Levite comes by, but both avoid the man. Finally, a Samaritan happens upon the traveller. Samaritans and Jews despised each other, but the Samaritan helps the injured man.</p>
</blockquote>
<p>This inspired an experiment reported in <a href=""https://web.archive.org/web/20190614233620/http://www.2macs.com/wp-content/uploads/2016/07/From-Jerusalem-to-Jericho-A-study-of-Variables-in-Helping-Behaviours-Darley-Batson-1973.pdf""><em>""From Jerusalem to Jericho"": A study of situational and dispositional variables in helping behavior.</em> (Darley &amp; Batson, 1973)</a>, participants were theology students asked to give a short talk in another building.</p>
<blockquote>
<p>People going between two buildings encountered a shabbily dressed person slumped by the side of the road. Subjects in a hurry to reach their destination were more likely to pass by without stopping.</p>
</blockquote>
<p>The experiment was 2 x 3: the participant was asked to either give a short talk on the parable of the Good Samaritan, or on an irrelevant topic. They were either very hurried, hurried, or not hurried by the experimenter.</p>
<p>Hurrying made significant difference in the likelihood of their giving the victim help. The topic of the talk had some influence, according to a reanalysis by <a href=""https://psycnet.apa.org/record/1977-03240-001"">(Greenwald, 1975)</a>, despite the original paper's claim of no influence. Self-reported personality and religiosity made no difference.</p>
<p>The lesson from this as well as many other social psychology experiments is that seemingly trivial situational variables have a greater impact than personality variables, even though people tend to explain behaviors using personality. See <em>The Person and the Situation: Perspectives of Social Psychology</em> (Lee Ross, Richard E. Nisbett, 2011)</p>
<h2>Chap 9. External motivation harms internal motivation</h2>
<p>Extrinsic motivations are motivations that ""come from the outside"", such as money, praise, food. Intrinsic motivations are from the inside, such as self-esteem, happiness. Both can motivate behaviors. However, it's interesting that sometimes extrinsic motivations can harm internal motivation.</p>
<p>In <a href=""https://psycnet.apa.org/record/1974-10497-001""><em>Undermining children's intrinsic interest with extrinsic reward: A test of the"" overjustification"" hypothesis</em> (Lepper et al, 1973)</a>, children are given markers to draw with. Some were told that they would be rewarded with a prize for playing, others got a prize unexpectedly, others were left alone as control group.</p>
<p>After some days, the amount of time children spent playing the markers were:
got expected prize &lt; control group &lt; got unexpected prize</p>
<p>The book didn't talk much about why the unexpected prize created higher motivation, but I think it is similar to how gambling addiction comes from <a href=""https://en.wikipedia.org/wiki/Reinforcement#Gambling_%E2%80%93_variable_ratio_scheduling"">variable reward</a>.</p>
<p>There are some explanations for why extrinsic reward lowered subsequent motivation. One is that extrinsic reward provides <a href=""https://en.wikipedia.org/wiki/Overjustification_effect""><em>overjustification effect</em></a>, where external rewards <a href=""https://en.wikipedia.org/wiki/Motivation_crowding_theory"">""crowd out""</a> internal rewards,</p>
<blockquote>
<p>Once rewards are no longer offered, interest in the activity is lost; prior intrinsic motivation does not return, and extrinsic rewards must be continuously offered as motivation to sustain the activity.</p>
</blockquote>
<p>Another explanation is that humans heuristically view means to an end as undesirable. In <a href=""https://psycnet.apa.org/journals/psp/42/1/51/"">(Sagotsky et al, 1982)</a>, children were given two activities, playing with crayons and markers. They were equally fun at the beginning, but one group was told that in order to play with crayons, they had to play with markers first. After a while, they became less interested in playing with markers. The other group, the reverse.</p>
<p>I think this is the psychological basis of <a href=""https://plato.stanford.edu/entries/kant-moral/#HumFor"">some ethical intuitions in the style of Kant</a>:</p>
<blockquote>
<p>we should never act in such a way that we treat humanity as a means only but always as an end in itself.</p>
</blockquote>
<p>A third explanation is that people consider extrinsic rewards a threat to their freedom and autonomy, and thus tend to rebel against it. I saw a news today about <a href=""https://web.archive.org/save/https://www.businessinsider.com.au/amazon-makes-game-warehouse-workers-tasks-2019-5?r=US&amp;IR=T"">Amazon's program to gamify work</a>. Some complained that it was threatening the workers' autonomy, which is a strange complaint: if gamification actually increases intrinsic motivation for work, doesn't it increase autonomy? Autonomy is freedom to follow one's intrinsic motivation, and thus, if a worker acquires an intrinsic motivation to do a good job, they would have more autonomy.</p>
<p>I think this complaint can be explained as a different kind of autonomy: freedom from prediction. Humans are evolved to want to be unpredictable (at least by others), because to be predictable is to be threatened by manipulation, which often decreases fitness.</p>
<h2>Chap 10. <a href=""https://en.wikipedia.org/wiki/Actor%E2%80%93observer_asymmetry"">Actor-observer asymmetry</a></h2>
<blockquote>
<p>Other people did what they did because of who they are. We did what we did because of outside events.</p>
</blockquote>
<p>In 1975, parts of the <a href=""https://en.wikipedia.org/wiki/Watergate_scandal"">Watergate scandal</a> was recreated in a very dramatic psychology experiment, reported in <a href=""https://psycnet.apa.org/journals/psp/32/1/55/""><em>Ubiquitous Watergate: An attributional analysis</em> (West, 1975)</a>.</p>
<p>80 criminology students were asked to meet the experimenter privately for a mysterious reason. There, they were asked to join a burglary team for secret documents in an ad agency. There were four versions presented:</p>
<ol>
<li>The burglary plan was sponsored by a government agency, for secret investigation purposes. Government would provide immunity if caught.</li>
<li>Same, but without immunity.</li>
<li>The plan was sponsored by a rival ad agency, with $2000 reward.</li>
<li>The student was asked to only join a test run of the plan, without stealing anything.</li>
</ol>
<p>Afterwards, they were debriefed and asked to explain their decision to join/not join.</p>
<p>Separately, 238 psychology students were presented the above situation, and asked to guess what percentage would agree to the plan.</p>
<p>Then, half of the participants were asked, ""Suppose John agreed to participate, explain why John agreed.""</p>
<p>Results:</p>
<ul>
<li>About 45% of participants agreed to join the burglary in the government-with-immunity situation. Otherwise, about 10%.</li>
<li>Most students in the second part thought they would not agree to the burglary plan.</li>
<li>Students in the first part who agreed to join the burglary explained their behavior as due to the circumstances.</li>
<li>Students in the second part explained the hypothetical John's behavior as due to John's personality.</li>
</ul>
<p>The criminology students were ""actors"", and the psychology students were ""observers"". An asymmetry was that actors attributed their behavior to situations, while the observers attributed to personalities. This is the actor-observer asymmetry.</p>
<p>Complications in this asymmetry are noted in <a href=""https://psycnet.apa.org/record/2006-20202-004""><em>The actor-observer asymmetry in attribution: A (surprising) meta-analysis</em> (Malle, 2006)</a>. Malle found that there are two kinds of biases: when the behavior is negative, the actor blames the situation and the observer blames the person. When the behavior is positive, the reverse happens. As such, this can be explained as a self-serving bias.</p>
<p>The authors conclude with a funny note:</p>
<blockquote>
<p>it's interesting to how athletes often publically thank the Lord for a personal victory, but do not publically blame the Lord for a defeat!</p>
</blockquote>
<h2>Chap 11. We are number 1</h2>
<blockquote>
<p>They never shout, ""They are number 1.""</p>
</blockquote>
<p>People like to think good about themselves. Even in collectivistic societies, people regard themselves as above average in collectivistic traits, according to <a href=""https://www.researchgate.net/publication/10958513_Pancultural_Self-Enhancement""><em>Pancultural self-enhancement</em> (Sedikides et al, 2003)</a></p>
<blockquote>
<p>Americans... self-enhanced on individualistic attributes, whereas Japanese... self-enhanced on collectivistic attributes</p>
</blockquote>
<p>An experiment is reported in <a href=""https://psycnet.apa.org/record/1977-10287-001""><em>Basking in reflected glory: Three (football) field studies</em> (Ciadini et al, 1976)</a>, where students are asked to describe a recent university sports team's victory/defeat. Before that, half received criticisms that decreased to their self-esteem, and others received praises that increased their self-esteem.</p>
<p>The result was that among those who had higher self-esteem, they described the sports outcome using ""we won"" or ""we lost"" 1/4 of the times. For those who had lower self-esteem, they used ""we won"" 40% of the times when the team won, but used ""we lost"" only 14% of the times when the team lost.</p>
<p>The explanation is that people in need of boosts to self-esteem try to BIRG (Basking in reflected glory) and CORF (Cut off from reflected failures). Reflected glory also improves their social standing.</p>
<p>Methods of increasing one's social standing are called <a href=""https://en.wikipedia.org/wiki/Impression_management""><em>impression management</em></a>, and include:</p>
<ul>
<li>BIRG and CORF, as noted above;</li>
<li>ingratiation: we praise and agree with others, so as to be liked;</li>
<li>self-handicapping: a student gets drunk before a big test, so that if they fail, they could blame on the drunkenness instead of their study ability;</li>
<li>exemplification: behave virtuously and make sure others saw it.</li>
</ul>
<p>A lot of these techniques are listed in <a href=""https://www.researchgate.net/publication/248124372_Toward_a_general_theory_of_strategic_self-presentation"">(Jones and Pittman, 1982)</a>.</p>
<h2>Chap 12. <a href=""https://en.wikipedia.org/wiki/Deindividuation"">Deindividuation</a></h2>
<p><a href=""https://psycnet.apa.org/record/1976-20842-001""><em>Effects of deindividuation variables on stealing among Halloween trick-or-treaters</em> (Diener et al, 1976)</a> reported an <a href=""https://en.wikipedia.org/wiki/Deindividuation#Diener,_Fraser,_Beaman,_and_Kelem_(1976)"">experiment in real life</a>.</p>
<p>The experiment was run in a Halloween. An experimenter place a bowl of candy in her living room for trick-or-treaters. A hidden recorder observes. In one condition, the woman asked the children identification questions such as their names. In the other condition, children were completely anonymous.  Some children came individually, others in a group.</p>
<p>In each condition, the woman invited the children in, claimed she had something in the kitchen she had to tend to, and told each child to take only one candy.</p>
<p>Result: being in a group and being anonymous both increased frequency of transgression (taking more than one candy). If the first child to take candies in a group transgressed, other children were also more likely to transgress.</p>
<p>The authors then defined deindividuation as when private self-awareness is reduced.</p>
<blockquote>
<p>The truly deindividuated person is alleged to pay scant atetntion to personal values and moral codes... to be inordinately sensitive to cues in the immediate environment.</p>
</blockquote>
<p>One study, <a href=""https://psycnet.apa.org/record/1982-09774-001""><em>The baiting crowd in episodes of threatened suicide</em> (Mann, 1981)</a>, examined 21 cases from newspapers, in which crowds were present when a person threatened to jump off a high place.</p>
<blockquote>
<p>Baiting or jeering occurred in 10 of the cases. Analysis of newspaper accounts of the episodes suggested several deindividuation factors that might contribute to the baiting phenomenon: membership in a large crowd, the cover of nighttime, and physical distance between crowd and victim (all factors associated with anonymity).</p>
</blockquote>
<p>Two theories of why deindividuation were given. One is that anonymity makes people feel safe to transgress. Another is that <a href=""https://www.tandfonline.com/doi/abs/10.1080/14792779443000049"">(Reicher &amp; Postmes, 1995)</a> people in a crowd would categorize themselves mainly by their social identity, and their behaviors would reflect the group norm than their personal norms.</p>
<p>I was disappointed that the authors did not give evolutionary psychological explanations for deindividuation. Humans are the only animals that wage wars. A deindividuation effect can be an evolutionary adaptation to prepare humans to fight more effectively in a crowd.</p>
<h2>Chap 13. <a href=""https://en.wikipedia.org/wiki/Mere-exposure_effect"">Mere exposure effect</a></h2>
<p>People prefer familiar things. Really, that's quite a banal observation. What's delightful about this chapter is the ingenuity of the experiment design.</p>
<p>Think about your own face. You see them in a mirror image (unless you take a selfie), but others see it directly. This means that you are familiar with your face in the mirror image, but others in the direct image.</p>
<p>This is exploited in <a href=""https://psycnet.apa.org/record/1979-31008-001""><em>Reversed facial images and the mere-exposure hypothesis</em> (Mita et al, 1977)</a>. Couples were separately shown photos of the female one's face, some mirrored, others not. They were asked to pick the one they prefer. The female one preferred the mirrored photo, and the male one preferred the direct photo.</p>
<p>Mere exposure effect is robust in real life and across species. <a href=""https://psycnet.apa.org/record/1979-00907-001"">(Grush et al, 1978)</a> found that</p>
<blockquote>
<p>previous or media exposure alone successfully predicted 83% of the [US congress election] primary winners</p>
</blockquote>
<p>And <a href=""https://link.springer.com/article/10.3758/BF03331092"">(Cross et al, 1967)</a> found rats who heard Mozart music in infancy preferred Mozart over Schoenberg as adults, and vice versa.</p>
<p>One possible evolutionary psychological explanation were given: preference familiarity is safer, and thus more adaptive. The authors warned however that it's not so simple, as people also have a preference for mild novelty.</p>
<h2>Chap 14. Shortcomings of introspection</h2>
<p>This chapter reviews a study that shows a particular instance of introspection failure:</p>
<blockquote>
<p>people's ideas about how their minds work stem not from private insights but from public knowledge. Unfortunately, however, this public knowledge is often not accurate. It is based on intuitive theories, widely shared throughout society, that are often mistaken.</p>
</blockquote>
<p>The book referenced <a href=""https://web.archive.org/web/20170818074812/https://deepblue.lib.umich.edu/bitstream/handle/2027.42/92159/VerbalReportsOnCausalInfluences.pdf?sequence=1""><em>Verbal reports about causal influences on social judgments: Private access versus public theories</em> (Nisbett, 1977)</a>, although I find <a href=""https://web.archive.org/web/20190126113112/https://deepblue.lib.umich.edu/bitstream/handle/2027.42/92167/TellingMoreThanWeCanKnow.pdf""><em>Telling More Than We Can Know: Verbal Reports on Mental Processes</em> (Nisbett &amp; Wilson, 1977)</a> to be better.</p>
<blockquote>
<p>Subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response. It is proposed that when people attempt to report on their cognitive processes... they do not do so on the basis of any true introspection. Instead, their reports are based on <em>a priori</em>, implicit causal theories, or judgments about the extent to which a particular stimulus is a plausible cause of a given response.</p>
</blockquote>
<p>In the experiment, a subject is given a fictitious application from Jill for the job of staff at crisis center. These applications are the same except on a few attributes of the applicant: attractiveness, intelligence, etc. Then, the subject is asked how much each attribute is correlated with the decision to accept.</p>
<p>The situation is then described to some observers (who didn't do the job application review), who are asked how much each attribute is correlated with the decision of the subject to accept.</p>
<blockquote>
<p>Subjects who read that Jill had once been involved in a serious car accident claimed that the event had made them view her as a more sympathetic person. However, according to the ratings they later gave, this event had exerted no impact... the only exception pertained to ratings of Jill's intelligence. Here, an almost perfect correlation emerged between how subjects' judgments had actually shifted and how much they believed they had shifted. Why so? The researchers argued that there are explicit rules, widely known throughout a culture, for ascribing intelligence to people. Because subjects could readily recognize whether a given factor was relevant to intelligence, they could reliably guess whether they would have taken it into consideration.</p>
</blockquote>
<blockquote>
<p>The determinations of subjects and observers coincided almost exactly.</p>
</blockquote>
<p>There are other introspection failures demonstrated by social psychology. People are unaware of the <a href=""https://en.wikipedia.org/wiki/Halo_effect"">halo effect</a> at work in their own judgments of others <a href=""https://web.archive.org/web/20170822050650/https://deepblue.lib.umich.edu/bitstream/handle/2027.42/92158/TheHaloEffect.pdf?sequence=1"">(Nisbett &amp;Wilson, 1977)</a>. People are unaware of <a href=""https://en.wikipedia.org/wiki/Misattribution_of_arousal"">the source of their own arousal</a>. People are unaware of their bias even if they know of such bias <a href=""https://journals.sagepub.com/doi/abs/10.1177/0146167202286008"">(Pronin et al, 2002)</a>.</p>
<p>In a further twist, introspection can degrade judgment. In <a href=""https://journals.sagepub.com/doi/abs/10.1177/0146167293194006"">(Wilson &amp; Kraft, 1993)</a>, participants reported how they felt about their romantic partners. Their expressed feelings correlated well with the duration of relationship. However, if they introspected on the reason of their feelings, before reporting their feelings, the correlation disappeared.</p>
<p>The authors conclude by suggesting that traveling, by putting oneself into novel situations, would be particularly helpful for one to know oneself.</p>
<h2>Chap 15. <a href=""https://en.wikipedia.org/wiki/Self-fulfilling_prophecy"">Self-fulfilling prophecies</a></h2>
<p>Again, a very well-known subject with a lot already written. This chapter reviews <a href=""https://web.archive.org/web/20170808155657/http://www.radford.edu/jaspelme/443/spring-2007/Articles/Snyder-Tanke-Bersheid-1977.pdf""><em>Social perception and interpersonal behavior: On the self-fulfilling nature of social stereotypes</em> (Snyder et al, 1977)</a></p>
<blockquote>
<p>Male ""perceivers"" interacted with female ""targets"" whom they believed to be physically attractive/unattractive. Tape recordings of each participant's conversational behavior were analyzed by naive observer judges for evidence of behavioral confirmation... targets who were perceived to be physically attractive came to behave in a friendly, likeable, and sociable manner in comparison with targets whose perceivers regarded them as unattractive. It is suggested that theories in cognitive social psychology attend to the ways in which perceivers create the information that they process in addition to the ways that they process that information.</p>
</blockquote>
<p>Philosophically, a self-fulfilling prophecy is a prediction about a future that is true iff the act of prediction is done. Usually, predictions themselves are supposed to be independent of the future that they talk about. Of course, all useful predictions must affect the future -- the predictor would try to profit from the prediction. However, such effects on the future are on the <em>predictor</em>, not on the <em>predicted</em>.</p>
<p>Social psychologists have found that human behaviors are more influenced by the situation than the personality (as noted in <em>The Person and the Situation</em> book). Snyder et al suggested that, in fact, personality traits are one of those self-fulfilling prophecies.</p>
<blockquote>
<p>our believing that others possess certain traits may cause us to behave in certain consistent ways toward them. This may cause them to behave in consistent ways in our presence.</p>
</blockquote>
<p>In other words, a lot of the persistence of personality could arise from the <a href=""https://en.wikipedia.org/wiki/Fundamental_attribution_error"">fundamental attribution error</a>.</p>
<h2>Chap 16. How to live like a predeterminist</h2>
<blockquote>
<p>So then, God has mercy on whom he chooses to have mercy, and he hardens whom he chooses to harden. -- <em>Romans</em> 9:18, which Calvinists quote a lot.</p>
</blockquote>
<p>Suppose an urge to smoke and a propensity to lung cancer are both genetically determined, and smoking does not cause lung cancer, why not smoke? If you feel the urge to smoke, it's already too late.</p>
<p>Believers of <a href=""https://en.wikipedia.org/wiki/Calvinism"">Calvinism</a> think that God has chosen some people to be saved, and others are damned. Those who are favored by God would both be naturally free from the urge to sin in this world, and enjoy paradise after death. Those who are not, would feel the urge to sin in this world, and go to hell after death.</p>
<p>So if a Calvinist feels an urge to sin, it's already too late. Why not sin? Instead, Calvinists keep resisting the urge to sin, and moreover, deny that they are resisting such urges, and insisting that they are effortlessly virtuous, evidence of God's favor.</p>
<p>In <a href=""https://web.archive.org/web/20190615050556/https://apps.dtic.mil/dtic/tr/fulltext/u2/a131586.pdf""><em>Causal versus diagnostic contingencies: On self-deception and on the voter's illusion</em> (Quattrone &amp; Tversky, 1984)</a> two experiments are reported.</p>
<p>In the first one, participants exercised, then were asked to put their hands in ice water until the pain makes them withdraw. Then they were told a version of the lung cancer puzzle: There are two kinds of hearts, type 1 and type 2, caused by unchangeable genetics. Type 1 heart is associated both with health and with a higher tolerance to the ice water after exercise. Type 2 heart is associated with early death and a lower tolerance. They then did the ice water test again, and they exhibited longer tolerance to the ice water, even though many of them denied that they were trying to do so.</p>
<blockquote>
<p>In the second experiment, subjects encountered one of two theories about the sort of voters who determine the margin of victory in an election. Only one of the theories would enable voting subjects to imagine that they could ""induce"" other like-minded persons to vote. As predicted, more subjects indicated that they would vote given that theory than given a theory in which the subject's vote would not be diagnostic of the electoral outcome, although the causal impact of the subject's vote is the same under both theories</p>
</blockquote>
<p>One explanation is that the unconsciousness deceived the consciousness, but the authors find this unreasonable, for it still does not explain what motivates the unconsciousness to deceive. They instead favored <a href=""https://faculty.washington.edu/agg/pdf/Gwald_Self-decept_1997.OCR.pdf"">Greenwald's theory</a> that people avoid analyzing in detail threatening information, just like how we throw away junk mail without looking in detail.</p>
<blockquote>
<p>In conclusion, self-deception is not the result of one center of intelligence hoodwinking the other. Rather, it is the result of a low-level screening process that banishes suspicious cognitions before they have the opportunity to be fully entertained by the conscious mind.</p>
</blockquote>
<h4>Similarity to superrationality and acausal trade.</h4>
<p>The behavior of Calvinists is similar to <a href=""https://en.wikipedia.org/wiki/Superrationality"">superrationality</a> and <a href=""https://wiki.lesswrong.com/wiki/Acausal_Trade"">acausal trade</a>, in which agents behave in a way that is <em>diagnostic</em> of good outcomes, even if it does not <em>cause</em> good outcomes.</p>
<p>Assuming the superrational player has access to their opponents' source codes/simulations, the superrationality strategy can be justified, but then it would just be usual rationality.</p>
<p>I think normative decision theories are incompatible with sufficiently good prediction. Normative decisions are only defined for agents with <em>apparent</em> free will. An agent apparently has free will only to someone who cannot predict the agent's behavior well. Superrationality and acausal trade both attempt to make a decision theory for agents that are aware that they are too predictable (to themselves or to someone they play with). This is similar to the situation where someone sees the future and then ""decides"" to rebel against the future. Either they saw the true future and did not rebel, or they did not see the true future at all. It's illogical to say they both saw the future and rebelled against it.</p>
<p>Similar problems happen with <a href=""https://www.scottaaronson.com/blog/?p=30"">Scott Aaronson's</a> solution to <a href=""https://en.wikipedia.org/wiki/Newcomb's_paradox"">Newcomb's paradox</a> (I'm a ""Wittengenstein""). A determinist who is self-aware of their determinism would, instead of offering a decision theory (""I should take one box because...""), offer a prediction theory (""I probably would take one box because..."").</p>
<h2>Chap 17. Partisan perceptions of media bias</h2>
<p>People often complain of media biases. People report differently about the same event. Why?</p>
<p>In <a href=""https://psycnet.apa.org/record/1986-03718-001""><em>The hostile media phenomenon: biased perception and perceptions of media bias in coverage of the Beirut massacre</em> (Vallone et al, 1985)</a>, the researchers studied how people perceived news about the <a href=""https://en.wikipedia.org/wiki/Sabra_and_Shatila_massacre"">Bairut massacre</a>,</p>
<blockquote>
<p>killing of civilians, mostly Palestinians and Lebanese Shiites... carried out by the militia under the eyes of their Israeli allies.</p>
</blockquote>
<p>The researchers took some neutral reports on the event, and as expected, pro-Israel people thought they are biased to be anti-Israel, while anti-Israel people thought they are biased to be pro-Israel.</p>
<p>In a study on biases <a href=""https://psycnet.apa.org/journals/psp/47/6/1231/"">(Lord et al, 1984)</a>, participants avoided bias by this command:</p>
<blockquote>
<p>""Ask yourself at each step whether you would have made the same evaluations had exactly the same study produced results on the other side of the issue.</p>
</blockquote>
<h2>Chap 18. <a href=""https://en.wikipedia.org/wiki/Empathy-altruism"">Empathy-altruism</a> hypothesis</h2>
<p>Several theorized psychological mechanisms of human altruistic actions are studied in <a href=""https://en.wikipedia.org/wiki/Empathy-altruism#Evidence""><em>More evidence that empathy is a source of altruistic motivation</em> (Batson, 1982)</a> reported an experiment on whether people would help a person in need.</p>
<p>It was found that: If (empathy OR guilt), then (helping). That is, people can be motivated to act altruistically by empathy without expectation of gain, or to gain relief from guilt. This argues against the theory of <em>psychological hedonism</em>.</p>
<p>Other potential sources of altruism are collectivism (act for the benefit of a group) and principlism (uphold a principle for its own sake). Effective altruism is one example of principlism based on utilitarianism.</p>
<h2>Chap 19. Expanding the self to include the other</h2>
<p>A psychological phenomenon of love (close personal relationships, such as lover, best friend) is to include that person in one's self. This involves perceiving, and allocating resources to, that person, in a similar way as to one's self.</p>
<p>Three experiments are described, from <a href=""https://psycnet.apa.org/journals/psp/60/2/241.html?uid=1991-18305-001""><em>Close relationships as including other in the self</em> (Aron, 1991)</a>.</p>
<ol>
<li>
<p>When allocating money, they allocate about the same to themself as to their friend.</p>
</li>
<li>
<p>They were asked to imagine nouns paired with their selves, mothers, or strangers. They recalled fewer nouns imagined with self or mother than nouns imagined with a stranger, suggesting that mother was processed more like self than a stranger.<br>
They explained the reason why it was recalled <em>less</em> by that we usually look at strangers directly, but only ourselves upon reflection (literal or not), and so it's harder to imagine ourselves than strangers.</p>
</li>
<li>
<p>When faced with a task to sort a list of adjectives into 4 piles: ""true/false about me, and true/false about my spouse"", they reacted slower on adjectives that were true about one but false about the other. This was explained by that differences between one's own and a close other's properties caused dissonance in the same way that holding opposite attitudes within oneself can cause dissonance.</p>
</li>
</ol>
<h2>Chap 20. Believing precedes disbelieving</h2>
<p>Descartes divided the mind up into intellect and will. The intellect writes up potential beliefs about the world; the will then chooses which to endorse. Spinoza said that we believe everything that we happen to understand, and then disbelieve only if we find it necessary. <a href=""https://web.archive.org/web/20170319174158/http://www.danielgilbert.com/Gilbert%20et%20al%20(EVERYTHING%20YOU%20READ).pdf""><em>You Can't Not Believe Everything You Read</em> (Gilbert, 1993)</a> presented three experiments that supports Spinoza's theory, and discussed its sociological effect.</p>
<blockquote>
<p>... we asked subjects in Experiment 1 to play the role of a trial judge and to make sentencing decisions about an ostensibly real criminal defendant. Subjects were given some information about the defendant that was known to be false and were occasionally interrupted [by a distraction task]... We predicted that interruption would cause subjects to continue to believe the false information they accepted on comprehension and that these beliefs would exert a profound influence on their sentencing of the defendant...</p>
</blockquote>
<blockquote>
<p>Experiments 1 and 2 provide support for the Spinozan hypothesis: When people are prevented from unbelieving the assertions they comprehend... they did not merely recall that such assertions were said to be true, but they actually behaved as though they believed the assertions.</p>
</blockquote>
<p>If you want to read more, <a href=""https://sad-pony-mathematician.blogspot.com/2018/10/lets-read-spinozas-ethics-you-cant-not.html"">I have written in detail about this</a>.</p>
<h2>Chap 21. Inferred memories</h2>
<p>When we recall a memory, that memory is an inference about past based on a number of clues that we have in the present. It is not necessarily accurate.</p>
<p>Experiment from <a href=""https://www.ncbi.nlm.nih.gov/pubmed/2778636""><em>Women's theories of menstruation and biases in recall of menstrual symptoms</em> (McFarland, 1989)</a> found that when women report, day-to-day, their unpleasant emotions, there is no difference between premenstrual, menstrual, and inter-menstrual days (they feel equally unpleasant). But when asked to <em>recall</em> how unpleasant it was, they recall significantly more unpleasant pre-menstrual and menstrual days, and <em>less</em> unpleasant inter-menstrual days.</p>
<p>This is explained by that, when they recall, they used intuitive theories about PMS to infer ""how it must have felt"" instead of ""how it actually felt"". This also, as a side effect, casts doubt on <a href=""https://en.wikipedia.org/wiki/Premenstrual_syndrome#Alternative_views"">whether PMS actually exists</a>.</p>
<p>Memories can be completely made up, as in <a href=""https://en.wikipedia.org/wiki/Recovered-memory_therapy"">repressed memory therapies</a>.</p>
<p>The fact that those inferences about the past are felt as genuine recalls, shows how little conscious introspection can give true knowledge about the self.</p>
<h2>Chap 22. <a href=""https://en.wikipedia.org/wiki/Ironic_process_theory"">Ironic process theory</a></h2>
<blockquote>
<p>Try to not to think of a polar bear!</p>
</blockquote>
<p>The theory of ironic process is that there is a cognitive process called <em>intender</em> who is looking for contents that matches some desired mental state. There is also a <em>monitor</em> who notifies consciousness about errant thoughts.</p>
<p>The intender is a costly process, and the monitor is a cheap process, so when one is under cognitive load, the intender doesn't work well, but the monitor still works well, and ironically, trying to not think of something results in thinking of it.</p>
<p><a href=""https://web.archive.org/web/20160901174616/https://www.redirectanxiety.com/wp-content/uploads/2015/04/Wegner1994.pdf""><em>Ironic Processes of Mental Control</em> (Wegner, 1994)</a> reported an experiment. Participants were asked to consciously improve/deprove their moods with happy/sad thoughts. Half were also asked to do a memory task as <a href=""https://en.wikipedia.org/wiki/Cognitive_load"">cognitive load</a>.</p>
<p>Those not under cognitive load were successful in their mood control, while those under cognitive load achieved the opposite.</p>
<p>This suggests that if you are under some cognitive load (such as busy studying), and you want to improve your mood, you should try consciously to feel worse. Also, if you are in a noisy and distracting environment, and want to sleep, you should try to stay awake.</p>
<p>Another experiment showed that people who try to avoid sexist language become ironically more prone to sexist language when under cognitive load. This is true no matter if they are sexist or not.</p>
<h2>Chap 23. <a href=""https://en.wikipedia.org/wiki/Implicit-association_test"">Implicit Association Test</a></h2>
<p>In <a href=""https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0163872""><em>Single-target implicit association tests (ST-IAT) predict voting behavior of decided and undecided voters in swiss referendums</em> (Raccuia, 2016)</a>, compared to self-reported political orientation, implicit association was found to be a weaker, but somewhat independent, predictor of voting behavior.</p>
<p>Other similar methods to probe the unconsciousness are studied, and the results are new and mixed.</p>
<h2>Chap 24. <a href=""https://en.wikipedia.org/wiki/Prospect_theory"">Prospect theory</a></h2>
<p>People don't behave as expectation-maximizers. Instead they are better modelled by prospect theory:</p>
<ol>
<li>Gains and losses are measured compared to a changeable default, instead of an absolute zero.</li>
<li>Losses are weighted more than gains, and both have decreasing marginal utilities.</li>
<li>People are more risk-averse with respect to gains, and more risk-loving with respect to losses.
<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Prospect_Theory_Graph.svg/640px-Prospect_Theory_Graph.svg.png"" alt=""prospect-theory-wikipedia""></li>
</ol>
<p>An experiment <a href=""https://journals.sagepub.com/doi/pdf/10.1177/0146167299259003""><em>The systematic influence of gain-and loss-framed messages on interest in and use of different types of health behavior</em> (Rothman et al, 1999)</a>. It was found that people used more bacteria-killing mouth wash, if they received positive advertising (about <em>maintaining</em> good health). They used more disclosing mouth wash (which merely detects dental diseases) if they received negative advertising (about the potential disease).</p>
<p>This theory, along with some others, is explained in great detail in <a href=""https://en.wikipedia.org/wiki/Thinking%2C_Fast_and_Slow""><em>Thinking, Fast and Slow</em> (Kahnemann, 2011)</a>, which I recommend.</p>
<p>Other mental heuristics include <a href=""https://en.wikipedia.org/wiki/Mental_accounting""><em>mental accounting</em> (Thaler, 1980)</a>, with its own set of irrational effects.</p>
<h2>Chap 25. Social isolation increases aggression</h2>
<p><a href=""https://www.ncbi.nlm.nih.gov/pubmed/11761307""><em>If you can't join them, beat them: Effects of social exclusion on aggressive behavior</em> (Twenge, 2001)</a></p>
<blockquote>
<p>Social exclusion was manipulated by telling people that they would end up alone later in life or that other participants had rejected them. These manipulations caused participants to behave more aggressively. Excluded people issued a more negative job evaluation against someone who insulted them, blasted a target with higher levels of aversive noise both when the target had insulted them and when no interaction had occurred. However, excluded people were not more aggressive toward someone who issued praise.</p>
</blockquote>
<p>In particular,</p>
<blockquote>
<p>These responses were specific to social exclusion and were not mediated by emotion.</p>
</blockquote>
<p>This was shown by two experimental facts:</p>
<ol>
<li>
<p>Participants who were told they would end up alone later in life or that other participants had rejected them, did not feel worse than average.</p>
</li>
<li>
<p>Participants who were told they would end up unlucky later in life, did not act more aggressively than average.</p>
</li>
</ol>
<p>Some psychological theories are given. One is <em>self-determination theory</em> from <a href=""http://www.jwalkonline.org/docs/Grad%20Classes/Fall%2007/Org%20Psy/Cases/motivation%20articles/PERUSED/deci_ryan_2000.pdf""><em>Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being</em> (Deci and Ryan, 2000)</a>, which says that people have three needs:</p>
<ul>
<li>relatedness (to some other people)</li>
<li>efficacy (can do important things)</li>
<li>autonomy (can control their own future)</li>
</ul>
<p>Other relevant factors are self-esteem, and stability over time. <a href=""https://www.ncbi.nlm.nih.gov/pubmed/2746456""><em>Stability and level of self-esteem as predictors of anger arousal and hostility</em> (Kernis et al, 1989)</a> found that in feelings of anger and hostility,</p>
<p>unstable high self-esteem &gt; low self-esteem &gt; stable high self-esteem</p>
<p>There is no evolutionary explanation, though. Social exclusion causes fewer offsprings, and aggression only worsens it. An evolutionary psychological explanation would be good. Either it has evolutionary benefit, or it is a side effect of something else.</p>
<h2>Chap 26. Social effects of gossiping</h2>
<p>Gossip is found to have a prosocial function. <a href=""https://psycnet.apa.org/record/2012-00030-001?doi=1""><em>The virtues of gossip: Reputational information sharing as prosocial behavior</em> (Feinberg, 2012)</a></p>
<blockquote>
<p>... <em>prosocial gossip</em>, the sharing of negative evaluative information about a target in a way that protects others from antisocial or exploitative behavior.</p>
</blockquote>
<p>In the study, they found experimental support for four hypotheses about the function of gossip:</p>
<ul>
<li><em>prosocial</em>: gossip is motivated by a desire to protect vulnerable people, without promise of material reward.</li>
<li><em>frustration</em>: seeing antisocial behavior makes people feel bad, which . Prosocial people are more prone to this frustration.</li>
<li><em>relief</em>: gossiping reduces the frustration.</li>
<li><em>deterrence</em>: threat of gossip makes antisocial people behave more prosocially.</li>
</ul>
<h2>Chap 27. Fear of death</h2>
<p>Good news: we will be worm food one day!</p>
<p>Good news for worms, I meant.</p>
<p><a href=""https://en.wikipedia.org/wiki/Terror_management_theory"">Terror management theory</a> argues that the terror of death creates such a profound, subconscious, anxiety, that humans spend their lives denying it in various ways, creating culture, religion, and many other social phenomena in the process.</p>
<p>In this chapter are reviewed the first 4 of the 7 experiments from <a href=""https://psycnet.apa.org/doiLanding?doi=10.1037/a0025947""><em>How sweet it is to be loved by you: the role of perceived regard in the terror management of close relationships</em> (CR Cox, J Arndt, 2012)</a>. This paper studies</p>
<blockquote>
<p>... whether people turn to close relationships to manage the awareness of mortality because they serve as a source of perceived regard.</p>
</blockquote>
<p><em>Perceived regard</em> means ""am I a good person as viewed by someone else?"" The paper in particular showed that people who have death on their mind exaggerate how much they think they are loved by a partner. Perceived regard from their own selves, and from average strangers, did not change. Having intense physical pain on the mind also did nothing.</p>
<p>They also found that having death on the mind makes people claim to love their partners more. They theorized that this is <em>mediated</em> by increased perceived regard:</p>
<p>death on the mind -&gt; more perceived regard from their partner -&gt; more love for their partner</p>
<blockquote>
<p>Study 4 revealed that activating thoughts of perceived regard from a partner in response to MS reduced death-thought accessibility. Studies 5 and 6 demonstrated that MS led high relationship contingent self-esteem individuals to exaggerate perceived regard from a partner, and this heightened regard led to greater commitment to one's partner. Study 7 examined attachment style differences and found that after MS, anxious individuals exaggerated how positively their parents see them, whereas secure individuals exaggerated how positively their romantic partners see them. Together, the present results suggest that perceptions of regard play an important role in why people pursue close relationships in the face of existential concerns.</p>
</blockquote>
<p><em>Personal comment</em>: It has been commented that Transhumanism can be analyzed as a religion. Is there value in analyzing transhumanism through terror management theory? There is at least one paper, <a href=""https://link.springer.com/chapter/10.1007/978-3-642-32560-1_18""><em>Software immortals: Science or faith?</em> (Proudfoot, 2012)</a>, that did so. This is important, because if transhumanism is indeed a religion, then the chance is high that it is deluded/unfalsifiable, like most religions have been shown to be.</p>
<p>Also, this would explain why moral nihilism is usually suffered as a mental disease than accepted as a working hypothesis. Despite its theoretical simplicity and moderate empirical support, it just doesn't offer any protection against terror of death.</p>
<h2>Chap 28. Motivated belief in free will</h2>
<p><a href=""https://pdfs.semanticscholar.org/fc63/33d24da4dc21ff942bd9f9fe8aad761d7603.pdf""><em>Free to punish: A motivated account of free will belief</em> (Clark, 2014)</a></p>
<blockquote>
<p>a key factor promoting belief in free will is a fundamental desire to hold others morally responsible for their wrongful behaviors</p>
</blockquote>
<p>Five experiments from the paper are recounted in detail. The authors praised the paper highly for its comprehensiveness.</p>
<blockquote>
<p>participants reported greater belief in free will after considering an immoral action than a morally neutral one... due to heightened punitive motivations... reading about others’ immoral behaviors reduced the perceived merit of anti-free-will research... the real-world prevalence of immoral behavior (as measured by crime and homicide rates) predicted free will belief on a country level.</p>
</blockquote>
<blockquote>
<p>Taken together, these results provide a potential explanation for the strength and prevalence of belief in free will: It is functional for holding others morally responsible and facilitates justifiably punishing harmful members of society.</p>
</blockquote>
<p><em>Personal comment</em>: Instead of philosophically studying whether free will exists, it's more productive to <a href=""https://en.wikipedia.org/wiki/Pierre-Simon_Laplace#I_had_no_need_of_that_hypothesis"">assume it doesn't exist</a>, and see what behaviors can be explained. If everything can be explained without free will, then the problem of free will dissolves. Else, we will have concentrated what free will is for, and made subsequent studies more focused.</p>
<p>It is also useful to study the human intuitive belief in free will, as important phenomena about humans, independent of whether they are right or wrong. This is analogous to the study of <a href=""https://plato.stanford.edu/entries/folkpsych-theory/"">folk psychology</a> and <a href=""https://web.archive.org/web/20190614010335/https://humans101.wordpress.com/tag/naive-physics/"">naive physics</a>. See <a href=""https://link.springer.com/article/10.1007/s13164-009-0010-7""><em>From Uncaused Will to Conscious Choice: The Need to Study, Not Speculate About People’s Folk Concept of Free Will</em> (Monroe, 2009)</a></p>
<blockquote>
<p>the core of people’s concept of free will is a choice that fulfills one’s desires and is free from internal or external constraints. No evidence was found for metaphysical assumptions about dualism or indeterminism.</p>
</blockquote>
<p>In the ""Afterthoughts"", the authors considered what a post-free-will society could be like. I think that such a society's theory of crime and punishment would be more like ""because this follows the natural order of things"", than ""because criminals are morally bad"".</p>
<p>Think of the joke about ""my brain made me commit the crime""</p>
<blockquote>
<p>The criminal: ""My brain made me commit the crime.""
The judge: ""My brain made me sentence you.""</p>
</blockquote>
<p>And now, instead of taking it as a joke, imagine both of them saying them very seriously. That's what I think could be true in the future.</p>
</body></html>",Yuxi_Liu,yuxi_liu,Yuxi_Liu,
cu7YY7WdgJBs3DpmJ,The Univariate Fallacy,the-univariate-fallacy-1,https://www.lesswrong.com/posts/cu7YY7WdgJBs3DpmJ/the-univariate-fallacy-1,2019-06-15T21:43:14.315Z,34,17,15,False,False,,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p><em>(A standalone math post that I want to be able to link back to later/elsewhere)</em></p>
<p>There's this statistical phenomenon where it's possible for two multivariate distributions to overlap along any one variable, but be cleanly separable when you look at the entire <a href=""https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace"">configuration space</a> at once. This is perhaps easiest to see with an illustrative diagram—</p>
<p><img src=""https://i.imgur.com/Q5Gzfp3.png"" alt=""""></p>
<p>The denial of this possibility (in arguments of the form, ""the distributions overlap along this variable, therefore you can't say that they're different"") is sometimes called the ""univariate fallacy."" (Eliezer Yudkowsky <a href=""https://twitter.com/ESYudkowsky/status/1124757043997372416"">proposes ""covariance denial fallacy"" or ""cluster erasure fallacy""</a> as potential alternative names.)</p>
<p>Let's make this more concrete by making up an example with actual numbers instead of just a pretty diagram. Imagine we have some datapoints that live in the forty-dimensional space {1, 2, 3, 4}⁴⁰ that are sampled from one of two probability distibutions, which we'll call <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_A""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span></span></span></span></span></span></span> and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_B""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span></span></span></span></span></span></span>.</p>
<p>For simplicity, let's suppose that the individual variables <em>x₁</em>, <em>x₂</em>, ... <em>x₄₀</em>—the coördinates of a point in our forty-dimensional space—are statistically independent and identically distributed. For every individual <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_i""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span>, the marginal distribution of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_A""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span></span></span></span></span></span></span> is—</p>
<p><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""P_A(x_i) = \begin{cases} 1/4 &amp; x_i = 1 \\ 7/16 &amp; x_i = 2 \\ 1/4 &amp; x_i = 3 \\ 1/16 &amp; x_i = 4 \\ \end{cases}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mrow MJXc-space3""><span class=""mjx-mo"" style=""vertical-align: -1.276em;""><span class=""mjx-delim-v""><span class=""mjx-char MJXc-TeX-size4-R"" style=""padding-top: 0.667em; padding-bottom: 0.298em;"">⎧</span><span class=""mjx-char MJXc-TeX-size4-R"" style=""line-height: 0.202em; margin-bottom: 0.087em; margin-top: -0.137em;"">⎪
⎪
⎪</span><span class=""mjx-char MJXc-TeX-size4-R"" style=""padding-top: 0.961em; padding-bottom: 0.961em;"">⎨</span><span class=""mjx-char MJXc-TeX-size4-R"" style=""line-height: 0.202em; margin-bottom: 0.087em; margin-top: -0.137em;"">⎪
⎪
⎪</span><span class=""mjx-char MJXc-TeX-size4-R"" style=""margin-top: -0.217em; padding-bottom: 1.182em;"">⎩</span></span></span><span class=""mjx-mtable"" style=""vertical-align: -2.2em; padding: 0px 0.167em;""><span class=""mjx-table""><span class=""mjx-mtr"" style=""height: 1.175em;""><span class=""mjx-mtd"" style=""padding: 0px 0.5em 0px 0px; text-align: left; width: 2em;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0px 0px 0px 0.5em; text-align: left; width: 2.685em;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-strut""></span></span></span></span><span class=""mjx-mtr"" style=""height: 1.275em;""><span class=""mjx-mtd"" style=""padding: 0.1em 0.5em 0px 0px; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">7</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">16</span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0.1em 0px 0px 0.5em; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span><span class=""mjx-strut""></span></span></span></span><span class=""mjx-mtr"" style=""height: 1.275em;""><span class=""mjx-mtd"" style=""padding: 0.1em 0.5em 0px 0px; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0.1em 0px 0px 0.5em; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3</span></span><span class=""mjx-strut""></span></span></span></span><span class=""mjx-mtr"" style=""height: 1.175em;""><span class=""mjx-mtd"" style=""padding: 0.1em 0.5em 0px 0px; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">16</span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0.1em 0px 0px 0.5em; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span><span class=""mjx-strut""></span></span></span></span></span></span><span class=""mjx-mo"" style=""width: 0.12em;""></span></span></span></span></span></span></p>
<p>And for <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_B""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span></span></span></span></span></span></span>—</p>
<p><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""P_B(x_i) = \begin{cases} 1/16 &amp; x_i = 1 \\ 1/4 &amp; x_i = 2 \\ 7/16 &amp; x_i = 3 \\ 1/4 &amp; x_i = 4 \\ \end{cases}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mrow MJXc-space3""><span class=""mjx-mo"" style=""vertical-align: -1.276em;""><span class=""mjx-delim-v""><span class=""mjx-char MJXc-TeX-size4-R"" style=""padding-top: 0.667em; padding-bottom: 0.298em;"">⎧</span><span class=""mjx-char MJXc-TeX-size4-R"" style=""line-height: 0.202em; margin-bottom: 0.087em; margin-top: -0.137em;"">⎪
⎪
⎪</span><span class=""mjx-char MJXc-TeX-size4-R"" style=""padding-top: 0.961em; padding-bottom: 0.961em;"">⎨</span><span class=""mjx-char MJXc-TeX-size4-R"" style=""line-height: 0.202em; margin-bottom: 0.087em; margin-top: -0.137em;"">⎪
⎪
⎪</span><span class=""mjx-char MJXc-TeX-size4-R"" style=""margin-top: -0.217em; padding-bottom: 1.182em;"">⎩</span></span></span><span class=""mjx-mtable"" style=""vertical-align: -2.2em; padding: 0px 0.167em;""><span class=""mjx-table""><span class=""mjx-mtr"" style=""height: 1.175em;""><span class=""mjx-mtd"" style=""padding: 0px 0.5em 0px 0px; text-align: left; width: 2em;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">16</span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0px 0px 0px 0.5em; text-align: left; width: 2.685em;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-strut""></span></span></span></span><span class=""mjx-mtr"" style=""height: 1.275em;""><span class=""mjx-mtd"" style=""padding: 0.1em 0.5em 0px 0px; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0.1em 0px 0px 0.5em; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span><span class=""mjx-strut""></span></span></span></span><span class=""mjx-mtr"" style=""height: 1.275em;""><span class=""mjx-mtd"" style=""padding: 0.1em 0.5em 0px 0px; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">7</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">16</span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0.1em 0px 0px 0.5em; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3</span></span><span class=""mjx-strut""></span></span></span></span><span class=""mjx-mtr"" style=""height: 1.175em;""><span class=""mjx-mtd"" style=""padding: 0.1em 0.5em 0px 0px; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">/</span></span></span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0.1em 0px 0px 0.5em; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">4</span></span><span class=""mjx-strut""></span></span></span></span></span></span><span class=""mjx-mo"" style=""width: 0.12em;""></span></span></span></span></span></span></p>
<p>If you look at any one <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x_i""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span>-coördinate for a point, you can't be confident which distribution the point was sampled from. For example, seeing that <em>x₁</em> takes the value 2 gives you a 7/4 (= 1.75) likelihood ratio in favor of that the point having been sampled from <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_A""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span></span></span></span></span></span></span> rather than <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_B""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span></span></span></span></span></span></span>, which is log₂(7/4) ≈ 0.807 <a href=""http://yudkowsky.net/rational/technical/"">bits of evidence</a>.</p>
<p>That's ... not a whole lot of evidence. If you guessed that the datapoint came from <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_A""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span></span></span></span></span></span></span> based on that much evidence, you'd be wrong about 4 times out of 10. (Given equal (1:1) prior odds, an odds ratio of 7:4 amounts to a probability of (7/4)/(1 + 7/4) ≈ 0.636.)</p>
<p>And yet if we look at <em>many</em> variables, we can achieve <em>supreme, godlike</em> confidence about which distribution a point was sampled from. <em>Proving</em> this is left as an exercise to the particularly intrepid reader, but a concrete <em>demonstration</em> is probably simpler and should be pretty convincing! Let's write some Python code to sample a point <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\vec{x}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-munderover""><span class=""mjx-stack""><span class=""mjx-over"" style=""height: 0.248em; padding-bottom: 0.06em; padding-left: 0.064em;""><span class=""mjx-mo"" style=""vertical-align: top;""><span class=""mjx-char MJXc-TeX-vec-R"" style=""padding-top: 0.519em; padding-bottom: 0.225em;"">→</span></span></span><span class=""mjx-op""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span></span></span></span></span></span> ∈ {1, 2, 3, 4}⁴⁰ from <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_A""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span></span></span></span></span></span></span>—</p>
<pre><code>import random

def a():
    return random.sample(
        [1]*4 +  # 1/4
        [2]*7 +  # 7/16
        [3]*4 +  # 1/4
        [4],     # 1/16
        1
    )[0]

x = [a() for _ in range(40)]
print(x)
</code></pre>
<p>Go ahead and run the code yourself. (With an <a href=""https://repl.it/languages/python3"">online REPL</a> if you don't have Python installed locally.) You'll <em>probably</em> get a value of <code>x</code> that ""looks something like""</p>
<pre><code>[2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 4, 4, 2, 2, 3, 3, 1, 2, 2, 2, 4, 2, 2, 1, 2, 1, 4, 3, 3, 2, 1, 1, 3, 3, 2, 2, 3, 3, 4]
</code></pre>
<p>If someone off the street just handed you this <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\vec{x}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-munderover""><span class=""mjx-stack""><span class=""mjx-over"" style=""height: 0.248em; padding-bottom: 0.06em; padding-left: 0.064em;""><span class=""mjx-mo"" style=""vertical-align: top;""><span class=""mjx-char MJXc-TeX-vec-R"" style=""padding-top: 0.519em; padding-bottom: 0.225em;"">→</span></span></span><span class=""mjx-op""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span></span></span></span></span></span> without telling you whether she got it from <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_A""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span></span></span></span></span></span></span> or <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_B""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">B</span></span></span></span></span></span></span></span>, how would you compute the probability that it came from <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_A""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span></span></span></span></span></span></span>?</p>
<p>Well, because the coördinates/variables are statistically independent, you can just tally up (multiply) the individual likelihood ratios from each variable. That's only a little bit more code—</p>
<pre><code>import logging

logging.basicConfig(level=logging.INFO)

def odds_to_probability(o):
    return o/(1+o)

def tally_likelihoods(x, p_a, p_b):
    total_odds = 1
    for i, x_i in enumerate(x, start=1):
        lr = p_a[x_i-1]/p_b[x_i-1]  # (-1s because of zero-based array indexing)
        logging.info(""x_%s = %s, likelihood ratio is %s"", i, x_i, lr)
        total_odds *= lr
    return total_odds

print(
    odds_to_probability(
        tally_likelihoods(
            x,
            [1/4, 7/16, 1/4, 1/16],
            [1/16, 1/4, 7/16, 1/4]
        )
    )
)
</code></pre>
<p>If you run that code, you'll <em>probably</em> see ""something like"" this—</p>
<pre><code>INFO:root:x_1 = 2, likelihood ratio is 1.75
INFO:root:x_2 = 1, likelihood ratio is 4.0
INFO:root:x_3 = 2, likelihood ratio is 1.75
INFO:root:x_4 = 2, likelihood ratio is 1.75
INFO:root:x_5 = 1, likelihood ratio is 4.0
[blah blah, redacting some lines to save vertical space in the blog post, blah blah]
INFO:root:x_37 = 2, likelihood ratio is 1.75
INFO:root:x_38 = 3, likelihood ratio is 0.5714285714285714
INFO:root:x_39 = 3, likelihood ratio is 0.5714285714285714
INFO:root:x_40 = 4, likelihood ratio is 0.25
0.9999936561215961
</code></pre>
<p>Our computed probability that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\vec{x}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-munderover""><span class=""mjx-stack""><span class=""mjx-over"" style=""height: 0.248em; padding-bottom: 0.06em; padding-left: 0.064em;""><span class=""mjx-mo"" style=""vertical-align: top;""><span class=""mjx-char MJXc-TeX-vec-R"" style=""padding-top: 0.519em; padding-bottom: 0.225em;"">→</span></span></span><span class=""mjx-op""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span></span></span></span></span></span> came from <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P_A""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.109em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span></span></span></span></span></span></span> has several nines in it. Wow! That's pretty confident!</p>
<p>Thanks for reading!</p>
</body></html>",Zack_M_Davis,zack_m_davis,Zack_M_Davis,
LRE9SeAEYXf6Et8w5,On Seeing Through 'On Seeing Through: A Unified Theory': A Unified Theory,on-seeing-through-on-seeing-through-a-unified-theory-a,https://www.lesswrong.com/posts/LRE9SeAEYXf6Et8w5/on-seeing-through-on-seeing-through-a-unified-theory-a,2019-06-15T18:57:25.436Z,26,9,0,False,False,https://www.gwern.net/Turing-complete#on-seeing-through-and-unseeing,,gwern,gwern,gwern,
ssgafq584SckcT3yC,"Do policy makers frequently underestimate non-human threats (ie: epidemic, natural disasters, climate change) when compared to threats that are human in nature (ie: military conflict, economic competition, Cold War)?",do-policy-makers-frequently-underestimate-non-human-threats,https://www.lesswrong.com/posts/ssgafq584SckcT3yC/do-policy-makers-frequently-underestimate-non-human-threats,2019-06-15T16:29:47.755Z,5,3,3,False,True,,"<p>  </p><p>Epidemic status: A random thought that I don’t feel adequate talking about. <br/> However, I believe this topic must’ve been discussed by somebody somewhere, so if my question reminds you of something similar, please point me to it. </p><p>It appears to me that policy makers seem to frequently prioritise dealing with threat posed by human players and neglect threat posed by the natural environment, even when the latter could be demonstrably more worrisome than the former. </p><p>Almost as if we are “designed” to deal with hostility from other humans, but are badly equipped to perceive and deal with a hostile natural environment. </p><p>I still struggle to express my point, so I’ll put in a couple examples and see how it goes. These examples don&#x27;t serve as evidence, as I made some sweeping statements about politics that won&#x27;t necessarily be true (stereotypically true, perhaps), just to show what my question is concerning. </p><p>Example 1: Cold War and climate change. <br/> For example, the USSR (ignoring the effect of nuclear weapons) during the Cold War probably didn’t pose an existential threat to the Western countries, while an impeding climate catastrophe might be. <br/> Yet policy makers during the Cold War felt the need to commit significant amount of resources to counteract Soviet influence across the globe, including the costly (and arguably futile and nonsensical) wars in Korea and Vietnam, while political actions on the climate issue seems rather half-minded and still controversial.<br/> Was it because climate change is too far away while the threat of communism seemed imminent? I don’t think so, since the reasoning behind the wars and many other geopolitical manoeuvres were likely long term, on the scale of decades if not longer—our environmental policy won’t differ much in this regard. </p><p>Example 2: WWI and influenza</p><p>In year 1918, even after the Influenza was observed to be extremely dangerous, authorities in the US were reluctant to implement public health measures and suppressed truthful reporting of the epidemic hoping to keep public morale high for WWI.<br/> Which certainly backfired when the epidemic became obvious to the public, dismantling their trust in the authority. Critical judgement error indeed. <br/> Again, it seems that the threat of the epidemic was downplayed in the mind of policy makers, even though it was already observed to be potent and deadly, and ultimately proven to be more destructive than the war itself. (Which shouldn’t come as too much a surprise)</p><p>Hopefully I have explained my question well enough.</p><p>I would like to know if there are similar discussions about this topic. </p><p>Maybe we can extend the guess further: we humans might be more adapted in sorting out our relationship with other humans compared to sorting out our relationship with non-human subjects. This is a very general statement that I don’t have any idea if we can back it up. </p>",RorscHak,rocksbasil,RocksBasil,
nLxH8jsK73i7YGi9m,Press Your Luck,press-your-luck,https://www.lesswrong.com/posts/nLxH8jsK73i7YGi9m/press-your-luck,2019-06-15T15:30:00.702Z,14,7,0,False,False,,"<p>Epistemic Status: Oh, we’re doing this.</p>
<p>Press Your Luck is back! Press Your Luck is back! Wednesday Nights on ABC! Woo-hoo!</p>
<p></p>
<p></p>
<p>The great classic game shows each bring together several unique elements into a synergistic whole with a consistent aesthetic and central theme. Each winning game has its own style of questions, its own attitude, its own game theoretic issues. You can’t tweak an existing game show to create a great or even good new one. You must do something truly new, with its own logic.</p>
<p>Thus, we have <em>very </em>long runs of the games that have hit upon a winning formula, and revivals of them when they fail. Within a revival or long run, now you can tweak the game and improve your place in local design space. Otherwise, you can’t, even if the existing implementation left a lot on the table. Family Feud is a great example of a unique game that has severe design flaws, but which likely isn’t going to change or be improved upon.</p>
<p>Some of the games that I count on this list along with Press Your Luck, mostly but not entirely from my youth, would be Jeopardy, Wheel of Fortune, $100,000 Pyramid, Scrabble, Sale of the Century, Who Wants to be a Millionaire, Greed, Let’s Make a Deal, The Price is Right, Family Feud, Deal or no Deal, Hollywood Squares and The Weakest Link.</p>
<p>Some of these are more evergreen and well-balanced products, with base content that informs and entertains, that one can watch for a long time, like Jeopardy. Others focus more on problems game theory,</p>
<p>The best reality competitions also feature this, but are much more vulnerable to cloning via minor variation (e.g. National Idol and its variations).</p>
<p>Enough general chatter. The new revival of Press Your Luck is here.</p>
<p>For those not familiar with Press Your Luck, the game takes place in two rounds with two stages each. In the first stage, contestants buzz in to answer questions. The questions are general (mostly easy) trivia. You can buzz in at any time, interrupting the question, which will finish after you answer. The first person to buzz in gets three spins if right, the others are given multiple choice between their answer and two other choices, and get one spin if right. There are four questions each round (three in the first round of the new version).</p>
<p>The strategy in this round mostly involves knowing when it is worth buzzing in despite not knowing for sure what the question is (they sometimes meander on purpose to trick those who try to guess too early),  and knowing when the person buzzing in knows their stuff and should be trusted. Guessing is generally very good.</p>
<p>In the second stage of the round, players take spins. Each spin randomly results in a prize that is added to one’s total, usually money but sometimes a trip or product that is valued at its retail price for total score, and sometimes includes an extra spin. There are also several whammies on the board. If you get one of those, you lose everything and go back to zero, and if you get four whammies you lose the game.</p>
<p>In the first round, whoever has the fewest spins goes first. In the second round, whoever has the least money from round one goes first.</p>
<p>The trick is that you can <em>pass </em>your spins to another player, unless the spins were already passed to you. If someone passes a spin, you have to take it yourself, but any extra spins you earn off those spins can be passed along, and if you get a whammy then you are free to again pass anything you have left. Passing always goes to the opponent with the most money.</p>
<p>Because of the constant danger of the whammy, and the ability of passed spins to be passed back to you, and the importance of going last in the second round (which far exceeds the value of the cash you bring to that round because of the frequency of whammies), passing at the right time is often a surprisingly complex decision. Key questions include whether you have enough time to recover should you hit a whammy, how likely the other player is to have enough spins to pass you, and whether you’re giving the other player enough spins that they are likely to be able to pass too many of them back to you even in scenarios that seem otherwise good.</p>
<p>The player with the most money at the end wins, and keeps those funds and prizes. In the old version, the player could then return the next day. Due to the way the game works, it <em>always </em>comes down to the last spin.</p>
<p>The great news is that this is still Press Your Luck.</p>
<p>Alas, the news isn’t all great.</p>
<p>There are changes. Some of them tiny, one of them large. They’re not great.</p>
<p>They all revolve around the problem of <em>not knowing what your show is about and what its aesthetic is trying to be. </em>It’s not that the show <em>has </em>to retain the tacky and vaguely sketchy 80s vibe, as much I would have voted for that. It’s that the show has to choose something and go with it, and it just… didn’t.</p>
<p>Let’s talk about the changes in detail.</p>
<p>Press Your Luck’s original run had numbers represented by numbers displayed from individual pixels displayed on a sign below each player. Thus, we used to see this:</p>
<p><img src=""https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSomTpGQk4ERJxTBJaJ5gO9J_CuVY1kVvCT3MEvBZT9yaEX86xHig"" /></p>
<p>Pictured: The Man Who Broke The Format, Michael Larson</p>
<p>Which fit perfectly into this:</p>
<p><img src=""https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRyRD-EI5M1cB7zh6oonKaiS7ML8GCDl6cWbM_QMBcnTL1sBNjD"" /></p>
<p>And with this board:</p>
<p><img src=""https://i1.wp.com/www.liketotally80s.com/wp-content/uploads/2011/11/press-your-luck.jpg"" /></p>
<p>It was consistent, it was unique, it was <em>incredibly </em>tacky and perfectly eighties. And it was awesome. It was mechanical, you could see the wheels turning. It was visceral. It was real.</p>
<p>Michael Larson’s trick was that the board movements were deterministic and predictable, so if you timed the buzzer right, you could chain extra spins forever. That’s obviously a bug one must fix, but the <em>possibility </em>of doing so, the fact that the game was mechanical and not up to the invisible and possibly fixed whim of a computer, and you could see the timing <em>matter </em>and the player having the fun, was all really great.</p>
<p>And now we still have the overall set in the background, which suddenly looks out of place, because we see this:</p>
<p><img src=""https://www.thewrap.com/wp-content/uploads/2019/06/Press-Your-Luck-1.jpg"" /></p>
<p>Pictured: Press Your Luck 2019 players being way, way too sympathetic to each other.</p>
<p>Look how warm and inviting that font is. So easy on the eyes. No! Bad design! Bad!</p>
<p>And this trying-to-be-non-tacky and thus missing the whole point board:</p>
<p><img src=""https://townsquare.media/site/696/files/2019/06/Press-Your-Luck-2019.jpg?w=1200&amp;h=0&amp;zc=1&amp;s=0&amp;a=t&amp;q=89"" /></p>
<p>Pictured: Someone who really wanted Maid For a Year. Which isn’t weird.</p>
<p>The board used to have personality. It used to look like it was tied together in the cheapest and flimsiest way possible, except on purpose. Now it’s vastly more generic, living off the echoes of an elusive age.</p>
<p>Those whammies are exactly as they used to be, and feel like a part of the right experience. They have new animations and things that happen, reflecting changing times and to mix it up, and they capture the old feel perfectly. The box that pops up after is exactly as it was, my only complaint is that if you lose a whammy (as one player did in their bonus round) it receded automatically rather than forcing the player to shove it down manually. Great job.</p>
<p>The numbers, on the other hand, feel digital, clear, warm and inviting. The coin stacks and other pictures feel fleshed out and realistic. What the hell is up with that? What kind of game do you think this is? Give us our tacky lights and hostile attitude back!</p>
<p>That’s the essence of the problem. They’re a cargo cult, following traditions they don’t understand. They’ve greatly weakened what they were doing, but retained enough of it to prevent them from doing anything else. They’re living off nostalgia without knowing what makes that nostalgia tick.</p>
<p>On a similar note, the hosting is most certainly not working for me.</p>
<p>Consider our old host, Peter Tomarken:</p>
<p><img src=""https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQM9Ms8HvIoxc3WQZzTCALquTOEjBZLs7R7uTiBW_8b8pXCYaXw"" /></p>
<p>Pictured: A man you should definitely <em>not</em> buy a used car from.</p>
<p>Peter was a perfect fit for the show, because he was clearly not quite on the level. There was something deliciously fake about his smile. There was something wonderfully deadpan about his reaction to answers and decisions. He was the operator of a highly suspect machine, and doing his best to sound like a neutral arbiter while hinting that he was up to something the whole time. He’d react to events, but in a just-aloof-enough kind of way. It was a great character.</p>
<p>Now, lets compare that to new host, the far more talented actress Elizabeth Banks, who I’ve greatly enjoyed on many other projects.</p>
<p>Alas, the producers that hired her gave her nothing to work with and did her no favors. Their thinking seemed to be something like “we need a big name celebrity because that’s who hosts game shows now, and the biggest name we could get that people like was Elizabeth Banks. So here you go.” And that was it, without any vision for why she would fit in or what persona she would use. So what happens?</p>
<p><img src=""https://i0.wp.com/www.pajiba.com/assets_c/2019/06/banks-press-your-luck-thumb-700xauto-212579.png"" /></p>
<p>Pictured: The first Google Image hit for “Elizabeth Banks Press Your Luck.”</p>
<p>All right, that freeze frame was chosen by someone who wanted her to seem like an escaped mental patient who let Wyldstyle apply her eye liner. I’ve literally never seen her look worse.</p>
<p>Lets look at a more intentional picture:</p>
<p><img src=""https://fashionmaniac.com/wp-content/uploads/2019/05/unnamed2-24-1080x1000-c.jpg"" /></p>
<p>Pictured: Me! Big star! Me! Me!</p>
<p>The real issue is only made clearer. Somewhere, someone has hosted a game show in a louder outfit. I assume. There are a lot of game shows.</p>
<p>She’s drawing attention to the host, and presenting a completely different aesthetic, instead of being part of the machinery of the game and complementing what the set is doing. None of her artistic choices hosting play into the game of Press Your Luck.</p>
<p>In fact, it was like she and those reviving the show made zero artistic choices at all. They did generically reasonable things at each step. They had her show up as Elizabeth Banks, a smart, empathetic human being who knows how to explain the game rules and hope the contestants win some money.</p>
<p>The best hosts infuse things with their personality. Every moment she showed us anything hinting at having one, the show got more interesting. I’d love to see her be more mocking of contestants who (quite correctly) buzz in early and end up giving nonsensical answers, as a clear example. I’d love to see disdainful Elizabeth who subtly acts as if every decision made is a mistake and as risky as possible. I’d love to see hyper-confident and flirty Elizabeth (e.g. that moment when she said “You can drive me any time” and I was really hoping she’d get a response that would let her coin the new catchphrase “don’t press your luck!”). I’d love to see I’m-too-big-a-star-for-this Elizabeth. I’d love to see her ham it up as acting-being-a-terrible-overactor-who-treats-every-spin-as-huge Elizabeth as long as she didn’t keep that up for too long. I’d especially like to see her be tacky and vaguely sketchy, like she’s constantly tricking you into pressing your luck (or passing) when you shouldn’t and wants to somehow cheat you out of the money. And so on.</p>
<p>I’m much less excited by her ‘let’s try and win you your dream experience, that would be good’ thing she’s doing a lot, because that’s completely generic and uninteresting, and plays completely against the show’s aesthetic.</p>
<p>Who would be some great hosts, SNL-audition-sketch style, that we could use to take the game in a unique and interesting direction?</p>
<p>Emilia Clarke</p>
<p><img src=""https://i.kinja-img.com/gawker-media/image/upload/s--9FNQ4u1u--/c_scale,f_auto,fl_progressive,q_80,w_800/lvcpxcdykghuihkjxkuf.png"" /></p>
<p>Pictured: The breaker of many things.</p>
<p>I’d have her come out and <em>scare the hell out of everyone, all the time. </em>Give us that Mother of Dragons stare that indicates the wrong answer will get you burned alive.</p>
<p>“You want to press your luck?”</p>
<p>Betty White</p>
<p><img src=""https://media.vanityfair.com/photos/5c40fee5b6173e6d16293ab5/master/w_768,c_limit/Betty-White-97th-Birthday%2520(1).jpg"" /></p>
<p>Pictured: A national treasure who is way too good and too old for this stuff.</p>
<p>No way to get her, of course, but wouldn’t this one have been amazing? This is the whole I-give-zero-anythings and you are all a bunch of idiots approach, with lots of great one liners. Everything’s better with Betty.</p>
<p>Martin Shkreli</p>
<p><img src=""https://d3i6fh83elv35t.cloudfront.net/static/2018/03/RTS1M9VV-1024x682.jpg"" /></p>
<p>Pictured: One whose goal is to be the sleaziest man on the face of the Earth</p>
<p>Ethical implications of giving this man a job aside, this seems freaking <em>perfect. </em>You’d take the whole tacky 80s used car salesman thing and crank it up to 11. When whammies arrive, he’d smile and deny he was doing it.</p>
<p>Zazie Beetz</p>
<p><img src=""https://cdn.vox-cdn.com/uploads/chorus_image/image/59840569/domino.0.jpg"" /></p>
<p>Pictured: Dominos falling into place</p>
<p>I love the idea of someone known for playing a character whose superpower is “I’m lucky” running Press Your Luck and acting like every bad roll was entirely the contestant’s fault.</p>
<p>Donald Trump</p>
<p><img src=""https://upload.wikimedia.org/wikipedia/commons/5/56/Donald_Trump_official_portrait.jpg"" /></p>
<p>Pictured: A carnival barker in a tacky outfit</p>
<p>He’s already known to love hosting game shows on network television. You know he’d revel it. He did a great job on The Apprentice. And every minute spent on set is a minute he’s not being president.</p>
<p>Charlie Sheen:</p>
<p><img src=""https://www.biography.com/.image/ar_1:1%2Cc_fill%2Ccs_srgb%2Cg_face%2Cq_auto:good%2Cw_300/MTQyMDI3NzIxMjI3MTE3NzM1/charlie-sheen_gettyimages-166403357jpg.jpg"" /></p>
<p>Pictured: A lot of cocaine</p>
<p>The theme here would be, he’s hit a few whammies and is constantly pressing his own luck, and is getting behind the idea of you doing so a little too much. Maybe a lot too much. Many similar good choices exist.</p>
<p>Winning!</p>
<p>James Holzhauer</p>
<p><img src=""https://www.latimes.com/resizer/NdrC8m89HzxjFepsIRfZvz3EZLQ=/800x0/www.trbimg.com/img-5cf6b36e/turbine/la-1559671660-1uakr7g4zl-snap-image"" /></p>
<p>Pictured: A man no longer in jeopardy.</p>
<p>Destiny calls, my friend! A professional gambler, who broke the smartest game in town by betting big at all the right times, but eventually hit his whammy just short of his goal, goes on to host a game of gambling it up. People said he was a shy nerd, if you pay attention to his interviews, you know that’s not true. The kid’s got game.</p>
<p>We could go on. Craig Ferguson. Alisha Tyler. Whitney Cummings. Michael Douglass. Peter Davidson. Oprah Winfrey. Sasha Cohen. Almost any iconic washed-up 80s star that went through hard times and is willing to play up being a little too desperate is a good idea. Or someone completely unknown. No reason this has to be a celebrity at all. Let’s go do a thing.</p>
<p>There’s also the rotating cast idea, where you bring in someone in need of a comeback, and if the players win too much money or too little (both ways are interesting), <em>you fire the host, who has pressed their luck too far. </em>Give them a rooting interest, then let someone else come in and ham it up a different way. Ideally, encourage them to say things that could get them in trouble.</p>
<p>Could the show succeed with Elizabeth Banks? Definitely, if they give her room to make it her own and develop a strong host persona that works with the game. And if they fix the big structural flaw that is the bonus game.</p>
<p>The bonus game, along with not allowing returning champions, is really, really bad, guys.</p>
<p>It’s bad in at least three important ways.</p>
<p>It makes the show too long. It kills the incentives, flow and tension in the main game. And the bonus game is horribly designed and not interesting.</p>
<p>The first point is the easiest to make. Press Your Luck, at its heart, is a simple game. There’s strategy and interesting decisions, and an arc you walk through, but at the core you’re watching random results from a board of prizes and whammies. A half-hour network show contains a good dose of that. At an hour, the whole thing is stretched quite thin. This is doubly true because the second half of it is watching one player continuously spin the wheel and not make interesting strategic decisions other than whether to stop.</p>
<p>By the end, you’ve had enough of the spinning. The show has worn out its welcome. It does this <em>much more </em>than watching two episodes in a row in syndication used to do. Press Your Luck simply isn’t an hour long show. One change I actively like was to take a question away from the first round, to make things go faster. Trying to make the show longer is a mistake.</p>
<p>The second point is an even bigger deal. Before, <em>how much you won mattered. </em>You keep your winnings, and you’re invited back on the show. But Press Your Luck is no Jeopardy, where a good player can ride a streak for weeks. Or a show like the old Sale of the Century, where players actively trade off chance of winning, and thus returning to play again, against locking in prizes. Press Your Luck is random as hell, and a top player (who didn’t pull a Michael Larson) might be 50% to win any given show. That means on average, you’ll win <em>one </em>additional time. It’s worth taking big risks to get a better payoff today. That tension made the show about a lot more than a binary win or loss, which made things much more fun and interesting.</p>
<p>When a player wins, what they just won means something. It’s a great moment, and an interestingly different moment depending on the circumstances.</p>
<p>Contrast this to the situation now, with the bonus game. The bonus game on average will give players <em>several times more money </em>than they win in the base game. It’s correct to pay <em>almost zero </em>attention to what one wins in the base game, and focus only on reaching the bonus game. The first half hour is about choosing which of three players will proceed to an almost purely random event outcome.</p>
<p>In the bonus game, you lose if you hit your fourth whammy, same as in the main game. This gave an opportunity to let <em>something </em>from the main game naturally carry to the bonus game – the whammy count. And they didn’t even do that. When you move to the bonus game, your whammies are cleared. If anything, they should do the opposite, and allow the money you win in the main game to have a real impact on the bonus game’s structure.</p>
<p>The lack of returning champions boggles my mind. It is such an unforced error. The only reason I knew Press Your Luck was returning at all was I saw an ad for it while watching Jeopardy. The only reason I was watching Jeopardy was because of James Holzhauer putting on a historic streak and show of trivia knowledge, as I’ve long ago worn out that show under normal circumstances. The old system of retirement after five shows would have been a huge lost opportunity for them. Having champions return provides an outer loop and story. There’s always someone you’re somewhat familiar with, for you to root for or against, from the last show. You can have streaks, stories, records and all that good stuff.</p>
<p>But lets say, for whatever reason, you are determined to have a bonus game. And you’re determined, again for whatever reason, to give up returning champions to do this, and make the show an hour long. Why in the world would you choose such a boring, completely random format? Do you think that ‘spin a random board a lot’ is the core product you need more of this badly?</p>
<p>And do you need to lie this much about how much someone might win?</p>
<p>Think about the game theory of the bonus game as I describe it to you, and what the player should care about when it happens. It’s all ludicrously simple.</p>
<p>You start with the board from round two of the base game, plus two customized prizes that the player would properly appreciate. A third special prize is added in round two. Should the player lose these prizes, they are returned to the board. Like in the base game, whammies take away everything, and the fourth whammy is game over.</p>
<p>Each round the player is given some number of spins. In round one this is five, later rounds it is less. The player must take these spins, as well as all additional spins the board provides. At the end of each round, the player can choose to stop, walk away and keep what they have, or they can continue. If they continue, they go to the next round, the big bucks and other prizes increase (to an eventual maximum of $100,000 for big bucks, up from a starting $10,000), and the number of spins per round goes down over time at least somewhat, from what we’ve seen so far. For later rounds, there is a spot where the player can choose to take money or lose a whammy, where losing a whammy is always the correct choice unless the player intends to walk away right now.</p>
<p>If at any time you have cash and prizes worth $500,000, the game doubles it to $1,000,000 and you win, which is a number they emphasize. Million dollar prizes are cool, you see.</p>
<p>Whammy frequency is unclear since the number on the board constantly changes between one and about five, out of eighteen total slots. And I wasn’t able to find it on the internet. Lets say for now we can assume a 15% chance. We can bump that estimate to 20% to account for potential extra spins, which must be taken.</p>
<p>If it isn’t obvious to you how this plays out, pause for a few minutes to analyze this, or until you think you understand how the bonus game works, and what matters.</p>
<p>In the early rounds, all that matters is avoiding whammies and getting rid of your spins to get to later rounds and enhance the board. The chance that you’ll be able to keep your initial funds, and leave without hitting the first whammy, are exceptionally low, because future rounds increase the jackpot by up to an order of magnitude, and fast enough that you expect to get there – plus, if you never hit a whammy, you definitely get there, unless you walk away when you shouldn’t. So it’s just a spin with three results: Whammy (boo!), No Whammy (yay!), or Extra Spin (no effect). Not so exciting.</p>
<p>In particular, the contestant hitting one of their dream prizes like their fantasy vacation doesn’t matter, because they’re almost certainly going to lose it. Tearing up in this spot, as both early contestants did and were doubtless encouraged to do, is good short-term television, but already seems lame by the end of episode two. When the player started saying purely “anything but a whammy” instead of calling for big bucks or a special prize, I was so happy seeing someone realize what mattered, and focusing on it.</p>
<p>Once the second whammy is hit, taking another round becomes risky, and it’s time to think about walking away even if there isn’t that much in the bank. Because if you hit a <em>third </em>whammy, then you’re back to nothing and continuing to spin is super risky – a round of even three spins will leave you with nothing half the time given bonus spins. You can’t really hope to sustain more than one such round before quitting. So anything substantially better than one future round’s average non-whammy take starts to look really good.</p>
<p>Therefore, working backwards, walking away with only one whammy or even no whammies don’t look so bad, if one has an unexpectedly strong current count.</p>
<p>And that’s without risk aversion!</p>
<p>The exact numbers don’t matter too much. The important thing is that the game is hugely dangerous, doesn’t have that high an expected value, and thus must be played conservatively in the only place you make a decision – whether to walk away.</p>
<p><strong>No one will ever win the million dollar prize, unless they change the rules.</strong></p>
<p>Period. Lucky players will break $100,000. Very lucky players will break $200,000. But only a <em>completely insane </em>player will then try to get to $500,000 and claim the million, instead of walking away. This isn’t a skill game where you can be good enough to take a big risk. It’s a very-low-skill game where the only skill is knowing when to quit.</p>
<p>Will someone win the cool million <em>eventually </em>if they do this show enough times? Of course. There are two ways this happens. First, someone could get super duper lucky and chain a bunch of extra spins and/or big bucks all at once, to get close enough to the $500,000 mark that it makes sense to keep going, or even get there mostly within a single round. Eventually that would happen once.</p>
<p>Alternatively, someone could be <em>totally insane </em>and go for it when they shouldn’t, and again get super lucky. Which, again, will happen eventually if things go on long enough. Indeed do many things come to pass.</p>
<p>The chance that either of those things happens faster than cancellation or a rules change? Very, very small.</p>
<p>Thus, a core ‘selling point’ of the entire revival, the big cash prize, isn’t real. It’s a mirage. A theoretical possibility.</p>
<p>The bigger issue is the simpler one. What was the point of all that? A well-designed and balanced game is now just a random click-fest, the first half of which means nothing and takes forever.</p>
<p>A bonus game with its own unique board that includes things like multipliers, and things giving decision points (e.g. <em>perhaps with landing on an exit square being how you stop playing? Or perhaps you have to buy your spins with dollars or risk running out? Or assemble the grand prize through hitting a set of squares that spell out Big Bucks?) </em>Without way too many whammies to give? That might have been interesting.</p>
<p>What we have now is tragically terrible.</p>
<p>My kid still loves the game, though. How could he not? Lights! Numbers!</p>
<p>And anything we can watch together, and both enjoy, is pretty damn good.</p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>",Zvi,zvi,Zvi,
kJrYWQgRWPSDLLonn,"Paper on qualitative types or degrees of knowledge, with examples from medicine?",paper-on-qualitative-types-or-degrees-of-knowledge-with,https://www.lesswrong.com/posts/kJrYWQgRWPSDLLonn/paper-on-qualitative-types-or-degrees-of-knowledge-with,2019-06-15T00:31:56.912Z,5,2,2,False,True,,"<html><head></head><body><p>There's a paper (or essay or something similar) I can remember reading that I've been struggling to find for some time now. It described different qualitative types of knowledge and used various examples from medicine, e.g. the difference between various kinds of trauma or emergency care where we know exactly what a problem is and can fix it (almost) exactly/completely versus something like obesity where we basically don't know (at least in the sense of commonly shared knowledge) much of anything about what exactly the problem even is.</p>
<p>Any help in uncovering the paper would be greatly appreciated!</p>
</body></html>",Kenny,kenny,Kenny,
PfceWjEqdRDvGkKZ8,Recommendation Features on LessWrong,recommendation-features-on-lesswrong-1,https://www.lesswrong.com/posts/PfceWjEqdRDvGkKZ8/recommendation-features-on-lesswrong-1,2019-06-15T00:23:18.102Z,62,19,36,True,False,,"<p>Today, we&#x27;re rolling out several new beta features on the home page, which display recommended posts to read. The first is a Continue Reading section: if you start reading a multi-post sequence, it will suggest that you read another post from that sequence. The second is a From the Archives section, which recommends highly-rated posts that you haven&#x27;t read, from all of LessWrong&#x27;s history.</p><p>To use these features, please ensure you are <a href=""https://www.lesswrong.com/login"">logged-in</a>.</p><br/><h1>Continue Reading</h1><p>Sequences are a mechanism on LessWrong for organizing collections of related posts. Anyone can create a Sequence from the <a href=""https://www.lesswrong.com/library"">library page</a>. If you write a series of posts and add them to a Sequence, they will have Previous and Next links at the top and bottom; if you create a Sequence out of posts by other authors, they will have Previous and Next links for readers who came to them via the Sequence.</p><p>When you are logged in and read a post from a Sequence, the first unread post from that sequence will be added as a recommendation in the Continue Reading section, like this:</p><span><figure><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/v1560557935/Screen_Shot_2019-06-14_at_15.47.45_pulq5h.png"" class=""draft-image center"" style=""width:100%"" /></figure></span><p>If you decide not to finish, hover over the recommendation and click the X button to dismiss the recommendation. For logged-out users, the Continue Reading section is replaced with a Core Reading section, which suggests the first posts of <a href=""https://www.lesswrong.com/rationality"">Rationality: A-Z</a>, <a href=""https://www.lesswrong.com/codex"">The Codex</a>, and <a href=""https://www.lesswrong.com/hpmor"">Harry Potter and the Methods of Rationality</a>.</p><br/><h1>From the Archives</h1><p>The home page now has a <em>From the Archives</em> section, which displays three posts randomly selected from the entire history of LessWrong. Currently, a post can appear in this section if: </p><p>(1) you&#x27;ve never read it while logged in (including on old-LessWrong), </p><p>(2) it has a score of at least 50, and </p><p>(3) it is not in the Meta section, or manually excluded from recommendation by moderators. (We manually exclude posts if they aged poorly in a way that wouldn&#x27;t be captured by votes at the time -- for example, announcements of conferences that have already happened, and reporting of studies that later failed to replicate.) </p><span><figure><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/v1560558324/Screen_Shot_2019-06-14_at_5.24.58_PM_iwk3ks.png"" class=""draft-image center"" style=""width:100%"" /></figure></span><p>Currently, if a post is eligible to appear in the From the Archives section, it will appear with probability proportional to the cube of its score.</p><br/><h1>Why These Features</h1><h2>LessWrong as a Repository for &quot;long content&quot;</h2><p><a href=""https://www.gwern.net/About#long-content"">Gwern’s about page</a> has influenced me a lot in thinking about the future of LessWrong. Gwern uses the following quote: </p><blockquote>The Internet is self destructing paper. A place where anything written is soon destroyed by rapacious competition and the only preservation is to forever copy writing from sheet to sheet faster than they can burn. If it’s worth writing, it’s worth keeping. If it can be kept, it might be worth writing…If you store your writing on a third party site like <a href=""https://en.wikipedia.org/wiki/Blogger"">Blogger</a>, <a href=""https://en.wikipedia.org/wiki/Livejournal"">Livejournal</a> or even on your own site, but in the complex format used by blog/wiki software du jour you will lose it forever as soon as hypersonic wings of Internet labor flows direct people’s energies elsewhere. For most information published on the Internet, perhaps that is not a moment too soon, but how can the muse of originality soar when immolating transience brushes every feather?</blockquote><blockquote>-- <a href=""https://en.wikipedia.org/wiki/Julian%20Assange"">Julian Assange</a> (<a href=""http://web.archive.org/web/20071020051936/http://iq.org/"">“Self destructing paper”</a>, 5 December 2006)</blockquote><p>Most of the content on the internet is designed to be read and forgotten in a very short period of time. Existing discussion platforms like Reddit and many forums even close threads automatically after a certain period of time to ensure that all discussion centers around the most recent activity on the site. </p><p>One of the goals I have with LessWrong is to be a place where we can build on each other&#x27;s ideas over the course of multiple decades, and where if you show up, you engage with the content on the site in a focused way, more similar to a textbook than a normal internet forum. And like our best textbooks, good introductions into core topics tend to stand the test of time quite well (e.g. the Feynman Lectures, which is still one of, if not the best introduction to physics even 60 years later). </p><p>The Continue Reading system is a key part of that goal, because it makes it much easier to use the site as a tool for focused study, since continuing to read the sequences you started is now one of the core actions on the site. </p><p>The recommendation system is also a key part of that goal, because it creates a way to discover content from the complete history of LessWrong, instead of just the last week, which strikes me as a necessary component to make collective intellectual progress that can span multiple decades. The best things for almost anyone to read have very likely not been written in the past week.</p><br/><h2>LessWrong as a Nudge</h2><p>Continue Reading is a nudge to encourage reading a few long things, rather than a lot of short things. Longer writing allows topics to be explored in greater depth, and also enables more explicit decisions about what to read, since making one decision per sequence is a lot less work than making one decision per post.</p><p>From the Archives is a nudge to read better posts. When we choose what to read, there is often a recency bias; the best writing of the past ten years will be better, on average, than the best writing of the past week, but active conversations will focus on the most recent things. A good information diet contains a mix of recent writing and of timeless classics; by putting From the Archives on the home page, we are saying, <em>on the margin, read more of the past</em>.</p><p>I also think that From the Archives will have a positive effect on what people write on LessWrong. There are many good ideas in LessWrong&#x27;s archives, waiting to be built upon, which haven&#x27;t received attention recently; my hope is that recommendations of older posts will inspire more good writing.</p><br/><h1>Caveat: Addictiveness</h1><p>Reading the latest posts on LessWrong is finite; you will run out of interesting-seeming recent posts, which creates a natural limit on time spent. Reading posts from the archives is effectively infinite; LessWrong&#x27;s archives are deep enough that you probably won&#x27;t ever run out of things to read. These new recommendation features therefore offer an opportunity to spend a lot of time by accident. We&#x27;d rather you make a deliberate decision about what and how much to read on LessWrong.</p><p>If you find you&#x27;re spending more time reading LessWrong&#x27;s recommended posts than you want, or expect that you would spend more time than you want to, you can turn off the Continue Reading and/or From the Archives sections by clicking the gear icon. (This requires that you be logged in to save the setting.)</p><br/><h1>These Are Beta</h1><p>These features are beta, and probably have bugs. The From the Archives post selection algorithm we&#x27;re currently using (based on post scores) seems to work okay for now, but scores are heavily affected by post visibility as well as quality, so some posts (especially imported posts) aren&#x27;t being recommended that should be, and post scores will suffer a positive-feedback effect where being recommended causes posts to be recommended more. So, we expect to rely less on the raw post score in the future, and more on other evaluation mechanisms such as asking users for retrospective evaluations of posts they&#x27;ve previously read, read-completion and clickthrough rates, vote-to-views ratios, etc. The recommendation algorithm is likely to become too complex to straightforwardly explain, though its workings will always be knowable to those willing to dive into the source code.</p>",habryka4,habryka4,habryka,
pWc6ZA6mcjXKvKwPd,Discourse Norms: Moderators Must Not Bully,discourse-norms-moderators-must-not-bully,https://www.lesswrong.com/posts/pWc6ZA6mcjXKvKwPd/discourse-norms-moderators-must-not-bully,2019-06-14T23:22:15.741Z,7,15,51,False,False,,"<p>One of the absolute worst things that can happen to a <a href=""https://www.lesswrong.com/posts/vHSrtmr3EBohcw6t8/norms-of-membership-for-voluntary-groups"">civic/public community </a>online is for moderators to be bullies or for moderators to take the side of the bullies. Once that happens, the community is at grave risk of ceasing to be a public community and instead embracing cliquism. If the moderators enforce the will of their friends rather than good discussion norms, the space is no longer going to be a space for good discussion but rather one for a certain friend group.</p><p>The most common way I&#x27;ve seen this happen goes something like this. A newcomer with locally unusual ideas joins the community. Conflict between their ideas and the more established norms arises. Because these ideas are unpopular, people push back against them, often in mean or uncharitable ways. If left unchecked, the newcomer may soon become a target of bullying and sniping. [1]</p><p>At this point, moderators need to intervene <em>in favor of the newcomer</em>, because mean and uncharitable behavior shouldn&#x27;t be allowed to stand in a civic/public space, even if it&#x27;s towards ideas that are locally unpopular. Moderation is needed to rein in the attacks and keep things civil and productive. However, in practice what often ends up happening is that the moderators intervene <em>against</em> the newcomer, enforcing the local social hierarchy rather than good discussion norms.</p><p>This is toxic to a civic/public space and, if left unchecked, drives out views or discussion styles other than those that are locally popular.</p><p>One potential antidote to this sort of behavior is holding moderators to significantly higher standards than users. If a moderator and a user are in an angry, insulting argument with one another, <em>the moderator should be removed from moderation or at minimum recuse themselves</em>. If a moderator posts insults against another user - <em>especially</em> someone who isn&#x27;t popular - they are at fault and should apologize or be removed from moderation.</p><p>Yes, this is a harsh standard. Yes, this means that being a moderator limits what you can say in some circumstances. But that&#x27;s what you need to do to keep the bullies at bay, and ultimately, being a moderator shouldn&#x27;t be a position of power but rather a position of responsibility.</p><p>Lastly, I want to point out that it&#x27;s totally fine for a space to exist for a friend group or for those who agree with certain perspectives - and for those sorts of spaces, it&#x27;s entirely fine for moderators to enforce local social norms or locally popular opinions! However, there&#x27;s a big difference between that and a civic/public space, and if you&#x27;re going for civic/public norms a higher standard is needed of moderators.</p><br/><p>[1] This obviously doesn&#x27;t apply to Nazis and the like, which should IMO be banned outright.</p><p>[2] Note that footnote [1] should not be construed as an excuse to go around calling everyone you don&#x27;t like a Nazi in hopes of getting them banned, and such rules should be clearly articulated beforehand - the intent is merely to point out that you can have a civic/public space that still prevents certain objectionable content.</p><br/><br/>",Davis_Kingsley,davis_kingsley,Davis_Kingsley,
CDYgSCFD6ioeCb5XT,SSC Sacramento Meetup,ssc-sacramento-meetup-1,https://www.lesswrong.com/events/CDYgSCFD6ioeCb5XT/ssc-sacramento-meetup-1,2019-06-14T23:08:21.396Z,8,2,3,False,False,,,jooyous,jooyous,jooyous,
bJ2haLkcGeLtTWaD5,Welcome to LessWrong!,welcome-to-lesswrong,https://www.lesswrong.com/posts/bJ2haLkcGeLtTWaD5/welcome-to-lesswrong,2019-06-14T19:42:26.128Z,480,423,74,False,False,,"<figure class=""table""><table style=""border:1px solid hsl(0, 0%, 100%)""><tbody><tr><td style=""border:1px solid hsl(0, 0%, 100%);text-align:center;width:400px""><i>﻿The road to wisdom? Well, it's plain</i><br><i>﻿and simple to express:</i><br><br><i>﻿Err</i><br><i>﻿and err</i><br><i>﻿and err again</i><br><i>﻿but <strong>less</strong></i><br><i>﻿and <strong>less</strong></i><br><i>﻿and <strong>less</strong>.</i><br><br>– Piet Hein</td></tr></tbody></table></figure><p>LessWrong is an online forum and community dedicated to improving human reasoning and decision-making. We seek to hold true beliefs and to be effective at accomplishing our goals. Each day, we aim to be less wrong about the world than the day before.</p><p><i>See also our</i><a href=""https://www.lesswrong.com/posts/LbbrnRvc9QwjJeics/new-user-s-guide-to-lesswrong""><i> New User's Guide</i></a><i>.</i></p><h2>Training Rationality</h2><p>Rationality has a number of definitions<span class=""footnote-reference"" data-footnote-reference="""" data-footnote-index=""1"" data-footnote-id=""ajdy57uko9d"" role=""doc-noteref"" id=""fnrefajdy57uko9d""><sup><a href=""#fnajdy57uko9d"">[1]</a></sup></span>&nbsp;on LessWrong, but perhaps the most canonical is that the more rational you are, the more likely your reasoning leads you to have accurate beliefs, and by extension, allows you to make decisions that most effectively advance your goals.</p><p>LessWrong contains a lot of content on this topic. How minds work (both human, artificial, and theoretical ideal), how to reason better, and how to have discussions that are productive. We're very big fans of <a href=""https://arbital.com/p/bayes_rule/?l=1zq"">Bayes Theorem</a> and other theories of normatively correct reasoning<span class=""footnote-reference"" data-footnote-reference="""" data-footnote-index=""2"" data-footnote-id=""l2ioxcywv4p"" role=""doc-noteref"" id=""fnrefl2ioxcywv4p""><sup><a href=""#fnl2ioxcywv4p"">[2]</a></sup></span>.</p><p>To get started improving your Rationality, we recommend reading the background-knowledge text of LessWrong, <a href=""https://www.lesswrong.com/rationality"">Rationality: A-Z </a>(aka ""The Sequences"") or at least <a href=""https://www.lesswrong.com/highlights"">selected highlights</a> from it. After that, looking through the Rationality section of the <a href=""https://www.lesswrong.com/concepts"">Concepts Portal</a> is a good thing to do.</p><h2>Applying Rationality</h2><p>You might value Rationality for its own sake, however, many people want to be better reasoners so they can have more accurate beliefs about topics they care about, and make better decisions.</p><p>Using LessWrong-style reasoning, contributors to LessWrong have written essays on an immense variety of topics on LessWrong, each time approaching the topic with a desire to know what's actually true (not just what's convenient or pleasant to believe), being deliberate about processing the evidence, and avoiding common pitfalls of human reason.</p><p>Check out the <a href=""https://www.lesswrong.com/concepts"">Concepts Portal</a> to find essays on topics such as <a href=""https://www.lesswrong.com/tag/ai"">artificial intelligence</a>, <a href=""https://www.lesswrong.com/tag/history"">history</a>, <a href=""https://www.lesswrong.com/tag/practice-and-philosophy-of-science"">philosophy of science</a>, <a href=""https://www.lesswrong.com/tag/philosophy-of-language"">language</a>, <a href=""https://www.lesswrong.com/tag/psychology"">psychology</a>, <a href=""https://www.lesswrong.com/tag/biology"">biology</a>, <a href=""https://www.lesswrong.com/tag/ethics-and-morality"">morality</a>, <a href=""https://www.lesswrong.com/tag/social-and-cultural-dynamics"">culture</a>, <a href=""https://www.lesswrong.com/tag/well-being"">self-care</a>, <a href=""https://www.lesswrong.com/tag/economics"">economics</a>, <a href=""https://www.lesswrong.com/tag/game-theory"">game theory</a>, <a href=""https://www.lesswrong.com/tag/productivity"">productivity</a>, <a href=""https://www.lesswrong.com/tag/art"">art</a>, <a href=""https://www.lesswrong.com/tag/nutrition"">nutrition</a>, <a href=""https://www.lesswrong.com/tag/relationships-interpersonal"">relationships</a> and hundreds of other topics broad and narrow.</p><h2>LessWrong and Artificial Intelligence</h2><p>For several reasons, LessWrong is a website and community with a strong interest in AI and specifically causing powerful AI systems to be safe and beneficial.</p><ul><li>AI is a field concerned with how minds and intelligence works, overlapping a lot with rationality.</li><li>Historically, LessWrong was seeded by the writings of Eliezer Yudkowsky, an artificial intelligence researcher.</li><li>Many members of the LessWrong community are heavily motivated by trying to improve the world as much as possible, and these people were convinced many years ago that AI was a very big deal for the future of humanity. Since then LessWrong has hosted a lot of discussion of AI Alignment/AI Safety, and that's only accelerated recently with further AI capabilities developments.<ul><li>LessWrong is also integrated with the <a href=""https://www.alignmentforum.org/about"">Alignment Forum</a></li><li>The LessWrong team who maintain and develop the site are predominantly motivated by trying to cause powerful AI outcomes to be good.</li></ul></li></ul><p>If you want to see more or less AI content, you can adjust your Frontpage Tag Filters according to taste<span class=""footnote-reference"" data-footnote-reference="""" data-footnote-index=""3"" data-footnote-id=""q85givw8h9"" role=""doc-noteref"" id=""fnrefq85givw8h9""><sup><a href=""#fnq85givw8h9"">[3]</a></sup></span>.</p><h2>Getting Started on LessWrong</h2><p>The <a href=""https://www.lesswrong.com/posts/LbbrnRvc9QwjJeics/new-user-s-guide-to-lesswrong"">New User's Guide</a> is a great place to start.</p><p>The core background text of LessWrong is the collection of essays, <a href=""https://www.lesswrong.com/rationality"">Rationality: A-Z</a> (aka ""The Sequences""). Reading these will help you understand the mindset and philosophy that defines the site. Those looking for a quick introduction can start with <a href=""https://www.lesswrong.com/highlights"">The Sequences Highlights</a></p><p>Other top writings include <a href=""https://www.lesswrong.com/codex"">The Codex</a> (writings by Scott Alexander) and <a href=""https://www.lesswrong.com/hpmor"">Harry Potter &amp; The Methods of Rationality</a>. Also see the <a href=""https://www.lesswrong.com/library"">Library Page</a> for many curated collections of posts and the <a href=""https://www.lesswrong.com/concepts"">Concepts Portal.</a></p><p>Also, feel free to introduce yourself in the monthly <a href=""https://www.lesswrong.com/tag/open-threads?sortedBy=new"">open and welcome thread</a>!</p><p>Lastly, we do recommend that new contributors (posters or commenters) take time to familiarize themselves with the sites norms and culture to maximize the chances that your contributions are well-received.</p><p>Thanks for your interest!</p><p>- The LW Team</p><p>&nbsp;</p><h2>Related Pages</h2><ul><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq""><strong>LessWrong FAQ</strong></a></li><li><a href=""https://www.lesswrong.com/posts/S69ogAGXcc9EQjpcZ/a-brief-history-of-lesswrong""><strong>A Brief History of LessWrong</strong></a></li><li><a href=""https://www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team-page-under-construction""><strong>Team</strong></a></li><li><a href=""https://www.lesswrong.com/concepts""><strong>LessWrong Concepts</strong></a></li></ul><p>&nbsp;</p><ol class=""footnote-section footnotes"" data-footnote-section="""" role=""doc-endnotes""><li class=""footnote-item"" data-footnote-item="""" data-footnote-index=""1"" data-footnote-id=""ajdy57uko9d"" role=""doc-endnote"" id=""fnajdy57uko9d""><span class=""footnote-back-link"" data-footnote-back-link="""" data-footnote-id=""ajdy57uko9d""><sup><strong><a href=""#fnrefajdy57uko9d"">^</a></strong></sup></span><div class=""footnote-content"" data-footnote-content=""""><p>Definitions of Rationality as used on LessWrong include:</p><p>- Rationality is thinking in ways that systematically arrive at truth.</p><p>- Rationality is thinking in ways that cause you to systematically achieve your goals.</p><p>- Rationality is trying to do better on purpose.</p><p>- Rationality is reasoning well even in the face of massive uncertainty.</p><p>- Rationality is making good decisions even when it’s hard.</p><p>-Rationality is being self-aware, understanding how your own mind works, and applying this knowledge to thinking better.</p></div></li><li class=""footnote-item"" data-footnote-item="""" data-footnote-index=""2"" data-footnote-id=""l2ioxcywv4p"" role=""doc-endnote"" id=""fnl2ioxcywv4p""><span class=""footnote-back-link"" data-footnote-back-link="""" data-footnote-id=""l2ioxcywv4p""><sup><strong><a href=""#fnrefl2ioxcywv4p"">^</a></strong></sup></span><div class=""footnote-content"" data-footnote-content=""""><p>There are in fact laws of thought no less ironclad than the law of physics [<a href=""https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition"">source</a>].</p></div></li><li class=""footnote-item"" data-footnote-item="""" data-footnote-index=""3"" data-footnote-id=""q85givw8h9"" role=""doc-endnote"" id=""fnq85givw8h9""><span class=""footnote-back-link"" data-footnote-back-link="""" data-footnote-id=""q85givw8h9""><sup><strong><a href=""#fnrefq85givw8h9"">^</a></strong></sup></span><div class=""footnote-content"" data-footnote-content=""""><p>Hover your mouse over the tags to be able to adjust their weighting in your Latest Posts feed.</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bJ2haLkcGeLtTWaD5/wd1btfriv8emb8il1xbg""></figure></div></li></ol>",Ruby,ruby,Ruby,
2rWKkWuPrgTMpLRbp,LessWrong FAQ,lesswrong-faq,https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq,2019-06-14T19:03:58.782Z,89,56,59,False,False,,"<p><i>This is a new FAQ written LessWrong 2.0. This is the first version and I apologize if it is a little rough. Please comment or message with further questions, typos, things that are unclear, etc.</i></p><p><i>The </i><a href=""https://wiki.lesswrong.com/wiki/FAQ""><i>old FAQ </i></a><i>on the LessWrong Wiki still contains much excellent information, however it has not been kept up to date.</i></p><p><strong>Advice!</strong> We suggest you navigate this guide with the help on the table of contents (ToC) in the left sidebar. You will need to scroll to see all of it. Mobile users need to click the menu icon in the top left.</p><p>The major sections of this FAQ are:</p><ul><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#LessWrong_Meta"">LessWrong Meta</a></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Getting_Started"">Getting Started</a></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Reading_Content"">Reading Content</a></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Posting___Commenting"">Posting &amp; Commenting</a><ul><li>special mention: <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#The_Editor"">The Editor</a></li></ul></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Karma___Voting"">Karma &amp; Voting</a></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Notifications___Subscriptions"">Notifications &amp; Subscriptions</a></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Messaging"">Messaging</a></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Questions"">Questions</a></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Community_Events_Page"">Community Events Page</a></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Moderation"">Moderation</a></li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_is_LessWrong_s_Privacy_Policy_and_Terms_of_Use_"">Privacy Policy &amp; Terms of Use</a></li></ul><h1>About LessWrong</h1><h2>What is LessWrong?</h2><p>LessWrong is a community dedicated to improving our reasoning and decision-making. We seek to hold true beliefs and to be effective at accomplishing our goals. More generally, we want to develop and practice the art of human rationality.</p><p><strong>To that end, LessWrong is a place to 1) develop and train rationality, and 2) apply one’s rationality to real-world problems.</strong></p><p>LessWrong serves these purposes with its <a href=""https://www.lesswrong.com/library"">library of rationality writings</a>, <a href=""https://www.lesswrong.com/allPosts"">community discussion forum</a>, <a href=""https://www.lesswrong.com/questions"">open questions research platform</a>, and <a href=""https://www.lesswrong.com/community"">community page for in-person events</a>.</p><p>See also: <a href=""https://www.lesswrong.com/posts/bJ2haLkcGeLtTWaD5/welcome-to-lesswrong"">Welcome to LessWrong!</a></p><h2>What is <i>rationality?</i></h2><p>Rationality is a term which can have different meanings to different people. You might already associate with a few things. On LessWrong, we mean something like the following:</p><ul><li>Rationality is thinking in ways which systematically arrive at truth.</li><li>Rationality is thinking in ways which cause you to systematically achieve your goals.</li><li>Rationality is trying to do better on purpose.</li><li>Rationality is reasoning well even in the face of massive uncertainty.</li><li>Rationality is making good decisions even when it’s hard.</li><li>Rationality is being self-aware, understanding how your own mind works, and applying this knowledge to thinking better.</li></ul><p>See also: <a href=""https://www.lesswrong.com/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality"">What Do We Mean By ""Rationality""?</a>, <a href=""https://intelligenceexplosion.com/2011/why-spock-is-not-rational/"">Why Spock is Not Rational</a>, <a href=""https://www.lesswrong.com/posts/gX8fcAwk3HGkFyJgk/what-are-the-open-problems-in-human-rationality#MPcuH6dWEDi8TDo96"">What are the open problems in Human Rationality?</a></p><h2>What is the history of LessWrong?</h2><p>In 2006, Eliezer Yudkowsky and others began writing on Overcoming Bias, a group blog with the general theme of how to move one’s beliefs closer to reality despite biases such as overconfidence and wishful thinking. In 2009, Eliezer moved to a new community blog, <i>LessWrong</i>. Eliezer seeded LessWrong with a series of daily blog posts which became known as <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Rationality__AI_to_Zombies__aka__the_Sequences___""><i>The Sequences</i></a><i>. </i>These writings attracted a large community of readers and writers interested in the art of human rationality.</p><p>See also: <a href=""https://www.lesswrong.com/posts/S69ogAGXcc9EQjpcZ/a-brief-history-of-lesswrong"">A Brief History of LessWrong</a></p><h2>What makes LessWrong different from other discussion forums?</h2><p>A combination of traits makes LessWrong distinct among online communities.</p><ol><li>We have unusually high standards of discourse. We emphasize <a href=""https://www.lesswrong.com/posts/3nZMgRTfFEfHp34Gb/the-meditation-on-curiosity"">curiosity</a>, <a href=""https://www.lesswrong.com/posts/YshRbqZHYFoEMqFAu/why-truth-and"">truth-seeking</a>, <a href=""https://www.lesswrong.com/s/GSqFqc646rsRd2oyz"">critical self-reflection</a>, <a href=""https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement"">intellectual collaboration</a>, and the long attention spans required to actually think through <a href=""https://www.lesswrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances"">complicated ideas</a>.</li><li>We are open to unusual ideas and are willing to doubt conventional wisdom. Curiosity and truth-seeking require a willingness to sometimes consider positions which are strange by ordinary standards, and in some cases, these positions will turn out to be <a href=""https://www.lesswrong.com/posts/WQFioaudEH8R7fyhm/local-validity-as-a-key-to-sanity-and-civilization"">credible</a>. As a result of this openness, some unconventional ideas are prevalent on LessWrong and many more are entertained.</li><li>We make intellectual progress by building on a large number of communally-<a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq#What_are_LessWrong_s_core_readings_"">shared background ideas and concepts</a>.</li></ol><h2>Why the name? It is a bit odd . . .</h2><p>I (Ruby) personally wasn’t there when the name was chosen so I’m not certain of the historical thought process, but I interpret the name “LessWrong” as expressing two important points:</p><ol><li>A humble recognition that no human is ever going to attain perfectly true beliefs and be right about everything. We should always believe that some of our beliefs are mistaken, we just don’t know which ones.</li><li>A bold recognition that notwithstanding the impossibility of being perfectly right, there is still the possibility of being <i>less wrong.</i> Everyone believes false things, but some believe <a href=""https://www.lesswrong.com/posts/dLJv2CoRCgeC2mPgj/the-fallacy-of-gray"">a lot fewer</a> wrong things than others.</li></ol><p>And so the aspiration of LessWrong is that by dedicating ourselves to learning how to think in ways which more systematically lead to truth (what we succinctly call <a href=""https://www.lesswrong.com/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality""><i>rationality</i></a>), we can meaningfully reduce our mistaken notions and have far more accurate models of reality.</p><h2>Who is this Eliezer guy I keep hearing about?</h2><p><a href=""https://www.lesswrong.com/users/eliezer_yudkowsky"">Eliezer Yudkowsky</a> was the original founder of LessWrong back in 2009. His writings on rationality attracted to the site a large number of people enthusiastic about learning to think better. Eliezer’s best-known works are <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq#Rationality__AI_to_Zombies__aka__the_Sequences___"">The Sequences</a>, (later renamed <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq#Rationality__AI_to_Zombies__aka__the_Sequences___"">Rationality: From AI to Zombies</a>) and <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq#Harry_Potter_and_Methods_of_Rationality__HPMOR_"">Harry Potter and the Methods of Rationality</a>. These texts are part of LessWrong’s philosophical foundation, and so unsurprisingly, you will see mentions of Eliezer not infrequently.</p><h2>How does LessWrong make money?</h2><p>We don’t. The LessWrong organization is a nonprofit funded by donations.</p><p>This hopefully has the benefit of reducing our incentives to optimize for clicks and pageviews. Instead, we can focus on our <a href=""https://www.lesswrong.com/posts/bJ2haLkcGeLtTWaD5/welcome-to-lesswrong-1"">stated purpose</a>.</p><h2>Can I see the source code for the site?</h2><p>Yes you can! We are open source, and you can find the code <a href=""https://github.com/ForumMagnum/ForumMagnum"">on Github here</a>.</p><h2>I have feedback, bug reports, or questions not answered in this FAQ. What should I do?</h2><p>You have several options for contacting the LessWrong team:</p><ol><li>Message the LessWrong team via Intercom (available in the bottom right). Ensure you don't have <i>Hide Intercom </i>set in your <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#How_do_I_edit_my_account_settings__What_can_I_do_"">account settings</a>. <ol><li>(Note: Intercom isn't displayed on mobile because it takes up too much space, so you'll need to use laptop or email us if you're on mobile)</li></ol></li><li>Email us team@lesswrong.com</li><li>Send a private message to a member of the LessWrong team (see these on the <a href=""https://www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team-page-under-construction"">team page</a>)</li><li>Open an issue on the <a href=""https://github.com/LessWrong2/Lesswrong2"">LessWrong Github repository</a>.</li><li><a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#How_do_I_ask_questions_"">Ask a question</a></li><li>For complaints and concerns regarding the LessWrong team, you can message <a href=""https://www.lesswrong.com/users/vaniver"">Vaniver</a>.</li></ol><p>We also have the <a href=""https://www.lesswrong.com/contact"">lesswrong.com/contact</a> page, which mostly repeats these options, in case that is reasier to remember in the future.</p><h2>Oh no! I think I lost my post/draft/sanity! What can I do?</h2><p>LessWrong stores revisions of posts as you’re drafting them. If you think you have lost content, please message the team via Intercom and we’ll see what we can do.</p><h1>Getting Started</h1><h2>I’m new. Where do I start?</h2><p><u>We encourage new users to read for a while before diving into discussions or making their own posts. This is helpful for new users to understand the site’s culture and background.</u></p><ul><li>Our <a href=""https://www.lesswrong.com/posts/bJ2haLkcGeLtTWaD5/welcome-to-lesswrong"">welcome page</a> offers a high-level description of LessWrong and includes a list of sample posts. It is a great way to get a feel for what LessWrong is like.</li><li>For new members who want to get up to speed, we direct you towards our <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_are_LessWrong_s_core_readings_"">core readings</a> which can be found on the <a href=""https://www.lesswrong.com/library"">Library page</a> and are <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_are_LessWrong_s_core_readings_"">described elsewhere</a> in this FAQ.</li><li>At the same time, feel free to browse more recent content. <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_are_all_the_ways_to_access_content_on_LessWrong_"">This answer</a> describes all the way you can locate content on LessWrong.</li><li>Unlike other places on the Internet, it is often worthwhile to read the comment sections on posts. Our commenting guidelines state that is preferable:<ul><li>Aim to explain, not persuade.</li><li>Present your own perspective rather than state group consensus or invoking authorities.</li><li>Get curious. If you disagree with some, try to figure out what they’re thinking. What’s their model? Don’t just assume they’re dumb or evil.</li></ul></li></ul><p>If you’re very new and you begin posting or commenting, you might find that you are quickly downvoted. This doesn’t mean you’re bad or unwelcome! But you are probably violating a norm or ignoring expected knowledge on the site. We suggest you read up a bit more before trying again later.</p><h2>What's a good and fast way to learn about how the website works?</h2><p>LessWrong extensively uses tool-tips and content previews to help users understand how the site works and see what content is even before they click.</p><p>We encourage you to mouse over most elements of the site to see what pops up. You will find:</p><ul><li>Items in the left sidebar have tool-tips.</li><li>Hovering over post titles displays an excerpt, reading time, and other meta info.</li><li>Hovering over usernames displays karma, join date, number of posts and comments, and a <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#How_do_I_edit_my_account_settings__What_can_I_do_"">bio if the user has set one</a>.</li><li>Hovering over karma scores displays the number of votes (in our <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Karma___Voting"">karma system</a>, karma does not usually equal the number of votes).</li></ul><h2>How do I create an account? (And why should I?)</h2><p>Although not required to use the LessWrong website; we recommend creating an account so that you can:</p><ul><li>Subscribe to users and different classes of posts.</li><li>Save your user settings</li><li>Vote and comment on posts.</li><li>Store your reading history, enabling tailored recommendations and potentially new features such as viewing your reading history and creating custom reading lists.</li></ul><p>Creating an account takes under 30 seconds. Click <i>login </i>in the top right and enter a username, email, and password.</p><p><br>&nbsp;</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/ncu0urtbdfjzwdoxvwoz""></figure><p><br>&nbsp;</p><p>Once you have created an account, feel free to introduce yourself in the latest Open/Welcome thread. Let others know how you found LessWrong, your background, and what you’re hoping for from LessWrong. This allows existing members to point you in the direction of material which you might especially like.</p><h2>How do I <i>Ask Questions/Make Posts/Go to My Profile/Private Message/Log Out?</i></h2><p>For logged-in users, you can access all these options via the drop-down menu accessible by clicking your username.</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/p5qkts4blzkq3bm6bnta""></figure><p>The star to the right of your username is the <a href=""https://www.lesswrong.com/posts/yb3Js7ArenCiiHxKF/karma-change-notifications""><i>karma notifier</i></a><i> </i>(star icon) and button for notifications panel (bell icon).</p><h2>How do I edit my account settings? What can I do?</h2><p>By clicking on your username and clicking <i>Edit Account, </i>you access your account settings. There you can:</p><ul><li>Set a bio for your account to let other LessWrong members know about you. If you set one, it will show up when they mouse-over your username.</li><li>Hide or show Intercom (messaging service with the LessWrong team members).</li><li>Activate the markdown editor.</li><li>Toggle comment collapse settings.</li><li><strong>Opt into beta features (new)</strong></li><li>Adjust settings for notifications of responses to your posts and comments</li><li>Adjust settings for the karma notifier.</li><li>Unsubscribe from your email subscriptions.</li></ul><h1>Reading Content</h1><h2>What are all the ways to access content on LessWrong?</h2><p>Ah, there are many ways!</p><h3>Homepage</h3><p>LessWrong’s homepage has the following content sections:</p><ul><li>Core Reading (shown only to logged-out users)</li><li>Curated</li><li>Latest Posts</li><li>Recent Discussion</li></ul><p><strong>Core Readings</strong></p><p>The core readings section provides links to texts which describe the intellectual foundations of LessWrong. They are <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_are_LessWrong_s_core_readings_"">described here</a>.</p><p><strong>Curated</strong></p><p>Each week, LessWrong’s moderation team selects on average three posts which seem to us to be especially well-written, insightful, instructive, or otherwise important. These are tagged as <i>curated posts </i>and appear with a star icon next to the title.</p><p>Curated posts are emailed out to the mailing list of people who have signed up for them. As of January 2025, this mailing list has over 30,000 subscribers, and typically over half of them open the email.</p><p>The three most recently curated posts appear in the <i>Curated</i> section. You can view more Curated posts by clicking <i>View All Curated Posts </i>or selecting the Curated filter on the AllPosts page.</p><p>Beneath the Curated section is a button to subscribe via email or RSS to curated posts (~3/week).</p><p>All Curated posts are selected from the Frontpage posts (i.e. they are basically never organizational announcements or news that will be unimportant in a few years).</p><p><strong>Latest Posts</strong></p><p>The <i>Latest Posts</i> section displays all* recent posts to LessWrong. These sorted <i>magically</i>** to balance between recency and quality (as indicated by karma score), i.e. more upvoted posts remain higher up in the Latest Posts section for longer.</p><p>*By default, only <i>Frontpage posts</i> are displayed in the Latest Posts section. To enable Personal blogposts to appear as well, check the checkbox beneath the section. See more in <a href=""https://www.lesswrong.com/posts/5conQhfa4rgb4SaWx/site-guide-personal-blogposts-vs-frontpage-posts"">What’s the difference between Personal Blogposts and Frontpage Posts?</a></p><p><i>**</i>LessWrong uses the following formula to rank posts in <i>Latest Posts</i>:</p><span class=""math-tex""><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""\frac{karmaScore}{(ageInHours + 2)^{1.15}}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mfrac""><span class=""mjx-box MJXc-stacked"" style=""width: 9.437em; padding: 0px 0.12em;""><span class=""mjx-numerator"" style=""width: 9.437em; top: -1.407em;""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">k</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">m</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">c</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span><span class=""mjx-denominator"" style=""width: 9.437em; bottom: -1.093em;""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;"">I</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.409em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1.15</span></span></span></span></span></span></span></span><span style=""border-bottom: 1.3px solid; top: -0.296em; width: 9.437em;"" class=""mjx-line""></span></span><span style=""height: 2.499em; vertical-align: -1.093em;"" class=""mjx-vsize""></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span></span><p>This is same the formula as used by Hacker News. You can read about it <a href=""https://medium.com/hacking-and-gonzo/how-hacker-news-ranking-algorithm-works-1d9b0cf2c08d"">here</a>.</p><p><strong>Recent Discussion</strong></p><p>This section is a purely-time based feed of the most recent comment activity happening on posts. Currently, all posts (both Personal blogposts and Frontpage) are shown. Discussion is grouped by post but restricted to only showing a few comments per post.</p><h3>All Posts Page (aka Archive)</h3><p>Whereas the homepage displays posts ordered with a magical algorithm, the <i>All Posts</i> page gives you complete control over which posts are included and how they ordered.</p><p>The All Posts page can be accessed via the left sidebar and drop-down menu (desktop); buttons on the bottom of the screen (mobile); or directly via <a href=""http://www.lesswrong.com/allPosts"">www.lesswrong.com/allPosts</a></p><p>The gear icon allows you to select which posts:</p><ul><li>All Posts (absolutely everything)</li><li>Frontpage (pages given <a href=""http://What’s the difference between Frontpage posts and Personal blogposts?""><i>Frontpage status</i></a> by the moderation team)</li><li>Curated (pages given <i>Curated status</i> by the moderation team)</li><li>Questions (from our <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Questions"">Open Questions</a> platform)</li><li>Events (from the <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Community_Events_Page"">Community Events Page</a>)</li><li>Meta (deprecated category containing posts about the LessWrong website and similar)</li></ul><p>These can then be sorted by: Daily, Magic, has Recent Comments, New, Old, and Top.</p><h3>The Library</h3><p>The Library page is accessible from the left sidebar/drop-down menu (desktop) or the buttons at the bottom of the screen (mobile). The Library page contains sequences (ordered sets of posts) and collections (ordered sets of sequences) of LessWrong’s best writings. These are split into <i>Core Readings, Curated Sequences, and Community Sequences.</i></p><p>LessWrong’s developers have put effort into making the reading experience in The Library as convenient and enjoyable as possible.</p><p><strong>Core Readings</strong></p><p>These are <a href=""https://www.lesswrong.com/rationality""><i>Rationality: From AI to Zombies</i></a> (formerly <i>The Sequences</i>), <a href=""https://www.lesswrong.com/codex""><i>The Codex</i></a><i>, </i>and <a href=""https://www.lesswrong.com/hpmor""><i>Harry Potter and the Methods of Rationality</i></a>. They are described in <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_are_LessWrong_s_core_readings_"">What are LessWrong’s core readings?</a></p><p><strong>Curated Sequences</strong></p><p>Similar Curated posts, <i>Curated sequences</i> are sets of posts which LessWrong’s moderation team think are especially valuable and ought to be included in LessWrong’s intellectual canon.</p><p>Top curated sequences include:</p><ul><li><a href=""https://www.lesswrong.com/s/ZNNi2uNx9E6iwGKKG"">Introduction to Game Theory</a></li><li><a href=""https://www.lesswrong.com/s/pC6DYFLPMTCbEwH8W"">Babble and Prune</a></li><li><a href=""https://www.lesswrong.com/s/xmDeR64CivZiTAcLx"">Community and Cooperation</a></li></ul><p><strong>Community Sequences</strong></p><p>Any LessWrong site member, not just moderators, can create post sequences. These appear in the <i>Community Sequences</i> section.</p><p>Standout mentions include:</p><ul><li><a href=""https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm"">Concepts in formal epistemology</a></li><li><a href=""https://www.lesswrong.com/s/zucjLBpQ9S9eWPWGu"">Share models, not beliefs</a></li><li><a href=""https://www.lesswrong.com/s/n44Fqx5W4BhMugCMS"">Keith Stanovich: What Intelligence Tests Miss</a></li></ul><p>Sequences have qualitative benefits over posts in that an author can build towards a larger point or explain more nuanced concepts than is possible in single (even quite long) blog posts.</p><p>You can also create your own sequence on <a href=""https://www.lesswrong.com/sequencesnew"">this page</a>.</p><h3>User Page</h3><p>Lastly, you can access a User’s posts and comments directly from their user page.</p><p>Note that you have the same options available for sorting and filtering a user’s posts as you do on the <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1"">All Posts page</a>.</p><h2>What’s the difference between Frontpage posts and Personal blogposts?</h2><p>Although LessWrong’s focus is on the development and application of rationality, we invite posts on almost any topic. To ensure that the default experience is still one centered on rationality, LessWrong classifies posts into <i>Frontpage posts</i> and <i>Personal blogposts.</i></p><p>Frontpage posts must meet the criteria of being broadly relevant to LessWrong’s main interests; timeless, i.e. not about recent events; and are attempts to explain not persuade. In contrast, Personal blogposts can be on any topic of interest to the author including divisive topics (which we generally keep off the frontpage), discussions about the community, and meta posts about LessWrong itself.</p><p>Frontpage posts have visibility by default. Personal blogposts can be viewed by: i) checking the “show Personal blogposts” checkbox on the homepage, ii) via the All Posts page if “All Posts” filter option is selected, iii) via a user’s profile page, iv) in the Recent Discussion section of the homepage.</p><p>See also: <a href=""https://www.lesswrong.com/posts/5conQhfa4rgb4SaWx/site-guide-personal-blogposts-vs-frontpage-posts""><i>Site Guide: Personal Blogposts vs Frontpage Posts</i></a></p><h2>What are Curated posts?</h2><p>Each week, LessWrong’s moderation team selects on average three posts which seem to us to be especially well-written, insightful, instructive, or otherwise important. These are tagged as <i>curated posts </i>and appear with a star icon next to the title.</p><p>The three most recently curated posts appear in the <i>Curated</i> section. You can view more Curated posts by clicking <i>View All Curated Posts </i>or selecting the Curated filter on the AllPosts page.</p><p>Beneath the Curated section is a button to subscribe via email or RSS to curated posts (~3/week).</p><p>All Curated posts will also be Frontpage posts.</p><h2>What are LessWrong’s core readings?</h2><p>The following texts lay the philosophical foundations of the LessWrong website and community. They are widely regarded as excellent, and, even when the ideas are not universally agreed upon, they are still commonly assumed background knowledge in the community.</p><p><strong>Rationality: AI to Zombies (aka “the Sequences”)</strong></p><p>In 2006, Eliezer began posting on a precursor to LessWrong, the shared blog, <i>Overcoming Bias</i> before the current site was launched in 2019<i>. </i>He posted nearly daily for several years and those writings became known as <i>the Sequences.</i> Later they were edited into a book, <i>Rationality: A-Z (or RAZ).</i></p><p>Rationality: A-Z is a deep exploration of how human minds can come to understand the world they exist in - and all the reasons they so commonly fail to do. The comprehensive work:</p><ul><li>lays foundational conceptions of <a href=""https://www.lesswrong.com/s/7gRSERQZbqTuLX5re"">belief</a>, <a href=""https://www.lesswrong.com/s/zpCiuR4T343j9WkcK"">evidence</a>, and <a href=""https://www.lesswrong.com/s/5uZQHpecjn7955faL"">understanding</a></li><li>reviews the <a href=""https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM"">systematic biases</a> and <a href=""https://www.lesswrong.com/s/FrqfoG3LJeCZs96Ym"">common excuses</a> which cause us to believe false things</li><li>offers guidance on <a href=""https://www.lesswrong.com/s/GSqFqc646rsRd2oyz"">how to change our minds</a> and <a href=""https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb"">how to use language effectively</a> to describe the world</li><li>depicts the <a href=""https://www.lesswrong.com/posts/8GhSZzsQmusCN9is7/minds-an-introduction"">nature of human psychology</a> with reference to how <a href=""https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8"">evolution</a> produced us</li><li>clarifies the kind of <a href=""https://www.lesswrong.com/rationality/ends-an-introduction"">morality</a> humans like us can have in a <a href=""https://www.lesswrong.com/s/p3TndjYbdYaiWwm9x"">reducible</a>, <a href=""https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo"">physical</a> world</li><li>and repeatedly reminds us that <a href=""https://www.lesswrong.com/s/5uZQHpecjn7955faL"">confusion and mystery exist only in our minds</a>.</li></ul><p>Eliezer covers these topics and others through allegory, anecdote, and scientific theory. He demonstrates the ideas by applying them to debates in <a href=""https://www.lesswrong.com/s/3HyeNiEpvbQQaqeoH"">artificial intelligence</a> (AI), <a href=""https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ"">physics</a>, <a href=""https://www.lesswrong.com/s/9bvAELWc8y2gYjRav"">metaethics</a>, and consciousness.</p><p>To start reading R:A-Z, visit <a href=""http://www.lesswrong.com/rationality""><i>www.lesswrong.com/rationality</i></a><i> or visit </i><a href=""https://www.amazon.com/Rationality-AI-Zombies-Eliezer-Yudkowsky-ebook/dp/B00ULP6EW2""><i>Amazon</i></a><i> to purchase the e-book or audiobook.</i></p><p><strong>The Codex</strong></p><p>Scott Alexander’s, one of LessWrong’s earliest and most prolific contributors, wrote many essays on good reasoning, learning from the institution of science, and different ways society has and could be organized. These have been organized into <a href=""https://www.lesswrong.com/codex""><i>the Codex</i></a>. Scott’s sequences include:</p><ul><li><a href=""https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi"">Argument and Analysis</a></li><li><a href=""https://www.lesswrong.com/s/BQBqPowfxjvoee8jw"">Studies and Statistics</a></li><li><a href=""https://www.lesswrong.com/s/xmDeR64CivZiTAcLx"">Community and Cooperation</a></li></ul><p>His exemplary essays include:</p><ul><li><a href=""https://www.lesswrong.com/posts/fzeoYhKoYPR3tDYFT/beware-isolated-demands-for-rigor"">Beware Isolated Demands for Rigor</a></li><li><a href=""https://www.lesswrong.com/posts/yCWPkLi8wJvewPbEp/the-noncentral-fallacy-the-worst-argument-in-the-world"">The noncentral fallacy - the worst argument in the world?</a></li><li><a href=""https://www.lesswrong.com/s/xmDeR64CivZiTAcLx/p/qajfiXo5qRThZQG7s"">Guided By The Beauty of Our Weapons</a></li><li><a href=""https://www.lesswrong.com/s/rNuPrZvabXe2MaZv8/p/GLMFmFvXGyAcG25ni"">I Can Tolerate Anything Except the Outgroup</a></li><li><a href=""https://www.lesswrong.com/s/xmDeR64CivZiTAcLx/p/TxcRbCYHaeL59aY7E"">Meditations on Moloch</a></li></ul><p><strong>Harry Potter and Methods of Rationality (HPMOR)</strong></p><p>A side project of Eliezer’s grew to be one of the most highly rated Harry Potter fanfictions of all time <i>and </i>an excellent primer on rationality. Eliezer imagined an alternate-universe Harry Potter who grew up with loving adopted parents, one of them an Oxford scientist. In this version, Harry enters the wizarding world with Enlightenment ideals and the experimental spirit.</p><p>We recommend HPMOR to interested in an introduction to rationality via a highly entertaining narrative. Click here to <a href=""https://www.lesswrong.com/s/PtgH6ALi5CoJnPmGS"">read HPMOR</a> through LessWrong or try the <a href=""http://www.hpmorpodcast.com/?page_id=56"">audiobook</a>.</p><h2>What’s with all the AI and math posts?</h2><p>For both historical reasons and because these topics are relevant to human rationality, many members of the LessWrong community are interested in AI, decision-theory, math, and related topics.</p><ul><li>Historically: LessWrong’s founder and author of its foundational works, <a href=""https://www.lesswrong.com/users/eliezer_yudkowsky"">Eliezer Yudkowsky</a>, is a co-founder of the Machine Intelligence Research Institute and major propopent for <a href=""https://intelligence.org/why-ai-safety/"">AI safety</a>. His writings on LessWrong attracted many people who were interested in both rationality and AI/AI safety, causing these to be ongoing overlap between LessWrong an AI safety communities.</li><li>Relevancy: Artificial intelligence is very much the study of intelligence and how “minds” work. Even if you are more interested in how human minds work and in improving your human rationality, there is much to learn from thinking generally about how intelligence works (for humans or non-humans). In particular, the fields of AI often brings technical precision and rigor to thinking to the gnarly, complicated topics of intelligence and optimal decision-making.<ul><li>Because of this relevance, many writings about human rationality on LessWrong (from Eliezer and others) make reference to concepts from AI and formal decision-theory.</li></ul></li></ul><p>See also: <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_is_the_AI_Alignment_Forum__AIAF__and_what_does_it_have_to_do_with_LessWrong_"">What is the AI Alignment Forum (AIAF) and what does it have to do with LessWrong?</a></p><h2>What is the AI Alignment Forum (AIAF) and what does it have to do with LessWrong?</h2><p>The <a href=""https://www.alignmentforum.org/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq"">AI Alignment Forum</a> is an online hub for AI Safety (aka AI alignment) researchers to discuss topics in the field. The AI Alignment forum is another project of the LessWrong team’s and resultantly shares some infrastructure with LessWrong proper, i.e. shared user accounts.</p><p>Because of the overlaps between the LessWrong and AI Safety communiites and relevance of AI content to rationality, posts made to the AI Alignment forum are automatically cross-posted to LessWrong.</p><ul><li>These posts will have the AIAF symbol (Omega/ 𝛀) shown next to the title and contain a warning that the content may especially technical.</li></ul><p>I (Ruby) am advocating strongly for there to be an easy way to filter these out for users who are not interested in AIAF content.</p><h2>What is that Omega symbol I see on some posts? Oh, it’s AIAF karma.</h2><p>Posts and comments which been cross-posted from the Alignment Forum will display their <i>Alignment Forum karma </i>(symbol: Omega/ 𝛀). When users with the ability to vote on Alignment Forum content vote on cross-posted AIAF on LessWrong, this will cause both the contents ordinary LessWrong karma and Alignment Forum karma to update.</p><h1>Posting &amp; Commenting</h1><h2>What can I post on LessWrong?</h2><p>Posts on practically any topic are welcomed on LessWrong. I (and others on the team) feel it is important that members are able to “bring their entire selves” to LessWrong and are able to share all their thoughts, ideas, and experiences without fearing whether they are “on topic” for LessWrong. Rationality is not restricted to only specific domains of one’s life and neither should LessWrong be.</p><p>However, to maintain its overall focus while still allowing posts on any topic, LessWrong classifies posts as either Personal blogposts or as Frontpage posts. See more in the post on <a href=""https://www.lesswrong.com/posts/5conQhfa4rgb4SaWx/site-guide-personal-blogposts-vs-frontpage-posts#5mXJAzz3f9Aov6iEJ"">Personal Blogpost vs Frontpage Posts</a>.</p><h2>The Editor</h2><p>LessWrong’s editor is what use you to enter posts and comments.</p><h3>How do I use Markdown? (And not the Draft.js default editor)</h3><p>By default, LessWrong uses an implementation of <a href=""https://draftjs.org/"">Draft.js</a>, however, if you prefer, you can switch to entering your text with markdown syntax. To do, check <i>Activate Markdown Editor </i>checkbox in your <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#How_do_I_edit_my_account_settings__What_can_I_do_"">account settings</a>.</p><p>With the Markdown editor activated, you can use <a href=""https://www.markdownguide.org/cheat-sheet/"">Markdown syntax</a> for formatting.</p><h3>How do I insert images?</h3><p>If you are using the Draft.Js editor, select some text (or whitespace) and click the image icon in the toolbar that appears your text. Insert a URL to a <i>hosted image</i>. The image must be hosted! Use a free online service like Imgur or similar. Ensure you use the url to the hosted image itself, <i>not</i> the page displaying uploaded image (common mistake).</p><p>Note: image insertions are only enabled for posts, not comments.</p><p>If you are using the Markdown editor, using the Markdown syntax for inserting images. It is:</p><blockquote><p>![image text](https://www.example.com)</p></blockquote><p>As above, the link must be to a hosted image.</p><h3>How do I insert spoiler protections?</h3><p>LessWrong gives you a way to “avoid spoiling” your readers. Text is concealed until a user mouses over it (it works a bit less well on mobile right now). This functionality is useful for creating exercises in your posts, e.g. ask a question in your post and conceal with answer beneath spoiler protection so users don’t accidentally see it. See <a href=""https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities"">this post</a> as an example.</p><p>In the LW docs editor, type <code>&gt;!</code> on a new line, then a spoiler box should appear</p><p>In the Markdown editor, surround your text with <code>:::spoiler</code> at the beginning, and <code>:::</code> at the end.</p><h3>How do I insert footnotes?</h3><p><strong>Using the Markdown Editor</strong><br>Use the <a href=""https://www.markdownguide.org/extended-syntax/#footnotes"">syntax described here</a>.</p><p><strong>Using the LW Docs Editor</strong></p><p>You can insert footnotes via:</p><p><strong>1.</strong> Manually selecting text in the text box and selecting <i>insert footnote</i> from the footnotes menu icon.</p><figure class=""image image_resized"" style=""width:513.264px""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/xeii8wiviwuwcqchuerh"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/apckh3fic9soeno0jfgk 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/beecsyqhciffmjn80xpf 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/gvtcjjjnjid3ibwnjmmn 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/pzabvdtjlfkvhyhzmgfd 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/zqa6aauqj0mneulycya3 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/n3rqahsqsoyvrvjpllui 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/yfhjbf0w8bxxkt78pxwp 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/fh7uyiapsyyhoqwd3k4a 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/bvbryrj8z1iafnlqcvzb 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/bbj6qtdf3poriijfrbqs 928w""><figcaption>The footnote icon is the <strong>[*] </strong>on the right.</figcaption></figure><p><strong>2. </strong>Using Markdown syntax</p><ul><li>Type [^<i>n</i>] where is the number of the footnote you wish to insert.</li><li>To insert a new footnote, use <i>n </i>that is &lt;number of existing footnotes + 1&gt;; to reuse an existing footnote, set <i>n </i>to be whichever footnote you are reusing.</li></ul><p>Footnotes will automatically renumber as you add and delete them!</p><h3>How do I use Latex?</h3><p>If using the Draft.js editor, press Cmd-4 for inline and Cmd-M for block-level. (Ctrl on Windows).</p><p>If using Markdown, surround your LaTeX text with $, for example:</p><p><i>$&lt;LaTeX text&gt;$</i></p><h3>How do I embed interact prediction widgets? (Elicit Predictions)</h3><p>Follow the instructions in this post <a href=""https://www.lesswrong.com/posts/JLrnbThMyCYDBa6Gu/embedded-interactive-predictions-on-lesswrong-2"">to insert Embedded Interactive Predictions</a>.</p><h2>How do I add multiple authors to a post?</h2><p>If you have 10 or more karma, you can add a co-author to your post in post settings section (bottom of the post edit page). If you don't yet have 10 karma, send us a message on Intercom (bottom right) or email us at <a href=""team@lesswrong.com"">team@lesswrong.com</a> and we'll do it for you.</p><h2>How do I make a linkpost?</h2><p>At the top of the post editor (underneath ""title"") is a ""link"" button. If you click on it, you'll see a field where you can enter a link from another site.</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/eqfb7tytbqam0ojhijhe"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/mkrc3mhu286goekajbqw 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/uh5cswskhvu1ajmiicyi 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/pyeufnkesiwqwipoovbt 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/vwbu0rmmu5bf3ls9agb6 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/uxxpjjc3wyptsjlyqqdd 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/t8sut1jz0spogwkcqrsf 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/yuguxilsgb8dwyg3jelq 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/khgwpyxmczd7xfhixvif 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/lp9bb83rj5ukvc9zedvm 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/xr9i4oi0jwsl0bqusnoj 1582w""></figure><p>Linkposts that include at least a short description of why the topic is relevant/interesting to LessWrongers tend to get more engagement than linkposts that just include the link by itself.</p><h1>Creating Sequences</h1><p>New users can create sequences by going to the Library page, scrolling to the Community Sequences section, and clicking ""Create New Sequence."" Once created, the sequence will be visible on your User Profile page, and in the Community Sequences section of the Library.</p><p>Once you've created at least one sequence, you'll also have access to a ""Create New Sequence"" button on your User Profile page.</p><p>Users with 1000+ karma also have a ""New Sequence"" menu item in their user menu.</p><p>Once a sequence is created, you can add posts to it by clicking ""Add/Remove Posts.""</p><h1>Karma &amp; Voting</h1><h2>How do I vote?</h2><p>Posts and comments have buttons for <i>upvoting </i>and <i>downvoting </i>them displayed around the posts current <i>karma</i> score.</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/k9buffrpbqpgrizqvxby""></figure><p><br>&nbsp;</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/pxf4sey8dcxgzhpy6elu""></figure><p><br>&nbsp;</p><p>Further, you have the option to <i>strong </i>upvotes or downvote posts and comments. On desktop: hold the vote button until you see the double bars appear. On mobile: double-tap the vote button (ignore a tool-tip telling you to hold).</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/zff3unpvckzlkxw40nyx""></figure><h2>What should my votes mean?</h2><p>We encourage people to vote such that <i>upvote </i>means “I want to see more of this” and <i>downvote</i> means “I want to see less of this.”&nbsp;</p><p>Votes should apply to individual posts/comments, not to overall users. (So, please do not downvote all of a user's historical posts).&nbsp;</p><h2>What’s the relationship between votes and karma? Why aren’t they the same?</h2><p>Posts and comments have a karma score. A single vote will increase or decrease the karma by an integer value. Upvotes increase the karma, downvotes decrease - and these can cancel out.</p><p>Further, users have karma scores too. A user’s karma score is the sum of all the karma on their posts and comments. <strong>The votes of users with more karma have more power under LessWrong’s voting system</strong>, ensuring that users who have earned the community’s respect and trust have more influence than new sign-ups. Because some users have votes which are worth more than a single point, the karma score of a post is usually greater than the number of votes on it.</p><h2>What’s the mapping between users’ karma and voting power?</h2><p>A user’s vote power is determined by the code implemented in <a href=""https://github.com/LessWrong2/Lesswrong2/blob/devel/packages/lesswrong/lib/voting/voteTypes.ts"">this file</a>.</p><h2>Agree/Disagree Voting</h2><p>We've recently added agree/disagree voting to comments on posts. As of July 2022, this is still a bit of an experiment. <a href=""https://www.lesswrong.com/posts/HALKHS4pMbfghxsjD/lesswrong-has-agree-disagree-voting-on-all-new-comment"">Read more here</a>.</p><h1>Notifications &amp; Subscriptions</h1><p><i>The notification and subscriptions system are currently undergoing a significant upgrade. Expect the functionality to be expanded in the next week or two. We will update this documentation then.</i></p><h3>Where do I get notifications?</h3><p>See the bell icon in the upper right-hand corner. There are four tabs.</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/nbq5wtwnvbxonxceq6i2""></figure><p>Bell: combined responses to your posts and comments + private message notifications</p><ol><li>Paper/Doc: New post notifications</li><li>Speech Bubble: Notifications of comments on your posts</li><li>Two Speech Bubbles: Notifications of private messages on.</li></ol><h3>What can I get notifications for?</h3><p>In your <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#How_do_I_edit_my_account_settings__What_can_I_do_"">account settings</a> you can toggle notifications on and off for responses to your posts and comments.</p><h3>Can I subscribe by email? What can I subscribe to?</h3><p>Right now, you can subscribe to receive <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_are_Curated_posts_"">Curated posts</a> by email or RSS. See the subscribe buttons beneath the Curated posts section on the homepage or in your <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#How_do_I_edit_my_account_settings__What_can_I_do_"">account settings</a>.</p><h1>Messaging</h1><h2>How do I sent private messages to other users?</h2><p>Navigate to a user's page by clicking on an appearance of their username or finding them via search. Click <i>send message</i>.</p><p>To read your messages, click on the notification icon (bell icon, top right) &gt; click the two speech bubbles on the right. Or visit <a href=""http://www.lesswrong.com/inbox"">www.lesswrong.com/inbox</a>.</p><h1>Questions</h1><h2>What do you mean, questions?</h2><p>The LessWrong team is actively developing a new experimental Open Questions Research Platform. The vision is to build a system which allows the LessWrong community to apply its high standards of reasoning and scholarship to solving large, important questions.</p><p>We expect LessWrong’s Open Questions to be valuable beyond existing platforms, e.g. Quora and StackExchange, for <a href=""https://www.lesswrong.com/posts/rFcbBbpK9yBSEFzZo/list-of-q-and-a-assumptions-and-uncertainties-lw2-0-internal#kZoEWqYcdg38tMNJN"">multiple reasons</a>. Among them:</p><ul><li>The LessWrong community’s focus on good reasoning and commitment to truth</li><li>The design of our tool to be for large [distributed] research questions.</li></ul><p>The LessWrong team thinks this is an excellent way to train and apply rationality.</p><h2>What kind of questions can I ask?</h2><p>If you have a question which seems like the LessWrong community could answer better than any other Q&amp;A platform, we welcome you to ask it here.</p><p>We will handle making sure questions of the right type are shown in each place, so don’t worry too much about whether your question is relevant. Like with posts, we welcome questions on most topics and then categorize them appropriately.</p><p>Existing questions have been of all the following types:</p><ul><li>Requests for facts</li><li>Requests for answers to difficult research questions</li><li>Requests for explanations of difficult topics</li><li>Requests for arguments for or against a position</li><li>Requests for opinions and insights on a given topic</li><li>Requests for personal advice</li><li>Recommendations, feedback, or request to hear other’s personal experiences</li><li>Questions about the LessWrong website</li></ul><p>These are all good. Get a sense of what people ask on LessWrong by viewing the <a href=""https://www.lesswrong.com/allPosts?filter=questions&amp;view=magic"">questions page</a>.</p><h2>How do I ask questions?</h2><p>To ask a question, click on your Username (top right, you must have an account), and click <i>Ask Question [Beta]</i>.</p><h2>How do I write a good question?</h2><p>The site StackOverflow has a <a href=""https://stackoverflow.com/help/how-to-ask"">lot of good advice</a> on writing questions. I've copied over some bits and reworded them to fit LessWrong:</p><p><strong>Search, and research.</strong></p><p>Before posting a question, we strongly recommend that you spend a reasonable amount of time researching the problem and searching for existing questions on this site that may provide an answer. LessWrong has been around for a long time now. Many common confusions are covered in <a href=""https://www.lesswrong.com/rationality"">Rationality A-Z</a>, or in the <a href=""https://www.lesswrong.com/bestoflesswrong"">Best of LessWrong</a> . any common questions have already been answered.)</p><p>Make sure to keep track of what you find when researching, even if it doesn't help! If you ultimately aren't able to find the answer to your question elsewhere on this site, then including links to related questions (as well as an explanation of why they didn't help in your specific case).</p><p><strong>Write a title that summarizes the specific question.&nbsp;</strong></p><p>The title is the first thing that potential answerers will see. If your title isn't interesting, they won't read the rest. Also, without a good title, people may not even be able to find your question. So, <i>make the title count</i>:</p><ul><li><strong>Pretend you're talking to a busy colleague</strong> and have to sum up your entire question in one sentence: what details can you include that will help someone identify and solve your problem? Include any error messages, key APIs, or unusual circumstances that make your question different from similar questions already on the site.</li><li>If you're having trouble summarizing the problem, <strong>write the title </strong><i><strong>last</strong></i>—sometimes, writing the rest of the question first can make it easier to describe the problem.</li></ul><h2>How can I helpfully answer questions?</h2><p>You can probably help more than you think! Even if it’s not easy to answer a question outright, small contributions of information or insight can still go a long way.</p><p>We encourage you to look through the <a href=""https://www.lesswrong.com/allPosts?filter=questions&amp;view=magic"">questions page</a> to find questions that either have existing knowledge about or catch your curiosity about. Read through existing answers and then see what you can add. All of the following can be useful contributions in addition to direct answers:</p><ul><li>A link or recommendation to a resource which might help answer the question.</li><li>A recommendation of who might know the answer that you could talk to.</li><li>A suggestion for what things, if observed, would be evidence about a question one way or another.</li><li>An explanation of how the question is maybe “confused” and should be <a href=""https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question"">dissolved</a>.</li><li>Identifying a related or “sub-question” you think will help answer the bigger question.<ul><li>Note the <i>Ask related question </i>feature in question pages.</li></ul></li></ul><p>Answering questions is also a great way to practice <a href=""https://www.lesswrong.com/posts/64FdKLwmea8MCLWkE/the-neglected-virtue-of-scholarship""><i>the neglected virtue of scholarship</i></a><i>. </i>A couple of LessWrong members have written guides helpful for getting started with scholarship. <a href=""https://www.lesswrong.com/users/lukeprog"">Lukeprog</a> wrote <a href=""https://www.lesswrong.com/posts/37sHjeisS9uJufi4u/scholarship-how-to-do-it-efficiently""><i>Scholarship: How to Do It Efficiently</i></a> and <a href=""https://www.lesswrong.com/users/gwern"">gwern</a> wrote a lengthy <a href=""https://www.gwern.net/Search"">Internet Search Tips</a> guide.</p><h2>How do I interact with questions?</h2><p>Question pages might seem confusing at first. They’re not so bad. Beneath the question text you will see a textbook with three options: “New Answer”, “Ask Related Question”, and “New Comment” as pictured.</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2rWKkWuPrgTMpLRbp/m9pb1dkyarpk9vionrno""></figure><p><strong>New Answer: </strong>An answer can be any response which sheds light on the question being asked, even if it’s not a complete or comprehensive answer. Some users choose to make smaller contributions as comments. There’s a bit of fuzzy line here so don’t worry about it too much. You have the ability to move responses back and forth between being comments or answers if you change your mind.</p><p><strong>New Comment: </strong>Comments on questions can be used to ask clarifying questions and other thoughts which aren’t really answers to the question asked. You can also comment on other people’s Answers, allowing for discussion of those answers.</p><p><strong>Ask Related Question: </strong>For large questions, sometimes you can’t answer a question directly and instead to ask another question first. You can respond to a question by asking what you think is a related question. These will then be linked in the Question UI.</p><p>Asking a (smaller) related question and then making progress on answering it is a great way to help get large research questions answered by the community.</p><h1>Community Events Page</h1><h2>What is the LessWrong community event page?</h2><p>LessWrong is both an online and offline community where members around the globe meet up in person for small and large gatherings including local meetups, regional retreats, and conferences.</p><p>The community events page is where LessWrong members can find each other in the physical world and create events and groups.</p><p>You can find the page at <a href=""http://www.lesswrong.com/community"">www.lesswrong.com/community</a>, via the left sidebar (desktop) or bottom buttons (mobile).</p><h2>What are all these categories of meetups?</h2><p>The community page displays four non-exclusive categories of events and groups. These include explicitly “LessWrong” themed events plus those overlapping and adjacent communities.</p><ul><li>LessWrong</li><li>SlateStarCodex (SSC)<ul><li><a href=""https://slatestarcodex.com/"">SlateStarCodex</a> (SSC) is the personal blog of <a href=""https://www.lesswrong.com/users/yvain"">Scott Alexander</a> who made core contributions to LessWrong. Many of his posts are still cross-posted to LessWrong. The global SSC has much overlap and much in common with the LessWrong community.</li></ul></li><li>Effective Altruism (EA)<ul><li><a href=""https://www.effectivealtruism.org/articles/introduction-to-effective-altruism/"">Effective Altruism</a> (EA) is a movement and community of people trying to use reason and evidence to do the most good possible. Many LessWrong members also affiliate with the EA community.</li></ul></li><li>MIRIx<ul><li><a href=""https://intelligence.org/mirix/"">MIRIx</a> are workshops for those wishing to be involved in the work of the <a href=""http://www.intelligence.org/"">Machine Intelligence Research Institute</a> (MIRI), an organization working on <a href=""https://intelligence.org/why-ai-safety/"">AI Safety</a>. See <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_s_with_all_the_AI_and_math_posts_"">What’s with all the AI and math posts?</a></li></ul></li></ul><p>These four include explicitly LessWrong themed events plus those from overlapping and adjacent communities.</p><h2>What happens at rationality meetups?</h2><p>Depends on the meetup! Some meetups focus on formal rationality practice while others are just opportunity’s for like-minded people to socialize - many meetups or groups split their time between the two.</p><h2>What are the larger community events?</h2><p>The community events page has information for large events too. Examples include the <a href=""https://www.lesswrong.com/events/cHxzEwzXQ8hyN5228/bay-summer-solstice-2019"">Bay Area Summer Solstice Celebration</a>, <a href=""https://www.lesswrong.com/events/9PJQMwTuaEhEKkSgK/the-athena-rationality-workshop-june-7th-10th-at-ea-hotel"">Athena Rationality Workshop</a>, <a href=""https://www.lesswrong.com/events/8fzBPHx8aQjrBNRqg/european-community-weekend-2019"">European Community Weekend</a>, and <a href=""https://www.lesswrong.com/events/xFGQdgJndLcthgWoE/miri-summer-fellows-program"">MIRI Summer Fellows Program</a>.</p><h2>What resources can help me run my local rationality meetup?</h2><p>There is a resources section on the bottom of the <a href=""https://www.lesswrong.com/community"">community events page</a>. Just scroll to the bottom!</p><h1>Moderation</h1><h2>What do LessWrong moderators do?</h2><p>LessWrong aims to be a <a href=""https://www.lesswrong.com/posts/tscc3e5eujrsEeFN4/well-kept-gardens-die-by-pacifism"">well-kept garden</a>. It is warded by a team of active moderators who ensure that discussion and content are of high quality, and that behaviors which would diminish the value of LessWrong are prevented.</p><h2>Who can moderate on LessWrong?</h2><p>LessWrong has a split moderation system. Most moderation activity is performed by LessWrong’s moderation team; however, users who meet certain karma thresholds can moderate their own posts plus set the <i>moderation guidelines </i>that appear on their posts.</p><p>Users with over 50 karma can moderate their own posts when they remain as Personal blogposts.</p><p>Users with over 2000 karma can moderate their own posts even when they have been promoted to Frontpage status.</p><p>The site <a href=""https://www.lesswrong.com/posts/hHyYph9CcYfdnoC5j/automatic-rate-limiting-on-lesswrong"">automatically rate limits people who have been heavily downvoted</a>.</p><h2>What moderation actions can I take on my own posts?</h2><p>If you meet the karma thresholds (50 on Personal blogposts, 2000 on Frontpage posts), you can perform the following moderation actions on your posts:</p><ul><li>Delete comments<ul><li>Optionally with a public notice and reason.</li></ul></li><li>Delete comment thread without a trace (deletes all comments and children)<ul><li>Optionally with a private reason sent to the author.</li></ul></li><li>Ban users from commenting on a given post of yours</li><li>Ban users from commenting on any of your posts</li></ul><p>Before you can moderate your own posts, you must set <i>moderation style</i> on your post. The following options are available:</p><ul><li>Easy Going - I just delete obvious spam and trolling</li><li>Norm Enforcing - I try to enforce particular rules (See moderation guidelines)</li><li>Reign of Terror - I delete anything I judge to be annoying or counterproductive</li></ul><p>If you select <i>norm enforcing, </i>you should set your custom moderation policy which will be shown at the top of the comment section and at the bottom of the new comment form of posts you can moderate.</p><p>We encourage you to take moderation actions consistent with the moderation policy you have set on your posts.</p><h2>What actions and duties do the LessWrong team moderators perform?</h2><p>Moderators perform the following routine regular duties:</p><ul><li>Reviewing all new posts and assigns them spam, personal blogpost, or Frontpage (if the author has permitted Frontpage promotion).</li><li>Reviewing new users when they first comment or post. We may reject content from new users if we don't think it's a good fit for the site. See <a href=""https://www.lesswrong.com/posts/PKGTKdRDneLA7bY54/new-rejected-content-section"">here</a>.</li><li>Deleting spam caught by our automatic filters.</li><li>Selecting posts for <a href=""https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#What_are_Curated_posts_"">Curation</a>.</li><li>Keeping an eye on discussions and ensuring they remain productive and civil.</li></ul><p>Moderations perform the following less-common actions:</p><ul><li>Issuing feedback and warnings to users who behave in ways harmful to LessWrong's discourse quality and culture.<ul><li>These will usually start with private feedback but escalate to public warnings.</li></ul></li><li>Banning users. For established users we generally start with temporary bans.</li><li>Locking comment threads (usually temporarily) if they become overly heated and divisive.</li><li>Limiting the visibility of divisive, heated conversations on the site to protect the culture and what people are exposed to.<ul><li>One example: we moved a comment thread on a post to a separate post.</li><li>Moderators can hide discussion threads from the Recent discussion feed on the homepage.</li></ul></li></ul><p>In <a href=""https://www.lesswrong.com/posts/5conQhfa4rgb4SaWx/site-guide-personal-blogposts-vs-frontpage-posts#5mXJAzz3f9Aov6iEJ"">extremely extreme and exceedingly severe cases</a>:</p><ul><li>The LessWrong team may decide that we cannot display certain content on the site. In this case, we will likely move that content back to a user’s drafts.</li></ul><h2>What powers do moderators have?</h2><p>Moderators generally have access to the site data, most of this time this is accessed at the request of a user in the process of debugging a technical issue. We take data privacy seriously. We don’t just read private messages.</p><p><i>Note: if a comment of yours is ever deleted, you will automatically receive a private message with its contents.</i></p><ul><li>The ability to delete comments<ul><li>Usually with public notice and reason.</li></ul></li><li>The ability to delete comment threads without trace (deletes all comments and all its children)<ul><li>Usually, with a private reason send to the author.</li></ul></li><li>The ability to move content between different classifications, e.g. Personal blogpost, Frontpage post, Curated, Meta (deprecated category) and drafts.</li><li>Moderators can view drafts, but they almost never will unless they’re helping you debug something.</li><li>The ability to edit posts.<ul><li>Moderators usually use this to fix awry formatting for you, e.g. your LaTeX is screwed up or egregious typos, leaving a comment saying they have done so.</li></ul></li><li>The ability to ban users from commenting on given posts or comment threads.</li><li>The ability to ban users from the site (typically done temporarily).</li><li>The ability to lock comment threads.</li><li>The ability to reject content from new users (in which the content will appear in the <a href=""https://www.lesswrong.com/posts/PKGTKdRDneLA7bY54/new-rejected-content-section"">rejected section</a>)</li><li>The ability to remove a user's voting privileges (and reverse their past votes).</li><li>The ability to rate limit user's ability to comment or post.</li></ul><p>More generally, most moderators are also site developers who may build new tools over time. We'll try to keep this section updated but it may not always be comprehensive.</p><h2>What is the LessWrong moderation policy/philosophy?</h2><p>Unfortunately, we do not have a recent and up to date document that speaks coherently for the whole site, however habryka’s post on <a href=""https://www.lesswrong.com/posts/bGpRGnhparqXm5GL7/models-of-moderation"">Models of moderation</a> is a good start. Our moderation policy tends to happen via case-law (i.e. a particular situation comes up, we make a decision based on our overall assessment of the situation, and write up our reasoning). We do try to be pretty public about our moderation reasoning when it comes up.</p><p>We do have a hard rule against mass downvoting, mass upvoting, or voting with sockpuppet accounts. The line between ""downvoting a lot, when you legitimately want to"" and ""mass downvoting"" is somewhat blurry, but if it seems like you are doing so in an adversarial way we are likely to ban your account and/or remove your ability to vote.</p><p>We &nbsp;also have a hard rule against ban evasion. If you make a new account to avoid a ban, or rate limit or other restriction, we're likely to notice, ban the new account and potentially ban your old account as well depending on the circumstances.</p><h2>Who are the moderators?</h2><p>The LessWrong core team plus a few others form the current moderation team. You can see who they are on the <a href=""https://www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team-page-under-construction"">team page</a>.</p><h2>How do I become a moderator?</h2><p>We are not currently recruiting any new moderators and there is no current process.</p><p>That said, moderators would be recruited from among those we believe possess excellent judgment and understand LessWrong, its purpose, its culture, and its values. The best way to demonstrate this would be through consistently valuable participation on LessWrong.</p><h1>What is LessWrong’s Privacy Policy and Terms of Use?</h1><p>Our Privacy Policy and Terms of Use can be <a href=""https://docs.google.com/viewer?url=https%3A%2F%2Fintelligence.org%2Ffiles%2FPrivacyandTerms-Lesswrong.com.pdf"">viewed here</a>.</p><p>Note that the <a href=""https://intelligence.org/"">Machine Intelligence Research Institute</a> (MIRI) is the relevant legal party for this privacy policy and terms of use. When Eliezer Yudkowsky founded the LessWrong website in 2009, he created it as the property of MIRI (then named the Singularity Institute for Artificial Intelligence, aka SIAI).</p><p>While we’re at it, we can add that the current LessWrong team operates legally as a part of a related organization, the <a href=""http://www.rationality.org/"">Center for Applied Rationality</a> (CFAR) while retaining autonomy over its internal decision-making and all decisions about the LessWrong website.</p><p>For the intertwined history of MIRI and CFAR, see <a href=""https://www.lesswrong.com/posts/b6AB7LhSG5krtKEje/was-cfar-always-intended-to-be-a-distinct-organization-from#cYyXLADw8XB8zsRsk"">this answer</a> to a LessWrong question.</p>",Ruby,ruby,Ruby,
KnFYhD5GvC3RbLHjq,On Freud's examination of the Uncanny in Literature,on-freud-s-examination-of-the-uncanny-in-literature-1,https://www.lesswrong.com/posts/KnFYhD5GvC3RbLHjq/on-freud-s-examination-of-the-uncanny-in-literature-1,2019-06-14T17:40:25.520Z,2,1,0,False,False,,"<br/><p>It is often mentioned that Freud’s Psychoanalytical theory has influenced literature very considerably. Regarding stories that aspire to cause dread, his article on the Uncanny in Literature provides good insight – particularly his interpretation of “The Sandman”,  E.T.A. Hoffmann’s bleak and magical story of fiery circles, monsters and doppelgangers,  is worth mentioning...</p><p>The Sandman, the eponymous fiend of this story, is both a magical entity (a flying beaked demon that abducts little children and uses their eyes so as to feed his own offspring) and a dyad of mysterious men: an ominously looking old lawyer, Coppelius, and a merchant of eyeglasses, called Giuseppe Coppola. Both names etymologically are derived from the Italian word for “eye”, and their links to the monstrous Sandman of legend do not stop there.</p><p><strong>The Lawyer Coppelius</strong></p><p>Coppelius is a very unpleasant-looking man, who seems to despise children. The protagonist of the story, the student Nathaniel, recalls how he hated and feared Coppelius ever since he was a boy. While at first the antipathy was caused by disgust at how the old man looked, as well as due to the antagonistic attitude he consistently demonstrated when invited to Nathaniel&#x27;s family home by his father, later on Nathaniel comes to regard Coppelius as the actual murderer of his father – whose death took place during a chemical, possibly alchemy-related, experiment.  </p><p>Coppelius had already been fused in Nathaniel&#x27;s mind with the mythical Sandman – the child’s governess was reckless enough to fill his mind with tales of horror, and it should be noted that she was the one who first suggested that his father’s night-time guest was the Sandman. Nathaniel always loved stories of mystery and the macabre, so the repugnant and terrifying figure of the winged and beaked Sandman soon assumed a central position in his personal pantheon of ghouls.</p><p>Coppelius manages to escape after the apparent accident that killed Nathaniel&#x27;s father, and Nathaniel will only see him again – or at any rate believe he saw him – years later, while studying far away from his home city.</p><p><strong>The optometrist Giuseppe Coppola </strong> </p><p>Coppola first appears to the student Nathaniel wishing to sell him some of his wares – lenses, small telescopes and eyeglasses. Nathaniel immediately feels repulsed, because the visage of this merchant is uncannily similar to the lawyer Coppelius&#x27;. At length, he decides to buy one of the elegant lenses, which he will soon put to use so to have a better look at the object of his desires: the university professor’s beautiful daughter, Olympia.</p><p>Unfortunately for Nathaniel the gained ability to have a closer look at Olympia - a girl normally isolated and confined to her room and only making her appearance by the window - results to dangerous infatuation and the dreaded sense that something is not quite right with her... For the rest of the story he will persistently attempt to negate his worries, despite the fact that they are consequently fueled by rumors circulating among the students, according to which Olympia is bizarrely wooden and barely ever speaks. Nathaniel is enamored and distances himself from his old friends as well as his old fiancee who stayed back at their hometown.</p><p><strong>Two fathers, two father-killers and two sons</strong></p><p>Hoffmann uses doppelgangers in most of his works. In the Sandman there are at least two notable pairs: Nathaniel has two fathers, the one who died during the alchemy experiment and his university professor (who wishes Nathaniel to marry Olympia, his daughter, and thus become his son-in-law). There are also two killers of the father figure: Coppelius (said to have caused the death of the father) and Coppola, who comes to fight with the university professor over ownership of the wooden automaton known as Olympia and mortally wounds him…</p><p>There are also, according to Freud, far more crucially two sons:</p><p>Freud does make a very convincing case when he argues that Olympia, the life-like piece of machinery, appears to be in reality part of Nathaniel. Indeed, the reader should note that while Nathaniel lost his father, Olympia is virtually next to her father all of the time, and whereas Nathaniel was scared by Coppola/Coppelius and the Sandman, Olympia is perfectly fine with being restricted and ordered around, docile and well-behaved. Freud argues that Olympia alludes to what the child, threatened by a potentially dangerous father, created as a means to avoid friction with the source of dread. Olympia can never antagonize the father, whereas Nathaniel keeps getting into considerable trouble in his attempt to come to terms with the various splits of the father-image.</p><p>In the end, Nathaniel only wishes to become one with Olympia, and if Freud’s analysis is correct then this wish is only one for self-completion. The split part of the youth can no longer stay away, it cannot be pushed away to another city and live in perpetual exile. Of course Nathaniel himself is not aware of the special tie to Olympia, yet everything about the story leads to the conclusion that this uncanny dance of living and wooden forms is orchestrated as an unwitting ceremony in honor of the father-image: Nathaniel and Olympia risk losing their very eyes – with Freud referring his reader to the psychoanalytic theory that links fear of losing one’s eyes to fear of castration.</p><p><strong>In conclusion</strong></p><p>E.T.A. Hoffmann’s The Sandman is, arguably, one of the most elegant works of dark romanticism. An uncanny story, presented masterfully – and one where the deeper meaning is allowed to remain hidden, so that the reader is free to be dazzled, surprised, horrified and indeed take part mentally in this macabre dance of hidden emotions and repressed memories. It was certainly a poignant decision by Freud to focus on this work in his article, since its use of the uncanny in high literature is paradigmatic.</p><p>By Kyriakos Chalkopoulos <a href=""(https://www.patreon.com/posts/27630785)"">(https://www.patreon.com/posts/27630785)</a></p>",KyriakosCH,kyriakosch,KyriakosCH,
PRMB3ptsZMeQpvG3b,Unknown Unknowns in AI Alignment,unknown-unknowns-in-ai-alignment-1,https://www.lesswrong.com/posts/PRMB3ptsZMeQpvG3b/unknown-unknowns-in-ai-alignment-1,2019-06-14T05:07:21.193Z,6,3,3,False,True,,"<html><head></head><body><p>It seems to me that no matter how many problems from different research agendas we solve, there is always the possibility that some  'unknown unknown' misalignment scenario can occur. I can imagine an approach of building model-agnostic, environment-agnostic minimal assumption alignment guarantees (which seems to be super hard), but I feel like things can go wrong in myriad other ways, even then.</p>
<p>Has there been any discussion about how we might go about this?</p>
</body></html>",sayan,sayan,sayan,
ePviE4mnjtLZ9GjKj,SlateStarCodex Madison Meetup: Mental Health and Moloch,slatestarcodex-madison-meetup-mental-health-and-moloch,https://www.lesswrong.com/events/ePviE4mnjtLZ9GjKj/slatestarcodex-madison-meetup-mental-health-and-moloch,2019-06-14T02:59:14.772Z,2,3,1,False,False,,,marywang,marywang,marywang,
zMBcDmMdCwJtXWrBs,Storytelling and the evolution of human intelligence,storytelling-and-the-evolution-of-human-intelligence-1,https://www.lesswrong.com/posts/zMBcDmMdCwJtXWrBs/storytelling-and-the-evolution-of-human-intelligence-1,2019-06-13T20:13:03.547Z,14,7,0,False,False,,"<p>This is a notice of a recent paper that may be of interest here, <a href=""https://www.nature.com/articles/s41437-019-0214-2"">&quot;The storytelling arms race: origin of human intelligence and the scientific mind&quot;</a> by Enrico Coen. It is a more specific working out of the <a href=""https://en.wikipedia.org/wiki/Machiavellian_intelligence"">Machiavellian Intelligence</a> hypothesis, taking storytelling as the means through which the deception arms race operated, and which (it suggests) also raised up language alongside intelligence. It also discusses the relationship between storytelling (where, as in war, truth is the first casualty) and science (where truth is the goal).</p><p>Disclosure: I work closely with the author, although not on the subject of this paper.</p>",Richard_Kennaway,richard_kennaway,Richard_Kennaway,
TqM6sfPuCX7ksjLoj,Real-World Coordination Problems are Usually Information Problems,real-world-coordination-problems-are-usually-information,https://www.lesswrong.com/posts/TqM6sfPuCX7ksjLoj/real-world-coordination-problems-are-usually-information,2019-06-13T18:21:55.586Z,35,14,2,False,False,,"<p>Let’s start with a few examples of very common real-world coordination problems.</p><ul><li>The marketing department at a car dealership posts ads for specific cars, but the salespeople don’t know which cars were advertised, causing confusion when a customer calls in asking about a specific car. There’s no intentional information-hoarding, it’s just that the marketing and sales people don’t sit next to each other or talk very often. Even if the info were shared, it would need to be translated to a format usable by the salespeople.</li><li>Various hard problems in analysis of large-scale biological data likely have close analogues in econometrics. The econometricians have good methods to solve the problems, and would probably be quite happy to apply those methods to biological data, and the bio experimentalists would love some analytic help. But these people hardly ever talk to each other, and use different language for the same things anyway.</li><li>When the US invaded Grenada in the ‘80’s, the marines occupied one side of the island and the army occupied the other. Their radios were not compatible, so if an army office needed to contact their counterpart in the marines, they had to walk to the nearest pay phone and get routed through Fort Bragg on commercial telephone lines.</li><li>Various US intelligence agencies had all of the pieces necessary to stop the 9/11 attacks. There were agencies which knew something was planned for that day, and knew who the actors were. There were agencies which knew the terrorists were getting on the planes. There were agencies which could have moved to stop them, but unfortunately the fax(!) from the agencies which knew what was happening wasn’t checked in time.</li><li>There are about 300 million people in the US. If I have a small company producing doilies, chances are there are plenty of people in the US alone who’d love my doilies and be happy to pay for them. But it’s hard to figure out exactly which people those are, and even once that’s done it’s hard to get them a message showing off my product. And even if all that works out, if the customers really want a slightly different pattern, it’s hard for them to communicate back to me what they want - even if I’d be happy to make it.</li><li>Just yesterday I was looking for data on turnover time of atherosclerotic plaques. I know plaques increase with age, but is it the same plaques in the same places growing, or is it an increase in equilibrium number of plaques (each appearing and dissipating quickly)? There’s probably thousands of people who can easily answer that question and would be happy to do so, yet finding a clear answer is still nontrivial.</li></ul><p>Obviously these are all specific examples of problems which happen all the time.</p><p>To some extent, coordination problems are universal and have always been with us. But humans evolved to solve coordination problems in Dunbar’s-number-sized groups (plus or minus an order of magnitude) regularly talking face-to-face. Even just two hundred years ago, most people operated in relatively small communities. It’s only since the rise of cheap long-distance communication that large-scale coordination problems have crept into everyday life. The cheaper and more ubiquitous long-distance communication becomes, the more coordination problems are going to be a bottleneck. Not all coordination problems look like this, but these are the sort of coordination problems which we’d expect to become more common over time. (See “<u><a href=""https://www.lesswrong.com/posts/sYt3ZCrBq2QAf3rak/from-personal-to-prison-gangs-enforcing-prosocial-behavior"">From Personal to Prison Gangs</a></u>” for a more fleshed-out version of this argument, and related problems.)</p><p>Look over the list of coordination problems above, and a few major themes jump out:</p><ul><li>Matching problems: Doily-makers know there are customers out there who want their product. Bio experimentalists know there are analysts out there who want their data. I know there’s someone out there who can answer my plaques question. But finding those people, in a world of 6 billion, is a needles-in-haystack problem. Just figuring out who to talk to is hard.</li><li>Lack of communication channels: Cheap, fast communication channels just don’t exist between company and customer or between departments of an organization. Even if you know who to talk to, you still need a way to talk to them.</li><li>Language difficulties: Econometricians and biologists use different language or even different frameworks for similar systems. Different departments use different data formats. The army and the marines had incompatible radios. Even when you know who to talk to and have a channel, communication can still be hard.</li></ul><p>Standard discussions of coordination problems tend to focus on cases where a dictator could easily solve the problem. Need to meet up with someone in New York City at a specific place and time without communicating in advance? The dictator can declare “Empire State building at noon is the official meet-up spot and time”, and there we go, we’re done. But the harder sorts of real-world coordination problems usually aren’t that easy. Having a designated dictator on hand doesn’t help a doily company find enthusiastic customers, or help a biologist and an econometrician realize they should collaborate, or help translate data from one format to another (assuming they do in fact need different formats).</p><p>The biggest problem is that there’s a combinatorially huge space of possible coordination problems, and any particular coordination problem won’t happen many times. How many people have asked my exact question about atherosclerotic plaques? In order to be useful, a coordination mechanism has to address a very wide class of coordination problems in one fell swoop - e.g. the question-answering site Quora. But simply declaring “this is the canonical question-answering site” doesn’t solve the problem - in order for it to actually work, we still need a good matching engine, so that askers and answerers can find each other without having to search through the haystack themselves.</p><p>A combinatorially huge space of problems directly leads to a more insidious issue: humans have limited processing ability, so there will inevitably be coordination problems where nobody involved even knows what’s possible. The biologist and the econometrician don’t know that their fields complement each other. In order to solve that sort of problem, a third party has to proactively look for opportunities to coordinate. Once the opportunity is found, actually connecting people is the relatively easy part - lots of academics are interested in opportunities to collaborate across fields (I hear grantmakers love that stuff).</p>",johnswentworth,johnswentworth,johnswentworth,
Hc3z58hNmGF3bwcM2,On Having Enough Socks,on-having-enough-socks,https://www.lesswrong.com/posts/Hc3z58hNmGF3bwcM2/on-having-enough-socks,2019-06-13T15:15:21.946Z,21,6,9,False,False,https://www.gwern.net/Socks,,gwern,gwern,gwern,
2d6DGNbf732y6YNTv,Learning Over Time for AI and Humans and Rationality,learning-over-time-for-ai-and-humans-and-rationality,https://www.lesswrong.com/posts/2d6DGNbf732y6YNTv/learning-over-time-for-ai-and-humans-and-rationality,2019-06-13T13:23:58.639Z,4,2,0,False,True,,"<p> <a href=""https://www.lesswrong.com/posts/pLZ3bdeng4u5W8Yft/let-s-talk-about-convergent-rationality-1"">https://www.lesswrong.com/posts/pLZ3bdeng4u5W8Yft/let-s-talk-about-convergent-rationality-1</a> </p><p>make the observation that natural and artificial intelligence will learn over time. Given the post&#x27;s title that got me thinking about the two learning settings and how that might apply to concepts of rationality.</p><p>AI will presumably not face the same life time limitation that humans currently do. The implication is that learning will be different in the two settings. Human learning is very much dependent on prior generation and various social/group type aspects.</p><p>But I&#x27;ve generally thought of rationality (as generally understood) bound to a single mind, as it were. The idea of applying many rules about rational thought or behavior to aggregates, such as &quot;the mob&quot; largely a misapplication.</p><p>I wonder if the learning process based on knowledge learned (including the potential for mis-knowledge transmission) via a larger social process performs better in both general learning and development of rational though processes or if the AI single, long lived &quot;mind&quot; has some advantage.</p><p>I might also wonder a bit about how this might apply to questions such as that asked regarding why China did not develop science.</p><p>I suspect various aspects of my though have been discussed here -- or at least can inform on the though but there are lots of post to search.</p>",jmh,jmh,jmh,
6XfeAxBQw2sxvbTMR,Some Ways Coordination is Hard,some-ways-coordination-is-hard,https://www.lesswrong.com/posts/6XfeAxBQw2sxvbTMR/some-ways-coordination-is-hard,2019-06-13T13:00:00.443Z,55,16,11,False,False,,"<p>Response to (Raymond Arnold at Less Wrong): <a href=""https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag"">The Schelling Choice is Rabbit, Not Stag</a> and by implication Duncan&#8217;s <a href=""https://medium.com/@ThingMaker/open-problems-in-group-rationality-5636440a2cd1"">Open Problems in Group Rationality</a></p>
<p>Stag Hunt is the game whereby if everyone gets together, they can hunt the Stag and win big. Those who do not hunt Stag instead hunt Rabbit, and win small. But if even one person instead decides to hunt Rabbit rather than hunt Stag, everyone hunting Stag loses rather than wins. This makes hunting Stag risky, which makes Stag risky (since others view it as risky, and thus might not do it, and view others as viewing it as risky, making it even more likely they won&#8217;t do it, and so on). Sometimes this can be overcome, and sometimes it can&#8217;t.</p>
<p>Raymond claims that the Schelling point of this game, by default, is Rabbit, not Stag.</p>
<p>Whether or not this is true depends on the exact rules of the game and the exact game state, what one might call the <em>margin of coordination. </em></p>
<p>If you haven&#8217;t yet, click through to at <a href=""https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag"">least to Raymond&#8217;s article</a> and his quote from Duncan&#8217;s original description, and consider reading <a href=""https://medium.com/@ThingMaker/open-problems-in-group-rationality-5636440a2cd1"">Duncan&#8217;s full article</a>.</p>
<p><span id=""more-20234""></span></p>
<h3>Questions</h3>
<ol>
<li>What is the risk versus reward on the stag hunt? How often must it work to be worth trying?</li>
<li>Can we afford to fail? Can others? For how long?</li>
<li>Can the stag hunt fail even if everyone chooses stag?</li>
<li>Do we have time to iterate or communicate to establish future coordination, and will our actions now act as a signal?</li>
<li>How many people need to cooperate before the stag hunt is worthwhile? Can we afford to lose anyone? Is there a lesser stag we can go after instead with a smaller group?</li>
<li>Is there a particular player who has reason to worry, or might cause others to worry?</li>
<li>Are the players known to be aware of the stag hunt? If there are multiple possible stag hunts, do we all know which one we&#8217;re going for?</li>
<li>Are the players confident others know the payoff and agree it is known to be there for everyone? Does everyone even know what stag is and what rabbit is?</li>
<li>Does this group have a history of going on stag hunts? Is going on the stag hunt praiseworthy or blameworthy if no one follows you, or not enough people do?</li>
<li>Do the rabbit hunts have network effects such that failure to coordinate on them is bad for everyone, not only for the person going stag?</li>
<li>Are there players who value <em>relative </em>success rather than absolute success, and want others to fail?</li>
<li>Do we trust other players to behave in their own best interests and trust them to trust others in this way? Do we trust them to act in the best interest of the group knowing the group rewards that?</li>
<li>Do we trust others to <em>do what they say they are going to do?</em></li>
<li>Are people capable of overcoming small short-term personal incentives to achieve or stand for something worthwhile, to help the group or world, or to cultivate and encourage virtue? Do they realize that cooperation <em>is a thing and is possible at all? </em>Do they realize that <a href=""https://www.lesswrong.com/out?url=https%3A%2F%2Fwww.talyarkoni.org%2Fblog%2F2018%2F10%2F02%2Fno-its-not-the-incentives-its-you%2F"">It&#8217;s Not the Incentives, It&#8217;s You</a>?</li>
<li>Is the stag hunt even worth asking all these questions?</li>
</ol>
<p>Note that most of these require <em>common knowledge</em>. We need everyone to know, and for everyone to know that everyone knows that everyone knows that everyone knows for however many levels it takes. Alternatively, we need habits of stag hunting that are self-sustaining.</p>
<p>Man, coordination is <em>complicated. </em>And hard. So many questions! Such variation.</p>
<h3>Duncan&#8217;s Example</h3>
<p>In Duncan&#8217;s example, we have full common knowledge of the situation, and full agreement on payoffs, which is very good. Alas, we still suffer severe problems.</p>
<p>We have a very bad answer to questions two, five and six. And also fourteen. If we lose even one person, the stag hunt fails, and there is no alternative hunt with fewer people. And we have players who have reason to worry, because they can ill afford a failed stag hunt. One of them will be stranded without the ability to even hunt rabbit, should the stag hunt fail.</p>
<p>It seems that everyone involved is reasoning as if each member is looking out mostly or entirely for themselves and their short-term success, and expecting all others to do so as well, even when this is an obviously bad medium-term plan.</p>
<p>The result will often be cascading loss of faith, resulting in full abandonment of the stag hunt. Certainly the stag hunt is no longer the <em>Schelling point </em>of selection, given the risks everyone sees. You wouldn&#8217;t do this implicitly, without everyone agreeing to it first, and you&#8217;d only do it with an explicit agreement if you had common knowledge of everyone being trustworthy to follow through.</p>
<p>Or, if there was a long history that the moment everyone had the resources to do it, they coordinate on the stag hunt. But that still only works with common knowledge of the full game state, so getting it without explicit communication is still super rare.</p>
<p>The obvious thing this group can do, in this situation, is to <em>explicitly agree to go on the stag hunt. </em></p>
<p>But they&#8217;re explicitly <em>already trying to do that, </em>and finding it won&#8217;t work, because these people do not trust each other. They fail question thirteen.</p>
<p>What are some other things this group might try? With so many advantages, it seems a shame to give up.</p>
<h3>Solutions in Duncan&#8217;s Example</h3>
<p>D1. Alexis gives one utility to Elliot (solve question two)</p>
<p>This actually should be enough! Alexis gives one of their 15 banked resources to Elliot. Now everyone has at least 6 banked resources, and will be able to choose rabbit even if the hunt fails. This makes the situation clear to everyone, and removes the worry that Elliot will need to choose rabbit.</p>
<p>D2. Wait until next hunt (solve question two)</p>
<p>Even simpler than option D1. If everyone hunts rabbit now, everyone goes up in stored resources, and next time has enough buffer to coordinate on a stag hunt.</p>
<p>Both point to the principle of <a href=""https://thezvi.wordpress.com/2017/09/30/slack/"">slack</a> that Raymond reminds us about, and extend this to the whole group. Don&#8217;t only keep your own slack high, <em>don&#8217;t ask anyone else to fully deplete theirs </em>under normal circumstances, even if it means everyone doing less efficient things for a bit.</p>
<p>D3. Build trust (solve question thirteen)</p>
<p>Note that if the group is sufficiently unreliable, that alone will prevent all stag hunts no matter what else is done. If the group could trust each other to follow through, knew that their words had meanings and promises meant something, then they could coordinate reliably despite their other handicaps here. With sufficient lack of trust, the stag hunt isn&#8217;t positive expectation to participate in, so there&#8217;s no point and everyone hunts rabbits until this is fixed.</p>
<p>D4. Use punishment or other incentives</p>
<p>A solution for any game-theoretic situation is to change the rules of the game, by coordinating to reallocate blame and resources based on actions. This is often the solution <em>within </em>the game, but can also happen by extending the situation outside the game. Raymond&#8217;s example shows that the game of Stag Hunt often inevitably causes punishment to take place. Using enough of it, reliably enough, predictably enough, should work, at least for the failures in Duncan&#8217;s example.</p>
<p>Improving any of the other answers would also help tilt the scales. Stag hunts are threshold effects at their core, so helping the situation in any way improves your chances more than one might think, and any problem causes more issues than you&#8217;d naively predict.</p>
<h3>Solutions in Raymond&#8217;s Example</h3>
<p>Raymond&#8217;s scenario faces different problems than Duncan&#8217;s. Where Duncan had problems with questions 2, 5, 6 and 13, Raymond faced a problem with questions 3, 7 and especially 8. He thought that staying focused had the big payoff, while his coworker thought that staying narrowly focused was a failure mode.</p>
<p>Coordination is especially hard if some of the people coordinating think that the result of coordination would be bad. What are the solutions now?</p>
<p>R1. Talk things over and create common knowledge (solve seven and eight)</p>
<p>R2. Propose a different coordinated approach designed to appeal to all participants</p>
<p>Once Raymond and his colleague talked things over and had common knowledge of each others&#8217; preferences for conversation types, coordination became possible. It became clear that Raymond&#8217;s preferred approach <em>didn&#8217;t </em>count as a stag hunt, because it didn&#8217;t benefit all parties, and there was a disagreement about whether it was net beneficial slash efficient to do it. Instead, a hybrid approach seemed likely to be better.</p>
<p>In cases where there is a clearly correct approach, making that clear to everyone, and knowing this has happened, makes it much more likely that coordination can successfully take place. In cases where there turns out not to be such an approach, this prevents those who previously thought such an approach existed from having false expectations, and minimizes conflict and frustration.</p>
<p>R3. Bid on what approach to take</p>
<p>Often coordination on <em>any </em>solution is better than failure to coordinate. Some level of meandering versus focus that all parties agree to is better than fighting over that ratio and having the meeting collapse, provided the meeting is worthwhile. Thus, if the participants can&#8217;t agree on what&#8217;s best, or find a solution that works well enough for everyone to prompt coordination, then a transfer of some kind can fix that.</p>
<p>Doing this with <em>dollars </em>in social situations is usually a terrible idea. That introduces bad dynamics, in ways I won&#8217;t go into here. Instead, one should strive to offer similar consideration in other coordination questions or trade-offs in the future. The greater the social trust, the more implicit this can be while still working. This then takes the form of an intentionally poorly specified number of amorphous points, that then can be cashed in at a future time. The points matter. They don&#8217;t need to balance, but they can&#8217;t be allowed to get too imbalanced.</p>
<h3>The Facebook Exodus Example</h3>
<p>A while back, I realized I was very much <a href=""https://thezvi.wordpress.com/2017/04/22/against-facebook/"">Against Facebook</a>. The problem was that the entire rationalist community was doing most of their discourse and communication there, as was a large portion of my other friend groups. I&#8217;d failed to find a good Magic: The Gathering team that didn&#8217;t want to do its (if you can still call it) coordination on Facebook. This was a major factor in ending my Magic comeback.</p>
<p>Many, but far from all, of those I queried agreed that Facebook was terrible and wished for a better alternative. But all of them initially despaired. The problem looked too hard. The network effects were too strong. Even if we could agree Facebook was bad, what was the alternative? What could possibly meet everyone&#8217;s needs as well as Facebook was doing, even if it was much better at not ruining lives and wasting time? Even if a good alternative was found, could we get everyone to agree on it?</p>
<p>Look at that list of questions. Consider that success depends to a large extent on <em>common knowledge </em>of the answers to those questions.</p>
<ol>
<li>What is the risk versus reward on the stag hunt? How often must it work to be worth trying?</li>
<li>Can we afford to fail? Can others? For how long?</li>
<li>Can the stag hunt fail even if everyone chooses stag?</li>
<li>Do we have time to iterate or communicate to establish future coordination, and will our actions now act as a signal?</li>
<li>How many people need to cooperate before the stag hunt is worthwhile? Can we afford to lose anyone? Is there a lesser stag we can go after instead with a smaller group?</li>
<li>Is there a particular player who has reason to worry, or might cause others to worry?</li>
<li>Are the players known to be aware of the stag hunt? If there are multiple possible stag hunts, do we all know which one we&#8217;re going for?</li>
<li>Are the players confident others know the payoff and agree it is known to be there for everyone? Does everyone even know what stag is and what rabbit is?</li>
<li>Does this group have a history of going on stag hunts? Is going on the stag hunt praiseworthy or blameworthy if no one follows you, or not enough people do?</li>
<li>Do the rabbit hunts have network effects such that failure to coordinate on them is bad for everyone, not only for the person going stag?</li>
<li>Are there players who value <em>relative </em>success rather than absolute success, and want others to fail?</li>
<li>Do we trust other players to behave in their own best interests and trust them to trust others in this way? Do we trust them to act in the best interest of the group knowing the group rewards that?</li>
<li>Do we trust others to <em>do what they say they are going to do?</em></li>
<li>Is the stag hunt even worth asking all these questions?</li>
</ol>
<p>Which ones were problems?</p>
<p>Most of them.</p>
<p>We had (1) uncertain risk versus reward of switching to another platform or set of platforms, (3) even if coordination on the switch was successful, with (2) continuous and potentially painful social consequences and blameworthiness for being &#8216;out of the loop&#8217; even temporarily. To be better off, often (5) the entire group would have to agree to the new location and method, with (6)(8) some people who would dislike any given proposal, or like staying with Facebook because they didn&#8217;t agree with my assessments, or because they&#8217;d need to coordinate elsewhere. (10) The attempt would hurt our network effects and cause non-trivial communication interruptions, even if it eventually worked. (7) Getting the word out would be a non-trivial issue, as this would include <em>common knowledge </em>that the word was out and the coordination was on, when it likely wasn&#8217;t going to be on at any given time.</p>
<p>(12) Facebook has many addictive qualities, so even many people who would say they &#8216;should&#8217; quit or even were quitting would often fail. (13) Even when people agreed to switch and announced this intention, they&#8217;d often find themselves coming back.</p>
<p>There was a lot of excusing one&#8217;s actions because of (14) Network Effects and The Incentives.</p>
<p>A lot of people (15) reasonably didn&#8217;t want to even think about it, under these circumstances.</p>
<p>The good news is we had (4) plenty of time to make this work, and (9) even most of those who didn&#8217;t think the switch was a good idea understood that it was a noble thing to <em>attempt </em>and would make sense under some scenarios. And no one was (11) thinking of their relative social media position. And of course, that the stag hunt wouldn&#8217;t <em>automatically </em>or <em>fully </em>fail if one person didn&#8217;t cooperate, and if we got enough cooperation, critical mass would take over.</p>
<p>But that&#8217;s still 11 out of 14 problems that remained importantly unsolved.</p>
<p>The better news was we had one other important advantage. <em>I hated hunting rabbit. </em>Rabbit hunting was not a productive exercise for me, and I&#8217;d rather be off hunting stag unsuccessfully on my own, than hunting rabbit. It&#8217;s not a great spot, certainly there would be better uses of time, but that was a great advantage &#8211; that I didn&#8217;t feel too bad about failures. Otherwise, the whole thing would never have had a chance.</p>
<p>It also helped that many others are increasingly making similar cases, for a wide variety of reasons, some of which I don&#8217;t even agree with or I don&#8217;t think are big deals.</p>
<p>The solution I went with was three&#8211;fold.</p>
<p>F1. First, to explain why I believed Facebook was awful, in order to help create common knowledge. Starting with an article, then continuing to make the case.</p>
<p>F2. Second, go out and start stag hunting on my own, and make it clear I wasn&#8217;t going anywhere. This does not work when stag hunts are all-or-nothing with a fixed &#8216;all&#8217; group, but this is rare. More often, stag hunts work if <em>those who people count on, do their jobs </em>rather than <em>everyone who might show up in theory shows up and does their job. </em>That&#8217;s a crucial distinction, and a dramatic drop in difficulty.</p>
<p>F3. Third, to reward with engagement, praise and when helpful direct encouragement and assistance those who wanted to make the switch to blogs or other less toxic communication forms. To some extent there was shaming of Facebook participation, but that&#8217;s not much use when everyone&#8217;s doing it.</p>
<p>Without the effort to first create common knowledge, the attempt would have had no chance of success. And of course, a combination of factors helped out, from the emergence of Less Wrong 2.0 to a variety of others waking up to the Facebook problem at about the same time, for their own reasons.</p>
<p>The solution of &#8216;be the coordination you want to see in the world even if it doesn&#8217;t make sense on its own&#8217; is <em>very powerful. </em>That&#8217;s how Jason Murray kick-started the New York rationalist group, and how many other groups have started &#8211; show up at the same time and same place, even if you end up eating alone, to ensure others can come. Doing it for a full-on stag hunt with fixed potential participation is more expensive, <em>but it is still a very strong signal that can encourage a cascade. </em>We need to accept that coordination is <em>hard, </em>and therefore <em>solutions that are expensive are worth considering.</em></p>
<p>Solving coordination problems is not only <em>expensive, </em>it&#8217;s also often <em>highly unpleasant </em>and non-intuitively <em>difficult </em>work, that superficially doesn&#8217;t look like the &#8216;real work.&#8217; Thus, those who solve coordination problems often end up resented as those who didn&#8217;t do the real work and are pulling down the big bucks, as something that one should obviously be able to get along without, discouraging and lowering reward and thus discouraging this hard and risky work. Often many people correctly say &#8216;oh, the problem is people can&#8217;t coordinate&#8217; go out to solve it, and make things worse, because the problems are indeed hard, and competition to solve them makes them harder.</p>
<p>If everyone required to successfully hunt a stag can agree on common knowledge of what the stag is, that the stag would be well worth hunting, and how and when the stag is hunted, one could argue that&#8217;s <em>a lot more than half </em>of the battle. The rest is still hard, but relatively hard part <em>really, really </em>should be over. Ninety percent of life after that, one could say, is the virtue of reliably showing up.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>",Zvi,zvi,Zvi,
FaBoqddGarNjwDyZk,Can we use ideas from ecosystem management to cultivate a healthy rationality memespace?,can-we-use-ideas-from-ecosystem-management-to-cultivate-a-1,https://www.lesswrong.com/posts/FaBoqddGarNjwDyZk/can-we-use-ideas-from-ecosystem-management-to-cultivate-a-1,2019-06-13T12:38:42.809Z,36,7,1,False,True,,"<p> <strong>Background: ecosystems management practices for improving community memespaces</strong></p><p>One can model individual human minds, as well as a community of minds, as an “ecosystem” of “memes”. These memes might be things like: </p><ul><li>Bayesian epistemology</li><li>a habit of checking Facebook when one wakes up</li><li>wiggling one’s fingers to indicate agreement with a statement</li><li>prefacing things one says with “My model of this is that...” </li><li>Doing calibration training</li><li>Referring to blog posts in conversation</li><li>Taking silent pauses to think mid-conversation</li></ul><p>Etc. etc. </p><p>Calling this set an “ecosystem”, seems to me to be mechanistically very close to what’s actually going on. At least, this is because:</p><ol type=""1""><li>Memes mutate as they are transmitted between minds</li><li>Memes undergo selection pressure as they are transmitted</li><li>The underlying topology/geography of social, cultural and geographical networks of people influence their spread</li><li>Memes can be in equilibrium with other memes </li><li>Memes can act as “invasive species” </li></ol><p>Now there is an emerging European rationality community, largely driven by efforts from the Prague rationalists. This community imports many memes from the Bay area rationality community. For a high-level, historical examination of this memespace, see <u><a href=""https://juliagalef.com/2017/02/20/map-of-bay-area-memespace/"">Julia Galef’s map of bay area memespace</a></u>.</p><p>At a recent CFAR workshop, we discussed how we can ensure that this interaction is successful. </p><p>Five of us, felt that the ecosystem model carried sufficient mechanistic similarity to the actual situation that it would be helpful to read up on things like: protocols for deliberate introductions of new species into new environments, invasive species regulations and protection programs, pest control, and more. </p><p><strong>Collection of background notes</strong></p><p>We spent 1h researching this. Now the outside view predicts that if we were to leave it at that, the 16-page Google doc would never be used again. Hence we’re experimenting with releasing our notes together with a LessWrong question, in order to allow others to benefit from and build on our progress. </p><p>You can find our notes <u><a href=""https://docs.google.com/document/d/13bGtCz7zwZ00qYWd1xgipKCXh0JIDgNM9kneWkdA2cQ/edit?usp=sharing"">here</a></u>.</p><p>These notes are provided “as is”. I briefly went through them to make them more readable, but apart from that this should not be interpreted as something the authors endorse as being true, and despite originating at a CFAR workshop it is not official CFAR content. </p><p><strong>Open questions</strong></p><p>We’d be interested in using further research to answer questions such as: </p><ul><li>What are warning signs of a memespace/ecosystem being harmed?</li><li>What are best practices for introducing a new meme into a memespace, and what can we learn from actual ecosystems?</li><li>What are some useful models for thinking about this problem?</li></ul>",jacobjacob,bird-concept,Bird Concept,
b5hjucTui3jB43Qba,Episode 6 of Tsuyoku Naritai (the 'becoming stronger' podcast),episode-6-of-tsuyoku-naritai-the-becoming-stronger-podcast,https://www.lesswrong.com/posts/b5hjucTui3jB43Qba/episode-6-of-tsuyoku-naritai-the-becoming-stronger-podcast,2019-06-12T23:31:06.803Z,3,1,0,False,False,,"<p>Sorry for the delay; I fell victim to the planning fallacy. Again.</p><p><a href=""https://anchor.fm/tsuyokunaritai/episodes/Episode-6---Design-e4arua"">https://anchor.fm/tsuyokunaritai/episodes/Episode-6---Design-e4arua</a></p>",Senarin,senarin,Bae's Theorem,
qXwCz2J4e2SGhaMQk,What kind of thing is logic in an ontological sense?,what-kind-of-thing-is-logic-in-an-ontological-sense-1,https://www.lesswrong.com/posts/qXwCz2J4e2SGhaMQk/what-kind-of-thing-is-logic-in-an-ontological-sense-1,2019-06-12T22:28:47.443Z,12,5,10,False,True,,"<p>The existence of logic seems somewhat mysterious. It&#x27;s this thing that seems to exist, but unlike other things that exist, it doesn&#x27;t seem to exist anywhere in specific or in any tangible form. Further, while it is easy to mock Plato for mysticism when he posits perfect forms existing in some kind of mysterious Platonic Realm, that&#x27;s actually uncomfortably close to a description of what logic is often seen as.</p>",Chris_Leong,chris_leong,Chris_Leong,
pLZ3bdeng4u5W8Yft,"Let's talk about ""Convergent Rationality""",let-s-talk-about-convergent-rationality-1,https://www.lesswrong.com/posts/pLZ3bdeng4u5W8Yft/let-s-talk-about-convergent-rationality-1,2019-06-12T21:53:35.356Z,44,13,33,False,False,,"<p><strong>What this post is about: </strong>I&#x27;m outlining some thoughts on what I&#x27;ve been calling &quot;convergent rationality&quot;.  I think this is an important core concept for AI-Xrisk, and probably a big crux for a lot of disagreements.  <strong><u>It&#x27;s going to be hand-wavy!</u></strong>  It also ended up being a lot longer than I anticipated.  </p><p><strong>Abstract:</strong> Natural and artificial intelligences tend to learn over time, becoming more intelligent with more experience and opportunity for reflection.  Do they also tend to become more &quot;rational&quot; (i.e. &quot;<a href=""https://arbital.com/p/consequentialist/"">consequentialist</a>&quot;, i.e. &quot;agenty&quot; in CFAR speak)?  Steve Omohundro&#x27;s <a href=""https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf"">classic 2008 paper</a> argues that they will, and the &quot;<a href=""https://www.lesswrong.com/posts/FTpPC4umEiREZMMRu/disambiguating-alignment-and-related-notions-1"">traditional AI safety view</a>&quot; and MIRI seem to agree.  But I think this assumes an AI that already has a certain sufficient &quot;level of rationality&quot;, and it&#x27;s not clear that all AIs (e.g. supervised learning algorithms) will exhibit or develop a sufficient level of rationality.  <a href=""https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section2"">Deconfusion research</a> around convergent rationality seems important, and we should strive to understand the conditions under which it is a concern as thoroughly as possible.</p><p><strong>I&#x27;m writing this for at least these 3 reasons:</strong></p><ul><li>I think it&#x27;d be useful to have a term (&quot;convergent rationality&quot;) for talking about this stuff. </li><li>I want to express, and clarify, (some of) my thoughts on the matter.</li><li>I think it&#x27;s likely a crux for a lot of disagreements, and isn&#x27;t widely or quickly recognized as such.  Optimistically, I think this article might lead to significantly more clear and productive discussions about AI-Xrisk strategy and technical work.</li></ul><p><strong>Outline:</strong></p><ul><li>Characterizing convergent rationality</li><li>My impression of attitudes towards convergent rationality</li><li>Relation to capability control</li><li>Relevance of convergent rationality to AI-Xrisk</li><li>Conclusions, some arguments pro/con convergent rationality</li></ul><h2>Characterizing convergent rationality</h2><p>Consider a supervised learner trying to maximize accuracy.  The <a href=""https://en.wikipedia.org/wiki/Bayes_error_rate"">Bayes error rate</a> is typically non-0, meaning it&#x27;s not possible to get 100% test accuracy just by making better predictions.  If, however, <a href=""https://drive.google.com/uc?export=download&id=1k93292JCoIHU0h6xVO3qmeRwLyOSlS4o"">the test data(/data distribution) were modified</a>, for example to only contain examples of a single class, the learner could achieve 100% accuracy.  If the learner were a consequentialist with accuracy as its utility function, it would prefer to modify the test distribution in this way in order to increase its utility.  Yet, even when given the opportunity to do so, typical gradient-based supervised learning algorithms do not seem to pursue such solutions (at least in my personal experience as an ML researcher).  </p><p>We can view the supervised learning algorithm as either ignorant of, or indifferent to, the strategy of modifying the test data.  But we can also this behavior as a failure of rationality, where the learner is &quot;irrationally&quot; averse or blind to this strategy, by construction.  A strong version of <strong>the convergent rationality thesis (CRT)</strong> would then predict that given sufficient capacity and &quot;optimization pressure&quot;, the supervised learner would &quot;become more rational&quot;, and begin to pursue the &quot;modify the test data&quot; strategy.  (I don&#x27;t think I&#x27;ve formulated CRT well enough to really call it a thesis, but I&#x27;ll continue using it informally).</p><p>More generally, CRT would imply that deontological ethics are not stable, and deontologists must converge towards consequentialists.  <em>(As a caveat, however, note that in general environments, deontological behavior can be described as optimizing a (somewhat contrived) utility function (grep &quot;existence proof&quot; in <a href=""https://arxiv.org/pdf/1811.07871.pdf"">the reward modeling agenda</a>)).  </em>The alarming implication would be that we cannot hope to build agents that will not develop instrumental goals.</p><p><strong><u>I suspect this picture is wrong.</u></strong>  At the moment, the picture I have is: imperfectly rational agents will sometimes seek to become more rational, but there may be limits on rationality which the &quot;self-improvement operator&quot; will not cross.  This would be analogous to the limit of ω which the &quot;add 1 operator&quot; approaches, but does not cross, in the ordinal numbers.  In other words, order to reach &quot;rationality level&quot; ω+1, it&#x27;s necessary for an agent to already start out at &quot;rationality level&quot; ω.  A caveat: I think &quot;rationality&quot; is not uni-dimensional, but I will continue to write as if it is.  </p><h2>My impression of attitudes towards convergent rationality</h2><p>Broadly speaking, MIRI seem to be strong believers in convergent rationality, but their reasons for this view haven&#x27;t been very well-articulated (TODO: except the inner optimizer paper?).  AI safety people more broadly seem to have a wide range of views, with many people disagreeing with MIRI&#x27;s views and/or not feeling confident that they understand them well/fully.</p><p>Again, broadly speaking, machine learning (ML) people often seem to think it&#x27;s a confused viewpoint bred out of anthropomorphism, ignorance of current/practical ML, and paranoia.  People who are more familiar with evolutionary/genetic algorithms and artificial life communities might be a bit more sympathetic, and similarly for people who are concerned with feedback loops in the context of algorithmic decision making.</p><p>I think a lot of people with working on ML-based AI safety consider convergent rationality to be less relevant than MIRI does, because 1) so far it is more of a hypothetical/theoretical concern, whereas we&#x27;ve done a lot of and 2) current ML (e.g. deep RL with bells and whistles) seems dangerous enough because of known and demonstrated specification and robustness problems (e.g. reward hacking and adversarial examples).</p><p>In the many conversations I&#x27;ve had with people from all these groups, I&#x27;ve found it pretty hard to find concrete points of disagreement that don&#x27;t reduce to differences in values (e.g. regarding long-termism), time-lines, or bare intuition.  I think &quot;level of paranoia about convergent rationality&quot; is likely an important underlying crux.</p><h2>Relation to capability control</h2><p>A plethora of naive approaches to solving safety problems by limiting what agents can do have been proposed and rejected on the grounds that advanced AIs will be smart and rational enough to subvert them.  Hyperbolically, the traditional AI safety view is that &quot;<a href=""https://www.lesswrong.com/posts/398Swu6jmczzSRvHy/superintelligence-13-capability-control-methods"">capability control</a>&quot; is useless.  Irrationality can be viewed as a form of capability control.  </p><p>Naively, approaches which deliberately reduce an agent&#x27;s intelligence or rationality should be an effective form of capability control method (I&#x27;m guessing that&#x27;s a proposal in the <a href=""https://arxiv.org/abs/1808.03644"">Artificial Stupidity</a> paper, but I haven&#x27;t read it).  If this were true, then we might be able to build very intelligent and useful AI systems, but control them by, e.g. <a href=""http://www.tinyurl.com/bomai-anon"">making them myopic</a>, or restricting the hypothesis class / search space.  This would reduce the &quot;burden&quot; on technical solutions to AI-Xrisk, making it (even) more of a global coordination problem.</p><p>But CRT suggests that these methods of capability control might fail unexpectedly.  There is at least one example (I&#x27;ve struggled to dig up) of a memory-less RL agent learning to encode memory information in the state of the world.  More generally, agents can recruit resources from their environments, implicitly expanding their intellectual capabilities, without actually &quot;self-modifying&quot;.</p><h2>Relevance of convergent rationality to AI-Xrisk</h2><p>Believing CRT should lead to higher levels of &quot;paranoia&quot;.  Technically, I think this should lead to more focus on things that look more like <a href=""https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1"">assurance (vs. robustness or specification)</a>.  Believing CRT should make us concerned that non-agenty systems (e.g. trained with supervised learning) might start behaving more like agents.</p><p>Strategically, it seems like the main implication of believing in CRT pertains to situations where we already have fairly robust global coordination and a sufficiently concerned AI community.  CRT implies that these conditions are not sufficient for a good prognosis: even if everyone using AI makes a good-faith effort to make it safe, if they mistakenly don&#x27;t believe CRT, they can fail.  So we&#x27;d also want the AI community to behave as if CRT were true unless or until we had overwhelming evidence that it was not a concern.</p><p>On the other hand, disbelief in CRT shouldn&#x27;t allay our fears overly much; AIs need not be hyperrational in order to pose significant Xrisk.  For example, we might be wiped out by something more &quot;grey goo&quot;-like, i.e. an AI that is basically a policy hyperoptimized for the niche of the Earth, and doesn&#x27;t even have anything resembling a world(/universe) model, planning procedure, etc.  Or we might create AIs that are like superintelligent humans: having many cognitive biases, but still agenty enough to thoroughly outcompete us, and considering lesser intelligences of dubious moral significance.</p><h2>Conclusions, some arguments pro/con convergent rationality</h2><p>My impression is that intelligence (as in IQ/g) and rationality are considered to be only loosely correlated.  My current model is that ML systems become more intelligent with more capacity/compute/information, but not necessarily more rational.  If this is true, is creates exciting prospects for forms of capability control.  On the other hand, if CRT is true, this supports the practice of modelling all sufficiently advanced AIs as rational agents.</p><p>I think the main argument against CRT is that, from an ML perspective, it seems like &quot;rationality&quot; is more or less a design choice: we can make agents myopic, we can hard-code flawed environment models or reasoning procedures, etc.The main counter-arguments arise from VNMUT, which can be interpreted as saying &quot;rational agents are more fit&quot; (in an evolutionary sense).  At the same time, it seems like the complexity of the real world (e.g. physical limits of communication and information processing) makes this a pretty weak argument.  Humans certainly seem highly irrational, and distinguishing biases and heuristics can be difficult.</p><p>A special case of this is the &quot;inner optimizers&quot; idea.  The strongest argument for inner optimizers I&#x27;m aware of goes like: &quot;the simplest solution to a complex enough task (and therefor the easiest for weakly guided search, e.g. by SGD) is to instantiate a more agenty process, and have it solve the problem for you&quot;.  The <strong>&quot;inner&quot;</strong> part comes from the postulate that a complex and flexible enough class of models will instantiate such a agenty process internally (i.e. using a subset of the model&#x27;s capacity).  I currently think this picture is broadly speaking correct, and is the third major (technical) pillar supporting AI-Xrisk concerns (along with Goodhart&#x27;s law and instrumental goals).</p><p>The issues with tiling agents also suggest that the analogy with ordinals I made might be stronger than it seems; it may be impossible for an agent to rationally endorse a qualitatively different form of reasoning.  Similarly, while &quot;CDT wants to become UDT&quot; (supporting CRT), my understanding is that it is not actually capable of doing so (opposing CRT) because &quot;you have to have been UDT all along&quot; (thanks to Jessica Taylor for explaining this stuff to me a few years back).</p><p>While I think MIRI&#x27;s work on idealized reasoners has shed some light on these questions, I think in practice, random(ish) &quot;mutation&quot; (whether intentionally designed or imposed by the physical environment) and evolutionary-like pressures may push AIs across boundaries that the &quot;self-improvement operator&quot; will not cross, making analyses of idealized reasoners less useful than they might naively appear.</p><p><em>This article is inspired by conversations with Alex Zhu, Scott Garrabrant, Jan Leike, Rohin Shah, Micah Carrol, and many others over the past year and years.</em> </p>",capybaralet,david-scott-krueger-formerly-capybaralet,David Scott Krueger (formerly: capybaralet),
kXFYEjnLsJr7R9d7Y,The Ninth Atlanta SSC Meetup,the-ninth-atlanta-ssc-meetup-1,https://www.lesswrong.com/events/kXFYEjnLsJr7R9d7Y/the-ninth-atlanta-ssc-meetup-1,2019-06-12T21:02:21.398Z,1,1,0,False,False,,"<br/><p>The next Slate Star Codex Meetup will be held on Saturday June 22nd at 2:00 PM</p><p>It will be at our usual meeting space of </p><p>Hodge Podge Coffee House (the more southerly location)</p><p>720 Moreland Ave SE, </p><p>Atlanta, GA 30316<br/></p><p>No set agenda - we&#x27;ll let things ramble like always.<br/><br/><br/>Please RSVP so I can determine how much space we will need.<br/><br/><br/>Hope to see everyone there!</p>",steve-french,steve-french,Steve French,
uhtpGBKXdi7arBa8Q,MLU: New Blog!,mlu-new-blog,https://www.lesswrong.com/posts/uhtpGBKXdi7arBa8Q/mlu-new-blog,2019-06-12T04:20:37.499Z,16,5,6,False,False,,"<p>I&#x27;m in the process of moving mindlevelup from Wordpress to a new static site hosted by Netlify. I like this because now I have more control over scripts and the visuals. It also finally puts into place my goal of having a place to have short/longform posts. </p><p>The plan is to slowly update posts and incrementally update the site.</p><p>The new site is <a href=""https://mlu.red/"">here</a>.</p><p>My short-form blog, Muse, has also been moved <a href=""https://mlu.red/muse/"">here</a>.</p><p>RSS feeds for both main-line blog posts and short-form posts can be found the <a href=""https://mlu.red/about.html"">About</a> page.</p>",,,,
euE7MT4yuRDDhNK57,The Outsider and the Onlooker (on esoteric meaning in fiction),the-outsider-and-the-onlooker-on-esoteric-meaning-in-fiction,https://www.lesswrong.com/posts/euE7MT4yuRDDhNK57/the-outsider-and-the-onlooker-on-esoteric-meaning-in-fiction,2019-06-11T20:02:41.386Z,4,2,0,False,False,,"<br/><p>“<em>Unhappy is he to whom the memories of childhood bring only fear and sadness. Wretched is he who looks back upon lone hours in vast and dismal chambers with brown hangings and maddening rows of antique books, or upon awed watches in twilight groves of grotesque, gigantic, and vine-encumbered trees that silently wave twisted branches far aloft. Such a lot the gods gave to me—to me, the dazed, the disappointed; the barren, the broken. And yet I am strangely content, and cling desperately to those sere memories, when my mind momentarily threatens to reach beyond to the other.</em> <em>” H.P. Lovecraft, The Outsider</em></p><p>The Outsider is, in many ways, the most remarkable short story that H.P. Lovecraft wrote. Certainly he has produced quite a few other interesting and elegant very short stories (<em>Dagon, The Statement of Randolph Carter </em>and<em> The Transition of Juan Romero </em>come to mind), yet the Outsider follows a different, possibly unique structure.  </p><p>Many readers focus on the ending, where the nature of the protagonist is revealed. To be sure it is memorable, although perhaps not entirely unexpected. What I always found far more striking, and what made me love this piece immediately, was what had been clarified half-way through the story: the <em>whereabouts</em> of the protagonist.</p><p>The Outsider had spent innumerable years in a desolate and morbid castle. He finally decides to risk climbing on the circular wall of the tallest tower, hoping that he may get to rise above the ominous forest he so despises, and for once in his life see the light of day… Lovecraft carefully prevents the reader from wondering whether this continuous rise to dizzying heights is somehow not what it was made out to be, so we indeed share the sense of wonder and surprise the Outsider has when we get to understand just what was above the terrible forest and the eternal night of the area with the castle. The aforementioned environment, with its imposingly tall and dense forest, the ancient fort and moat and the silent labyrinth of shadows beyond, was left behind by the hapless narrator – yet what he wished for came at the price of a horrible realization: His entire personal realm was not part of the actual world, but a subterranean, <em>chthonic</em> region.</p><p>At first I was impressed by the revelation itself. Later on, though, I did focus on what it connoted. It is known that Lovecraft wished to weave a narration of <em>cosmic horror</em>, that is horror stories where the cause of alarm isn’t tied to psychological reasons or mental illness. Of course he was entirely aware of the fact that the very sense of horror rests on the depths of our mental world and the unexamined, deep emotions and other mental phenomena which are seated there and which rarely are to rise above the surface and become to some degree conscious. In letters to his fellow writers, as well as in his treatise on Weird Literature, he refers to the inherent dependence of the “cosmic horror” narrative in regards to the dark ocean of unknowns we inevitably host in subterranean caverns of our psyche. There is, therefore, good reason to suspect that in essence the revelation about the world the Outsider comes from is tied to the deep depression and decade-long isolation of Lovecraft himself from society.  </p><p>There is an alluring image in a prose-poem by the celebrated Constantine Cavafy (1863-1933), titled “The ships”, where an observer in a dock happens to see a number of splendid ships filled with treasures. The poet explains that those ships symbolize the goods brought from the realm of imagination; and in most docks one gets to see only a few well-built vessels carrying notable merchandise. Indeed, most ships that get to arrive at our docks won’t be very exceptional; perhaps one or two might bring a treasure which is worth commemorating in a story. And his poem ends with the statement that there exist, moreover, other types of ships, ships which are so rare and carry commodities of such mythical value that we can never hope to see one even near our dock and may only aspire to listen to the enchanting songs of the sailors on those rarest of ships coming from the deepest realms of our mental world.</p><p>Much like the person standing in that dock, Lovecraft too managed to commemorate the arrival of at least a number of rare and beautiful ships from the uncharted territories of pananthopic imagination. And he also spoke and wrote at great length about the quest a writer should have, which is to remain vigilant and prepare for the treasures of the mind; those treasures which – with a little bit of luck! – may at some time reveal themselves to the persistent onlooker.</p><p>By Kyriakos Chalkopoulos - <a href=""https://www.patreon.com/Kyriakos"">https://www.patreon.com/Kyriakos</a></p>",KyriakosCH,kyriakosch,KyriakosCH,
AdwAqk9xYmKWkppbC,How much does neatly eating matter? What about other food manners? ,how-much-does-neatly-eating-matter-what-about-other-food-1,https://www.lesswrong.com/posts/AdwAqk9xYmKWkppbC/how-much-does-neatly-eating-matter-what-about-other-food-1,2019-06-11T19:40:45.539Z,8,9,6,False,True,,"<p>I just read this <a href=""https://reverseincentives.wordpress.com/2019/06/11/the-art-of-sandwich-eating/"">post</a> about the importance of sandwhich eating skill. The author describes how his investment firm served very hard to eat food to potential clients. </p><blockquote>During my internship, a Prestigious Private Equity Firm was looking to improve its stock price/shareholder base. So a delegation of higher-ups (the COO/CFO/Head of IR/Head of Legal) went around pitching their stock to institutions that they thought might be interested in buying and holding it for a long time. These sorts of meetings were a fairly common occurrence at my old firm—we’d have perhaps an average of three or four a week—and would sometimes be catered.</blockquote><blockquote>The meeting with PPEF was catered. The meal seemed like an almost-intentional<a href=""https://reverseincentives.wordpress.com/2019/06/11/the-art-of-sandwich-eating/#_edn2"">[ii]</a>selection of food items that are difficult to consume in a professional setting—sandwiches with way too much mayo, kettle-cooked potato chips (the extra crunchy kind), and chocolate chip cookies that crumbled if you bit them. There were napkins, but there were not enough napkins.</blockquote><blockquote>The people from my firm almost uniformly avoided the food. A few nibbled carefully on the cookies; only one—a portfolio manager with a fierce intellect and a lack of regard for what others might think of his presentation—dared eat a sandwich. Much like any normal human would<a href=""https://reverseincentives.wordpress.com/2019/06/11/the-art-of-sandwich-eating/#_edn3"">[iii]</a>, he went through several napkins and looked rather undignified at times. (Though this was unimportant, because he was the one who would decide if PPEF would get the investment it wanted.) I of course ate nothing, because I was an intern focused on taking good notes and not appearing overly intimidated.</blockquote><blockquote>All four PPEF delegates ate every food item we provided—to do otherwise might have been rude. What’s more, they did it with a shocking amount of grace. Chips seemed not to crunch; any filling that threatened to escape a sandwich was carefully corralled. Napkins were almost unnecessary and were fastidiously refolded if used.  In their manners and mannerisms, the PPEF delegates were precise and uniform. None of this appeared to take any attention. It all looked as natural as breathing. In fact, though food was surely being eaten, it almost seemed that <em>they</em> were not eating at all. When they later typed on their iPads—while making frequent eye contact with everyone across the table—their fingers did not so much as smudge the glass.</blockquote><p>The author correctly points out that the executives must have been selected for sandwhich eating skill. Obviously it is bad to leave food on your face/teeth or have bad breath. I don&#x27;t do those things but I tend to eat in a pretty messy way. How much is that going to hurt me if I stay in software? What if I move to other fields? </p>",deluks917,deluks917,sapphire,
5nH5Qtax9ae8CQjZ9,"Tal Yarkoni: No, it's not The Incentives—it's you",tal-yarkoni-no-it-s-not-the-incentives-it-s-you,https://www.lesswrong.com/posts/5nH5Qtax9ae8CQjZ9/tal-yarkoni-no-it-s-not-the-incentives-it-s-you,2019-06-11T07:09:16.405Z,91,42,119,False,False,https://www.talyarkoni.org/blog/2018/10/02/no-its-not-the-incentives-its-you/,"<p>Neuroscientist Tal Yarkoni denounces many of his colleagues' tendency to appeal to publish-or-perish incentives as an excuse for sloppy science (October 2018, ~4600 words). Perhaps read as a complement to our <a href=""https://www.lesswrong.com/posts/45mNHCMaZgsvfDXbw/quotes-from-moral-mazes"">recent</a> <a href=""https://www.lesswrong.com/posts/2Zsuv5uPFPNTACwzg/moral-mazes-and-short-termism"">discussion</a> of <em>Moral Mazes</em>?</p>
",Zack_M_Davis,zack_m_davis,Zack_M_Davis,
MXiGuBiCu3NHaPHrG,"Long Term Future Fund applications open until June 28th
",long-term-future-fund-applications-open-until-june-28th-1,https://www.lesswrong.com/posts/MXiGuBiCu3NHaPHrG/long-term-future-fund-applications-open-until-june-28th-1,2019-06-10T20:39:58.183Z,30,10,9,False,False,,"<p>The Long Term Future just reopened its applications. You can apply here:</p><p><strong><a href=""https://forms.gle/uvqAerU5zjAA6kay8"">Apply to the Long Term Future Fund</a></strong></p><p>We will from now on have rolling applications, with a window of about 3-4 months between responses. The application window for the coming round will end on the <strong>28th of June 2019.</strong> Any application received after that will receive a response around four months later during the next evaluation period (unless it indicates that it is urgent, though we are less likely to fund out-of-cycle applications).</p><p>We continue to be particularly interested in small teams and individuals that are trying to get projects off the ground, or that need less money than existing grant-making institutions are likely to give out (i.e. less than ~$100k, but more than $10k, since we can’t give grants below $10k). Here are some concrete examples:</p><ul><li>To spend a few months (perhaps during the summer) to research an open problem in AI alignment or AI strategy and produce a few blog posts or videos on their ideas</li><li>To spend a few months building a web app with the potential to solve an operations bottleneck at x-risk organisations</li><li>To spend a few months up-skilling in a field to prepare for future work (e.g. microeconomics, functional programming, etc)</li><li>To spend a year testing an idea that has the potential to be built into an org</li></ul><p>You are also likely to find reading the writeups of our past grant decisions valuable to help you decide whether your project is a good fit:</p><ul><li><a href=""https://forum.effectivealtruism.org/posts/sTvepoxCDMgskGDDN/long-term-future-fund-november-grant-decisions"">November 2018 writeup</a></li><li><a href=""https://forum.effectivealtruism.org/posts/CJJDwgyqT4gXktq6g/long-term-future-fund-april-2019-grant-recommendations"">April 2019 writeup</a></li></ul><p><em><a href=""https://forms.gle/uvqAerU5zjAA6kay8"">Apply Here</a></em></p><p><strong>What kind of applications can we fund?</strong></p><p>After last round, CEA clarified what kinds of grants we are likely able to make, which includes the vast majority of applications we have received in past rounds. In general you should err on the side of applying, since I think it is very likely we will be able to make something work. However, because of organizational overhead we are more likely to fund applications to registered charities and less likely to fund projects that require complicated arrangements to be compliant with charity law.</p><p>For grants to individuals, we can definitely fund the following types of grants:</p><ul><li>Events/workshops</li><li>Scholarships</li><li>Self-study</li><li>Research project</li><li>Content creation</li><li>Product creation (eg: tool/resource that can be used by community)</li></ul><p>We will likely not be able to make the following types of grants:</p><ul><li>Grantees requesting funding for a list of possible projects</li><ul><li>In this case, we would fund only a single project of the proposed ones. Feel free to apply with multiple projects, but we will have to reach out to confirm a specific project.</li></ul><li>Self-development that is not directly related to community benefit</li><ul><li>In order to make grants the public benefit needs to be greater than the private benefit to any individual. So we cannot make grants that focus on helping a single individual in a way that isn’t directly connected to public benefit.</li></ul></ul><p>If you have any questions about the application process or other questions related to the funds, feel free to submit them in the comments. You can also contact me directly under <a href=""(ealongtermfuture@gmail.com)."">(ealongtermfuture@gmail.com).</a></p>",habryka4,habryka4,habryka,
p9PM2Wc3CSvLKRMwG,Get Rich Real Slowly,get-rich-real-slowly,https://www.lesswrong.com/posts/p9PM2Wc3CSvLKRMwG/get-rich-real-slowly,2019-06-10T17:51:32.654Z,35,19,13,False,False,,"<html><head></head><body><p><em>Cross-posted from <a href=""https://putanumonit.com/2019/06/03/get-rich-real-slowly/"">Putanumonit</a>.</em></p>
<p>Some of you have read&nbsp;<em><a href=""https://putanumonit.com/2017/02/10/get-rich-slowly/"">Get Rich Slowly</a></em> and thought: no, this is not slow enough. The dizzying pace of 6-7% annual return is too much. You want more safety and less volatility,&nbsp;something towards the bottom-left corner of the <a href=""https://www.youtube.com/watch?v=vnAbsNN3SbA"">efficient frontier</a>.</p>
<p><img src=""https://putanumonit.files.wordpress.com/2019/06/efficient-frontier.jpg?w=900"" alt=""""></p>
<p>Concrete example: you have $10,000 today, and you may want to spend them sometime in the next 2-3 years on a car, a <a href=""https://www.moneyunder30.com/how-to-take-a-mini-retirement-and-why-you-should"">mini-retirement</a>, or just an emergency. You would like there to be more than $10,000 when you need the money, but what you really want is to be confident that there wouldn’t be any less than that. Emerging market stock index funds can turn $10,000 to $15,000 in two years, or to $6,000. You just want there to be $10,500.</p>
<p>This post will briefly cover the basics of low-risk-low-return saving, with general principles and particular examples. With apologies to my international readers, the examples are all USA-specific. The general approach, however, should be easily transferrable – and you’ll know what scams to watch out for.</p>
<h2>“Risk-free” rate</h2>
<p>The benchmark for the return rate on low-risk investments is the federal funds rate, which is currently set by the Federal Reserve at 2.5% <sup class=""footnote-ref""><a href=""#fn-jfq6qYpKJ2LTYWLnL-1"" id=""fnref-jfq6qYpKJ2LTYWLnL-1"">[1]</a></sup>. This is the rate at which big institutional banks borrow dollars <sup class=""footnote-ref""><a href=""#fn-jfq6qYpKJ2LTYWLnL-2"" id=""fnref-jfq6qYpKJ2LTYWLnL-2"">[2]</a></sup> from each other and from the central bank overnight. This number also tracks very closely the rate at which the <a href=""https://www.stlouisfed.org/on-the-economy/2017/october/increases-fed-funds-rate-impact-other-interest-rates"">US government borrows money</a> for one year.</p>
<p><img src=""https://putanumonit.files.wordpress.com/2019/05/fed-funds-treasury.png"" alt=""""></p>
<p>The chance of any bank going bankrupt overnight or the US government defaulting on its debt within 1 year are both very close to zero, which is why the federal funds rate is often referred to as the “risk-free rate of return”. Of course, nothing is ever truly risk-free when finance is concerned. “Risk-free” is a euphemism for “this won’t blow up unless the entire rest of the financial system blows up as well, and at that point, you should care about your stocks canned food more than about your dollar savings.”</p>
<p>So, big banks can borrow and lend “safely” at 2.5%. Let’s see what normal schlubs like us can get when we go to the big banks ourselves.</p>
<h2>Checking and Savings Accounts</h2>
<p>There are roughly 70,000 retail bank branches in the United States. A third of them belong to the biggest 5 retail banks: Wells Fargo, JP Morgan Chase, Bank of America, US Bank, and PNC. Either one would be happy to open you a checking account – a simple account where your money is insured by the government, accessible from any ATM and online, and earns 0% interest.</p>
<p>When the bank sees that your pockets are bulging with 100 Benjamins, they will offer to open you a savings account as well. Savings accounts usually have limitations such as a minimum balance needed to open, a cap on monthly transfers, and fees. On the plus side, your money will earn an astonishing interest rate of… <a href=""https://www.bankofamerica.com/deposits/savings/savings-accounts/"">0.03%</a>.</p>
<p><img src=""https://putanumonit.files.wordpress.com/2019/06/bofa-savings.png?w=900"" alt=""""></p>
<p>That’s right, at the end of two years in a Bank of America savings account your $10,000 will accrue six whole dollars! If you reach the Platinum Honors Tier, which requires an endless amount of bureaucratic hoops to jump through and also sacrificing your firstborn to <a href=""https://en.wikipedia.org/wiki/Classification_of_demons#Binsfeld.27s_classification_of_demons"">Mammon the prince of Hell</a>, they will toss in an extra $6 and a lollipop.</p>
<p>And then, they’ll charge you $192 in monthly service fees.</p>
<p><strong>Actual rate of return:</strong> <em>negative</em> 2%.</p>
<p><strong>Scam meter:</strong> totally a scam.</p>
<h2>Certificates of Deposit at Big Banks</h2>
<p>0.03% interest will double your money in a mere 2,310 years, not counting taxes. If that’s a bit too slow, banks will offer you a <a href=""https://www.investopedia.com/terms/c/certificateofdeposit.asp"">certificate of deposit</a>, or CD. CDs offer a higher rate of return in exchange for placing more limits on your money. CDs have a set maturity date and withdrawing money prior to that date incurs a penalty. In the few places I’ve checked, the penalty is about a quarter of the total interest that would be earned for the full term.</p>
<p>CD rates at the big banks vary from 0.1% at Bank of America to 2% at Wells Fargo if you lock the money down for two years. So $10k in a Wells Fargo CD will turn into $10,201 after two years, or $10,050 if the money is withdrawn after one year: $100 of interest minus $50 in early withdrawal penalty.</p>
<p>There may also be fees associated, and interest on CDs is taxed as income. Marginal income tax rates are between 22%-37% for Americans earning $38,000 or more, so even in the best case scenario a two year CD will only net $201 * (1-.22) = $157.</p>
<p>**Actual rate of return:&nbsp;**0-1.5%.</p>
<p><strong>Scam meter:</strong>&nbsp;only a bit of a scam.</p>
<h2>Savings Accounts and CDs at New Banks</h2>
<p>Nerdwallet has <a href=""https://www.nerdwallet.com/rates?active_offers=true&amp;bank_type=bank&amp;bank_type=internet_bank&amp;deposit_minimum=10000&amp;display_page=TOOLS&amp;length_of_term=720&amp;min_ratings&amp;page=1&amp;sort_key=apy&amp;sort_order=desc&amp;sub_product_type=CD&amp;sub_product_type=CHECKING&amp;sub_product_type=MONEY_MARKET&amp;sub_product_type=SAVINGS&amp;zip_code=94103"">a list of rates</a> offered on CDs and savings accounts at various institutions. The bigger established banks are clustered towards the end of the list, while the top of it is populated by smaller, newer, and online-only banks looking to aggressively grow their customer base. Some of these banks offer 2% on savings accounts and 2.6-2.7% on CDs. If you think there’s a chance you may need the money before the CD matures, the two options are probably equivalent in terms of expectancy.</p>
<p>In any case, if you want a high-yield account with a bank it makes sense to shop around for a young bank desperate for love, not one of the old fat cats.</p>
<p><strong>Actual rate of return:</strong> 1.5-2% after tax.</p>
<p><strong>Scam meter:</strong>&nbsp;barely a scam at all.</p>
<h2>CD Secured Loans</h2>
<p>If you do open a CD, and especially if you ask about withdrawing the money early, the bank will start marketing to you a miraculous financial product called a CD secured loan. Are you cynical enough to guess what that is?</p>
<p>The bank, via a well-dressed “relationship technician” or a brochure with glossy print, will inform you that while normal bank loans have an interest rate of 10-12%, you can get a loan&nbsp;at interest rates of just 4-8% as long as it’s fully secured by your CD. In case it’s not clear: the bank will give&nbsp;<em>your own money</em> back to you, with <em>zero</em> risk to the bank itself (since the loan is fully collateralized), while charging you 2-6% interest for the pleasure.</p>
<p>And if you ask why on Earth you would pay 6% interest on your own money when you can just withdraw it and pay 0% at worst, the bank will tell you about the wonders it will do for your credit score <sup class=""footnote-ref""><a href=""#fn-jfq6qYpKJ2LTYWLnL-3"" id=""fnref-jfq6qYpKJ2LTYWLnL-3"">[3]</a></sup>. At this point I recommend shouting <em>“Begone, demon!”</em> at the top of your lungs and running out of the bank branch while your soul is still intact.</p>
<p><strong>Actual rate of return:</strong> <em>negative</em> 6%.</p>
<p><strong>Scam meter:</strong>&nbsp;Shameless and disgusting scam. I wrote 5,500 words <a href=""https://putanumonit.com/2018/12/14/defense-of-finance/"">defending the finance industry</a>&nbsp;but then added a caveat: finance turns bad when it <a href=""https://putanumonit.com/2019/01/01/finance-followups/#turkeys"">collides with the astounding financial illiteracy</a> of the average American. CD secured loans are as bad an example of this as I know. It’s a financial product designed solely for people who are easily persuadable, financially ignorant, and flunked middle school math.</p>
<h2>Index Funds</h2>
<p>By and large, the best tool for low-yield investments is the same as the best tool for high-yield investments: index funds. Instead of lending money to a single bank, bond ETFs (exchange traded funds, the easiest way to invest in indices) allow you to buy pieces of loans to the US government and other low-risk institutions.</p>
<p>Because of their equity-like structure, the value of ETFs is a bit more volatile day-to-day but on a scale of a year or more bond ETFs basically replicate the yield of the underlying bonds. If an ETF just keeps buying treasuries that have 2% yield, the ETF will inevitably yield 2%. More importantly, ETF gains are taxed at the capital gains tax rate (15% if held for more than one year) instead of as income tax (22%-37%).</p>
<p>Two great options are Vanguard’s money market fund, <a href=""https://investor.vanguard.com/mutual-funds/profile/portfolio/vmfxx"">VMFXX</a>, and bond fund, <a href=""https://investor.vanguard.com/mutual-funds/profile/portfolio/vbmfx"">VBMFX</a>. VMFXX holds short-term US government treasuries and bank repos, with an expected yield of around 2-2.5%. VBMFX holds two-thirds long-term US treasuries and one-third corporate bonds. The expected return is around 3% today, but the average maturity of the bonds is 8 years which can lead to some short-term divergence between the ETF return and the bond yield if interest rates change.</p>
<p><strong>Actual rate of return:</strong>&nbsp;2-2.5% after taxes and Vanguard’s tiny fees (0-0.15%).</p>
<p><strong>Scam meter:</strong>&nbsp;Basically, everything Vanguard does is the opposite of a scam. RIP <a href=""https://www.cnbc.com/2018/12/14/jack-bogle-founder-of-vanguard-group-and-creator-of-the-index-fund-dies-at-age-89.html"">Jack Bogle</a>, the trillion-dollar real-life Robin Hood.</p>
<h2>Wealthfront</h2>
<p>Investing is a story of trade-offs. High-returns, low volatility, low tail-risk, flexible access to money – you have to pick some and compromise on the others. You can’t have a return higher than the federal funds rate, zero volatility, FDIC insurance, and no-limit access to your cash at any time.</p>
<p>Oh, wait, <a href=""https://www.wealthfront.com/cash"">you totally can with Wealthfront</a>.</p>
<p>The Wealthfront cash account offers 2.51% return with no fees, FDIC insurance up to $1,000,000, and unlimited free transfers. I swear they’re not paying me to recommend them (although I do get a small bonus if you <a href=""http://wlth.fr/1CuZ1ZZ"">use my referral link</a>). I just researched financial brochures for several hours, and then Wealthfront just turned out to be better than every single bank on practically all parameters.</p>
<p>I’m not entirely sure why this is the case. It could be that Wealthfront just has much lower costs, being a small startup with no physical branches or fancy investment managers with MBAs. Perhaps they’re eating through some VC money in the hunt for market share growth. And perhaps, the number of Americans who can actually do the math and figure out the best deal is so small that every big bank would rather spend money on marketing to idiots than on paying their customers actual interest on deposits.</p>
<p>My goal to expand the number of the financially literate, one blog post at a time.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-jfq6qYpKJ2LTYWLnL-1"" class=""footnote-item""><p>All rates in this post are in annualized terms, so 2.5% means 2.5%-a-year. <a href=""#fnref-jfq6qYpKJ2LTYWLnL-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-jfq6qYpKJ2LTYWLnL-2"" class=""footnote-item""><p>Other currencies have different rates set by their respective central banks. <a href=""#fnref-jfq6qYpKJ2LTYWLnL-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-jfq6qYpKJ2LTYWLnL-3"" class=""footnote-item""><p>The credit score system is itself a meta-scam. It’s a way for financial institutions to sucker you into paying interest on debt, which is good for them and bad for you. You can build up an OK credit score by doing sensible things like opening a few good credit cards and paying them off every month with no interest. But then, once you’re emotionally invested in the system, the only way to improve your score is to take on debt with interest.</p>
<p>Spoiler alert: the way to pay less interest is to pay less interest, not to pay <em>more</em> interest in order to “build up your credit score”. <a href=""#fnref-jfq6qYpKJ2LTYWLnL-3"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
</body></html>",Jacobian,jacob-falkovich,Jacob Falkovich,
7pSzHo34PKk2aJtZD,Honors Fuel Achievement,honors-fuel-achievement,https://www.lesswrong.com/posts/7pSzHo34PKk2aJtZD/honors-fuel-achievement,2019-06-10T16:10:01.002Z,38,15,2,False,False,,"<p><em>This is an excerpt from the draft of</em> <em><u><a href=""http://samoburja.com/gft"">my upcoming book</a></u></em> <em>on great founder theory. It was originally published on SamoBurja.com. You can</em> <em><u><a href=""http://samoburja.com/honors-fuel-achievement/"">access the original here.</a></u></em></p><p>It is a cherished dream for many people to win a Nobel Prize, or an Oscar, or a knighthood, or whatever honor is most respected in the field they dedicate themselves to. These ritualized honors are very important to us, but do we fully understand them?</p><p>We usually think honors are about the recipient, but the giver of honors also gains. The giver and recipient collaborate to publicly assert that the recipient is worthy of prestige, and that the giver has the authority to grant it. Honors are thus acts of an alliance to mutually boost prestige. </p><p>This meaning is even codified in diplomatic protocol; representatives of countries often exchange honors for the explicit purpose of signalling alliance.</p><p>The audience also participates in this transaction of prestige. They either accept the whole affair and the implied claims of the giver and the recipient, or reject or ignore them. The honors only have meaning&#x2014;and thus the primary parties only gain&#x2014;if the onlookers take them seriously. The performance of honor-giving is a bid for that audience&#x2019;s assent, both the literal immediate audience, as well as the broader public who will hear about the honors bestowed or see them televised.</p><p>The audience accepts the frame because they recognize the preexisting prestige of someone involved. Honors can be prestigious because prestigious people receive them, because prestigious people give them, or both.</p><p>Consider the Nobel Prize in science. Its purpose is to tell the public who the most notable experts in a field are. In other words, it makes the recipient&#x2019;s standing within a given scientific community more visible to the rest of society, fortifying their standing within that particular scientific community in the process. This is a useful service to the scientific community and the public.</p><p>The Nobel Prize has different functions depending on the field in which it is awarded. In the case of the Literature and Peace Prizes, its function is at least partially to advance the political goals of the overseeing organization. Rather than making the existing distribution of prestige more legible, these prizes alter it by granting prestige to the proponents of preferred causes. Looking at a list of Nobel Peace Prize winners leaves an impression of a particular political orientation, but the public story of the prize, from which it gets much of its prestige, is much more neutral. These more political Nobel prizes also derive much of their prestige from the scientific Nobel prizes.</p><p>The Nobel&#x2019;s initial prestige came from the reputation of Alfred Nobel and of the institutions named to oversee the prize (the Swedish Academy, the Royal Swedish Academy of Sciences, the Karolinska Institutet, and the Norwegian Parliament), as well as some money attached to it, which came from the fortune Nobel made by inventing dynamite. Money, however, is a limited source of prestige. The negative connotations of the term &#x201C;nouveau riche&#x201D; reflect this. This begs the question: what, then, are sources of prestige?</p><p><strong>The ruler is the fount of honor</strong></p><p>A ruler is a source of prestige and, moreover, usually the primary source of prestige in a society. This follows naturally from their status as the society&#x2019;s leader, that is, the person who has the highest authority in decision-making, who is deferred to above all. This authority extends to the domain of prestige. For example, Queen Elizabeth I granted minor titles to former pirates, like <a href=""https://en.wikipedia.org/wiki/Sea_Dogs"">Sir Francis Drake and Sir John Hawkins</a>, who helped harass the Spanish and set the course for later English naval domination. King Charles II granted a charter creating the Royal Society, which would play a crucial role in the scientific revolution. By conferring the highest honor in the land on naval warfare and scientific exploration, later mainstays of British power, these may have been the most important decisions these rulers ever made.</p><p>Sometimes the ruler is also the recipient of honor. Comrade Stalin is a genius of literature. And biology. And architecture. Because if he isn&#x2019;t, you go to the gulag. He has a monopoly on violence. He uses this monopoly to monopolize prestige. He can then quite effectively award it, pushing nearly any status system in the direction he chooses to. If he has a good understanding of experts and isn&#x2019;t too afraid of being deposed from his monopoly, he can use his standing to reward excellent generals, scientists, and poets.</p><p>Comrade Stalin, however, has a problem. His authority, the legitimacy of his monopoly on violence, formally rests on him being the Genius of Socialism, and thus on the quality of all those papers. The insecurity of this legitimacy requires him to aggressively prop it up by hoarding prestige.</p><p>Things don&#x2019;t have to be this way. If the legitimacy of Stalin&#x2019;s monopoly on violence was officially grounded in something more secure and more true, he could dispense with biology and geology papers being written in his name. He could dispense with the papers being enshrined as obligatory reading in the relevant fields. He would be not just the monopolist of violence, but the monopolist of legitimacy much more directly. People feel the need to prove themselves where they are insecure. A secure ruler does not need to prove his legitimacy. In turn, a more direct claim of legitimacy is less falsifiable, and thus requires less upkeep and less distortion.</p><p>So while power can be used to create prestige, some ways to do this are more functional, in terms of costing less and having fewer negative side effects, than others. Stalin&#x2019;s elevation of Trofim Lysenko and that biologists rejection of mendelian genetics, was perhaps useful for politically bolstering Stalin&#x2019;s preferred agricultural politics, but set back Soviet genetics by decades as well as contributed to the Great Ukrainian Famine of 1932-1933 and the Great Chinese Famine of  1959-1961. </p><p>A ruler trying to gain standing by playing football is silly, because if he truly is the ruler, people will feel obliged to lose, ruining the game. Of course there are the unwise, like the Roman Emperor Commodus, who fancied himself a gladiator. Commodus always won his fights in the arena, and his subjects viewed his predilection for gladiatorial combat as a disgrace. For rulers trying to gain standing, what remains is the role of the status referee, the one who confers honor across domains. Distortions introduced by having to praise his work are thus reduced. This is one of the most important roles of the ruler: the ruler uses his fount of prestige to regulate overall status and prestige competition, so that the right people and the right behaviors win, solving coordination problems and tragedies of the commons.</p><p>There are brilliant rulers who really might have something to contribute to a field, and some who aren&#x2019;t particularly brilliant but wish to engage in hobbies for personal fulfillment. A common practice for both of these kinds of rulers is to be active under assumed identities or proxies, sometimes convincingly, sometimes not. Frederick the Great of Prussia, for example, anonymously published a <a href=""https://archive.org/details/AntiMachiavelFriedericktheGreat/page/n39"">political treatise</a> shortly after assuming the throne. The anonymity prevents the prestige distortions that might come from the ruler visibly competing in one of the domains that he rules over.</p><p>The prestige of rulers and, more generally, the prestige landscape created by power, is the fount from which most other prestige flows. If someone tries to grant prestige out of line with this source, it may not be taken seriously, or may find itself undermined by power. If something is not being taken seriously, power can be applied behind the scenes to promote it until it is.</p><p>For example, after World War II, American officials in the State Department and the CIA wanted to undermine the dominance of pro-Soviet communists in the Western highbrow cultural scene. To do this, they planned to promote artists and intellectuals who were either anti-Soviet or at least not especially sympathetic to the Soviets &#x2014; at the time this was often the best you could do in highbrow circles. They considered abstract expressionist painting, which was then a new and obscure movement, a promising candidate. Though no one would call it patriotic, it was American and it wasn&#x2019;t especially communist.</p><p>In 1946, the State Department organized an international exhibition of abstract painting called &#x201C;Advancing American Art&#x201D;. It was so poorly received that the tour was cancelled and the paintings sold off for next to nothing. Undeterred, the CIA, under a front organization called the Congress for Cultural Freedom, <a href=""https://www.independent.co.uk/news/world/modern-art-was-cia-weapon-1578808.html"">continued to arrange international exhibitions for abstract expressionists</a>. Eventually, the movement caught on. It would be an oversimplification to say that the CIA made abstract expressionism famous&#x2014;there were other influential promoters, like the critic Clement Greenberg&#x2014;but their support was not irrelevant.</p><p>If one looks closely at any society, one will observe that its rulers&#x2014;and their prestige&#x2014;subsidize all other sources of prestige. Thus, when the landscape of power shifts, the landscape of prestige shifts accordingly. It is then critical that rulers are incentivized to allocate prestige well&#x2014;that is, in accordance with the actual distribution of excellence. If they aren&#x2019;t, as in the case of Stalin, the resulting distortions in the allocation of prestige produce distortions in their society&#x2019;s understanding of what is good and what is true. <a href=""https://en.wikipedia.org/wiki/Lysenkoism"">Lysenkoism</a> was an epistemic and moral disaster. This kind of corruption can ultimately have catastrophic effects on the society&#x2019;s health, because the ability to ascertain the truth is fundamental to the functionality of a society&#x2019;s people and its institutions.</p><p><strong>Awards are better than prizes</strong></p><p>Among the many different kinds of honors, we can pick out two especially common ones: those meant to incentivize a particular achievement with a financial reward, which I call prizes, and those meant to afford prestige on the basis of past achievement, which I call awards. Prizes aim to get some specific thing done, whereas awards aim to affect the distribution of prestige, incentivizing achievement in a more indirect way. With a prize, money is fundamental. With an award, it is incidental. The <a href=""https://en.wikipedia.org/wiki/Millennium_Prize_Problems"">Millennium Prizes</a> are a prime example of the former, the Academy Awards of the latter.</p><p>This distinction is often muddled, leading honors to be less effective than they could be. I have to clarify what I mean by each term, because in practice they aren&#x2019;t used in a reliable way. There are awards that are called prizes and prizes that are called awards. Despite its name, the Nobel Prize is a hybrid case that is more of an award. Though it comes with a financial reward, it is primarily about affording prestige, and this is what those who try to win it are after. The money is nice, but the glory is better.</p><p>It&#x2019;s for this reason that I think that awards are more effective than prizes in incentivizing the production of knowledge. Glory is a greater motivator than money. Furthermore, the money attached to prizes is often insufficient for justifying the investment of money, time, energy, social capital, and so on required to achieve the relevant goal.</p><p>A better use of prize money is to directly fund projects aimed at the desired achievement. The venture capitalists of Silicon Valley and grantmakers like the Mercatus Center&#x2019;s <a href=""https://www.mercatus.org/emergentventures"">Emergent Ventures program</a> are good examples. Before any project begins, it&#x2019;s possible to determine which individuals or teams have the best chance of success. Giving them the money beforehand solves the financing problem, and even if success won&#x2019;t make them a fortune, the glory of the achievement -- perhaps augmented by an award -- should be incentive enough.</p><p>A prize also provides less return on its creator&#x2019;s investment of social capital than an award. Once the goal is achieved and the prize won, there is no longer a reason for it to exist. It is self-abolishing. An award, on the other hand, can continue to be given out year after year, compounding the investment of prestige. Recognizing this fact, prize-giving organizations often convert their prizes into awards, contributing to confusion about the distinction.</p><p>The X Prize illustrates some of these flaws. Created by entrepreneur and space enthusiast Peter Diamandis in the 1990s, the prizes are meant to incentivize breakthroughs in solving the world&#x2019;s biggest problems. Their <a href=""https://www.xprize.org/about/about-us"">website</a> says, &#x201C;Rather than throw money at a problem, we incentivize the solution and challenge the world to solve it.&#x201D; Perhaps the most well-known past prize is the Ansari X Prize, which promised a $10 million reward for the creation of a reusable spacecraft. Many of the other X Prizes are also about breakthroughs in space technology. Since their founding, the X Prize has directly collaborated with firms as well-known as Google, IBM&#x2019;s Watson, and Northrop Grumman, and today counts Google co-founder Larry Page on its board of trustees.</p><p>And yet, the great advancements towards space exploration in the past twenty years have had little to do with the X Prize. $10 million is a paltry sum compared to the money required to finance serious efforts in the area, and even less compared to the rewards of success, as SpaceX and Blue Origin have demonstrated. It&#x2019;s safe to say that an X Prize and $10 million played no part in Musk and Bezos&#x2019; motivations. Even the project that won the Ansari Prize had $100 million in financing. Either the prize money wasn&#x2019;t much of an incentive, or the winning team was very confused.</p><p>If it&#x2019;s not really incentivizing breakthroughs, then what is the real use of the X Prize money? It&#x2019;s to garner publicity. The idea of monetary prizes excites our imagination and so lends them virality, and for this narrow purpose the X Prize money has worked. Its creators may understand this, and hope that the publicity brings attention to the relevant problems and so itself incentivizes breakthroughs. The evidence doesn&#x2019;t bear this out, however. The X Prize has garnered its fair share of media coverage, but it has failed to lend massive prestige to the sector of technological innovation, and thus has not institutionalized newly-legible professional communities of practice in the manner that the Nobel prize did. After all, we forget that much of what we think of as the immutably prestigious &#x201C;scientific community,&#x201D; and even the field of professional economics, is a result downstream of such shifts in the landscape of prestige. Imagine how different society would be today if we had a Nobel Prize for technology!  </p><p>While publicity is good, it&#x2019;s even better to be able to <a href=""http://samoburja.com/how-elon-musk-is-making-engineers-cool-again/"">affect the distribution of prestige throughout society</a>. The more closely social status corresponds to activity that&#x2019;s ultimately beneficial for society, the more such activity is incentivized, much more strongly than by even a large financial reward. Wisely distributing status makes the difference between a world where most kids dream of becoming YouTubers and one where they dream of taking us to space.</p><p><em>Read more from Samo Burja</em> <em><u><a href=""http://samoburja.com/essays"">here.</a></u></em></p>",Samo Burja,samo-burja,Samo Burja,
yJanLQZY4jfFk5KkM,On pointless waiting,on-pointless-waiting,https://www.lesswrong.com/posts/yJanLQZY4jfFk5KkM/on-pointless-waiting,2019-06-10T08:58:56.018Z,43,25,6,False,False,,"<p>I’ve often noticed in myself a tendency, if I am not doing something immediately engrossing, to find myself waiting.</p><p>Waiting, waiting, waiting, not really being present, just willing time to pass.</p><p>But the weird thing is, frequently there isn’t anything in particular that I’m waiting <em>for</em>. Getting out of that situation, yes, but I don’t have anything in particular that I’d want to do when I do get out.</p><p>I have a suspicion that this might have to do with mental habits ingrained in school.</p><p>In elementary school, there’s no real <em>goal</em> for your studies. Mostly it’s just coming there, doing the things that teachers want you to do, until the day is over and you get to go.</p><p>In that environment, every minute that passes means winning. Every minute takes you a bit closer to being out of there. That’s the real goal: getting out so you can finally do something fun.</p><p>During a lesson you are waiting for recess, during recess you are waiting for the end of the day. Outside school you are waiting for the weekend, on the weekend you are waiting for the bliss of the long summer leave.</p><p>Waiting, waiting, waiting.</p><p>So you learn to pay attention to the time. Human minds are tuned to feedback, things that let them know how well they are doing. And since each passing minute takes you closer to the goal, the passing of time becomes its own reward.</p><p>Time having passed means that you have achieved something. Time having passed means that you can feel a tiny bit of satisfaction.</p><p>And then that habit, diligently trained for a decade, can carry over to the rest of your life. Even as an adult, you find yourself waiting, waiting, waiting.</p><p>You don’t know what it is that you are waiting for, because you are not really waiting for anything in particular. Even if it would actually be more pleasant to stay engaged with the present moment, you keep tracking the time. Because waiting feels like winning, and every passing minute feels like it takes you closer to your goal.</p><p>Even if you don’t actually know what your goal is. Even if reaching your goal will only give you a new situation where you can again wait, so that you are never actually present.</p><p>Still, you keep waiting, waiting, waiting.</p><p>—</p><p><em>(<strong><u><a href=""https://www.lesswrong.com/posts/baTWMegR42PAsH9qJ/generalizing-from-one-example"">typical mind fallacy</a></u></strong></em> <em>employed for the sake of artistic license; I am describing my own experience, without claiming this to be a universal one)</em></p>",Kaj_Sotala,kaj_sotala,Kaj_Sotala,
W4kdfpvRafKys4nua,Dissolving the zombie argument,dissolving-the-zombie-argument,https://www.lesswrong.com/posts/W4kdfpvRafKys4nua/dissolving-the-zombie-argument,2019-06-10T04:54:54.716Z,-1,5,14,False,False,,"<p><strong>Update</strong>: Upon reflection, I&apos;m not entirely satisfied with this post. I think I definitely managed to identify some of the confusion around these kinds of discussion, but a smaller proportion than I would have liked.</p><p>The Zombie argument (<a href=""http://consc.net/zombies-on-the-web/"">David Chalmer&apos;s website</a>,  <a href=""https://plato.stanford.edu/entries/zombies/"">Stanford Encyclopedia of Philosophy</a>) is one of the most famous arguments against materialism, so I&apos;ll assume that you can find an explanation yourself if you aren&apos;t already familiar with it.</p><p>I always find it fascinating when you have two sides that can&apos;t seem to communicate with or understand one another. I think the root of the problem is that both sides have a different notion of what counts as a zombie. The Dualist Conception of consciousness involves qualia, so their conception of a philosophical zombie is an entity that lacks qualia. This is a notoriously hard term to define - some would say because it is meaningless - but all that matters here is that they have a stricter conception of consciousness that the Materialist. The Materialist Conception of consciousness involves certain information processing taking place, so a Materialist Conception of a zombie would involve these processes taking place, but also not taking place, which would be a contradiction.</p><p>Here&apos;s the confusion. If a someone were to claim that humans don&apos;t fit the Dualist Conception of a zombie and that Materialism is true, they&apos;d be contradicting themselves, because Dualists have a wide conception of what counts as a zombie that all entities in a Materialist world would fit this definition. On the other hand, if someone were to claim that that Materialist Conception of a zombie were logically possible, which is merely to claim that they can posit this without contradiction, they would be mistaken since Materialist&apos;s have such a narrow conception of what would count as a zombie that this class is an empty set.</p><p>Once the definition of what counts as a zombie has been fixed, so too has the outcome of the argument. And this is really contingent on what counts as consciousness, so the Zombie argument isn&apos;t actually where the fundamental difference lies. This isn&apos;t a mere linguistic difference, it&apos;s a question of what <a href=""https://www.lesswrong.com/posts/peCFP4zGowfe7Xccz/natural-structures-and-definitions"">natural structures exist</a> that cry out to be given a label. Or as <a href=""https://www.lesswrong.com/posts/peCFP4zGowfe7Xccz/natural-structures-and-definitions#CxxeFpHHwLoDoWpdx"">Richard Kennaway</a> might frame it, an attempt to understand the nature of a phenomenon which we already have some experience with, without foreclosing the possibility that we might end up tossing away the concept if we find it confused.</p><p>One last clarification: many people find this argument persuasive. In so far as this is the case, it&apos;s usually because they had an inconsistency in their thoughts. For example, perhaps they identified as materialists, without thinking through exactly what a materialist view of consciousness would entail, and when they realised this, they discovered it was something that they didn&apos;t endorse.</p>",Chris_Leong,chris_leong,Chris_Leong,
4LNjrip3p4R6YGDMa,Knights and Knaves,knights-and-knaves,https://www.lesswrong.com/posts/4LNjrip3p4R6YGDMa/knights-and-knaves,2019-06-10T01:51:10.058Z,4,3,1,False,False,,"<p>In the Knights and Knaves riddle you are facing a fork in the road, with one way leading to freedom and the other to death. There are two persons, a knight and a knave. The former always tells the truth while the latter always lies. You got to ask one yes/no question to find your way into freedom.</p><p>One solution is to use truth tables. For example in that the statements of both persons are concatenated together. According to the AND table it does not matter in which order true and false are combinated, the result is false. So if your question goes like »What would the other person say, if I&#x27;d ask him if this way leads to freedom?«, you always get a falsified answer and are able to identify the way into freedom.</p><p>A general assumption for this riddle is that both persons know the truth about whereto the ways lead. That introduces another approach, in that the knave must diversify between inner and outer opinion. To be able to always lie outwardly, he has to know the truth for himself, so his inner opinion is the truth. To take advantage of that, one could ask »Would you say for yourself, that this path leads to freedom?«. This provokes a contradiction in the knave&#x27;s answer and can therefore be spotted. </p><p>Finally a similiar approach that uses the inner opinion is possible too. If both know of the truth, but are still acting differently, this must be on purpose. So in other words, one wants to harm you and the other not. A simpler question would therefore be »Do you want me to go this way?«. The good guy, you can take at his word, because he has your best interests in mind. The bad guy on the other hand would like to send you to 	death, but since he&#x27;s forced to lie, you can take him at his word too.  </p>",filius,filius,filius,
cnBGXGSFGpfvknFc3,"Ramifications of limited positive value, unlimited negative value?",ramifications-of-limited-positive-value-unlimited-negative,https://www.lesswrong.com/posts/cnBGXGSFGpfvknFc3/ramifications-of-limited-positive-value-unlimited-negative,2019-06-09T23:17:37.826Z,10,6,18,False,True,,"<p><em>This assumes you&#x27;ve read some stuff on acausal trade, and various philosophical stuff on what is valuable from the sequences and elsewhere. If this post seems fundamentally confusing it&#x27;s probably not asking for your help at this moment. If it seems fundamentally *confused* and you have a decent sense of why, it *is* asking for your help to deconfuse it.</em></p><p><em>Also, a bit rambly. Sorry.</em></p><p>Recently, I had a realization that my intuition says something like:</p><ul><li>positive experiences can only add up to some finite[1] amount, with diminishing returns</li><li>negative experiences get added up linearly</li></ul><p>[edited to add]</p><p>This seems surprising and confusing and probably paradoxical. But I&#x27;ve reflected on it for a month and the intuition seems reasonably stable.</p><p>I can&#x27;t tell if it&#x27;s more surprising and paradoxical than other various flavors of utilitarianism, or other moral frameworks. Sometimes intuitions are just wrong, and sometimes they&#x27;re wrong but pointing at something useful, and it&#x27;s hard to know in advance. </p><p>I&#x27;m looking to get a better sense of where these intuitions come from and why. My goal with this question is to basically get good critiques or examinations of &quot;what ramifications would this worldview have&quot;, which can help me figure out whether and how this outlook is confused. So far I haven&#x27;t found a single moral framework that seems to capture all my moral intuitions, and in this question I&#x27;m asking for help sorting through some related philosophical confusions.</p><p>[/edit]</p><p>[1] or, positive experiences might be infinite, but a smaller infinity than the negative ones?</p><p>Basically, when I ask myself:</p><p>Once we&#x27;ve done literally all the things – there are as many humans or human like things that could possibly exist, having all the experiences they could possibly have...</p><p>...and we&#x27;ve created all the mind-designs that seem possibly cogent and good, that can have positive, non-human-like experiences...</p><p>...and we&#x27;ve created all the non-sentient universes that seem plausibly good from some sort of weird aesthetic artistic standpoint, i.e. maybe there&#x27;s a universe of elegant beautiful math forms where nobody gets to directly experience it but it&#x27;s sort of beautiful that it exists in an abstract way...</p><p>...and then maybe we&#x27;ve duplicated each of these a couple times (or a couple million times, just to be sure)...</p><p>...I feel like that&#x27;s it. We won. You can&#x27;t get a higher score than that.</p><p>By contrast, if there is one person out there experiencing suffering, that is sad. And if there are two it&#x27;s twice as sad, even if they have identical experiences. And if there are 1,000,000,000,000,000 it&#x27;s 1,000,000,000,000,000x as sad, even if they&#x27;re all identical.</p><p><strong>Querying myself</strong></p><p>This comes from asking myself: &quot;do I want to have all the possible good experiences I could have?&quot; I think the answer is probably yes. And when I ask &quot;do I want to have all the possible good experiences that are somewhat contradictory, such that I&#x27;d need to clone myself and experience them separately&quot; the answer is still probably yes. </p><p>And when I ask &quot;once I have all that, would it be useful to duplicate myself?&quot; And... I&#x27;m not sure. Maybe? I&#x27;m not very excited about it. Seems like maybe nice to do, just in as a hedge against weird philosophical confusion. But when I imagine doing that the millionth time, I don&#x27;t think I&#x27;ve gotten anything extra.</p><p>But when I imagine the millionth copy of Raemon-experiencing-hell, it still seems pretty bad.</p><p><strong>Clarification on humancentricness</strong></p><p>Unlike some other LessWrong folk, I&#x27;m only medium enthusiastic about the singularity, and not all that enthusiastic about exponential growth. I care about things that human-Ray cares about. I care about Weird Future Ray&#x27;s preferences in roughly the same way I care about other people&#x27;s preferences, and other Weird Future People&#x27;s preferences. (Which is a fair bit, but more as a &quot;it seems nice to help them out if I have the resources, and in particular if they are suffering.&quot;)</p><p><strong>Counterargument – Measure/Magical Reality Fluid</strong></p><p>The main counterargument is that maybe you need to dedicate all of the multiverse to positive experiences to give the positive experiences more Magical Reality Fluid (i.e. something like &quot;more chance at existing&quot;, but try not to trick yourself into thinking you understand that concept if you don&#x27;t).</p><p>I sort of might begrudgingly accept this, but this feels something like &quot;the values of weird future Being That Shares a Causal Link With Me&quot;, rather than &quot;my values.&quot;</p><p><strong>Why is this relevant?</strong></p><p>If there&#x27;s a finite number of good experiences to have, then it&#x27;s an empirical question of &quot;how much computation or other resources does it take to cause them?&quot;</p><p>I&#x27;d... feel somewhat (although not majorly) surprised, if it turned out that you needed more than our light cone&#x27;s worth of resources to do that.</p><p>But then there&#x27;s the question of acausal trade, or trying to communicate with simulators, or &quot;being the sort of people such that whether we&#x27;re in a simulation or not, we adopt policies such that alternate versions of us with the same policies who are getting simulated are getting a good outcome.&quot;</p><p>And... that *only* seems relevant to my values if either <em>this</em> universe isn&#x27;t big enough to satisfy my human-values, <em>or</em> my human values care about things outside of this universe.</p><p>And basically,  it seems to me the only reason I care about other universes is that I think Hell Exists Out There Somewhere and Must Be Destroyed. </p><p>(Where &quot;hell&quot; is most likely to exist in the form AIs running incidental thought experiments, committing mind-crime in the process).</p><p>I expect to change my mind on this a bunch, and I don&#x27;t think it&#x27;s necessary (or even positive EV) for me to try to come to a firm opinion on this sort of thing before the singularity. </p><p>But it seems potentially important to have *meta* policies such that someone simulating me can easily tell (at lower resolutions of simulation) whether I&#x27;m the sort of agent who&#x27;d unfold into an agent-with-good-policies if they gave me more compute.</p><p><strong>tl;dr –</strong> <strong>what are the implications of the outlook listed above? What ramifications might I not be considering?</strong></p>",Raemon,raemon,Raemon,
QBFmvTnPhjMbyntJ8,On why mathematics appear to be non-cosmic,on-why-mathematics-appear-to-be-non-cosmic,https://www.lesswrong.com/posts/QBFmvTnPhjMbyntJ8/on-why-mathematics-appear-to-be-non-cosmic,2019-06-09T20:42:56.370Z,4,3,20,False,False,,"<p>Preface</p><p>I do fear that perhaps this post of mine (my fourth here) may cause a few negative reactions. I do try to approach this from a philosophical viewpoint, as befits my studies. It goes without saying that I may be wrong, and would very much like to read your views and even more so any reasons that my own position may be identified as untenable. I can only assure you that to me it currently seems that mathematics are not cosmic but anthropic.</p><br/><p>*</p><p>There are so many quotes about mathematics, from celebrated mathematicians, philosophers, even artists; some are witty yet too polemical to identify as useful in a treatise that aspires to discuss whether math is merely anthropic or cosmic, and others are perhaps too focused on the order itself and thus come across a bit like the expected fawning of an admirer to his or her muse.</p><p>Yet the question regarding math being only a human concept, or something which is actually cosmic, is an important one, and it does deserve honest examination. I will try to present a few of my own thoughts on this subject, hoping that they may be of use – even if their use is simply to allow for fruitful reflection and possible dismissal.</p><p>It is evident that mathematics have value. It is also evident that they allow for technological development. They do serve as a foundation for scientific orders that rest on experiment and thus are invaluable. However we should also consider what the primary difference between math as an order and scientific orders (physics, chemistry etc) easily let’s us know about math itself:</p><p>Primarily math differs from science in that it secures that its results are valid not from experiment, data and observation, but axiom-based proof. The use of proof in math is often attributed to the first Greek mathematicians, and specifically to either the first Philosopher, Thales of Miletus, or his students, Anaximander and Pythagoras. Euclid argued that the first Theorem that math presents is the one by Thales, which has to do with analogies between parts of 2D forms (eg triangles) inscribed in a circle. The idea of a proof proceeding from axioms, of a Theorem, is fundamental in mathematics – and it also is a crucial difference between math and orders such as physics. Fields of science that have to do with observing (and interacting with) the external world do significantly differ from a field (math) which only requires reflecting on axiomatic systems.</p><p>Given the above is true, it does follow that a human is far more connected to math than to any study of external objects: they are tied to math without even trying to be tied to it, given math exists as a mental creation and not one which requires the senses to intervene.  </p><p>But what does “being more connected” mean, in this context? Is math actually intertwined with human thought of all kinds? Obviously we do not innately know about basic “realities” of the external world, such as weight and impact; the risk of a free-fall is something that an infant has to first accept as a reality without grasping why it is so. On the contrary we do, by necessity, already have fundamental awareness of the (arguably) most basic notion in all of mathematics: the notion of the monad.</p><p>The monad is the idea of “one”. That anything distinct is a “one”, regardless of whether we mean to include it in a larger group or divide it to constituent parts: each of those larger groups are also “one”, and the same is true for any divisions. “Oneness”, therefore, as the pre-socratics already argued (and Plato examined in hundreds of pages) is arguably one of the most characteristic human notions, and a notion which is generally inescapable and ubiquitous. “One” is also the first digit and  the meter of the set of natural numbers (1,2,3,4…), and this is because the human mind fundamentally identifies differences as distinct, even when the difference may become (in advanced math) extremely complicated and of peculiar types. Yet the humble set of natural numbers also gives us an interesting sequence when altered a bit: the so-called Fibonacci sequence, which I think is a good example to use so as to show why I think that math are only human and not cosmic.</p><p>The Fibonacci sequence progresses in a very specific way: each part is formed by adding the two previous parts. The sequence begins with 1 (or 0 and 1), so the first parts of it are (0), 1, 1, 2, 3, 5, 8,13. The entire sequence diverges from both sides (alternating between the next part presenting a numerical difference just smaller or just larger) to the <strong>golden ratio</strong>, and forms a pretty spiral form (wiki image: <a href=""https://en.wikipedia.org/wiki/Fibonacci_number#/media/File:FibonacciSpiral.svg)."">https://en.wikipedia.org/wiki/Fibonacci_number#/media/File:FibonacciSpiral.svg</a>). Yet for me it is of more interest that humans do happen to observe a good approximation of this specific, mathematical spiral, on some external objects; namely the shells of a few small animals.</p><p>It is pretty clear that the shell of some external being is not itself aware of mathematics. One could argue, of course, that “nature” itself is filled with mathematics, and thus in some way a few external forms happen to approximate a specific spiral, and the tie to the <strong>golden ratio</strong> etc is only to be expected given nature (and by extension, perhaps, the Cosmos itself) is mathematical. Certainly this can appear to provide an answer; or to be precise it would at least present a cause for this appearance of mathematics and of a specific spiral in the external world. Is it really a good answer, though? In other words, do we observe the Fibonacci or golden ratio spiral approximation on the external world because the external world itself is tied to math, or do we do so because we are tied to math in an even deeper way than we realize and could only project what we have inside of our mental world onto anything external?</p><p>My view is that humans are so bound to math (regardless of how knowledgeable one is in mathematics) that we cannot but view the world mathematically. Rockets are built, using math, and by them we can even leave the orbit of our planet – yet consider whether what allowed us to realize how to achieve so impressive a result was not math alone, but math as a kind of very anthropic cane or leg by which we slowly learned to move about:</p><p>In essence I do think that due to the human species being so obstructed from developing far more advanced mathematics (to put it another way: due to how difficult advancing math can be even for the best mathematicians) we tend to not identify that math itself is not the cause of development, not the cause of movement and progression, but a leg - the only leg - we have to familiarize ourselves with because we aspire to move on this plane. Imagine a dog which wanted to move from A to B, but couldn’t use its legs. At some point it manages to move one of them, and then enough so as to finally get to B. It is undoubtedly a major achievement for the dog. But the dog shouldn’t proceed to claim that the dirt between A and B is made of <em>moving legs</em> – let alone that it is the case for the entire Cosmos.</p><p>I only meant to briefly present my thoughts on this subject, and wish to specify (what very likely is already clear to more mathematically-oriented readers of this post) that my personal knowledge of mathematics is quite basic. I approach the subject from a philosophical and epistemological viewpoint, which is more fitting to my own University studies (Philosophy).</p><p>by Kyriakos Chalkopoulos <a href=""(https://www.patreon.com/Kyriakos)"">(https://www.patreon.com/Kyriakos)</a></p>",KyriakosCH,kyriakosch,KyriakosCH,
DwBrNpnGCEdNLbZeL,An attempt to list out my core values and virtues,an-attempt-to-list-out-my-core-values-and-virtues,https://www.lesswrong.com/posts/DwBrNpnGCEdNLbZeL/an-attempt-to-list-out-my-core-values-and-virtues,2019-06-09T20:02:43.122Z,26,6,1,False,False,,"<p>Last week a friend pressed upon me the importance of <em>writing your own culture</em>. It was a small part of a multi-hour conversation, and I&#x27;m not sure if I&#x27;m interpreted their meaning correctly, but the correct-seeming-to-me position I took from it was something like this:</p><p><em>Write your own culture. Identify <u>your</u> values the things <u>you</u> consider to be virtues. Not those of the broader culture you exist in or those putatively held by the groups in which you have an identity. Those which are yours, for you, separate from what others might value or consider virtuous.</em></p><p>Perhaps for convenience, to date, I would round off my values/virtues to being Rationalist and EA values. Succinct, perhaps easier to communicate or even convenient as an internal mental handle. But with less personal ownership, or something. Like perhaps they&#x27;re only &quot;my values&quot; because I&#x27;m part of those groups. And that&#x27;s not true. While the groups might have helped me flesh out and identify my values, they are <em>my values</em>. Also when held this way there is far less nuance to them.</p><p>I already have them floating around in my head with my own particular characterization. Yet they float around individually, not as a coherent list. So here goes. Here&#x27;s a first attempt to capture my values and virtues.</p><p>(What&#x27;s the difference between a value and virtue? I&#x27;m not sure exactly, but my brain is labeling some items as more one than the other. Maybe values are things I optimize for and virtues are behaviors I endorse.)</p><p>(Also, this isn&#x27;t an exhaustive list of absolutely everything I&#x27;d say I care about or think is good. These are top high-level virtues which subsume all the other things for me. I value cake, but cake arises as something I value further down the chain than anything list here.)</p><h1>My Values/Virtues</h1><p>(very loose/hazy ordering of priority)</p><ul><li><strong>Curiosity/ Wanting to know and understand the world.</strong></li><li><strong>Truth</strong></li><ul><li>Over what is comfortable or &quot;instrumentally&quot; advantageous.</li><ul><li>It feels that I would choose truth even if it would destroy me. Though I wouldn&#x27;t choose it if it would destroy Miranda (or the world) . . . so I guess there are limits.</li></ul><li>An abhorrence of rationalization.</li><li>A revulsion of arguments (or even countenancing the possibility) that truth should be sacrificed for some other gain. I&#x27;m not saying that I never would, but I find the suggestion viscerally emotionally upsetting and offensive (perhaps irrationally and dogmatically so).</li><li>Related to truth, it feels I have an utmost virtue of accepting arguments and reasoning that seem correct and to have lead to true conclusions. Combined with <em>Integrity, </em>that means I will act on arguments even if they lead to unconventional places.</li><li>Eliezer&#x27;s <a href=""http://yudkowsky.net/rational/virtues/"">12 Virtues of [Epistemic] Rationality</a> are my virtues because they fall under truth. (The virtue of <em>scholarship </em>also falls under <em>Curiosity.)</em></li><li>Downstream of overall Truth and Curiosity is the desire for self-knowledge.</li></ul><li><strong>A Sense that More is Possible / A Will to Transcendence</strong></li><ul><li>This feels very core to who I am / want to be. It&#x27;s a value I pride myself on (my <a href=""https://asensethat.com/"">furtive attempt at a personal blog</a> had this title).</li><li>It&#x27;s something like, I believe there exist dimensions along which the world can be better or worse, and it is <em>good </em>to make it better. Surely if it is possible, you make it better? I feel like I somewhat lack further justification for this feeling, but that&#x27;s the feeling I have. We should make things as good as they can be. (<em>Perfect the universe </em>I say, as the goal to aim for even if it&#x27;s not a realistic/meaningful target).</li><li>I have uncertainty about what <em>better </em>and <em>more </em>mean exactly. I have guesses and strong feeling (like suffering is bad, knowledge is good), but I place extremely extremely low probability on all states of the world being equally good. For now, <a href=""http://mindingourway.com/you-dont-get-t/"">push in the obvious directions</a> [1].</li><li>I do have a very strong sense that the world is a <a href=""https://jesswhittlestone.com/blog/2015/2/25/reasons-for-hope"">hellova lot better than it used to be</a>. So much change in, in so little time. Much of the time I feel baffled that people don&#x27;t look at the last few hundred years (or even their own lifetimes), look at the progress of technology, and aren&#x27;t resultantly clamoring in the streets for why don&#x27;t we speed up the goddamn progress so we can get to the <em><a href=""https://nickbostrom.com/utopia.html"">goddamn utopian future</a></em> which is just a <em>super reasonable extrapolation</em> of what is possible given our knowledge of the laws of physics and recent history.</li><li>I am part of the world - and importantly, I am the part of the world I affect the world through - so my own self-improvement is especially important.</li><ul><li>And I have a solid sense of the many, many ways I could be better.</li></ul><li>This value/virtue is powerful. It motivates me. It&#x27;s also dangerous in that it pushes me towards dissatisfaction, always looking at what could be but isn&#x27;t. I struggle with this. I&#x27;ve been an advocate of Acceptance Commitment Therapy (an Eastern-influenced psychotherapy) for seven years since it helps with this kind of dilemma. I&#x27;m still working to enjoy the good things that already are while still striving hard for all that could be.</li></ul><li><strong>There are better and worse ways for the world to be.</strong></li><ul><li>This is already subsumed in the <em>Sense that More is Possible </em>but somewhat feels fundamental enough to have it&#x27;s own high-level bullet point.</li></ul><li><strong>Optimizing the whole / Long-termism / Making local sacrifices/ Foregoing marshmallows</strong></li><ul><li>Part of a Sense that More is Possible is wanting to optimize everything across all of time and space. And global optimization often requires local sacrifice. That&#x27;s just like super basic.</li><li>I cultivate this as a core virtue. Always be aware if you are sacrificing the whole for the sake of a part. When there is possibility for so much, don&#x27;t get short-sighted. </li><li>The result is I&#x27;m willing to work on very long and slow feedback loops, generally delaying gratification for some time.</li><ul><li>Recently I fear I&#x27;ve been doing this too much (i.e. in a way that does not optimize the whole) and am dialing back a little.  Maybe it&#x27;s that being a <a href=""https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip"">multiagent agent</a> is hard.</li></ul><li>I think this value/virtue/attitude is why I&#x27;ve always disliked the question of <em><a href=""https://www.nytimes.com/2015/01/11/fashion/no-37-big-wedding-or-small.html"">what would a perfect day look like for you?&quot;</a> </em>Perfection isn&#x27;t a concept I can apply to days in isolation.</li></ul><li><strong>Quantitative-Sensitivity</strong></li><ul><li>One person suffering is bad. Two people suffering is worse. This seems kind of basically true even now I&#x27;m not sure how to argue for it. (I dislike that I don&#x27;t have a better basis, but maybe I can work on that.)</li><li>Accepting that quantities matter, I follow that through. <a href=""https://wiki.lesswrong.com/wiki/Shut_up_and_multiply"">I shut up and multiply</a>.</li><li>Shut up and multiplying is a core virtue.<strong> </strong></li></ul><li><strong>Empathy / Caring for Sentient Beings</strong></li><ul><li>It seems I just do. [Many times,] if I see someone suffering before me, I feel it, and I don&#x27;t like it. That is not how I want the world to be. I will invest my effort to change. Not just that, I want the flourishing of sentient beings. Being quantitatively-sensitive and trying to optimize the whole, I try to scale up. This feels like the obvious thing to do.</li><li>A virtue here is that you don&#x27;t purchase your own benefit at the expense of others.</li><li>This is related to cooperation as well, but I take <em><strong>Consideration</strong> </em>to be an important virtue. You&#x27;re supposed to think about the effects of your actions on others. All things equal, you don&#x27;t put your own utility in front of there&#x27;s (if anything, apply a heavy discount around your own to counter self-serving biases). Most practically, I avoid being late and really, really hate canceling/rescheduling on people.</li></ul><li><strong>Cooperation</strong></li><ul><li>This just feels like so obviously the correct thing to do that you just do it. Obviously, we all gain more from cooperation, especially if we have shared values. If not for cooperation, we&#x27;d still be single-celled organisms, if that.</li><li>It&#x27;s hard, it takes effort, but obviously obviously you push to make it happen.</li><li>You try to have all the virtues that make it possible. You are honest (or at least <a href=""https://www.lesswrong.com/posts/xdwbX9pFEr7Pomaxv/meta-honesty-firming-up-honesty-around-its-edge-cases"">meta-honest</a>), you&#x27;re reliable, predictable, you do what you say will, and <a href=""https://www.lesswrong.com/posts/EQJfdqSaMcJyR5k73/habryka-s-shortform-feed#xQJYMNujxB3EqephZ"">act in accordance with your stated beliefs</a>. </li><li>I really, really prefer to be honest. I lump this under cooperation, but I think it&#x27;s also part of connection, (and also deception is stressful, but I don&#x27;t endorse honesty as a virtue because of that).</li><ul><li>I&#x27;m more ready to be dishonest with others though than with myself. By far. If others have chosen to enter into an adversarial situation with me, I don&#x27;t owe it to them to ensure they have an accurate map with which to harm me.</li></ul><li>I try very hard to do what I say I will. I honor contracts and agreements and pledges, even when it ends up being costly or if I regret the commitment. Most of the time it means I&#x27;ll go to lengths to avoid being late. Once I pledged to stay in a job for nine months longer and I did even when I wasn&#x27;t enjoying it.</li><li>Cooperation flows naturally from optimizing the whole too. Sure, you can get ahead today via defection, but in the long-term, cooperation wins out.</li></ul><li><strong>Connection</strong></li><ul><li>It seems that I value connecting with other minds. This feels kind of weird, but maybe only because I came to recognize it explicitly later than the other values and virtues listed her.</li><li>I&#x27;m not sure what &quot;connecting&quot; means exactly, but it&#x27;s a thing and it seems good and something I want and value.</li></ul><li><strong>Gratitude</strong></li><ul><li>I have always felt gratitude very strongly and deeply. If someone has done something which has benefited me, I am thankful and wish to do good by them too.</li><li>My feelings of gratitude are evoked by even small things and the feelings will last for years.</li><li>Is this ideal? I don&#x27;t know. I haven&#x27;t thought through the arguments, but I embrace it as a personal-virtue I am happy to have.</li><li>I suspect it gratitude is in large part what grows into loyalty for me. I think that I am rather loyal</li></ul><li><strong>Meta-Virtue: Integrity</strong></li><ul><li>Integrity is the meta-virtue of having your values and virtues and acting in accordance with them. The commitment to living by them. It&#x27;s kind of weird to need this meta-virtue, but I think you can praise a person for commitment to the values and virtues somewhat independently of sharing their values and virtues. </li></ul></ul><br/><br/><p>[1]</p><blockquote>I can&#x27;t tell you exactly where I&#x27;m going, but I can sure see which direction the arrow points.</blockquote><blockquote>It&#x27;s easier, in a way, to talk about the negative motivations — ending disease, decreasing existential risk, that sort of thing — because those are the things that I&#x27;m pretty sure of, in light of uncertainty about what really matters to me. I don&#x27;t know exactly what I want, but I&#x27;m pretty sure I want there to be humans (or post-humans) around to see it.</blockquote><blockquote>But don&#x27;t confuse what I&#x27;m <em>doing</em> with what I&#x27;m <em>fighting for.</em> The latter is much harder to describe, and I have no delusions of understanding.</blockquote><blockquote>You don&#x27;t get to know exactly what you&#x27;re fighting for, but the world&#x27;s in bad enough shape that you don&#x27;t <em>need</em> to.</blockquote><p>From<em> <a href=""http://mindingourway.com/you-dont-get-t/"">You don&#x27;t get to know what you&#x27;re fighting for</a> </em>on Minding Our Way</p><br/><br/>",Ruby,ruby,Ruby,
nJzBWzym4ySAwmk4G,The expected value of extinction risk reduction is positive,the-expected-value-of-extinction-risk-reduction-is-positive-1,https://www.lesswrong.com/posts/nJzBWzym4ySAwmk4G/the-expected-value-of-extinction-risk-reduction-is-positive-1,2019-06-09T15:49:32.352Z,23,6,0,False,False,,"<p><a href=""https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive"">(Cross-post from the EA forum)</a></p>
<h1>Short summary</h1>
<p>There are good reasons to care about sentient beings living in the millions of years to come. Caring about the future of sentience is sometimes taken to imply reducing the risk of human extinction as a moral priority. However, this implication is not obvious so long as one is uncertain whether a future with humanity would be better or worse than one without it.</p>
<p>In this article, we try to give an all-things-considered answer to the question: “Is the expected value of efforts to reduce the risk of human extinction positive or negative?”. Among others, we cover the following points:</p>
<ul>
<li>What happens if we simply tally up the welfare of current sentient beings on earth and extrapolate into the future; and why that isn’t a good idea</li>
<li>Thinking about the possible values and preferences of future generations, how these might align with ours, and what that implies</li>
<li>Why the “option value argument” for reducing extinction risk is weak</li>
<li>How the potential of a non-human animal civilisation or an extra-terrestrial civilisation taking over after human extinction increases the expected value of extinction risk reduction</li>
<li>Why, if we had more empirical insight or moral reflection, we might have moral concern for things outside of earth, and how that increases the value of extinction risk reduction</li>
<li>How avoiding a global catastrophe that would not lead to extinction can have very long-term effects</li>
</ul>
<h1>Long Summary</h1>
<p>If most expected value or disvalue lies in the billions of years to come, altruists should plausibly focus their efforts on improving the long-term future. It is not clear whether reducing the risk of human extinction would, in expectation, improve the long-term future, because a future with humanity may be better or worse than one without it.</p>
<p>From a consequentialist, welfarist view, most expected value (EV) or disvalue of the future comes from scenarios in which (post-)humanity colonizes space, because these scenarios contain most expected beings. Simply extrapolating the current welfare (part 1.1) of humans and farmed and wild animals, it is unclear whether we should support spreading sentient beings to other planets.</p>
<p>From a more general perspective (part 1.2), future agents will likely care morally about the same things we find valuable or about any of the things we are neutral towards. It seems very unlikely that they would see value exactly where we see disvalue. If future agents are powerful enough to shape the world according to their preferences, this asymmetry implies the EV of future agents colonizing space is positive from many welfarist perspectives.</p>
<p>If we can defer the decision about whether to colonize space to future agents with more moral and empirical insight, doing so creates option value (part 1.3). However, most expected future disvalue plausibly comes from futures controlled by indifferent or malicious agents. Such “bad” agents will make worse decisions than we, currently, could. Thus, the option value in reducing the risk of human extinction is small.</p>
<p>The universe may not stay empty, even if humanity goes extinct (part 2.1). A non-human animal civilization, extraterrestrials or uncontrolled artificial intelligence that was created by humanity might colonize space. These scenarios may be worse than (post-)human space colonization in expectation. Additionally, with more moral or empirical insight, we might realize that the universe is already filled with beings or things we care about (part 2.2). If the universe is already filled with disvalue that future agents could alleviate, this gives further reason to reduce extinction risk.</p>
<p>In practice, many efforts to reduce the risk of human extinction also have other effects of long-term significance. Such efforts might often reduce the risk of global catastrophes (part 3.1) from which humanity would recover, but which might set technological and social progress on a worse track than they are on now. Furthermore, such efforts often promote global coordination, peace and stability (part 3.2), which is crucial for safe development of pivotal technologies and to avoid negative trajectory changes in general.</p>
<p>Aggregating these considerations, efforts to reduce extinction risk seem positive in expectation from most consequentialist views, ranging from neutral on some views to extremely positive on others. As efforts to reduce extinction risk also seem highly leveraged and time-sensitive, they should probably hold prominent place in the long-termist EA portfolio.</p>
<h1>Introduction and background</h1>
<p>The future of Earth-originating life might be vast, lasting millions of years and containing many times more beings than currently alive <a href=""https://nickbostrom.com/astronomical/waste.html"">(Bostrom, 2003)</a>. If future beings matter morally, it should plausibly be a major <a href=""https://80000hours.org/articles/future-generations/"">moral</a> <a href=""https://www.effectivealtruism.org/articles/cause-profile-long-run-future/"">concern</a> that the future plays out well. So how should we, today,  prioritise our efforts aimed at improving the future?</p>
<p>We could try to reduce the <a href=""https://en.wikipedia.org/wiki/Global_catastrophic_risk"">risk of human extinction</a>. A future with humanity would be drastically different from one without it. Few other factors seems as pivotal for how the world will look like in the millions of years to come as whether or not humanity survives the next few centuries and millennia. Effective efforts to reduce the risk of human extinction could thus have immense long-term impact. If we were sure that this impact was positive, extinction risk reduction would plausibly be one of the most effective ways to improve the future.</p>
<p>However, it is not at first glance clear that reducing extinction risk is positive from an impartial altruistic perspective. For example, future humans might have terrible lives that they can’t escape from, or humane values might exert little control over the future, resulting in future agents causing great harm to other beings. If indeed it turned out that we weren’t sure if extinction risk reduction was positive, we would prioritize other ways to improve the future without making extinction risk reduction a primary goal.</p>
<p>To inform this prioritisation, in this article we estimate the expected value of efforts to reduce the risk of human extinction.</p>
<h2>Moral assumptions</h2>
<p>Throughout this article, we base our considerations on two assumptions:</p>
<ol>
<li>That it morally matters what happens in the billions of years to come. From this <a href=""https://80000hours.org/articles/future-generations/"">very long-term view</a>, making sure the future plays out well is a primary moral concern.</li>
<li>That we should aim to satisfy our reflected moral preferences. Most people would want to act according to the preferences they would have upon idealized reflection, rather than according to their current preferences. The process of idealized reflection will differ between people. Some people might want to revise their preferences after they became much smarter, more rational and had spent millions of years in philosophical discussion. Others might want to largely keep their current moral intuitions, but learn empirical facts about the world (e.g. about the nature of consciousness).</li>
</ol>
<p>Most arguments further assume that the state the world is brought into by one’s actions is what matters morally (as opposed to e.g. the actions following a specific rule). We thus take a <a href=""https://en.wikipedia.org/wiki/Consequentialism"">consequentialist</a> view, judging potential actions by their consequences.</p>
<p>Parts 1.1 and 1.2 further take a welfarist perspective, assuming that what matters morally in states of the world is the welfare of <a href=""https://en.wikipedia.org/wiki/Sentience"">sentient</a> beings. In a way, that means assuming our reflected preferences are welfarist. Welfare will be broadly defined as including pleasure and pain, but also complex values or the satisfaction of preferences. From this perspective, a state of the world is good if it is good for the individuals in this world. Across several beings, welfare will be aggregated additively<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-1"" id=""fnref-S8My2kBLixCNaJDFB-1"">[1]</a></sup>, no matter how far in the future an expected being lives. Additional beings with positive (negative) welfare coming into existence will count as morally good (bad). In short, parts 1.1 and 1.2 take the view of welfarist consequentialism with a total view on population ethics (see e.g. <a href=""http://users.ox.ac.uk/~mert2255/papers/population_axiology.pdf"">(Greaves, 2017)</a>), but the arguments also hold for other similar views.</p>
<p>If we make the assumptions outlined above, nearly all expected value or disvalue in a future with humanity arises from scenarios in which (post-)humans colonize space. The colonizable universe seems very large, so scenarios with space colonization likely contain a lot more beings than scenarios with earthbound life only (<a href=""https://nickbostrom.com/astronomical/waste.html"">Bostrom, 2003</a>). Conditional on human survival, space colonization also does <a href=""https://www.fhi.ox.ac.uk/will-we-eventually-be-able-to-colonize-other-stars-notes-from-a-preliminary-review/"">not seem too unlikely</a>, thus nearly all expected future beings live in scenarios with space colonization<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-2"" id=""fnref-S8My2kBLixCNaJDFB-2"">[2]</a></sup>. We thus take “a future with humanity” to mean “(post-)human space colonization” for the main text and briefly discuss what a future with only earthbound humanity might look like in Appendix 1.</p>
<h2>Outline of the article</h2>
<p>Ultimately, we want to know “What is the <a href=""https://en.wikipedia.org/wiki/Expected_value"">expected value</a> (EV) of efforts to reduce the risk of human extinction?”. We will address this question in three parts:</p>
<ul>
<li>
<p>In part 1, we ask “What is the EV of (post-)human space colonization<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-3"" id=""fnref-S8My2kBLixCNaJDFB-3"">[3]</a></sup>?”. We first attempt to extrapolate the EV from the amounts of value and disvalue in today’s world and how they would likely develop with space colonization. We then turn toward a more general examination of what future agents’ tools and preferences might look like and how they will, in expectation, shape the future. Finally, we consider if future agents could make a better decision on whether to colonize space (or not) than we can, so that it seems valuable to let them decide (option value).</p>
</li>
<li>
<p>In part 1 we tacitly assumed the universe without humanity is and stays empty. In part 2, we drop that assumption. We evaluate how the possibility of space colonization by alternative agents and the possibility of existing but tractable disvalue in the universe change the EV of keeping humans around.</p>
</li>
<li>
<p>In part 3, we ask “Besides reducing extinction risk, what will be the consequences of our efforts?”. We look at how different efforts to reduce extinction risk might influence the long-term future by reducing global catastrophic risk and by promoting global coordination and stability.</p>
</li>
</ul>
<p>We stress that the conclusions of the different parts should not be separated from the context. Since we are reasoning about a topic as complex and uncertain as the long-term future, we take several views, aiming to ultimately reach a verdict by aggregating across them.</p>
<h2>A note on disvalue-focus</h2>
<p>The moral view on which this article is based is very broad and can include enormously different value systems, in particular different degrees of ‘disvalue-focus’. We consider a moral view disvalue-focused if it holds the prevention/reduction of disvalue is (vastly) more important than the creation of value. One example are views that hold the prevention or <a href=""https://foundational-research.org/the-case-for-suffering-focused-ethics/"">reduction of</a> <a href=""https://docs.google.com/document/d/1hQI3otOAT39sonCHIM6B4na9BKeKjEl7wUKacgQ9qF8/edit"">suffering</a> as an especially high moral priority.</p>
<p>The degree of disvalue focus one takes chiefly influences the EV of reducing extinction risk.</p>
<p>From very disvalue-focused views, (post-) human space colonization may not seem desirable even if the future contains a much better ratio of value to disvalue than today. There is little to gain from space colonization if the creation of value (e.g. happy beings) morally matters little. On the other hand, space colonization would multiply the amount of sentient beings and thereby multiply the <em>absolute</em> amount of disvalue.</p>
<p>At first glance it thus seems that reducing the risk of human extinction is not a good idea from a strongly disvalue-focused perspective. However, the value of extinction risk reduction for disvalue-focused views gets shifted upwards considerably by the arguments in part 2 and 3 of this article.</p>
<h1>Part 1: What is the EV of (post-)human space colonization?<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-4"" id=""fnref-S8My2kBLixCNaJDFB-4"">[4]</a></sup></h1>
<h2>1.1: Extrapolating from today’s world</h2>
<p>Space colonization is <a href=""https://selfawarepatterns.com/2017/07/22/the-difficulty-of-interstellar-travel-for-humans/"">hard</a>. By the time our technology is advanced enough, human civilization will possibly have changed considerably in many ways. However, to get a first grasp of the expected value of the long-term future, we can model it as a rough extrapolation of the present. What if humanity as we know it colonized space? There would be vastly more sentient beings, including humans, farmed animals and wild animals<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-5"" id=""fnref-S8My2kBLixCNaJDFB-5"">[5]</a></sup>. To estimate the expected value of this future, we will consider three questions:</p>
<ol>
<li>How many humans, farmed animals and wild animals will exist?</li>
<li>How should we weigh the welfare of different beings?</li>
<li>For each of humans, farmed animals and wild animals:
<ol>
<li>Is the current average welfare net positive/average life worth living?</li>
<li>How will welfare develop in the future?</li>
</ol>
</li>
</ol>
<p>We will then attempt to draw a conclusion. Note that throughout this consideration, we take an individualistic <a href=""https://was-research.org/mission/"">welfarist</a> <a href=""https://digitalcommons.osgoode.yorku.ca/cgi/viewcontent.cgi?referer=https://www.google.co.uk/&amp;httpsredir=1&amp;article=1936&amp;context=ohlj&amp;sei-redir=1"">perspective </a>on wild animals. This perspective stands in contrast to e.g. valuing functional ecosystems and might seem unusual, but is increasingly popular.</p>
<h3>There will likely be more farmed and wild animals than humans, but the ratio will decrease compared to the present</h3>
<p>In today’s world, both farmed and wild animals outnumber humans by far. There are about 3-4 times more <a href=""https://www.economist.com/graphic-detail/2011/07/27/counting-chickens"">farmed</a> land animals and about 13 times more farmed fish<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-6"" id=""fnref-S8My2kBLixCNaJDFB-6"">[6]</a></sup> than humans alive. <a href=""https://reducing-suffering.org/how-many-wild-animals-are-there/"">Wild animals</a> prevail over farmed animals, with about 10 times more wild birds than farmed birds and 100 times more wild mammals than farmed mammals alive at any point. Moving on to smaller wild animals, the numbers increase again, with 10 000 times more vertebrates than humans, and between 100 000 000 - 10 000 000 000 times more insects and spiders than humans<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-7"" id=""fnref-S8My2kBLixCNaJDFB-7"">[7]</a></sup>.</p>
<p>In the future, the relative number of animals compared to humans will likely decrease considerably.</p>
<p>Farmed animals will not be alive if animal farming substantially decreases or stops, which seems more likely than not for both for moral and economical reasons. Humanity’s moral circle seems to have been expanding throughout history <a href=""https://www.goodreads.com/book/show/3026168-the-expanding-circle"">(Singer, 2011)</a> and further expansion to animals may well lead us to stop farming animals.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-8"" id=""fnref-S8My2kBLixCNaJDFB-8"">[8]</a></sup> Also financially, plant-based meat alternatives or lab-grown meat will likely develop to be more efficient than growing animals (Tuomisto and Teixeira de Mattos, 2011). However, none of these developments seems unequivocally destined to end factory-farming<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-9"" id=""fnref-S8My2kBLixCNaJDFB-9"">[9]</a></sup>, and the historical track record shows that meat consumption per head has been <a href=""http://www.fao.org/docrep/005/y4252e/y4252e05b.htm"">growing</a> for &gt; 50 years<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-10"" id=""fnref-S8My2kBLixCNaJDFB-10"">[10]</a></sup>. Overall, it seems likely but not absolutely clear that the number of farmed animals relative to humans will be smaller in the future. For wild animals, we can extrapolate from a <a href=""http://reducing-suffering.org/humanitys-net-impact-on-wild-animal-suffering/"">historical</a> <a href=""http://www.overcomingbias.com/2009/09/nature-is-doomed.html"">trend</a> of decreasing wild animal populations. Even if wild animals were spread to other planets for terraforming, the animal / human ratio would likely be lower than today.</p>
<h3>Welfare of different beings can be weighted by (expected) consciousness</h3>
<p>To determine the EV of the future, we need to aggregate welfare across different beings. It seems like we should weigh the experience of a human, a cow and a beetle differently when adding up, but by how much? This is a hard question with no clear answer, but we outline some approaches here. The degree to which an animal is conscious (“the lights are on”, the being is aware of its experiences, emotions and thoughts), or the confidence we have in an animal being conscious, can serve as a parameter by which to weight welfare. To arrive at a number for this parameter, we can use <a href=""http://reflectivedisequilibrium.blogspot.com/2015/11/some-considerations-for-prioritization.html"">proxies </a>such as brain mass, neuron count and mental abilities directly. Alternatively, we may aggregate these proxies with other considerations into an estimate of confidence that a being is conscious. For instance, the <a href=""https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#SummaryCurrentThinking"">Open Philanthropy Project estimates the probability that cows are conscious at 80%</a>.</p>
<h3>The EV of (post-)human lives is likely positive</h3>
<p>Currently, the average human life seems to be perceived as being worth living. Survey data and experience sampling suggests that most humans are quite content with their lives and experience more positive than negative emotions on a day-to-day basis<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-11"" id=""fnref-S8My2kBLixCNaJDFB-11"">[11]</a></sup>. If they find it not worth living, humans can take their life, but relatively few people commit suicide (<a href=""https://www.cdc.gov/nchs/fastats/deaths.htm"">Suicide</a> accounts for 1.7 % of all deaths in US).<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-12"" id=""fnref-S8My2kBLixCNaJDFB-12"">[12]</a></sup> We could conclude that human welfare is positive.</p>
<p>We should, however, note the two caveats in this conclusion. First, a live can be <em>perceived</em> as worth living even if it is negative from a welfarist perspective.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-13"" id=""fnref-S8My2kBLixCNaJDFB-13"">[13]</a></sup>  Second, the <em>average</em> life might not be worth living if the suffering of the worst off was sufficiently more intense than the happiness of the majority of people.</p>
<p>Overall, it seems that from a large majority of consequentialist views, the current aggregated human welfare is positive.</p>
<p>In the future, we will probably make progress that will improve the average human life. Historic trends have been positive across <a href=""http://lukemuehlhauser.com/three-wild-speculations-from-amateur-quantitative-macrohistory/"">many indicators of human well-being</a>, knowledge, intelligence and capability. On a global scale, violence is declining, cooperation increasing <a href=""https://stevenpinker.com/publications/better-angels-our-nature"">(Pinker, 2011)</a>. Yet, the trend does not include all indicators: subjective welfare has (in recent times) remained stable or improved very little, and mental health problems are more prevalent. These developments have sparked research into positive psychology and mental health treatment, which is slowly bearing fruit. As more fundamental issues are gradually improved, humanity will likely shift more resources towards actively improving welfare and mental health. Powerful tools like genetic design and virtual reality could be used to further improve the lives of the broad majority as well as the worst-off. While there are good reasons to assume that human welfare in the future will be more positive than now, we still face uncertainties (e.g. from low probability events like malicious, but very powerful autocratic regimes and unknown unknowns).</p>
<h3>EV of farmed animals’ lives is probably negative</h3>
<p>Currently, <a href=""https://docs.google.com/spreadsheets/d/1Njl_GS7jDOELjOtywvk3thIFpW_v10uZ5APJl1KgaY0/edit#gid=0"">93% </a>of farmed animals live on factory farms in conditions that likely make their lives not worth living. Although there are positive sides to animal life on farms compared to life in the wild<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-14"" id=""fnref-S8My2kBLixCNaJDFB-14"">[14]</a></sup>, these are likely outweighed by negative experiences<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-15"" id=""fnref-S8My2kBLixCNaJDFB-15"">[15]</a></sup>. Most farmed animals also lack opportunities to exhibit naturally desired behaviours like grooming. While there is clearly room for improvement in factory farming conditions, the question “is the average life worth living?” must be answered separately for each situation and remains controversial<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-16"" id=""fnref-S8My2kBLixCNaJDFB-16"">[16]</a></sup>. On average, a factory farm animal life today probably has negative welfare.</p>
<p>In the future, factory farming is likely to be abolished or modified to improve animal welfare as our moral circle expands to animals (see above). We can thus be moderately optimistic that farm animal welfare will improve and/or less farm animals will be alive.</p>
<h3>The EV of wild animals’ lives is very unclear, but potentially negative</h3>
<p>Currently, we know too little about the lives and perception of wild animals to judge whether their average welfare is positive or negative. We see evidence of both positive<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-17"" id=""fnref-S8My2kBLixCNaJDFB-17"">[17]</a></sup> and negative<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-18"" id=""fnref-S8My2kBLixCNaJDFB-18"">[18]</a></sup> experiences. Meanwhile, our perspective on wild animals might be skewed towards charismatic big mammals living relatively good lives. We thus overlook the vast majority of wild animals, based both on biomass and neural count. Most smaller wild animal species (invertebrates, insects etc) are <a href=""http://reducing-suffering.org/the-importance-of-insect-suffering/"">r-selected</a>, with most individuals living very short lives before dying painfully. While vast numbers of those lives seem negative from a welfarist perspective, we may chose to weight them less based on the considerations outlined above. In summary, most welfarist views would probably judge the aggregated welfare of wild animals as negative. The more one thinks that smaller, r-selected animals matter morally, the more negative average wild animal welfare becomes.</p>
<p>In future, we may reduce the suffering of wild animals, but it is unclear whether their welfare would be positive. Future humans may be driven by the expansion of the moral circle and empowered by technological progress (e.g. <a href=""https://www.hedweb.com/"">biotechnology</a>) to improve wild animal lives.  However, if average wild animal welfare remains negative, it would still be bad to increase wild animal numbers by space colonization.</p>
<h3>Conclusion</h3>
<p>It remains unclear whether the EV of a future in which a human civilization similar to the one we know colonized space is positive or negative.</p>
<p>To quantify the above considerations from a welfarist perspective, we created a mathematical model. This model yields a positive EV for a future with space colonization if different beings are weighted by <a href=""https://www.getguesstimate.com/models/10009"">neuron count</a> and a negative EV if they are weighted by <a href=""https://www.getguesstimate.com/models/10003"">sqrt(neuron count)</a>. In the first case, average welfare is positive, driven by the spreading of happy (post-)humans. In the second case, average welfare is negative as suffering wild animals are spread. The model is also based on a series of low-confidence assumptions<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-19"" id=""fnref-S8My2kBLixCNaJDFB-19"">[19]</a></sup>, alteration of which could flip the sign of the outcome again.</p>
<p>More qualitatively, the EV of an extrapolated future heavily depends on one’s moral views. The degree to which one is focused on avoiding disvalue seems especially important. Consider that every day, humans and animals are being tortured, murdered, or in psychological despair. Those who would <a href=""https://en.wikipedia.org/wiki/The_Ones_Who_Walk_Away_from_Omelas"">walk away from Omelas</a> might <a href=""http://reducing-suffering.org/omelas-and-space-colonization/"">also walk away</a> from current and extrapolated future worlds.</p>
<p>Finally, we should note how little we know about the world and how this impacts our confidence in considerations about an extrapolated future. To illustrate the extent of our empirical uncertainty, consider that we are extrapolating from 100 000 years of human existence, 10 000 years of civilizational history and 200 years of industrial history to potentially 500 million years on earth (and much longer in the rest of the universe). If people in the past had guessed about the EV of the future in a similar manner, they would most likely have gotten it wrong (e.g. they might not have considered moral relevance of animals, or not have known that there is a universe to potentially colonize). We might be missing crucial considerations now in analogous ways.</p>
<h2></h2>
<h2>1.2: Future agents’ tools and preferences</h2>
<p>While part 1.1 extrapolates directly from today’s world, part 1.2 takes a more abstract approach. To estimate the EV of (post-)human space-colonization in more broadly applicable terms, we consider three questions:</p>
<ol>
<li>Will future agents have the tools to shape the world according to their preferences?</li>
<li>Will future agents’ preferences resemble our 'reflected preferences' (see 'Moral assumptions' section)?</li>
<li>Can we expect the net welfare of future agents and powerless beings to be positive or negative?</li>
</ol>
<p>We then attempt to estimate the EV of future agents colonizing space from a welfarist consequentialist view.</p>
<h3>Future agents will have powerful tools to shape the world according to their preferences</h3>
<p>Since climbing down from the trees, humanity has changed the world a great deal. We have done this by developing increasingly powerful tools to satisfy our preferences (i.e. preferences to eat, stay healthy and warm, and communicate with friends (even if they are far away)). As far as humans have altruistic preferences, powerful tools have made acting on them less costly. For instance, if you see someone is badly hurt and want to help, you don’t have to carry them home and care for them yourself anymore, you can just call an ambulance. However, powerful tools have also made it easier to cause harm, either by satisfying harmful preferences (e.g. weapons of mass destruction) or as a side-effect of our actions that we are indifferent to. Technologies that enable factory farming do enormous harm to animals, although they were developed to satisfy a preference for eating meat, not for harming animals<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-20"" id=""fnref-S8My2kBLixCNaJDFB-20"">[20]</a></sup>.</p>
<p>It seems likely that future agents will have much more powerful tools than we do today. These tools could be used to make the future better or worse. For instance, biotechnology and genetic engineering could help us cure diseases and live longer, but they could also enforce inequality if treatments are too expensive for most people. Advanced AI could make all kinds of services much cheaper but could also be misused. For more potent and complex tools, the stakes are even higher. Consider the example of technologies that facilitate space colonization. These tools could be used to cause the existence of many times more happy lives than would be possible on Earth, but also to spread suffering.</p>
<p>In summary, future agents will have the tools to create enormous value (more examples <a href=""https://www.quora.com/Looking-1000-years-into-the-future-and-assuming-the-human-race-is-doing-well-what-will-society-be-like"">here</a>) or disvalue (more examples <a href=""https://foundational-research.org/risks-of-astronomical-future-suffering/#Some_scenarios_for_future_suffering"">here</a>).<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-21"" id=""fnref-S8My2kBLixCNaJDFB-21"">[21]</a></sup> It is thus important to consider the values/preferences that future agents might have.</p>
<h3>We can expect future agents to have other-regarding preferences that we would, after reflection, find somewhat positive</h3>
<p>When referring to future agents’ preferences, we distinguish between ‘self-regarding preferences’, i.e. preferences about states of affairs that directly affect an agent, and ‘other-regarding preferences’, i.e. preferences about the world that remain even if an agent is not directly affected (see footnote<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-22"" id=""fnref-S8My2kBLixCNaJDFB-22"">[22]</a></sup> for a precise definition). Future agents’ other-regarding preferences will be crucial for the value of the future. For example, if the future contains powerless beings in addition to powerful agents, the welfare of the former will depend to a large degree on the other-regarding preferences of the latter (much more about that later).</p>
<h4>We can expect a considerable fraction of future agents’ preferences to be other-regarding</h4>
<p>Most people alive today clearly have (positive and negative) other-regarding preferences, but will this be the case for future agents? It has been argued that over time, other-regarding preferences could be stripped away by Darwinian selection. We explore this argument and several counterarguments in appendix 2. We conclude that future agents will, in expectation, have a considerable fraction of other-regarding preferences.</p>
<h4>Future agents’ preferences will in expectation be parallel rather than anti-parallel to our reflected preferences</h4>
<p>We want to estimate the EV of a future shaped by powerful tools according to future agents’ other-regarding preferences. In this article we assume that we should ultimately aim to satisfy our reflected moral preferences, the preferences we would have after an idealized reflection process (as discussed in the ""Moral assumptions"" section above). Thus, we must establish how future agents’ other-regarding preferences (FAP) compare to our reflected other-regarding preferences (RP). Briefly put, we need to ask: “would we want the same things as these future agents who will shape the world?”</p>
<p>FAP can be somewhere on a spectrum from parallel to orthogonal to anti-parallel to RP. If FAP and RP are parallel, future agents agree exactly with our reflected preferences. If the are anti-parallel, future agents see value exactly where we see disvalue. And if the are orthogonal, future agents value what we regard as neutral, and vice versa.  We now examine how FAP will be distributed on this spectrum.</p>
<p>Assume that future agents care about moral reflection. They will then have better conditions for an idealized reflection process than we have, for several reasons:</p>
<ul>
<li>
<p>Future agents will probably be more intelligent and rational<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-23"" id=""fnref-S8My2kBLixCNaJDFB-23"">[23]</a></sup></p>
</li>
<li>
<p>Empirical advances will help inform moral intuitions (e.g. experience machines might allow agents to get a better idea of other beings’ experiences)</p>
</li>
<li>
<p>Philosophy will advance <a href=""https://fragile-credences.github.io/ps/"">further</a></p>
</li>
<li>
<p>Future agents will have more time and resources to deliberate</p>
</li>
</ul>
<p>Given these prerequisites, it seems that future agents’ moral reflection would in expectation lead to FAP that are parallel rather than anti-parallel to RP. How much overlap between FAP and RP to expect remains difficult to estimate.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-24"" id=""fnref-S8My2kBLixCNaJDFB-24"">[24]</a></sup></p>
<p>However, scenarios in which future agents do not care about moral reflection might substantially influence the EV of the future. For example, it might be likely  that humanity loses control and the agents shaping the future bear no resemblance to humans. This could be the case if developing controlled artificial general intelligence (AGI) is very hard, and the probability that misaligned AGI will be developed is high (in this case, the future agent <em>is</em> a misaligned AI).<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-25"" id=""fnref-S8My2kBLixCNaJDFB-25"">[25]</a></sup></p>
<p>Even if (post-)humans remain in control, human moral intuitions might turn out to be contingent the starting conditions of the reflection process and not very convergent across the species. Thus, FAP may not develop into any clear direction, but rather drift randomly<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-26"" id=""fnref-S8My2kBLixCNaJDFB-26"">[26]</a></sup>. Very strong and fast <a href=""http://reducing-suffering.org/will-future-civilization-eventually-achieve-goal-preservation/"">goal drift</a> might be possible if future agents include digital (human) minds because such minds would not be restrained by the cultural universals rooted in the physical brain architecture.</p>
<p>If it turns out that FAP develop differently from RP, FAP will in expectation be orthogonal to RP rather than anti-parallel. The space of possible preferences is vast, so it seems much more likely that FAP will be completely different from RP, rather than exactly opposite<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-27"" id=""fnref-S8My2kBLixCNaJDFB-27"">[27]</a></sup> (See footnote<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-28"" id=""fnref-S8My2kBLixCNaJDFB-28"">[28]</a></sup> for an example). In summary, FAP parallel or orthogonal to RP both seem likely, but a large fraction of FAP being anti-parallel to RP seems fairly unlikely. This main claim seems true for most “idealized reflection processes” that people would choose.</p>
<p>However, FAP being between parallel and orthogonal to RP in expectation does not necessarily imply the future will be good. Actions driven by (orthogonal) FAP could have very harmful side-effects, as judged by our reflected preferences. Harmful side-effects could be devastating especially if future agents are indifferent towards beings we (would on reflection) care about morally. Such negative side-effects might outweigh positive intended effects, as has happened in the past<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-29"" id=""fnref-S8My2kBLixCNaJDFB-29"">[29]</a></sup>. Indeed, some of the most discussed <a href=""https://foundational-research.org/risks-of-astronomical-future-suffering/"">“risks of astronomical future suffering”</a> are examples of negative side-effects.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-30"" id=""fnref-S8My2kBLixCNaJDFB-30"">[30]</a></sup></p>
<h3>Future agents’ tools and preferences will in expectation shape a world with probably net positive welfare</h3>
<p>Above we argued that we can expect some overlap between future agents’ other-regarding preferences (FAP) and our reflected other-regarding preferences (RP). We can thus be somewhat optimistic about the future in a very general way, independent of our first-order moral views, if we ultimately aim to satisfy our reflected preferences. In the following section, we will drop some of that generality. We will examine what future agents’ preferences will imply for the welfare of future beings. In doing so, we assume that we would on reflection hold an aggregative, welfarist altruistic view (as explained in the background-section).</p>
<p>If we assume these specific RP, can we still expect FAP to overlap with them? After all, other-regarding preferences anti-parallel to welfarist altruism – such as sadistic, hateful, revengeful preferences - clearly exist within present day humanity. If current human values transferred broadly into the future, should we then expect a large fraction of FAP being anti-parallel to welfarist altruism? Probably not. We argue in appendix 3 that although this is hard to quantify, the large majority of human other-regarding preferences seem positive.</p>
<p>Assuming somewhat welfarist FAP, we explore what the future might be like for two types of beings: Future agents (post-humans) who have powerful tools to shape the world, and powerless future beings. To aggregate welfare for moral evaluation, we need to estimate how many beings of each type will exist. Powerful agents will likely be able to create powerless beings as “tools” if this seems useful for them. Sentient “tools” could include animals, farmed for meat production or spread to other planets for terraforming (e.g. insects), but also digital sentient minds, like sentient robots for task performance or simulated minds created for scientific experimentation or entertainment. The last example seems especially relevant, as digital minds could be created in vast amounts if digital sentience is possible at all, which <a href=""https://foundational-research.org/altruists-should-prioritize-artificial-intelligence//#VII_Artificial_sentience_and_risks_of_astronomical_suffering"">does not seem unlikely</a>. If we find we morally care about these “tools” upon reflection, the future would contain many times more powerless beings than powerful agents.</p>
<p>The EV of the future thus depends on the welfare of both powerful agents and powerless beings, with the latter potentially much more relevant than the former. We now consider each in turn, asking:</p>
<ul>
<li>How will their expected welfare be affected by intended effects and side-effects of future agents’ actions?</li>
<li>How to evaluate this morally?</li>
</ul>
<h4>The aggregated welfare of powerful future agents is in expectation positive</h4>
<p>Future agents will have powerful tools to satisfy their self-regarding preferences and be somewhat benevolent towards each other. Thus, we can expect future agents’ welfare to be increased through intended effects of their actions.</p>
<p>Side-effects of future agents’ actions negative for other agents’ welfare would mainly arise if their civilization is not coordinated well. However, compromise and cooperation seem to usually benefit all involved parties, indicating that we can expect future agents to develop good tools for coordination and use them a lot.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-31"" id=""fnref-S8My2kBLixCNaJDFB-31"">[31]</a></sup> Coordination also seems essential to avert many extinction risks. Thus, a civilization that avoided extinction so successfully that it colonizes space is expected to be quite coordinated.</p>
<p>Taken together, vastly more resources will likely be used in ways that improve the welfare of powerful agents than in ways that diminish their welfare. From the big majority of welfarist views, future agents’ aggregated welfare is thus expected to be positive. This conclusion is also supported by human history, as improved tools, cooperation and altruism have increased the welfare of most humans and average human lives are seen as worth living by many (see part 1.1).</p>
<h4></h4>
<h4>The aggregated welfare of powerless future beings may in expectation be positive</h4>
<p>Assuming that future agents are mostly indifferent towards the welfare of their “tools”, their actions would affect powerless beings only via (in expectation random) side-effects. It is thus relevant to know the “default” level of welfare of powerless beings. If the affected powerless beings were animals shaped by evolution, their default welfare might be net negative. This is because evolutionary pressure might result in a pain-pleasure asymmetry with suffering being much more intense than pleasure (see footnote for further explanation<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-32"" id=""fnref-S8My2kBLixCNaJDFB-32"">[32]</a></sup>). Such evolutionary pressure would not apply for designed digital sentience. Given that our experience with welfare is restricted to animals (incl. humans) shaped by evolution, it is unclear what the default welfare of digital sentients would be. If there is at least some moral concern for digital sentience, it seems fairly likely that the creating agents would prefer to give their sentient tools net positive welfare<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-33"" id=""fnref-S8My2kBLixCNaJDFB-33"">[33]</a></sup>.</p>
<p>If future agents intend to affect the welfare of powerless beings, they might - besides from treating their sentient “tools” accordingly - create <a href=""http://reflectivedisequilibrium.blogspot.co.uk/2012/03/are-pain-and-pleasure-equally-energy.html"">(dis-)value optimized sentience</a>: minds that are optimized for extreme positive or negative welfare. For example, future agents could simulate many minds in bliss, or many minds in agony. The motivation for creating (dis-)value optimized sentience could be altruism, sadism or strategic reasons<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-34"" id=""fnref-S8My2kBLixCNaJDFB-34"">[34]</a></sup>. Creating (dis-)value optimized sentience would likely produce much more (negative) welfare per unit of invested resources than the side-effects on sentient tools mentioned above, as sentient tools are optimized for task performance, not production of (dis-)value<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-35"" id=""fnref-S8My2kBLixCNaJDFB-35"">[35]</a></sup>. (Dis-)value optimized sentience would then be the main determinant of the expected value of post-human space colonization, and not side-effects on sentient tools.</p>
<p>FAP may be orthogonal to welfarist altruism, in which case little (dis-)value optimized sentience will be produced. However, we expect a much larger fraction of FAP to be parallel to welfarist altruism than anti-parallel to it, and thus expect that future agents will use many more resources to create value-optimized sentience than disvalue-optimized sentience. The possibility of (dis-)value optimized sentience should increase the net expected welfare of powerless future beings. However, there is considerable uncertainty about the moral implications of one resource-unit spent optimized for value or disvalue (see e.g. <a href=""http://reflectivedisequilibrium.blogspot.co.uk/2012/03/are-pain-and-pleasure-equally-energy.html"">here</a> and <a href=""http://www.simonknutsson.com/reply-to-shulmans-are-pain-and-pleasure-equally-energy-efficient/"">here</a>). On the one hand, (dis)value optimized sentience created without evolutionary pressure might be equally efficient in producing moral (dis)value, but used a lot more to produce value. On the other hand, disvalue optimized sentience might lead to especially intense suffering. Many people intuitively give more moral importance to the prevention of suffering the worse it gets (e.g. <a href=""https://en.wikipedia.org/wiki/Prioritarianism"">prioritarianism</a>).</p>
<p>In summary, it seems plausible that a little concern for the welfare of sentient tools could go a long way. Even if most future agents were completely indifferent towards sentient tools (=majority of FAP orthogonal to RP), positive intended effects – creation of value-optimized sentience – could plausibly weigh heavier than side-effects.</p>
<h3>Conclusion</h3>
<p>Morally evaluating the future scenarios sketched in part 1.2 is hard because we are uncertain. Both empirically uncertain what the future will be like and morally uncertain what our intuitions will be like. The key unanswered questions are</p>
<ul>
<li>How much can we expect the preferences that shape the future to overlap with our reflected preferences?</li>
<li>In absence of concern for the welfare of sentient tools, how good or bad is their default welfare?</li>
<li>How will the scales of intended effects and side-effects compare?</li>
</ul>
<p>Taken together, we believe that the arguments in this section indicate that the EV of (post)-human space colonization would only be negative from relatively strongly disvalue-focused views. From the majority, but not overwhelming majority, of welfarist views the EV of (post)-human space colonization seems positive.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-36"" id=""fnref-S8My2kBLixCNaJDFB-36"">[36]</a></sup><sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-37"" id=""fnref-S8My2kBLixCNaJDFB-37"">[37]</a></sup></p>
<p>In parts 1.1 and 1.2, we directly estimated the EV of (post-)human space colonization and found it to be very uncertain. In the remaining parts, we will improve our estimate via other approaches that are less dependent on specific predictions about how (post-)humans will shape the future.</p>
<h2>1.3: Future agents could later decide not to colonize space (option value)</h2>
<p>We are often uncertain about what the right thing to do is. If we can defer the decision to someone wiser than ourselves, this is generally a good call. We can also defer across time: we can keep our options open for now, and hope our descendants will be able to make better decisions. This option value may give us a reason to prefer to keep our options open.</p>
<p>For instance, our descendants may be in a better position to judge whether space colonization would be good or bad. If they can see that space colonization would be negative, they can refrain from (further) colonizing space: They have the option to limit the harm. In contrast, if humanity goes extinct, the option of (post)-human space colonization is forever lost. So avoiding extinction creates ‘option value’(e.g. <a href=""https://docs.google.com/document/d/1hQI3otOAT39sonCHIM6B4na9BKeKjEl7wUKacgQ9qF8/"">Macaskill</a>).<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-38"" id=""fnref-S8My2kBLixCNaJDFB-38"">[38]</a></sup> This specific type of ‘option value’ - from future agents choosing not to colonize space - and not the more general value of keeping options open, is what we will be referring to throughout this section.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-39"" id=""fnref-S8My2kBLixCNaJDFB-39"">[39]</a></sup> This type of option value exist for nearly all moral views, and is very unlikely to be negative.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-40"" id=""fnref-S8My2kBLixCNaJDFB-40"">[40]</a></sup> However, as we will discuss in this chapter, this value is rather small compared to other considerations.</p>
<h3>A considerable fraction of futures contains option value</h3>
<p>Reducing the risk of human extinction only creates option value if future agents will make a better decision, by our (reflected) lights, about whether to colonize space than we could. If they will make worse decisions than us, we would rather decide ourselves.</p>
<p>In order for future agents to make better decisions than us and actually act on them, they need to surpass us in at least one of the following aspects:</p>
<ul>
<li>Better values</li>
<li>Better judgement what space colonization will be like (based on increased empirical understanding and rationality)</li>
<li>Greater willingness and ability to make decisions based on moral values (non-selfishness and coordination)</li>
</ul>
<h4>Values</h4>
<p>Human values change. We are disgusted by many of our ancestors’ moral views, and they would find ours equally repugnant. We can even look back on our own moral views and disagree. There is no reason for these trends to stop exactly now: human morality will likely continue to change.</p>
<p>Yet at each stage in the change, we are likely to view our values as obviously correct. This encourages a greater degree of <a href=""https://concepts.effectivealtruism.org/concepts/moral-uncertainty/"">moral uncertainty</a> than feels natural. We should expect that our moral views would change after idealized reflection (although this also depends on which meta-ethical theory is correct and how idealized reflection works).</p>
<p>We argued in part 1.2 that future agents’ preferences will in expectation have some overlap with our <em>reflected</em> preferences. Even if that overlap is not very high, a high degree of moral uncertainty would indicate that we would often prefer future agents’ preferences over our <em>current, unreflected</em> preferences. In a sizeable fraction of future scenarios, future agents with more time and better tools to reflect, can be expected to make better decisions than one could today.</p>
<h4>Empirical understanding and rationality</h4>
<p>We now understand the world better than our ancestors, and are able to think more clearly. If those trends continue, future agents may understand better what space colonization will be like, and so better understand how good it will be on a given set of values.</p>
<p>For example, future agents’ estimate of the EV of space colonization will benefit from</p>
<ul>
<li>Better empirical understanding of the universe (for instance about questions discussed in part 2.2)<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-41"" id=""fnref-S8My2kBLixCNaJDFB-41"">[41]</a></sup> and better predictions, fuelled by more scientific knowledge and better forecasting techniques</li>
<li>Increased intelligence and rationality<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-42"" id=""fnref-S8My2kBLixCNaJDFB-42"">[42]</a></sup>, allowing them to more accurately determine what the best action is based on their values.</li>
</ul>
<p>As long as there is some overlap between their preferences and one’s reflected preferences, this gives an additional reason to defer to future agents’ decisions (example see footnote).<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-43"" id=""fnref-S8My2kBLixCNaJDFB-43"">[43]</a></sup></p>
<h4>Non-selfishness and coordination</h4>
<p>We often know what’s right, but don’t follow through on it anyway. What is true for diets also applies here:</p>
<ul>
<li>
<p>Future agents would need to actually make the decision about space colonization based on moral reasoning<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-44"" id=""fnref-S8My2kBLixCNaJDFB-44"">[44]</a></sup>. This might imply acting against strong economic incentives pushing towards space colonization.</p>
</li>
<li>
<p>Future agents need to be coordinated well enough to avoid space colonization. That might be a challenge in non-singleton futures since future civilization would need ways to ensure that not a single agent starts space colonization.</p>
</li>
</ul>
<p>It seems likely that future agents would probably surpass our current level of empirical understanding, rationality, and coordination, and in a considerable fraction of possible futures they might also do better on values and non-selfishness. However, we should note that to actually not colonize space, they would have to surpass a certain threshold in all of these fields, which may be quite high. Thus, a little bit of progress doesn’t help - option value is only created in deferring the decision to future agents if they surpass this threshold.</p>
<h3>Only the relative good futures contain option value</h3>
<p>For any future scenario to contain option value, the agents in that future need to surpass us in various ways, as outlined above. This has an implication that further diminishes the relevance of the option value argument. Future agents need to have relatively good values and be relatively non-selfishness to decide not to colonize space for moral reasons. But even if these agents colonized space, they would probably do it in a <em>relatively</em> good manner. Most expected future disvalue <a href=""https://foundational-research.org/cause-prioritization-downside-focused-value-systems/"">plausibly</a> comes from futures controlled by indifferent or malicious agents (like misaligned AI). Such “bad” agents will make worse decisions about whether or not to colonize space than we, currently, could, because their preferences are very different from our (reflected) preferences. Potential space colonization by indifferent or malicious agents thus generates large amounts of expected future disvalue, which cannot be alleviated by option value. Option value doesn’t help in the cases where it is most needed (see footnote for an explanatory example)<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-45"" id=""fnref-S8My2kBLixCNaJDFB-45"">[45]</a></sup></p>
<h3>Conclusion</h3>
<p>If future agents are good enough, there is option value in deferring the decision whether to colonize space to them. In some not-too-small fraction of possible futures, agents will fulfill the criteria and thus option value adds positively to the EV of reducing extinction risk. However, the futures accounting for most expected future disvalue are likely controlled by indifferent or malicious agents. Such “bad” agents would likely make worse decisions than we could. A large amount of expected future disvalue is thus not amendable from alleviation through option value. Overall, we think the option value in reducing the risk of human extinction is probably fairly moderate, but there is a lot of uncertainty and contingency on one’s specific moral and empirical views<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-46"" id=""fnref-S8My2kBLixCNaJDFB-46"">[46]</a></sup>. <a href=""https://www.getguesstimate.com/models/10027"">Modelling</a> the considerations of this section showed that if the 90% confidence interval of value of the future was from -0.9 to 0.9 (arbitrary value units), option value was 0.07.</p>
<h1>Part 2: Absence of (post-)human space colonization does not imply a universe devoid of value or disvalue</h1>
<p>Up to now, we have tacitly assumed that the sign of EV of (post)-human space colonization determines whether extinction risk reduction is worthwhile. This only holds if without humanity, the EV of the future is roughly zero, because the (colonizable) universe is and will stay devoid of value or disvalue. We now consider two classes of scenarios in which this is not the case, with important implications especially for people who think that EV of (post-)human space colonization is likely negative.</p>
<h2>2.1 Whether (post-)humans colonizing space is good or bad, space colonization by other agents seems worse</h2>
<p>If humanity goes extinct without colonizing space, some kind of other beings would likely survive on earth<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-47"" id=""fnref-S8My2kBLixCNaJDFB-47"">[47]</a></sup>. These beings might evolve into a non-human technological civilization in the hundreds of millions of years left on earth and eventually colonize space. Similarly, extraterrestrials (that might already exist or come into existence in the future) might colonize (more of) our corner of the universe, if humanity does not.</p>
<p>In these cases, we must ask whether we prefer (post-)human space colonization over the alternatives. Whether alternative civilizations would be more or less <a href=""https://foundational-research.org/how-would-catastrophic-risks-affect-prospects-for-compromise/#Might_humans_be_replaced_by_other_species"">compassionate</a> or <a href=""http://effective-altruism.com/ea/14y/saving_expected_lives_at_10_apiece/9hp"">cooperative</a> than humans, we can only guess. We may however assume that our reflected preferences depend on some aspects of being human, such as human culture or the biological structure of the human brain<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-48"" id=""fnref-S8My2kBLixCNaJDFB-48"">[48]</a></sup>. Thus, our reflected preferences likely overlap more with a (post-)human civilization than alternative civilizations. As future agents will have powerful tools to shape the world according to their preferences, we should prefer (post-)human space colonization over space colonization by an alternative civilization.</p>
<p>To understand how we can factor this consideration into the overall EV of a future with (post-) human space colonization, consider the following example of Ana and Chris. Ana thinks the EV of (post-)human space colonization is negative. For her, the EV of potential alternative space colonization is thus even more negative. This should cause people who, like Ana, are pessimistic about the EV of (post-)human space colonization (and thus the value of reducing the risk of human extinction) to update towards reducing the risk of human extinction because the alternative is even worse (technical caveat in footnote<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-49"" id=""fnref-S8My2kBLixCNaJDFB-49"">[49]</a></sup>).</p>
<p>Chris thinks that the EV of (post-)human space colonization is positive. For him, the EV of potential alternative space colonization could be positive or negative. For people like Chris, who are optimistic about the EV of (post-)human space colonization (and thus the value of reducing the risk of human extinction), the direction of update is thus less clear. They should update towards reducing the risk of human extinction if the potential alternative civilization is bad, or away from it if the potential alternative civilization is merely less good. Taken together, this consideration implies a stronger update for future pessimists like Ana than for future optimists like Chris. This becomes clearer in the mathematical derivation<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-50"" id=""fnref-S8My2kBLixCNaJDFB-50"">[50]</a></sup> or when considering an example<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-51"" id=""fnref-S8My2kBLixCNaJDFB-51"">[51]</a></sup>.</p>
<p>It remains to estimate how big the update should be. Based on our best guesses about the relevant parameters (Fermi-estimate see <a href=""https://docs.google.com/spreadsheets/d/17m84p4ovCAcwL-uha1hdI3I7vcH8j7Wwx145gYJoEDQ/edit?usp=sharing"">here</a>), it seems like future pessimists should considerably shift their judgement of the EV of human extinction risk reduction into the less negative direction. Future optimists should moderately shift their judgement downwards. Therefore, if one was previously uncertain with roughly equal credence in future pessimism and future optimism, one’s estimate for the EV of human extinction risk reduction should increase.</p>
<p>We should note that this is a very broad consideration, with details contingent on the actual moral views people hold and specific empirical considerations<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-52"" id=""fnref-S8My2kBLixCNaJDFB-52"">[52]</a></sup>.</p>
<p>A specific case of alternative space colonization could arise if humanity gets extinguished by misaligned AGI. It seems likely that misaligned AI would <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8356&amp;rep=rep1&amp;type=pdf"">colonize</a> <a href=""https://wiki.lesswrong.com/wiki/Basic_AI_drives"">space</a>. Space colonization by an AI might include (among other things of value/disvalue to us) the creation of many digital minds for instrumental purposes. If the AI is only driven by values orthogonal to ours, it would likely not care about the welfare of those digital minds. Whether we should expect space colonization by a human-made, misaligned AI to be morally worse than space colonization by future agents with (post-)human values has been discussed <a href=""https://foundational-research.org/cause-prioritization-downside-focused-value-systems/"">extensively</a> <a href=""https://foundational-research.org/altruists-should-prioritize-artificial-intelligence/#VII_Artificial_sentience_and_risks_of_astronomical_suffering"">elsewhere</a>. Briefly, nearly all moral views would most likely rather have human value-inspired space colonization than space colonization by AI with arbitrary values, giving extra reason to work on AI alignment especially for future pessimists.</p>
<h2>2.2 Existing disvalue could be alleviated by colonizing space</h2>
<p>With more empirical knowledge and philosophical reflection, we may find that the universe is <em>already</em> filled with beings/things that we morally care about. Instead of just increasing the number of morally relevant things (i.e. earth originating sentient beings), future agents might then influence the states of morally relevant beings/things already existing in the universe<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-53"" id=""fnref-S8My2kBLixCNaJDFB-53"">[53]</a></sup>. This topic is highly speculative and we should stress that most of the EV probably comes from “unknown unknowns”, which humanity might discover during idealized reflection. Simply put, we might find some way in which future agents can make the existing world (a lot) better if they stick around. To illustrate this general concept, consider the following ideas.</p>
<p>We might find that we morally care about things other than sentient beings, which could be vastly abundant in the universe. For example, we may develop moral concern for <a href=""http://reducing-suffering.org/is-there-suffering-in-fundamental-physics/"">fundamental physics</a>, e.g. in the form of <a href=""https://en.wikipedia.org/wiki/Panpsychism"">panpsychicism</a>. Another possibility could arise if the solution to the simulation argument <a href=""https://www.simulation-argument.com/simulation.html"">(Bostrom, 2003)</a> is indeed that we live in a simulation, with most things of moral relevance positioned outside of our simulation but modifiable by us in yet unknown ways. It might also turn out that we can interact with other agents in the (potentially infinite) universe or multiverse by <a href=""https://wiki.lesswrong.com/wiki/Acausal_trade"">acausal trade </a>or <a href=""http://effective-altruism.com/ea/1gf/multiversewide_cooperation_in_a_nutshell/"">multiverse-wide cooperation</a>, thereby influencing existing things of moral relevance (to us) in their part of the universe/multiverse. These specific ideas may look weird. However, given humanity’s history of realizing that we care about more/other things than previously thought<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-54"" id=""fnref-S8My2kBLixCNaJDFB-54"">[54]</a></sup>, it should in principle seem likely that our reflected preferences include some yet unknown unknowns.</p>
<p>We argued in part 1.2 that future agents’ preferences will in expectation be parallel rather than anti-parallel to our reflected preferences. If the universe is already filled with things/beings of moral concern, we can thus assume that future agents will in expectation improve the state of these things<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-55"" id=""fnref-S8My2kBLixCNaJDFB-55"">[55]</a></sup>. This creates additional reason to reduce the risk of human extinction: There might be a moral responsibility for humanity to stick around and “improve the universe”. This perspective is especially relevant for disvalue-focused views. From a (strongly) disvalue-focused view, increasing the numbers of conscious beings by space colonization is negative because it generates suffering and disvalue. It might seem that there is little to gain if space colonization goes well, but much to lose if it goes wrong. If, however, future agents could alleviate existing disvalue, then humanity’s survival (potentially including space colonization) has upsides that may well be larger than the expected downsides (Fermi-estimate see footnote<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-56"" id=""fnref-S8My2kBLixCNaJDFB-56"">[56]</a></sup>).<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-57"" id=""fnref-S8My2kBLixCNaJDFB-57"">[57]</a></sup></p>
<h1>Part 3: Efforts to reduce extinction risk may also improve the future</h1>
<p>If we had a button that reduces human extinction risk, and has no other effect, we would only need the considerations in parts 1 and 2 to know whether we should press it. In practice, efforts to reduce extinction risk often have other morally relevant consequences, which we examine below.</p>
<h2>3.1: Efforts to reduce non-AI extinction risk reduce global catastrophic risk<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-58"" id=""fnref-S8My2kBLixCNaJDFB-58"">[58]</a></sup></h2>
<p>Global catastrophe here refers to a scenario of hundreds of millions of human deaths and resulting societal collapse. Many potential causes of human extinction, like a large scale epidemic, nuclear war, or runaway climate change, are far more likely to lead to a global catastrophe than to complete extinction. Thus, many efforts to reduce the risk of human extinction also reduce global catastrophic risk. In the following, we argue that this effect adds substantially to the EV of efforts to reduce extinction risk, even from the very-long term perspective of this article. This doesn’t hold for efforts to reduce risks that, like risks from misaligned AGI, are more likely to lead to complete extinction than to a global catastrophe.</p>
<p>Apart from being a dramatic event of immense magnitude for current generations, a global catastrophes could severely curb humanity’s long-term potential by destabilizing technological progress and derailing social progress<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-59"" id=""fnref-S8My2kBLixCNaJDFB-59"">[59]</a></sup>.</p>
<p>Technological progress might be <a href=""https://foundational-research.org/how-would-catastrophic-risks-affect-prospects-for-compromise/"">uncoordinated and incautious </a>in a world that is politically destabilized by global catastrophe. For pivotal technologies such as AGI, development in an arms race scenario (e.g. driven by post-catastrophe resource scarcity or war) could lead to adverse outcomes we cannot correct afterwards.</p>
<p>Social progress might likewise divert towards opposing open society and general utilitarian-type values. Can we expect the “new” value system emerging after a global catastrophe to be robustly worse than our current value system? While this issue is debated<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-60"" id=""fnref-S8My2kBLixCNaJDFB-60"">[60]</a></sup>, <a href=""https://blog.givewell.org/2015/08/13/the-long-term-significance-of-reducing-global-catastrophic-risks/"">Nick Beckstead</a> gives a strand of arguments suggesting the “new” values would in expectation be worse. Compared to the rest of human history, we currently seem to be on a unusually promising trajectory of social progress. What exactly would happen if this period was interrupted by a global catastrophe is a difficult question, and any answer will involve many judgements calls about the contingency and convergence of human values. However, as we hardly understand the driving factors behind the current period of social progress, we cannot be confident it would recommence if interrupted by a global catastrophe. Thus, if one sees the current trajectory as broadly positive, one should expect this value to be partially lost if a global catastrophe occurs.</p>
<p>Taken together, reducing global catastrophic risk seems to be a valuable effect of efforts to reduce extinction risk. This aspect is fairly relevant even from a very-long term perspective because catastrophes are much more likely than extinction. A <a href=""https://docs.google.com/spreadsheets/d/1XKfakFBWrLERqM7l5t0oNuJuvyipTsyEQyY_nNyW7yI/edit?usp=sharing"">Fermi-Estimate</a> suggests the long-term impact from the prevention of global catastrophes is about 50% of the impact from avoiding extinction events. The potential long-term consequences from a global catastrophe include worse values and an increase in the likelihood of misaligned AI scenarios. These consequences seem bad from most moral perspectives, including strongly disvalue-focused ones. Considering the effects on global catastrophic risk should suggest a significant update in the evaluation of the EV of efforts to reduce extinction risk towards more positive (or less negative) values.</p>
<h2>3.2: Efforts to reduce extinction risk often promote coordination, peace and stability, which is broadly good</h2>
<p>The shared future of humanity is a (transgenerational) global public good <a href=""http://www.existential-risk.org/concept.html"">(Bostrom, 2013)</a>, thus society needs to coordinate to preserve it, e.g. by providing funding and other incentives. Most extinction risk also arises from technologies that allow for one agent (intentionally or by mistake) to start a potential extinction event (e.g. release a harmful virus or start a nuclear war). Coordinated action and careful decisions are thus needed and indeed, the broadest efforts to reduce extinction risk directly promote global coordination, peace and stability. More focused efforts often promote “narrow cooperation” within a specific field (e.g. nuclear non-proliferation) or set up processes (e.g. pathogenic surveillance) that increase global stability by reducing perceived levels of threat from non-extinction events (e.g. bioterrorist attacks).</p>
<p>Taken together, efforts to reduce extinction risk also promote a more coordinated, peaceful and stable global society. Future agents in such a society will probably make wiser and more careful decisions, reducing the risk of unexpected negative trajectory changes in general. Safe development of AI will specifically <a href=""https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf"">depend on</a> <a href=""https://nickbostrom.com/papers/aipolicy.pdf"">these factors</a>. Therefore, efforts to reduce extinction risk may also steer the world away from some of the worst non-extinction outcomes, which likely involve war, violence and arms races.</p>
<p>Note that there may be a trade-off as most targeted efforts seem more neglected and therefore promising levers for extinction risk reduction. However, their effects on global coordination, peace and stability are less certain and likely smaller than the effects of broad efforts aimed directly at increasing these factors. Broad efforts to promote global coordination, peace and stability might be among the most promising approaches to robustly improve the future and reduce the risk of dystopian outcomes conditional on human survival.</p>
<h1>Conclusion</h1>
<h2>The expected value of efforts to reduce the risk of human extinction (from non-AI causes) seems robustly positive</h2>
<p>So all things considered, what is the expected value of efforts to reduce the risk of human extinction? In the first part, we considered what might happen if human extinction is prevented for long enough that future agents, maybe our biological descendants, digital humans, or (misaligned) AGI created by humans, colonize space. The EV of (post-)human space colonization is probably positive from many welfarist perspectives, but very uncertain. We also examined the ‘option value argument’, according to which we should try to avoid extinction and defer the decision to colonize space (or not) to wiser future agents. We concluded that option value, while mostly positive, is small and the option value argument hardly conclusive.</p>
<p>In part 2, we explored what the future universe might look like if humans do go extinct. Vast amounts of value or disvalue might (come to) exist in those scenarios as well. Some of this (dis-)value could be influenced by future agents if they survive. This insight has little impact for people who were optimistic about the future anyway, but shifts the EV of reducing extinction risk upwards for people who were previously pessimistic about the future. In part 3, we extended our considerations to additional effects of many efforts to reduce extinction risk, namely reducing the risk of “mere” global catastrophes and increasing global cooperation and stability. These effects generate considerable additional positive long-term impact. This is because global catastrophes would likely change the direction of technological and social progress in a bad way, while global cooperation and stability are prerequisites for a positive long-term trajectory.</p>
<p>Some aspects of moral views make the EV of reducing extinction risk looks less positive than suggested above. We will consider three such aspects:</p>
<ul>
<li>From a strongly disvalue-focused view, increasing the total number of sentient beings seems negative regardless of the empirical circumstances. The EV of (post-) human space colonization (part 1.1 and 1.2) is thus negative, at least if the universe is currently devoid of value.</li>
<li>From a very stable moral view (with low moral uncertainty, thus very little expected change in preferences upon idealized reflection), there are no moral insights for future agents to discover and act upon. Future agents could then only make better decisions than us about whether to colonize space through empirical insights. Likewise, future agents could only discover opportunities to alleviate astronomical disvalue that we currently do not see through empirical insights. Option value (part 1.3) and the effects from potentially existing disvalue (part 2.2) are reduced.</li>
<li>From a very unusual moral view (with some of one’s reflected other-regarding preferences expected to be anti-parallel to most of humanity’s reflected other-regarding preferences), future agents will sometimes do the opposite of what one would have wanted<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-61"" id=""fnref-S8My2kBLixCNaJDFB-61"">[61]</a></sup>. This would be true even if future agents are reflected and act altruistically (according to a different conception of ‘altruism’). From that view the future looks generally worse. There is less option value (part 1.3), and if the universe is already filled with beings/things that we morally care about (part 2.2), sometimes future agents might do the <em>wrong</em> thing upon this discovery.</li>
</ul>
<p>To generate the (hypothetical) moral view that is most sceptical about reducing extinction risk, we unite all of the three aspects above. We assume a strongly disvalue-focused, very stable and unusual moral view. Even from this perspective (in rough order of descending relevance):</p>
<ul>
<li>Efforts to reduce extinction risk may improve the long-term future by reducing the risk of global catastrophes and increasing global cooperation and stability (part 3).</li>
<li>There may be some opportunity for future agents to alleviate existing disvalue (as long as the moral view in question isn’t completely ‘unusual’ in all aspects) (part 2.2)</li>
<li>(Post-)humans space colonization might be preferable to space colonization by non-human animals or extraterrestrials (part 2.1)</li>
<li>Small amounts of option value might arise from empirical insights improving decisions (part 1.3).</li>
</ul>
<p>From this maximally sceptical view, targeted approaches to reduce the risk of human extinction likely seem somewhat unexciting or neutral, with high uncertainty (see footnote<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-62"" id=""fnref-S8My2kBLixCNaJDFB-62"">[62]</a></sup> for how advocates of strongly disvalue-focused views see the EV of efforts to reduce extinction risk). Reducing the risk of extinction by misaligned AI probably seems positive because misaligned AI would also colonize space (see part 2.1).</p>
<p>From views that value the creation of happy beings or creation of value more broadly, have considerable moral uncertainty, and believe future reflected and altruistic agents could make good decisions, the EV of efforts to reduce extinction risk is likely positive and extremely high.</p>
<p>In aggregation, efforts to reduce the risk of human extinction seem in expectation robustly positive from many consequentialist perspectives.</p>
<h2>Efforts to reduce extinction risk should be a key part of the EA long-termist portfolio</h2>
<p>Effective altruists whose primary moral concern is making sure the future plays out well will, in practice, need to allocate their resources between different possible efforts. Some of these efforts are optimized to reduce extinction risk (e.g. promoting biosecurity), others are optimized to improve the future conditional on human survival while also reducing extinction risk (e.g. promoting global coordination or otherwise preventing <a href=""https://www.effectivealtruism.org/articles/a-proposed-adjustment-to-the-astronomical-waste-argument-nick-beckstead/"">negative trajectory changes</a>) and some are optimized to improve the future without making extinction risk reduction a primary goal (e.g. promoting <a href=""https://sentience-politics.org/expanding-moral-circle-reduce-suffering-far-future/"">moral circle expansion</a> or <a href=""https://foundational-research.org/suffering-focused-ai-safety/"">""worst-case""</a> <a href=""http://s-risks.org/using-surrogate-goals-to-deflect-threats/"">AI safety</a> research).</p>
<p>We have argued above that the EV of efforts to reduce extinction risk is positive, but is it large enough to warrant investment of marginal resources? A thorough answer to this question requires detailed examination of the specific efforts in question and goes beyond the scope of this article. We are thus in no position to provide a definitive answer for the community. We will, however, present two arguments that favor including efforts to reduce extinction risk as a key part in the long-termist EA portfolio. Efforts to reduce the risks of human extinction are time-sensitive and seem very leveraged. We know of specific risks this century, we have reasonably good ideas for ways to reduce them, and if we actually avert an extinction event, this has robust impact for millions of years (at least in expectation) to come. As a very broad generalization, many efforts optimized to otherwise improve the future - such as improving today’s values in the hope that they will propagate to future generations - are less time-sensitive or leveraged. In short, it seems easier to prevent an event from happening in this century than to otherwise robustly influence the future millions of years down the line.</p>
<p>Key caveats to this argument include that it is not clear how big differences in time-sensitivity and leverage are<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-63"" id=""fnref-S8My2kBLixCNaJDFB-63"">[63]</a></sup> and that we may still discover <a href=""https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/#III_Reducing_s-risks_is_both_tractable_and_neglected"">highly leveraged ways</a> to “otherwise improve the future”. Therefore, it seems that the EA long-termist portfolio should contain all of the efforts described above, allowing each member of the <a href=""http://effective-altruism.com/ea/1cb/how_can_we_best_coordinate_as_a_community/"">community</a> to contribute to their comparative advantage. For those holding very disvalue-focused moral views, the more attractive efforts would plausibly be those optimized to improve the future without making extinction risk reduction a primary goal.</p>
<h1>Acknowledgments:</h1>
<p>We are grateful to Brian Tomasik, Max Dalton, Lukas Gloor, Gregory Lewis, Tyler John, Thomas Sittler, Alex Norman, William MacAskill and Fabienne Sandkühler for helpful comments on the manuscript. Additionally, we thank Max Daniel, Sören Mindermann, Carl Shulman and Sebastian Sudergaard Schmidt for discussions that helped inform our views on the matter.</p>
<h1>Author contributions:</h1>
<p>Jan conceived the article and the arguments presented in it. Friederike and Jan contributed to structuring the content and writing.</p>
<h1>Appendix 1: What if humanity stayed earthbound?</h1>
<p>In this appendix, we use the approach of part 1.1 and apply it to a situation in which humanity stays Earth-bound. It is recommended to first read part 1.1 before reading this appendix.</p>
<p>We think that scenarios in which humanity stays Earth-bound are of very limited relevance for the EV of the future for two reasons:</p>
<ul>
<li>Even if humanity staying Earth-bound was the most likely outcome, probably only a small fraction of expected beings live in these scenarios, so they only constitute a small fraction of expected value or disvalue (as argued in the introduction).</li>
<li>Humanity staying Earth-bound may not actually be a very likely scenario because reaching post-humanity and realizing astronomical value might be a default path, conditional on humanity not going extinct <a href=""https://nickbostrom.com/papers/future.html"">(Bostrom, 2009)</a></li>
</ul>
<p>If we assume humanity will stay Earth-bound, it seems that most welfarist views would probably favour reducing  extinction risk. If one thinks humans are much more important than animals, it is obvious (unless one combined that view with suffering-focused ethics, such as <a href=""https://en.wikipedia.org/wiki/Antinatalism"">antinatalism</a>). If one also cares about animals, then very plausibly humanity's impact on wild animals is more relevant than humanity’s impact on farmed animals, because of the enormous numbers of the former (and especially since it seems plausible that factory farming will not continue indefinitely). So far, humanity’s main effect on wild animals has been a permanent decrease of <a href=""http://reducing-suffering.org/humanitys-net-impact-on-wild-animal-suffering/"">population size</a> (through habitat destruction), which is expected to continue as human population size grows. Compared to that, direct influence on wild animal well-being currently is unclear and probably small (though it is less clear for <a href=""http://reducing-suffering.org/wild-caught-fishing-affects-wild-animal-suffering/"">aquatic life</a>):</p>
<ul>
<li>We kill significant numbers of wild animals, but we don’t know how painful human-caused death compared to non-human caused death is</li>
<li>Wild animal generation times are very short, so the number of animals affected by “never coming into existence” is probably much larger</li>
</ul>
<p>If one thinks that wild animals are on net suffering, future population size reduction seems beneficial. If one thinks that wild animal welfare is net positive, then habitat reduction would be bad. However, there is still unarguably a lot of suffering in nature. Humanity might eventually - if we have much more knowledge and better tools, that allow us to do so at limited costs to ourselves - improve wild animals’ lives (like we already do with e.g. vaccinations), so the prospect of that might offset some of the negative value of current habitat reduction. Obviously, habitat destruction is negative from a conservationist/environmentalist perspective.</p>
<h1>Appendix 2: Future agents will in expectation have a considerable fraction of other-regarding preferences</h1>
<p>Altruism in humans likely evolved as a “shortcut” solution to coordination problems. It was often impossible to forecast how much an altruistic act would help spread your own genes, but it often would (especially in small tribes, where all members were closely related). Thus, humans for whom altruism just felt good had a selective advantage.</p>
<p>As agents become more rational and long-term planning, a tendency to help for purely selfless reasons seems less adaptive. Agents can deliberately cooperate for strategic reasons whenever necessary and for the exactly optimal amount to optimize for their own reproductive fitness. One might fear that in the long run, only preferences for increasing one’s own power and influence (and that of one’s descendants) might remain under Darwinian selection.</p>
<p>But this is not necessarily the case, for two reasons:</p>
<h2>Darwinian processes will select for patience, not “selfishness” (<a href=""https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/"">Paul Christiano</a>)</h2>
<p>Agents reasoning from a long-term perspective, and the better the tools to preserve values and influence into the future, may reduce the need for altruistic preferences, but also strongly reduce selection pressure for selfishness. In contrast to short-term planning (overly) altruistic agents, long-term planning agents that want to create value would realize that amassing power is an instrumental goal for that, and will try to survive, get resources for instrumental reasons, and coordinate with others against unchecked expansion of selfish agents. Thus, future evolution might select not for selfishness, but for patience or how strongly an agent cares about the long-term. Such long-term preferences should be expected to be more altruistic.</p>
<p><a href=""http://reflectivedisequilibrium.blogspot.com/2012/09/spreading-happiness-to-stars-seems.html"">Carl Shulman</a> additionally makes the point that in a space colonization scenario, agents that want to create value would only be very slightly disadvantaged in direct competition with agents that only care about expanding.</p>
<p><a href=""http://reducing-suffering.org/the-future-of-darwinism/"">Brian Tomasik</a> thinks Christiano’s argument is valid and altruism might not be driven to zero in the future, but is doubtful that very-long term altruist will have strategic advantages over medium-term corporations and governments and cautions against putting too much weight on theoretical arguments: “Human(e) values have only a mild degree of control in the present. So it would be surprising if such values had significantly more control in the far future.”</p>
<h2>Preferences might not even be subject to Darwinian processes indefinitely</h2>
<p>If the losses from evolutionary pressure indeed loom large, it seems quite likely that future generations would coordinate against it, e.g. by forming a singleton <a href=""https://nickbostrom.com/fut/singleton.html"">(Bostrom, 2006)</a> (which broadly encompasses many forms of global coordination or value/goal-preservation). (Of course, there are also future scenarios that would strip away all other-regarding preferences, e.g. in <a href=""https://wiki.lesswrong.com/wiki/Malthusian_Scenarios"">Malthusian scenarios</a>.)</p>
<p>In conclusion,  we will end up somewhere between no other-regarding preferences and even more than today, with a considerable probability of future agents having a considerable fraction of other-regarding preferences.</p>
<h1>Appendix 3: What if current human values transferred broadly into the future?</h1>
<p>Most humans (past and present) intend to do what we now consider good (be loving, friendly, altruistic) more than they intend to harm (be sadistic, hateful, seek revenge). Positive<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-64"" id=""fnref-S8My2kBLixCNaJDFB-64"">[64]</a></sup> other-regarding preferences might be more universal: most people would, all else equal, prefer all human or animals to be happy, while fewer people would have such a general preference for suffering. This relative overhang of positive preferences in human society is evident from rules that ban hurting (some) others, but not helping others. These rules will (if they persist) also shape the future, as they increase the costs of doing harm.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-65"" id=""fnref-S8My2kBLixCNaJDFB-65"">[65]</a></sup></p>
<p>Throughout human history, there has been a trend away from cruelty and violence.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-66"" id=""fnref-S8My2kBLixCNaJDFB-66"">[66]</a></sup> Although humans cause a lot of suffering in the world today, this is mostly because people are indifferent or <a href=""http://effective-altruism.com/ea/1cl/an_argument_for_why_the_future_may_be_good/"">“lazy”</a>, rather than evil. All in all, it seems fair to say that the significant majority of human other-regarding preferences is positive, and that most people would, all else equal, prefer more happiness and less suffering. However, we admit this is hard to quantify.<sup class=""footnote-ref""><a href=""#fn-S8My2kBLixCNaJDFB-67"" id=""fnref-S8My2kBLixCNaJDFB-67"">[67]</a></sup></p>
<h1>References (only those published in peer-reviewed journals, and books):</h1>
<pre><code>    Bjørnskov, C., Boettke, P.J., Booth, P., Coyne, C.J., De Vos, M., Ormerod, P., Sacks, D.W., Schwartz, P., Shackleton, J.R., Snowdon, C., 2012. ... and the Pursuit of Happiness-Wellbeing and the Role of Government.


    Bostrom, N., 2013. Existential risk prevention as global priority. Global Policy 4, 15–31.


    Bostrom, N., 2011. INFINITE ETHICS. Analysis and Metaphysics 9–59.


    Bostrom, N., 2009. The Future of Humanity, in: New Waves in Philosophy of Technology, New Waves in Philosophy. Palgrave Macmillan, London, pp. 186–215.[ https://doi.org/10.1057/9780230227279_10](https://doi.org/10.1057/9780230227279_10)


    Bostrom, N., 2006. What is a singleton. Linguistic and Philosophical Investigations 5, 48–54.


    Bostrom, N., 2004. The future of human evolution. Death and anti-death: Two hundred years after Kant, fifty years after Turing 339–371.


    Bostrom, N., 2003a. Astronomical waste: The opportunity cost of delayed technological development. Utilitas 15, 308–314.


    Bostrom, N., 2003b. Are We Living in a Computer Simulation? The Philosophical Quarterly 53, 243–255.[ https://doi.org/10.1111/1467-9213.00309](https://doi.org/10.1111/1467-9213.00309)


    Greaves, H., 2017. Population axiology. Philosophy Compass 12, e12442.


    Killingsworth, M.A., Gilbert, D.T., 2010. A wandering mind is an unhappy mind. Science 330, 932.[ https://doi.org/10.1126/science.1192439](https://doi.org/10.1126/science.1192439)


    Pinker, S., 2011. The Better Angels of our Nature. New York, NY: Viking.


    Sagoff, M., 1984. Animal Liberation and Environmental Ethics: Bad Marriage, Quick Divorce. Philosophy &amp; Public Policy Quarterly 4, 6.[ https://doi.org/10.13021/G8PPPQ.41984.1177](https://doi.org/10.13021/G8PPPQ.41984.1177)


    Singer, P., 2011. The expanding circle: Ethics, evolution, and moral progress. Princeton University Press.


    Tuomisto, H.L., Teixeira de Mattos, M.J., 2011. Environmental Impacts of Cultured Meat Production. Environ. Sci. Technol. 45, 6117–6123.[ https://doi.org/10.1021/es200130u](https://doi.org/10.1021/es200130u)
</code></pre>
<h3>Footnotes</h3>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-S8My2kBLixCNaJDFB-1"" class=""footnote-item""><p>Simply put: two beings experiencing positive (or negative) welfare are morally twice as good (or bad) as one being experiencing the same welfare <a href=""#fnref-S8My2kBLixCNaJDFB-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-2"" class=""footnote-item""><p>Some considerations that might reduce our certainty that, even given the moral perspective of this article, most expected value or disvalue comes from space colonization:</p>
<ul>
<li>The <a href=""https://en.wikipedia.org/wiki/Doomsday_argument"">doomsday argument</a></li>
<li>Some explanations of the <a href=""https://en.wikipedia.org/wiki/Fermi_paradox"">Fermi-Paradox</a></li>
<li>Potential <a href=""https://foundational-research.org/how-the-simulation-argument-dampens-future-fanaticism"">implications</a> of the simulation argument <a href=""https://www.simulation-argument.com/simulation.pdf"">(Bostrom, 2003)</a></li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-2"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-3"" class=""footnote-item""><p>In this article, the term ‘(post-)human space colonization’ is meant to include any form of space colonization that originates from a human civilization, including cases in which (biological) humans or human values don’t play a role (e.g. because humanity lost control over artificial superintelligence, which then colonizes space). <a href=""#fnref-S8My2kBLixCNaJDFB-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-4"" class=""footnote-item""><p>… assuming that without (post-)human space colonization, the universe is and stays devoid of value or disvalue, as explained in “Outline of the article” <a href=""#fnref-S8My2kBLixCNaJDFB-4"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-5"" class=""footnote-item""><p>We here assume that humanity does not change substantially, excluding e.g. digital sentience from our considerations. This may be overly simplistic, as interstellar travel seems so difficult that a space-faring civilization will likely be extremely different from us today. <a href=""#fnref-S8My2kBLixCNaJDFB-5"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-6"" class=""footnote-item""><p>Around <a href=""http://fishcount.org.uk/published/std/fishcountstudy2.pdf"">80 billion</a> farmed fish, which live around one year, are raised and killed per year. <a href=""#fnref-S8My2kBLixCNaJDFB-6"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-7"" class=""footnote-item""><p>All estimates from <a href=""https://reducing-suffering.org/how-many-wild-animals-are-there/"">Brian Tomasik</a> <a href=""#fnref-S8My2kBLixCNaJDFB-7"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-8"" class=""footnote-item""><p>There are convincing anecdotes and examples for an expanding moral circle from family to nation to all humans: The abolishment of slavery; human rights; reduction in discrimination based on gender, sexual orientation, race. However, there doesn’t seem to be a lot of hard evidence. <a href=""https://www.gwern.net/The-Narrowing-Circle"">Gwern</a> lists a few examples of a narrowing moral circle (such as infanticide, torture, other examples being less convincing). <a href=""#fnref-S8My2kBLixCNaJDFB-8"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-9"" class=""footnote-item""><p>For example:</p>
<ul>
<li>lab-grown meat is very<a href=""https://www.openphilanthropy.org/research/cause-reports/animal-product-alternatives""> challenging</a> with few people working on it, little funding, …</li>
<li>Consumer adoption is far from <a href=""http://effective-altruism.com/ea/yu/scenarios_for_cellular_agriculture/"">inevitable</a></li>
<li><em>Some</em> people will certainly not want to eat in-vitro meat, so it is unlikely the number of factory-farmed will be abolished completely in the medium term, if the circle of empathy doesn’t increase or governments don’t regulate.</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-9"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-10"" class=""footnote-item""><p>There are also contrary trends. E.g. in Germany, meat consumption per head has been decreasing since 2011, from 62.8 kg in 2011 to 59.2 kg in 2015. In the US, it has been <a href=""http://www.humanesociety.org/news/resources/research/stats_slaughter_totals.html"">stagnant</a> for 10 years. <a href=""#fnref-S8My2kBLixCNaJDFB-10"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-11"" class=""footnote-item""><p>For example:</p>
<ul>
<li>Many more people remember feeling enjoyment or love than pain or depression across many countries (Figure 13, <a href=""https://www.nber.org/papers/w16441.pdf"">here</a>)</li>
<li>In nearly every country, (much) more than 50% of people report feeling very happy or rather happy (section “Economic growth and happiness”, <a href=""https://ourworldindata.org/happiness-and-life-satisfaction"">here</a>)</li>
<li>Average happiness in experience sampling in US: 65/100 (Killingsworth and Gilbert, 2010)</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-11"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-12"" class=""footnote-item""><p>One could claim that this just shows that people are afraid of dying or don’t commit suicide for other reasons, but people that suffer from depression have lifetime <a href=""http://www.allaboutdepression.com/gen_04.html"">suicide rates</a> of 2-15%, <a href=""https://www.suicidology.org/portals/14/docs/resources/factsheets/2011/depressionsuicide2014.pdf"">10-25 times</a> higher than general population. This at least indicates that suicide rates increase if quality of life decreases. <a href=""#fnref-S8My2kBLixCNaJDFB-12"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-13"" class=""footnote-item""><p>Reported well-being: People on average seem to report being content with their lives. This is only moderate evidence for their lives being positive from a welfarist view because people don’t generally think in welfarist terms when evaluating their lives and there might be optimism bias in reporting.
Suicide rates: There are many reasons why people with lives not worth living might refrain from suicide, for example:</p>
<ul>
<li>possibility of failing and then being institutionalized and/or living with serious disability</li>
<li>obligations to parents, children, friends</li>
<li>fear of hell</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-13"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-14"" class=""footnote-item""><p>For example:</p>
<ul>
<li>always enough food and water (with <a href=""https://en.wikipedia.org/wiki/Forced_molting"">some exceptions</a>)</li>
<li>Domesticated animals have been bred for a long time and now in general have lower basal stress levels and stress reactions than wild animals (because they don’t need them)</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-14"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-15"" class=""footnote-item""><p>For example:</p>
<ul>
<li>harmful breeding (e.g. broiler chicken are potentially in pain during the last 2 weeks of their life, because their joints cannot sustain their weight)</li>
<li>There is no incentive to satisfy the emotional and social needs of farmed animals. It is quite likely that e.g. pigs can’t exhibit their natural behavior (e.g. gestation crates). Pigs, hens, veal cattle are often kept in ways that they can’t move (or only very little) for weeks.</li>
<li>stress (intense confinement, chicken and pigs show self-mutilating behavior)</li>
<li>extreme suffering (some percentage of farmed animals suffering to death or experiencing <a href=""https://www.researchgate.net/publication/238675067_An_HSUS_Report_The_Welfare_of_Birds_at_Slaughter"">intense pain</a> during slaughter)</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-15"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-16"" class=""footnote-item""><p>The book <em>Compassion by the pound</em>, for example, rates the welfare of caged laying hens and pigs as negative, but beef cattle, dairy cows, free range laying hens and broiler chickens (market animals) as positive. Other experts disagree, especially on broiler chickens having lives worth living. <a href=""#fnref-S8My2kBLixCNaJDFB-16"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-17"" class=""footnote-item""><p>Ability to express natural behaviour, such as sex, eating, social behavior, etc. <a href=""#fnref-S8My2kBLixCNaJDFB-17"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-18"" class=""footnote-item""><p>Often painful deaths, disease, parasitism, predation, starvation, etc. In general, there is danger of anthropomorphism. Of course I would be cold in Antarctica, but a polar bear wouldn’t. <a href=""#fnref-S8My2kBLixCNaJDFB-18"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-19"" class=""footnote-item""><p>Specifically: moral weight for insects, probability that humanity will eventually improve wild animal welfare, future population size multiplier (insect relative to humans) and human and insect welfare. <a href=""#fnref-S8My2kBLixCNaJDFB-19"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-20"" class=""footnote-item""><p>If anything, attitudes towards animals have arguably become more empathetic. The majority of people around the globe express concern for farm animal well-being. (However, there is limited data, several confounders, and results from indirect questioning indicate that the actual concern for farmed animals might be much lower). See e.g.:
<a href=""http://ec.europa.eu/commfrontoffice/publicopinion/archives/ebs/ebs_270_en.pdf"">http://ec.europa.eu/commfrontoffice/publicopinion/archives/ebs/ebs_270_en.pdf</a>
<a href=""https://www.horizonpoll.co.nz/attachments/docs/horizon-research-factory-farming-survey-report.pdf"">https://www.horizonpoll.co.nz/attachments/docs/horizon-research-factory-farming-survey-report.pdf</a>
<a href=""http://www.tandfonline.com/doi/abs/10.2752/175303713X13636846944367"">http://www.tandfonline.com/doi/abs/10.2752/175303713X13636846944367</a>
<a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4196765/"">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4196765/</a>
But also: <a href=""https://link.springer.com/article/10.1007/s11205-009-9492-z"">https://link.springer.com/article/10.1007/s11205-009-9492-z</a> <a href=""#fnref-S8My2kBLixCNaJDFB-20"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-21"" class=""footnote-item""><p>Future technology, in combination with unchecked evolutionary pressure, might also lead to futures that contain very little of what we would value upon reflection <a href=""https://nickbostrom.com/fut/evolution.html"">(Bostrom, 2004)</a>. <a href=""#fnref-S8My2kBLixCNaJDFB-21"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-22"" class=""footnote-item""><p>Self-regarding preferences are preferences that depend on the expected effect of the preferred state of affairs <em>on the agent.</em> These are not synonymous with purely “selfish preferences”. Acting according to self-regarding preferences can lead to acts that benefit others, such as in trade.</p>
<p>Other-regarding preferences are preferences that don’t depend on the expected effect of the preferred state of affairs on the agent. Other-regarding preferences can lead to acts that also benefit the actor. E.g. parents are happy if they know their children are happy. However, the parents would also want their children to be happy if they wouldn’t come to know about it. As defined here, other-regarding preferences are not necessarily positive for others. They can be negative (e.g. sadistic/hateful preferences) or neutral (e.g. aesthetic preferences).</p>
<p>Example of two parties at war:</p>
<ul>
<li>Self-regarding preference: Members of the one party want members of the other party to die, so they can win the war and conquer the other party’s resources.</li>
<li>Other-regarding preference: Members of the one party want members of the other party to die, because they developed intense hate against them. Even if they don’t get any advantage from it, they would still want the enemy to suffer.</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-22"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-23"" class=""footnote-item""><p>Individual humans as well as human society have become more intelligent over time. See: <a href=""https://en.wikipedia.org/wiki/History_of_education"">history of education</a>, <a href=""https://en.wikipedia.org/wiki/Scientific_revolution"">scientific revolution</a>, <a href=""https://en.wikipedia.org/wiki/Flynn_effect"">Flynn effect</a>, information technology. Genetic engineering or artificial intelligence may further increase our individual and collective cognition. <a href=""#fnref-S8My2kBLixCNaJDFB-23"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-24"" class=""footnote-item""><p>Even if FAP and RP don’t have a lot of overlap, there might be additional reasons to defer to the values of future generations. Paul Christiano <a href=""https://rationalaltruist.com/2013/06/13/against-moral-advocacy/"">advocates</a> one should sympathize with future agents’ values, if they are reflected, for strategic cooperative reasons, and for a willingness to discard idiosyncratic judgements. <a href=""#fnref-S8My2kBLixCNaJDFB-24"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-25"" class=""footnote-item""><p>Even if earth-originating AI is initially controlled, this might not guarantee control over the future: Goal preservation might be costly, if there are trade-offs between learning and <a href=""http://reducing-suffering.org/will-future-civilization-eventually-achieve-goal-preservation/"">goal preservation</a> during self-improvement, especially in multipolar scenarios. <a href=""#fnref-S8My2kBLixCNaJDFB-25"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-26"" class=""footnote-item""><p>How meaningful moral reflection is, and whether we should expect human values to converge upon reflection, also depends on unsolved questions in <a href=""https://en.wikipedia.org/wiki/Meta-ethics"">meta-ethics</a>. <a href=""#fnref-S8My2kBLixCNaJDFB-26"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-27"" class=""footnote-item""><p>Of course, orthogonal other-regarding preferences can sometimes still lead to anti-parallel actions. Take as an example the debate of conservationism vs. wild animal suffering. Both parties have other-regarding preferences over wild animals. Conservationist don’t have a preferences for wild animal suffering, just for conserving eco-systems. Wild animal suffering advocates don’t have a preference against conserving eco-systems (per se), just against wild animal suffering. In practice, these orthogonal views likely recommend different actions regarding habitat destruction. However, if there will be future agents with preferences on both sides, then there is wildly more room for gains through trade and compromise (such as the implementation of David Pearce’s <a href=""https://www.hedweb.com/"">Hedonistic imperative</a>) in cases like this than if other-regarding preferences were actually anti-parallel. Still, as I also remark in the conclusion, people who think their reflected preferences will be sufficiently unusual to have only a small overlap with other-regarding preferences of other humans, even if they are reflected, will find the whole part 1.2 less compelling for that reason. <a href=""#fnref-S8My2kBLixCNaJDFB-27"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-28"" class=""footnote-item""><p>Maybe we would, after idealized reflection, include a certain class of beings into our other-regarding preferences, and we would want them to be able to experience, say, freedom. It seem quite likely that future agents won’t care about these being at all. However, it seems very unlikely that they would have a particular other-regarding preference for such being to be un-free.</p>
<p>Or consider the <a href=""https://wiki.lesswrong.com/wiki/Paperclip_maximizer"">paperclip-maximiser</a>, a canonical example for misaligned AI and thus a example for FAP certainly not being parallel to RP. Still, a paperclip-maximizer does not have a particular aversion against flourishing life, just as we don’t have a particular aversion against paperclips. <a href=""#fnref-S8My2kBLixCNaJDFB-28"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-29"" class=""footnote-item""><p>Examples of negative “side-effects” as defined here:</p>
<ul>
<li>The negative “side-effects” of warfare on the losing party are bigger than the positive effects for the winning party (assuming that the motivation for the war was not “harming the enemy”, but e.g. acquiring the enemy’s resources)
<ul>
<li>This is an example of side effects of powerful agents’ self-regarding preferences on other powerful agents.</li>
</ul>
</li>
<li>The negative “side-effects” of factory farming (animal suffering) are bigger than the positive effects for humanity (ability to eat meat). Many people do care about animals, so this is also an example of self-regarding preferences conflicting with other-regarding preferences.</li>
<li>The negative “side-effects” of slave-labor on the slave are bigger than the positive effects for the slave owner (gain in wealth)
<ul>
<li>These are both examples of side effects of powerful agents’ self-regarding preferences on powerless beings.</li>
</ul>
</li>
</ul>
<p>Of course there are also positive side-effects, cooperative and accidental: E.g.</p>
<ul>
<li>positive “side-effects” of powerful agents acting according to their preferences on other powerful agents: All gains from trade and cooperation</li>
<li>positive “side-effects” of powerful agents acting according to their preferences on powerless beings: Rabies vaccination for wild animals. Arguably, wild animal population size reduction.</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-29"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-30"" class=""footnote-item""><p>Additionally, one might object that FAP may not be the driving force shaping the future. Today, it seems that major decision are mediated by a complex system of economical and political structures that often leads to outcomes that don’t align with the preferences of individual humans and that overweights the interests of the economically and politically powerful. On that view, we might expect the influence of human(e) values over the world to remain small. We think that future agents will probably have better tools to actually shape the world according to their preferences, which includes better tools for mediating disagreement and reaching meaningful compromise. But insofar as the argument in this footnote applies, it gives an additional reason to expect orthogonal actions, even if FAP aren’t orthogonal. <a href=""#fnref-S8My2kBLixCNaJDFB-30"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-31"" class=""footnote-item""><p>Note that cooperation does not require caring about the partner one cooperates with. Even two agents that don’t care about each other at all may cooperate instead of waging war for the resources the other party holds, if they have good tools/institutions to arrange compromise, because the cost of warfare is high. <a href=""#fnref-S8My2kBLixCNaJDFB-31"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-32"" class=""footnote-item""><p>Evolutionary reasons for the asymmetry between biological pain and pleasure that would not necessarily remain in designed digital sentience (ideas owed to Carl Shulman):</p>
<ul>
<li>Animals try to minimize the duration of pain (e.g. by moving away from the source of pain), and try to maximize the duration of pleasurable events (e.g by continuing to eat). Thus, painful events are on average shorter than pleasurable events, and so need to be more intense to induce the same learning experience.</li>
<li>Losses in reproductive fitness from one single negative event (e.g. a deadly injury) can be much greater than the gains of reproductive fitness from any single positive event, so animals evolved to want to avoid these events at all cost.</li>
<li>Boredom/satiation can be seen as evolved protection against reward channel hacking. Animals for which one pleasant stimulus stayed pleasant indefinitely (e.g. animal that just continued eating) had less reproductive success. Pain channels need less protection against hacking, because pain channel hacking...:
<ul>
<li>only works if there is sustained pain in the first place, and</li>
<li>is much harder to learn than pleasure channel hacking (the former: after getting hurt, an animal would need to find and eat a pain-relieving plant; the latter: an animal just needs to continue eating despite not having any use for additional calories)</li>
</ul>
</li>
</ul>
<p>This might be part of the reason why pain seems much easier to instantiate on demand than happiness. <a href=""#fnref-S8My2kBLixCNaJDFB-32"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-33"" class=""footnote-item""><p>Even if future powerful agents have some concern for the welfare of sentient tools, sentient tools’ welfare might still be net negative, if there are reasons that make positive-welfare tools much more expensive than negative welfare tools (e.g. if suffering is very important for task performance). But even if maximal efficiency and welfare of tools are not completely correlated, we think that most suffering can be avoided while still keeping most productivity, so that a little concern for sentient tools could thus go a long way. <a href=""#fnref-S8My2kBLixCNaJDFB-33"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-34"" class=""footnote-item""><p>Strategic acts in scenarios with little cooperation could motivate the creation of disvalue-optimized sentience, especially in multipolar scenarios that contain both altruistic and indifferent agents (blackmailing). However, because uncooperative acts are bad for everyone, these scenarios in expectation seem to involve little resources. On the positive side, there can also be gains from trade between altruistic and indifferent agents. <a href=""#fnref-S8My2kBLixCNaJDFB-34"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-35"" class=""footnote-item""><p>Sentient tools are optimized for performance in the task they are created for. Per resource-unit, future agents would create: a number of minds as is most efficient, with hedonic experience as is most efficient, optimized for task.</p>
<p>(Dis)value-optimized sentience might be directly optimized for extent of consciousness or intensity of experience (if that is actually what future generations value altruistically). Per resource-unit, future agents would create: as many minds as is optimal for (dis)value, with as positive/negative as possible hedonic experience, optimized for conscious states.</p>
<p>Such sentience might be orders of magnitude more efficient in creating conscious experience than sentience not optimized for it. E.g. in humans, only a tiny fraction of energy is used for peak conscious experience: about 20% of energy is used for the brain, only a fraction of that is used for conscious experience, only a fraction of which are “peak” experiences. <a href=""#fnref-S8My2kBLixCNaJDFB-35"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-36"" class=""footnote-item""><p>The driving force behind this judgement is not necessarily the belief that most futures will be good. Rather, it is the belief that the ‘rather good’ futures will contain more net value than the ‘rather bad’ futures will contain net disvalue.</p>
<ul>
<li>The ‘rather good’ futures contain agents with other-regarding preferences highly parallel to our reflected preferences. Many resources will be spent in a way that optimizes for value (by our lights).</li>
<li>In the ‘rather bad’ futures, agents are largely selfish, or have other-regarding preferences completely orthogonal to our reflected other-regarding preferences. In these futures, most resources will be spent for goals that we do not care about, but very few resources will be spent to produce things we would disvalue in an optimized way. On whichever side of ”zero” these scenarios fall, they seem much closer to parity than the “rather good futures” (from most moral views).</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-36"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-37"" class=""footnote-item""><p>As also noted in the discussion at the end of the article, part 1 is less relevant for people who have other-regarding preferences very different from other people, and who believe their RP to be very different from the RP of the rest of humanity. <a href=""#fnref-S8My2kBLixCNaJDFB-37"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-38"" class=""footnote-item""><p>Option value is not a separate kind of value, and it would be already integrated in the perfect EV calculation. However, it is quite easy to overlook, and somewhat important in this context, so it is discussed separately here. <a href=""#fnref-S8My2kBLixCNaJDFB-38"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-39"" class=""footnote-item""><p>In a general sense, ‘option value’ includes the value of any change of strategy, for the better or worse, that future agents might take upon learning more. However, the general fact that agents can learn more and adapt their strategy is not surprising and was already factored into considerations 1, 2 and 4. <a href=""#fnref-S8My2kBLixCNaJDFB-39"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-40"" class=""footnote-item""><p>In the more general definition, option value is not always positive. In general, giving future agents the option to choose between different strategies can be bad, if the values of future agents are bad or their epistemics are worse. In this section, ‘option value’ only refers to the option of future agents not to colonize space, if they find colonizing space would be bad from an altruistic perspective. It seems very unlikely that, if future agents refrain from space colonization <em>for altruistic reasons</em> at all, they would do so exactly in those cases in which we (current generation) would have judge space colonization as positive (according to our reflected preferences). So this kind of option value is very unlikely to be negative. <a href=""#fnref-S8My2kBLixCNaJDFB-40"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-41"" class=""footnote-item""><p>Although empirical insights about the universe play a role in both option value and part 2.2, these two considerations are different:</p>
<ul>
<li>Part 2.2: Further insight about the universe might show that there already is a lot of disvalue out there. A benevolent civilization might reduce this disvalue.</li>
<li>Option value: Further insight about the universe might show that there already is a lot of value or disvalue out there. That means that we should be uncertain about the EV of (post-)human space colonization. Our descendants will be less uncertain, and can then, if they know there is NOT already a lot of disvalue out there, still decide to not spread to the stars.</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-41"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-42"" class=""footnote-item""><p>Individual humans as well as human society have become more intelligent over time. See: <a href=""https://en.wikipedia.org/wiki/History_of_education"">history of education</a>, <a href=""https://en.wikipedia.org/wiki/Scientific_revolution"">scientific revolution</a>, <a href=""https://en.wikipedia.org/wiki/Flynn_effect"">Flynn effect</a>, information technology. Genetic engineering or artificial intelligence may further increase our individual and collective cognition. <a href=""#fnref-S8My2kBLixCNaJDFB-42"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-43"" class=""footnote-item""><p>For example, if we care only about maximizing X, but future agents will care about maximizing X, Y and Z to equal parts, letting them decide whether or not to colonize space might still lead to more X than if we decided, because they have vastly more knowledge about the universe and are generally much more capable of making rational decisions. <a href=""#fnref-S8My2kBLixCNaJDFB-43"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-44"" class=""footnote-item""><p>Even if future agents can make better decisions regarding our other-regarding preferences than we (currently) could, future agents also need to be non-selfish enough to act accordingly - their other-regarding preferences need to constitute a sufficiently large fraction of their overall preferences. <a href=""#fnref-S8My2kBLixCNaJDFB-44"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-45"" class=""footnote-item""><p>Say we are uncertain about the value in the future in two ways:</p>
<ul>
<li>50% credence that disvalue-focused view would be my preferred moral view after idealized reflection, 50% credence in a ‘balanced view’ that also values the creation of value.</li>
<li>50% credence that the future will be controlled by indifferent actors, with preferences completely orthogonal to our reflected preferences, 50% credence that it will be controlled by good actors who have exactly the preferences we would have after idealized reflection.</li>
</ul>
<p>The following table shows expected net value of space colonization without considering option value (again: made-up numbers):</p>
<p>| | Indifferent actors | Good actors|
|----|----|----|----|
|<strong>Disvalue-focused view</strong> | -100	| -10|
|<strong>‘Balanced view’</strong> | - 5	| 100|</p>
<p>Now with option value, only the good actors would limit the harm if the disvalue-focused view was indeed our (and thus, their) preferred moral view after idealized reflection:</p>
<p>| | Indifferent actors | Good actors|
|----|----|----|----|
|<strong>Disvalue-focused view</strong> | -100	| 0 |
|<strong>‘Balanced view’</strong> | - 5	| 100| <a href=""#fnref-S8My2kBLixCNaJDFB-45"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-46"" class=""footnote-item""><p>There is more option value, if:</p>
<ul>
<li>One one currently has high moral uncertainty (one expects one’s views to change considerably upon idealized reflection). With high moral uncertainty, it is more likely that future agents will have significantly more accurate moral values. Expects future agents to have a significantly better empirical understanding</li>
<li>One’s uncertainty about the EV of the future comes mainly from moral, and not empirical, uncertainty. For example, say you are uncertain about the expected value of the future because you are unsure whether you would, in your reflected preferences, endorse a strongly disvalue-focused view. If you are generally optimistic about future agents, you can assume future generations to be better informed about which moral view to take. Thus, there is a lot of option value in reducing the risk of human extinction.  If, one the other hand, you are uncertain about the EV of the future because you think there is a high chance that future agents just won’t be altruistic, there is no option value in deferring the decision about space colonization to them.</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-46"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-47"" class=""footnote-item""><p>It seems likely that some life-forms would survive, except if human extinction is caused by some cosmic catastrophes (not a focus area for effective altruists, because unlikely and intractable) or by specific forms of nano-technology or by misaligned AI. <a href=""#fnref-S8My2kBLixCNaJDFB-47"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-48"" class=""footnote-item""><p>The extent to which it is true depends on the reflection process one chooses. Several people who read an early draft of this article commented that they would imagine their reflected preferences to be independent of human-specific factors. <a href=""#fnref-S8My2kBLixCNaJDFB-48"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-49"" class=""footnote-item""><p>The argument in the main text assumed that the alternative space colonization contains a comparable amount of things that we find morally relevant as the (post-)human colonization. But in many cases, the EV of an alternative space colonization would actually be (near) neutral, because the alternative civilization’s preferences would be orthogonal to ours. Our values would just be so different from the AI’s or extraterrestrial values that space colonization by these agents might often look neutral to us. The argument in the main text still applies, but only for those alternative space colonizations that contain comparable absolute amounts of value and disvalue.</p>
<p>However, a very similar argument applies even for alternative colonizations that contain less absolute amount of things we morally care about. The value of alternative space colonization would be shifted more towards zero, but future pessimists would in expectation always find alternative space colonization a worse outcome than no space colonization. From the future pessimistic perspective, human extinction leads to a bad outcome (alternative colonization), and not a neutral one (no space colonization). Future pessimists should thus update towards extinction risk reduction being less negative. Future optimists might find the alternative space colonization better or worse than no colonization.</p>
<p>The mathematical derivation in the next footnote takes this caveat into account. <a href=""#fnref-S8My2kBLixCNaJDFB-49"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-50"" class=""footnote-item""><p>Assumption: This derivation makes the assumption that people who think the EV of human space colonization is negative and those who think it is positive would still rank a set of potential future scenarios in the same order when evaluating them normatively. This seems plausible, but may not be the case.
Let’s simplify the value of human extinction risk reduction to:</p>
<p><em>EV(reduction of human extinction risk) = EV(human space colonization) - EV(human extinction)</em></p>
<p>(This simplification is very uncharitable towards extinction risk reduction, even if <em>only</em> considering the long-term effects, see parts 2 and 3 of this article).
Assuming that no non-human animal or extraterrestrial civilization would emerge in case of human extinction, then EV(human extinction)=0, and so future pessimists judge:</p>
<p><em>EV(reduction of human extinction risk) = EV(human space colonization) - EV(human extinction)= EV(human space colonization) &lt; 0</em></p>
<p>And future optimists believe:</p>
<p><em>EV(reduction of human extinction risk) = EV(human space colonization) - EV(human extinction) = EV(human space colonization) &gt; 0</em></p>
<p>Let’s say, if humanity goes extinct, there will be non-human space colonization eventually with the probability <strong><em>p</em></strong>. (p can be down-weighted in a way to account for the fact that later space colonization probably means less final area colonized). This means that:</p>
<p><em>EV(human extinction) = p * EV(non-human space colonization)</em></p>
<p>Let’s define the amount of value and disvalue created by human space colonization as <em>Vₕ</em> and <em>Dₕ</em>, and the amount value and disvalue created by the non-human civilization as <em>Vₙₕ</em> and <em>Dₙₕ</em>.</p>
<p>We can expect two relations:</p>
<ol>
<li>On average, a non-human civilization will care less about creating value and care less about reducing disvalue than a human civilization. We can expect the ratio of value to disvalue to be worse in the case of a non-human civilization:</li>
</ol>
<p><em>(i)	Vₙₕ/Dₙₕ = (Vₕ/Dₕ) * r</em>, with <em>0  &lt;= r &lt;= 1</em></p>
<ol start=""2"">
<li>On average, non-human animals and  extraterrestrial values will be alien to us, their preferences will be orthogonal to ours. I seems likely that on average these futures will contain less value <em>or</em> disvalue than a future with human space-colonization.</li>
</ol>
<p><em>(ii)	(Vₙₕ + Dₙₕ) = (Vₕ + Dₕ) * a</em>, with <em>0 &lt;= a &lt;= 1</em></p>
<p>Finally, the expected value of non-human space colonization can be expressed as (by definition):</p>
<p><em>(iii)	EV(non-human space colonization) = Vₙₕ - Dₙₕ</em></p>
<p>Using <em>(i)</em>, <em>(ii)</em>, and <em>(iii)</em> we get:</p>
<p><em>EV(human extinction) =</em>
<em>EV(non-human space colonization) * Probability(non-human space colonization)  =</em>
<em>(Vₙₕ - Dₙₕ) * p =</em>
<em>[a * (Vₕ + Dₕ) / ((Vₕ/ Dₕ) * r + 1)] * <strong>(r * Vₕ/ Dₕ - 1)</strong> * p</em></p>
<p>The first term [in square brackets] is always positive. The sign of the second term <strong>(in bold)</strong> can change depending on whether we were previously optimistic or pessimistic about the future.</p>
<p>If we were previously pessimistic about the future, we thought:</p>
<p><em>Vₕ - Dₕ &lt; 0 &nbsp; &nbsp; -&gt; &nbsp; &nbsp; Vₕ/ Dₕ &lt; 1</em></p>
<p>The second term is negative, EV of human extinction is negative. Compared to the “naive” pessimistic view (assuming <em>EV(human extinction) = 0</em>), pessimists should update their view into the direction of <em>EV(reducing human extinction risk)</em> being less negative.</p>
<p>If we were previously optimistic about the future, we thought:</p>
<p><em>Vₕ - Dₕ &gt; 0 &nbsp; &nbsp; -&gt; &nbsp; &nbsp; Vₕ/ Dₕ &gt; 1</em></p>
<p>Now the second term can be negative, neutral, or positive. Compared to the naive view, future optimists should sometimes be more enthusiastic (if <em>Vₙₕ/ Dₙₕ= r * Vₕ/ Dₕ &lt; 1</em>) and sometimes be less enthusiastic (if <em>Vₙₕ/ Dₙₕ= r * Vₕ/ Dₕ &gt; 1</em>) about extinction risk reduction than they previously were. <a href=""#fnref-S8My2kBLixCNaJDFB-50"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-51"" class=""footnote-item""><p>Let’s define future pessimists as people who judge the expected value of (post-)human space colonization as negative; future optimists analogously. Now consider the example of a non-human civilization significantly worse than human civilization (by our lights), such that future optimists would find it normatively neutral, and future pessimists find it significantly more negative than human civilization. Then future optimists would not update their judgement (compared to before considering the possibility of a non-human animal spacefaring civilization), but pessimists would update significantly into the direction of human extinction risk reduction being positive. <a href=""#fnref-S8My2kBLixCNaJDFB-51"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-52"" class=""footnote-item""><p>E.g. one might think that humanity might be comparatively bad at coordination (compared to e.g. intelligent ants), and so relatively likely to create uncontrolled AI wrong, which might be an exceptionally bad outcome, maybe even worse than an intelligent ant civilization. However, considerations like this seem to require highly specific judgements and are likely not very robust. <a href=""#fnref-S8My2kBLixCNaJDFB-52"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-53"" class=""footnote-item""><p>Section 4.2 is not dependent on a welfarist or even consequentialist view. More generally, it applies to any kind of empirical or moral insight that we might have, which would make us realize that other things than we previously thought are of great moral value or disvalue. <a href=""#fnref-S8My2kBLixCNaJDFB-53"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-54"" class=""footnote-item""><p>For example:</p>
<ul>
<li>The history of an “expanding moral circle” (Singer, 2011), from tribes to nations to all humans…</li>
<li>The relatively new notion of <a href=""https://en.wikipedia.org/wiki/Timeline_of_history_of_environmentalism"">environmentalism</a></li>
<li>The new notion of <a href=""https://was-research.org/mission/"">wild animal suffering</a></li>
<li>The new notion of future beings being (astronomically) important <a href=""https://nickbostrom.com/astronomical/waste.html"">(Bostrom, 2003)</a></li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-54"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-55"" class=""footnote-item""><p>Assuming that the side-effects of resources spent for self-regarding preferences of future agents are neutral/symmetric with regards to the beings/things out there (which seems to be a reasonable assumption). <a href=""#fnref-S8My2kBLixCNaJDFB-55"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-56"" class=""footnote-item""><p>Fermi-estimate (wild guesses, again):</p>
<ol>
<li>Assume a 20% probability that, with more moral and empirical insight, we would conclude that the universe is <em>already</em> filled with beings/things that we morally care about</li>
<li>Assume that the altruistic impact future agents could have is always proportional to the amount of resources spent for altruistic purposes. If the universe is devoid of value or disvalue, then altruistic resources will be spent on creating new value (e.g. happy beings). If the universe is already filled with beings/things that we morally care about, it will likely contain some disvalue. Assume that in these cases, 25% of altruistic resources will be used to reduce this disvalue (and only 75% to create new value). Also assume that resources can be used at the same efficiency <em>e</em> to create new disvalue, or to reduce existing disvalue.</li>
<li>Assume that resources spent for self-regarding preferences of future agents would on average not improve or worsen the situation for the things of (dis)value already out there.</li>
<li>Assume that in expectation, future agents will spend 40 times as many resources pursuing other-regarding preferences parallel to our reflected preferences (“altruistic”) than on pursuing other-regarding preferences anti-parallel to our reflected preferences (“anti-altruistic”). Note that this is compatible with future agents, in expectation, spending most of their resources on other-regarding preferences completely orthogonal to our reflected preferences.</li>
<li>From a disvalue-focused perspective, creation of new value does not matter, only creation of new disvalue, or reduction of already existing disvalue. From such a perspective: (<em>R</em>: total amount of resources spent on parallel or anti-parallel other-regarding preferences).</li>
</ol>
<ul>
<li>Expected creation of new disvalue = (1/40) * R * e = 2.5% * R * e</li>
<li>Expected reduction of already existing disvalue =  20% * 25% * (1-(1/40)) * R * e = 5% * R * e</li>
</ul>
<p>Thus, the expected reduction of disvalue through (post-)humanity is 2 times greater than expected creation of disvalue. This is, however, an upper bound. The calculation assumed that the universe contains enough disvalue that future agents could actually spend 25% altruistic resources on alleviating it, before having alleviated it all. In some cases, the universe might not contain that much disvalue, so some resources would go into the creation of value again. <a href=""#fnref-S8My2kBLixCNaJDFB-56"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-57"" class=""footnote-item""><p>Analogous to part 1.2, this part 2.2 is less relevant for people who believe that some of their reflected other-regarding preferences will be so unusual that they will be anti-parallel to most of humanity’s reflected other-regarding preferences. Such a view is e.g. defended by <a href=""http://reducing-suffering.org/is-there-suffering-in-fundamental-physics/#Should_negative-leaning_consequentialists_promote_this_issue."">Brian Tomasik</a> in the context of suffering in fundamental physics.  Tomasik argues that, even if he (after idealized reflection) and future generation both came around to care for sentience in fundamental physics, and even if future generations were to influence fundamental physics <em>for altruistic reasons</em>, they would still be more likely to do it in a way that increases the vivacity of physics, which Tomasik (after idealized reflection) would oppose. <a href=""#fnref-S8My2kBLixCNaJDFB-57"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-58"" class=""footnote-item""><p>This section draws heavily on <a href=""https://blog.givewell.org/2015/08/13/the-long-term-significance-of-reducing-global-catastrophic-risks/"">Nick Beckstead’s thoughts</a>. <a href=""#fnref-S8My2kBLixCNaJDFB-58"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-59"" class=""footnote-item""><p>Global catastrophes that do not directly cause human extinction may initiate developments that lead to extinction later on. For the purposes of this article, these cases are not different from direct extinction, and are omitted here. <a href=""#fnref-S8My2kBLixCNaJDFB-59"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-60"" class=""footnote-item""><p>E.g. <a href=""https://rationalaltruist.com/2013/06/13/against-moral-advocacy/"">Paul Christiano</a>: “<em>So if modern civilization is destroyed and eventually successfully rebuilt, I think we should treat that as recovering most of Earth’s altruistic potential (though I would certainly hate for it to happen).</em>”
In his article, Christiano outlines several empirical and moral judgement calls that lead him to his conclusion, such as:</p>
<ul>
<li>As long a moral reflection and sophistication process is ongoing, which seems likely, civilizations will reach very good values (by his lights).</li>
<li>He is willing to discard his idiosyncratic judgements.</li>
<li>He directly cares about others’ (reflected) values.</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-60"" class=""footnote-backref"">↩︎</a></li>
<li id=""fn-S8My2kBLixCNaJDFB-61"" class=""footnote-item""><p>It is of course a question whether one should stick with one’s own preferences, if the majority of reflected and altruistic agents have opposite preferences. According to some empirical and meta-ethical assumptions, one should. <a href=""#fnref-S8My2kBLixCNaJDFB-61"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-62"" class=""footnote-item""><p>Different advocates of strong suffering-focused views come to different judgements on the topic. They all seem to agree that, from a purely suffering-focused perspective, it is not clear whether efforts to reduce the risk of human extinction are positive or negative:</p>
<p><a href=""https://foundational-research.org/cause-prioritization-downside-focused-value-systems/#Extinction_risk_reduction_Unlikely_to_be_positive_according_to_downside-focused_views"">Lukas Gloor:</a> <em>""it tentatively seems to me that the effect of making cosmic stakes (and therefore downside risks) more likely is not sufficiently balanced by positive effects on stability, arms race prevention and civilizational values (factors which would make downside risks less likely). However, this is hard to assess and may change depending on novel insights.” …</em>
<em>“We have seen that efforts to reduce extinction risk (exception: AI alignment) are unpromising interventions for downside-focused value systems, and some of the interventions available in that space (especially if they do not simultaneously also improve the quality of the future) may even be negative when evaluated purely from this perspective.”</em></p>
<p><a href=""https://www.hedweb.com/social-media/pre2014.html"">David Pearce</a>: <em>“Should existential risk reduction be the primary goal of: a) negative utilitarians? b) classical hedonistic utilitarians? c) preference utilitarians? All, or none, of the above? The answer is far from obvious. For example, one might naively suppose that a negative utilitarian would welcome human extinction. But only (trans)humans - or our potential superintelligent successors - are technically capable of phasing out the cruelties of the rest of the living world on Earth. And only (trans)humans - or rather our potential superintelligent successors - are technically capable of assuming stewardship of our entire Hubble volume.” …</em>
<em>“In practice, I don't think it's ethically fruitful to contemplate destroying human civilisation, whether by thermonuclear Doomsday devices or utilitronium shockwaves. Until we understand the upper bounds of intelligent agency, the ultimate sphere of responsibility of posthuman superintelligence is unknown. Quite possibly, this ultimate sphere of responsibility will entail stewardship of our entire Hubble volume across multiple quasi-classical Everett branches, maybe extending even into what we naively call the past [...]. In short, we need to create full-spectrum superintelligence.”</em></p>
<p><a href=""https://foundational-research.org/how-would-catastrophic-risks-affect-prospects-for-compromise/"">Brian Tomasik</a>: <em>“I'm now less hopeful that catastrophic-risk reduction is plausibly good for pure negative utilitarians. The main reason is that some catastrophic risk, such as from malicious biotech, do seem to pose nontrivial risk of causing complete extinction relative to their probability of merely causing mayhem and conflict. So I now don't support efforts to reduce non-AGI ""existential risks"". [...] Regardless, negative utilitarians should just focus their sights on more clearly beneficial suffering-reduction projects”</em> <a href=""#fnref-S8My2kBLixCNaJDFB-62"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-63"" class=""footnote-item""><p>For example, interventions that aim at improving humanity’s values/increasing the circle of empathy might be highly leveraged and time-sensitive, if humanity achieves goal conservation soon, or values are otherwise sticky. <a href=""#fnref-S8My2kBLixCNaJDFB-63"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-64"" class=""footnote-item""><p>“Positive”/”negative” as defined from a welfarist perspective. <a href=""#fnref-S8My2kBLixCNaJDFB-64"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-65"" class=""footnote-item""><p>Societies may increase the costs, and thereby reducing the frequency, of acts following from negative other-regarding preferences, as long as negative other-regarding preferences are a minority. E.g. if 5% of a society have a other-regarding preference for inflicting suffering on a certain group (of powerless beings), but 95% have a preference against it, in many societal forms less than 5% of people will actually inflict suffering on this group of powerless beings, because there will be laws against it, ... <a href=""#fnref-S8My2kBLixCNaJDFB-65"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-66"" class=""footnote-item""><p>This fact could be interpreted either as human nature that we will revert to, or as a trend of moral progress. The latter seems more likely to us. <a href=""#fnref-S8My2kBLixCNaJDFB-66"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-S8My2kBLixCNaJDFB-67"" class=""footnote-item""><p>Another possible operationalization of the ratio between positive and negative other-regarding preferences: How much money is spent on pursuing positive and negative other-regarding preferences?</p>
<ul>
<li>Some state budgets are clearly pursuant to positive other-regarding preferences</li>
<li>It is less clear whether there are budgets that are clearly pursuant to negative other-regarding preferences, although at least a part of military spending is.</li>
</ul>
 <a href=""#fnref-S8My2kBLixCNaJDFB-67"" class=""footnote-backref"">↩︎</a></li>
</ol>
</section>
",JanBrauner,janb,JanB,
p4YbAXT2dnSAaKc2L,In Defense of Those Reclusive Authors,in-defense-of-those-reclusive-authors-1,https://www.lesswrong.com/posts/p4YbAXT2dnSAaKc2L/in-defense-of-those-reclusive-authors-1,2019-06-09T14:14:38.644Z,11,7,0,False,False,,"<h2>Notable Authors of the 20th Century Who Were Introverted</h2><p>Writing  fiction is usually a solitary profession. Among those individuals who  end up producing art to express themselves, one can logically assume  that there will be many who like to keep their distance from society.  While there have always been writers who are sociable, a few of the  greats were largely solitary and lonely, socially awkward, or even  reclusive individuals.</p><p>Apart from the 20th century’s very famous cases of this  type of creator—the Argentinian writer J.L. Borges, the Portuguese  author Fernando Pessoa and the Czech-Jewish allegorist Franz  Kafka—reclusive attitudes and highly introverted interests can be easily  identified in a number of notable artists who merely happen to have  earned less renown. H.P. Lovecraft, with his imagining of a world  populated by primordial monstrosities, or Robert Walzer, who despite  having been one of Kafka’s literary heroes, remains virtually unknown to  this day. Yet he penned hundreds of short stories as well as a few  large novels  which were all about the sense of alienation and lack of  belonging to the world. And Henry James (with his nominally secured  position in the literary canon of 20th century English  literature notwithstanding) who is by now only infrequently referenced  as an insightful anatomist of introversion and co-morbid indifference to  the external world.</p><h3>Regarding the Degree of Introversion</h3><p>Pronounced,  evident lack of interest – or at least professed such lack of interest –  about the external world, can be observed in a number of quotes by the  aforementioned writers. During the First World War, Franz Kafka wrote in  his diary that he was then being rewarded for never have been involved  in worldly affairs… Borges – far more reclusive than Kafka – had penned  silent cries, in which he accused his contemporary society of being even  unworthy of suffering in Hell; he argues, that is, that human malice is  just too crude to deserve a metaphysical punishment! Pessoa, who spent  his days as a shadow in the busy streets of downtown Lisbon, working as a  translator for various trading firms, claimed, in one of his most  famous poems, that he put on clothes which didn’t suit him, and was  taken for someone else, and was subsequently lost…</p><h3>Then and Now</h3><p>While  in more recent years – primarily, perhaps, due to the ubiquity of  television – writers have at times been presented – some of them  willingly – as another type of media celebrity, in the not so distant  past it was still quite difficult to reach an author from outside the  circuit of the publishing world. Writers used to mostly be identified  through their written work, and it was the norm for a reader to be aware  of an author, to like or even love their work, and yet be fully  ignorant of their physical likeness – and also unaware of most of the  biographical information that by now is routinely accessed; from the  opening pages of the book itself, or from external sources. This isn&#x27;t  of secondary importance in our examination, given one would scarcely  imagine Pessoa, Lovecraft, or even Kafka, giving a TV interview; and  perhaps many would question even if individuals with so reclusive  personalities would, had they lived now, be offered a publishing deal at  all.</p><h3>Are Highly Introverted Writers Actually Needed?</h3><p>Publishing  is a business, and a publishing house is not likely to invest on a  writer’s work if it stands to lose money... And yet an author is  arguably different to a performer of popular art; the latter is mostly  tied to entertainment, while the former – at least in theory –  incorporates a cerebral quality, and aspires to other heights of  artistry. In practice, of course, not all authors differ that  significantly from performance artists; but to – whether actively or  unwittingly – bring about an increase in links between the two  professions, will certainly result in fewer published authors who are  characterized by acute introversion.</p><p>Even assuming that the above is true, would it be necessarily a  negative outcome? Does the reader actually stand to gain something  specifically out of reading the fictional work of an introvert, or even a  recluse?</p><h3>An Allegory as the Epilogue</h3><p>A  brief answer may be provided, in the form of an allegory: In a group of  travelers, sharing stories, the more original ones would tend to come  from those who ventured further away. One shouldn’t lose patience with  the more estranged story-tellers, for journeys to the most distant lands  can make the traveler lose interest in the homeland; where everyone is  familiar with the geography, the customs and the people’s faces. And  such journeys also can make the person feel that the ties to his  countrymen have been practically severed, and the wondrous information  contained within him, from those distant lands he visited, can’t  actually interest this crowd...</p><p>Shouldn’t we, therefore, expect that if such a fellow decides, at  some point, to actually speak, the words we might then listen to could  indeed present us with material that we hadn’t yet the chance to reflect  upon?</p><p>After all, a book we take interest in is always going to function as a map to our own, mostly unexplored inner world.</p><p>by Kyriakos Chalkopoulos - <a href=""https://www.patreon.com/posts/in-defense-of-27095565"">https://www.patreon.com/posts/in-defense-of-27095565</a></p>",KyriakosCH,kyriakosch,KyriakosCH,
JBtGzP5shRoN6qeuv,Learning magic,learning-magic,https://www.lesswrong.com/posts/JBtGzP5shRoN6qeuv/learning-magic,2019-06-09T12:29:06.484Z,12,3,8,False,True,,"<p>Magic (primarily misdirection and cold-reading, but also the mechanics like sleight-of-hand) seems like an extremely good case-study in the study of how the human mind works and the predictable ways in which human maps differ from the territory. There are several magicians out there who offer to teach classes and so forth, but are there any who can be vouched for as really &quot;knowing their stuff&quot; as a teacher if I wanted to approach the subject in this light? Relatedly, can anyone vouch for the quality of the Penn and Teller course on the MasterClass platform?</p><p>Bonus points for teachers in London who can be hired for small (up to 10 people) groups.</p>",Smaug123,smaug123,Smaug123,
Dnk4qvGNXRddvFyJM,Paternal Formats,paternal-formats,https://www.lesswrong.com/posts/Dnk4qvGNXRddvFyJM/paternal-formats,2019-06-09T01:26:27.911Z,52,27,35,False,False,,"<p>In <a href=""https://users.speakeasy.net/~lion/nb/book.pdf"">How to Make a Complete Map of Every Thought You Think</a>, Lion Kimbro introduces the idea of coercive vs uncoercive formats for information. [Re-named to &quot;Paternal vs Navigable&quot; here, at <a href=""https://www.lesswrong.com/posts/Dnk4qvGNXRddvFyJM/coercive-formats#2dJaiCq7qSjirwsLz"">the suggestion of MakoYass</a>.]</p><p>Paternal:</p><ul><li>Stories. </li><li>Lesson plans; course prerequisites. </li><li>Videos. </li><li>Linear text which must be read in order. Introducing terminology and ideas once (rather than wherever they&#x27;re needed), without an index or other better reference mechanisms. </li><li>Explaining what you&#x27;re getting at only via your chain of argumentation, rather than up-front. </li><li>Intertwining development of many different themes (making it impossible to look for only the information you want).</li></ul><p>Navigable:</p><ul><li>Blueprints. </li><li>Wikis. </li><li>Reference manuals. </li><li>Hyperlinks. </li><li>TLDRs. </li><li>Tools which make it easy to quickly get an overview, such as paper abstracts or introductions which actually summarize a work (rather than teasing its conclusions or giving autobiographical context, etc). </li><li>Diagrams/tables/charts and other graphic elements. (The eye is free to roam to whichever part is of interest; also, these elements can often be made fairly self-explanatory, so that one doesn&#x27;t have to read everything else to see what is going on.)</li><li>Tables of contents; outlines.</li><li>Chapters and sections which can be read independently. </li></ul><p>I initially wrote the above lists as two paragraphs. Turning them into bulleted lists made them feel less paternal. The rest of this post is paternal, in that I don&#x27;t separate out different topics in an easily distinguishable way.</p><p>I&#x27;m not going to claim that &quot;less paternal&quot; is always better. Paternal formats are often more fun to read, for example. On this spectrum, HPMOR would be more paternal than the sequences. Furthermore, you might want to present information in a paternal format if getting information in the wrong order is likely to make a person <em>actively misunderstand</em> a concept rather than only fail to understand. (Mere prerequisite concepts are not really a good excuse -- a wiki-style format, where prerequisite concepts are obsessively hyperlinked, works fine.)</p><p>A &#x27;sequence&#x27; is a somewhat paternal format -- a set of posts intended to be read in a particular order. However, sequences on LW1.0 were less paternal than sequences as they exist on LW2.0. LW1.0 sequences (mostly?) listed the posts in a sequence <em>with summaries of each post,</em>, while LW2.0 sequences only have a summary of the entire sequence at the top, w/o a short description of each post.</p><p>A paternal format can also be better for attention management. Navigable formats can create a tendency for attention to jump all over the place and not spend enough time on any one topic.</p><p>Paternal formats seem workable for small and medium-sized chunks of information, but become unwieldy for large amounts of information. Imagine if an encyclopedia were written to be read in a sequential order.</p><p>Navigable formats often take more effort to create. Drawing a good diagram tends to take more effort than writing the information in text (though, obviously, this depends on a lot of factors). Maybe the ideal should be to move information from more-paternal to less-paternal formats: it is good to initially create a lot of content in very linear formats which have to be read from start to finish, but it is also good to eventually move things to a wiki or something like that.</p><p>Another idea is that information should just be available in as broad a variety of formats as possible.</p><p>Paternal formats seem to be better for convincing people of a target position, whereas navigable formats seem better for presenting all of the information. This would make paternal formats a <a href=""https://slatestarcodex.com/2017/03/24/guided-by-the-beauty-of-our-weapons/"">symmetric weapon</a>. This isn&#x27;t entirely true -- advertisements often seem to use information-delivery strategies which I would ordinarily characterize as less paternal (such as bulleted lists, text elements that don&#x27;t need to be read in a particular order, images). Maybe that&#x27;s because they need to work with a low attention span.</p><p>It is possible that rigorous argument and careful analysis fits better in linear, paternal formats. So, it isn&#x27;t clear which formatting style is better for healthy epistemics.</p><p>Maybe communities which share information in a manner heavily bias toward paternal formats have a greater risk of being convinced by long and overly clever arguments, whereas communities which share information heavily biased toward navigable formats suffer from overly quick impressions formed via merely glancing at a million things.</p><p>But, let&#x27;s not jump to the conclusion that a healthy balance is sufficient. The ideal would be to get the advantages of both.</p>",abramdemski,abramdemski,abramdemski,
ai559QFEanL6Fnb74,Word-Idols (or an examination of ties between philosophy and horror literature),word-idols-or-an-examination-of-ties-between-philosophy-and-1,https://www.lesswrong.com/posts/ai559QFEanL6Fnb74/word-idols-or-an-examination-of-ties-between-philosophy-and-1,2019-06-09T00:31:28.544Z,9,6,4,False,False,,"<p> An examination of any ties between Philosophy and horror literature is,  indeed, quite rare an undertaking... There are many reasons for the  scarcity of articles on this topic, ranging from a reluctance to  acknowledge horror literature as serious (literary) fiction, to  Philosophy itself being dismissed as overrated, superfluous or obsolete.  As with most cases of categorical nullification of entire genres or  orders, this one as well can largely be attributed to lack of  familiarity with the essential subjects they encompass.<br/> </p><p>It can be argued that there indeed are grounds to assert a link between  Philosophy and Horror literature. Socrates himself, while pondering a  definition of Philosophy, notes that the noun <em>thámvos -</em> the Greek term for <em>dazzle</em>  – was traditionally regarded as the progenitor of philosophical  thought, and goes on to speak favorably of this connection. Socrates  offers the insight that Philosophy is a hunt for the source of the  dazzling sense a thinker may have of there being unknown things in our  own mental world; the sense that we are, both by necessity and will,  progressing on a surface of things and sliding along, minding to steer  away from any chasms, while below the level of consciousness is  perpetuated a dark abyss of unknowns. <br/> </p><p>Anyone who has read H.P. Lovecraft would instantly recognize the  aforementioned image. A deep, unexplored abyss teeming with potentially  dangerous forces, juxtaposed to a relatively well-established surface  area where humans carry on their everyday lives with neither the ability  nor the will to investigate what lurks below. The lack of ability  itself is to be expected: the human mind has its own limitations, and so  does the conscious power of any individual. The absence of will,  however, does signify fear.<br/> </p><p>That said, in Philosophy the subject matter does not – usually – allow  for lack of will to manifest (what would a non-thinking philosopher  be?). Nevertheless, it can be regarded as self-evident that will to  examine the depths of one’s own mind is generally lacking in most  people. It can be lacking in philosophically-inclined individuals as  well, given there are topics which may cause even the supposedly  self-indulgent thinker to make the conscious decision to back down from  further examination; these topics primarily have to do with bringing  into light what hasn’t been formed stably before: to self-reflect, to  insist in examining one’s deeper world of thought is a little bit like  having to look at a bright and blinding light that cannot be immediately  softened. A dangerous and powerful beam which is potent enough to  reveal new and not entirely well-defined forms moving about below the  conscious mind. Sometimes – as in Plato’s Allegory of the Cave – one has  to first look away from the Sun, and prefer to observe not the forms  themselves but their idols as they are reflected on the surface of a  lake or river. Or choose to simply retain a memory of the first  impression, and then dealing only with the memory, having replaced the  striking and dazzling original with a replica sculpted out of more  familiar thoughts and notions.<br/> </p><p>Let us recall the opening paragraph of H.P. Lovecraft’s “The Call of Cthulhu”:<br/> </p><p>“The most merciful thing in the world, I think, is the inability of the  human mind to correlate all its contents. We live on a placid island of  ignorance in the midst of black seas of infinity, and it was not meant  that we should voyage far. The sciences, each straining in its own  direction, have hitherto harmed us little; but some day the piecing  together of dissociated knowledge will open up such terrifying vistas of  reality, and of our frightful position therein, that we shall either go  mad from the revelation or flee from the deadly light into the peace  and safety of a new dark age. ”<br/> </p><p>Reading the above one cannot help but notice that a discovery may lead  to disaster; for two reasons: The person made the discovery by stumbling  upon it, but in essence this lack of readiness can well be something  impossible to change and not even fruitful to attempt changing. It may  indeed be caused by inherent checks and balances in the human mind. The  sense of so intense and ominous a surprise is potent enough to demand  meticulous examination: In De Maupassant’s dark short stories we often  read of the narrator having to take notes in the aftermath of such a  pathos, and those are notes taken not with the end to further the  insight granted by the original revelation, but in fact with an almost  antithetical goal: they are conceived and – perhaps – scribbled down so  as to serve as another barrier between the frightened note-taker and the  dangerous glow of the dazzling revelation, since they aspire to dim the  light by burying it under pages and pages of a peculiar safety net. In  Lovecraft, again, we often read the narrator claim that he is writing  down his story not out of hope to establish some logical explanation  (and thus make his horror diminish) but because he wishes for an account  to remain, an account of a cursed barrier he stumbled upon. The  horrified thinker is forced to become a strange patent creator and come  up with means to repress a dangerous sense originating in the depths of  one’s own mind. The sculptor in H.P.L’s “The Call of Cthulhu” can bare  to look at the <em>idol</em> he created, but only out of sedation, while the <em>original,</em> witnessed in the dream, was impossible to withstand.<br/> </p><p>As stated, most of the issues dealt with in Philosophy do not  immediately border so dizzying or dazzling a sense. Socrates did say  that he was “almost afraid” of examining Parmenides, due to the nauseous  implications of the Eleatic Philosophy; yet that was a discussion on  Dialectics, a branch of Philosophy that deals with matters which by  their own nature are open-ended and theoretical. And while potentially  any examination of notions themselves may eventually lead the thinker to  sense he isn’t aware of what lies further below (or even if any set  foundation exists in those unlit depths of the unconscious from which  all notions spring and are later on crystallized into terms to be used  and communicated freely) it is obvious that the large majority of  philosophical subjects are more distinctly outlined and consequently  rendered quite fit for smooth and relatively unexcited discussion.<br/> </p><p>And yet, Lovecraft’s idea about an unintended revelation does echo other  philosophical-literary sentiments by celebrated authors. The sense of a  critical border – an <em>event horizon,</em> so to speak – in  consciousness, is perhaps one of the most common subjects in well-known  literary fiction, one examined by authors such as F. Kafka, J.L. Borges,  H. Hesse, C. Baudelaire and E.A. Poe. It is, I think, highly  unfortunate that when it becomes the centerpiece in horror literature –  as in the case of H.P. Lovecraft’s works – the focus usually rests on  the sentiment of fear and not on the arguably philosophical and  psychological cause: the fear of the unknown. <br/> </p><p>Perhaps Lovecraft himself is – at least partly – to blame for diverting  attention from the philosophical meaning of his allegorical “invasion”  or “colonization” by “alien” lifeforms; this type of furtive coexistence  may literally be alluding to the necessary lack of awareness in all of  us for what lurks deeper inside our mental cosmos. After all, don’t we  fossilize any sense of that deep into neat notions, and don’t we proceed  to carve – far less potent than the original – <em>idols</em> of those notions in the shape of words?  </p><p>by Kyriakos Chalkopoulos - article can also be found at <a href=""https://www.patreon.com/posts/word-idols-are-27458809"">https://www.patreon.com/posts/word-idols-are-27458809</a></p>",KyriakosCH,kyriakosch,KyriakosCH,
zTfSXQracE7TW8x4w,Mistakes with Conservation of Expected Evidence,mistakes-with-conservation-of-expected-evidence,https://www.lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence,2019-06-08T23:07:53.719Z,232,91,29,False,False,,"<p><em>Epistemic Status: I&apos;ve really spent some time wrestling with this one. I am highly confident in most of what I say. However, this differs from section to section. I&apos;ll put more specific epistemic statuses at the end of each section.</em></p><p>Some of this post is generated from mistakes I&apos;ve seen people make (or, heard people complain about) in applying <a href=""https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence"">conservation-of-expected-evidence</a> or related ideas. Other parts of this post are based on mistakes I made myself. I think that I used a wrong version of conservation-of-expected-evidence for some time, and propagated some wrong conclusions fairly deeply; so, this post is partly an attempt to work out the right conclusions for myself, and partly a warning to those who might make the same mistakes.</p><p>All of the mistakes I&apos;ll argue against have <em>some good insight behind them. </em>They may be something which is usually true, or something which points in the direction of a real phenomenon while making an error. I may come off as nitpicking.</p><h1>1. &quot;You can&apos;t predict that you&apos;ll update in a particular direction.&quot;</h1><p>Starting with an easy one. </p><p>It can be tempting to simplify conservation of expected evidence to say you can&apos;t predict the direction which your beliefs will change. This is often approximately true, and it&apos;s exactly true in symmetric cases where your starting belief is 50-50 and the evidence is equally likely to point in either direction. </p><p>To see why it is wrong in general, consider an extreme case: a universal law, which you mostly already believe to be true. At any time, you could see a counterexample, which would make you jump to complete disbelief. That&apos;s a small probability of a very large update downwards. Conservation of expected evidence implies that you must move your belief upwards when you don&apos;t see such a counterexample. But, you consider that case to be quite likely. So, considering only which <em>direction</em> your beliefs will change, you can be fairly confident that your belief in the universal law will increase -- in fact, as confident as you are in the universal law itself.</p><p>The critical point here is direction vs magnitude. Conservation of expected evidence takes magnitude as well as direction into account. The small but very probable increase is balanced by the large but very improbable decrease.</p><p>The fact that we&apos;re talking about universal laws and counterexamples may fool you into thinking about logical uncertainty. You <em>can</em> think about logical uncertainty if you want, but this phenomenon is present in the fully classical Bayesian setting; there&apos;s no funny business with non-Bayesian updates here.</p><p><em>Epistemic status: confidence at the level of mathematical reasoning.</em></p><h1>2. &quot;Yes requires the possibility of no.&quot;</h1><p>Scott&apos;s recent post, <a href=""https://www.lesswrong.com/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no"">yes requires the possibility of no</a>, is fine. I&apos;m referring to a possible mistake which one could make in applying the principle illustrated there.</p><blockquote>&#x201C;Those who dream do not know they dream, but when you are awake, you know you are awake.&#x201D; -- Eliezer, <a href=""https://www.lesswrong.com/posts/svoD5KLKHyAKEdwPo/against-modest-epistemology"">Against Modest Epistemology</a></blockquote><p>Sometimes, look around, and ask myself whether I&apos;m in a dream. When this happens, I generally conclude very confidently that I&apos;m awake.</p><p>I am not similarly capable of determining that I&apos;m dreaming. My dreaming self doesn&apos;t have the self-awareness to question whether he is dreaming in this way.</p><p>(Actually, very occasionally, I do. I either end up forcing myself awake, or I become lucid in the dream. Let&apos;s ignore that possibility for the purpose of the thought experiment.)</p><p>I am not claiming that my dreaming self is never deluded into thinking he is awake. On the contrary, I have those repeatedly-waking-up-only-to-find-I&apos;m-still-dreaming dreams occasionally. In those cases, I vividly believe myself to be awake. So, it&apos;s definitely possible for me to vividly believe I&apos;m awake and be mistaken. </p><p>What I&apos;m saying is that, when I&apos;m asleep, I am not able to perform <em>the actually good test</em>, where I look around and really consciously consider whether or not I might be dreaming. Nonetheless, when I can perform that check, it seems quite reliable. If I want to know if I&apos;m awake, I can just check.</p><p>A &quot;yes-requires-the-possibility-of-no&quot; mindset might conclude that my &quot;actually good test&quot; is no good at all, because it can&apos;t say &quot;no&quot;. I believe the exact opposite: my test seems really quite effective, because I only successfully complete it while awake.</p><p>Sometimes, your thought processes really are quite suspect; yet, there&apos;s a sanity check you can run which tells you the truth. If you&apos;re deluding yourself, the <em>general category</em> of &quot;things which you think are simple sanity checks you can run&quot; is not trustworthy. If you&apos;re deluding yourself, you&apos;re not even going to think about the real sanity checks. But, that <em>does not in itself detract from the effectiveness of the sanity check.</em></p><p>The general moral in terms of conservation of expected evidence is: <em>&quot;&apos;Yes&apos; only requires the possibility of silence.&quot;</em>. In many cases, you can meaningfully say yes without being able to meaningfully say no. For example, the axioms of set theory could prove their own inconsistency. They could <em>not</em> prove themselves consistent (without also proving themselves inconsistent). This does not detract from the effectiveness of a proof of inconsistency! Again, although the example involves logic, there&apos;s nothing funny going on with logical uncertainty; the phenomenon under discussion is understandable in fully Bayesian terms.</p><p>Symbolically: as is always the case, you don&apos;t really want to update on the raw proposition, but rather, <em>the fact that you observed the proposition</em>, to account for selection bias. Conservation of expected evidence can be written <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(H) = P(H|E)P(E) + P(H|\neg E)P(\neg E)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">&#xAC;</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">&#xAC;</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>, but if we re-write it to explicitly show the &quot;observation of evidence&quot;, it becomes <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(H) = P(H|obs(E))P(obs(E)) + P(H|\neg obs(E))P(\neg obs(E))""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">&#xAC;</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">&#xAC;</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>. It <em><strong>does not become </strong></em><span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P(H) = P(H|obs(E))P(obs(E)) + \color{red}{P(H|obs(\neg E))P(obs(\neg E))}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mstyle MJXc-space2"" style=""color: red;""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">&#xAC;</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">&#xAC;</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span></span></span>. In English: evidence is balanced between making the observation and not making the observation, <em><strong>not</strong></em> between the observation and the observation of the negation.</p><p><em>Epistemic status: confidence at the level of mathematical reasoning for the core claim of this section. However, some applications of the idea (such as to dreams, my central example) depend on trickier philosophical issues discussed in the next section. I&apos;m only moderately confident I have the right view there.</em></p><h1>3. &quot;But then what do you say to the Republican?&quot;</h1><p>I suspect that many readers are <em>less than fully on board</em> with the claims I made in the previous section. Perhaps you think I&apos;m grossly overconfident about being awake. Perhaps you think I&apos;m neglecting the outside view, or ignoring something to do with timeless decision theory.</p><p>A lot of my thinking in this post was generated by grappling with some points made in <a href=""https://www.lesswrong.com/s/oLGCcbnvabyibnG9d"">Inadequate Equilibria</a>. To quote the relevant paragraph of <a href=""https://www.lesswrong.com/posts/svoD5KLKHyAKEdwPo/against-modest-epistemology"">against modest epistemology</a>:</p><blockquote>Or as someone advocating what I took to be modesty recently said to me, after I explained why I thought it was sometimes okay to give yourself the discretion to disagree with mainstream expertise when the mainstream seems to be screwing up, in exactly the following words: &#x201C;But then what do you say to the Republican?&#x201D;</blockquote><p>Let&apos;s put that in (pseudo-)conservation-of-expected-evidence terms: we know that just applying one&apos;s best reasoning will often leave one overconfident in one&apos;s idiosyncratic beliefs. Doesn&apos;t that mean &quot;apply your best reasoning&quot; is a <a href=""https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence"">bad test</a>, which fails to conserve expected evidence? So, should we not adjust downward in general?</p><p>In the essay, Eliezer strongly advises allowing yourself to have an inside view even when there&apos;s an outside view which says inside views broadly similar to yours tend to be mistaken. But doesn&apos;t that go against what he said in <a href=""https://www.lesswrong.com/posts/dWTEtgBfFaz6vjwQf/ethical-injunctions"">Ethical Injunctions</a>?</p><p><em>Ethical Injunctions</em> argues that there are situations where you should not trust your reasoning, and fall back on a general rule. You do this because, in the vast majority of cases of that kind, your oh-so-clever reasoning is mistaken and the general rule saves you from the error.</p><p>In <em>Against Modest Epistemology</em>, Eliezer criticizes arguments which rely on putting arguments in very general categories and taking the outside view:</p><blockquote>At its epistemological core, modesty says that we should abstract up to a particular <em>very general</em> self-observation, condition on it, and then not condition on anything else because that would be inside-viewing. An observation like, &#x201C;I&#x2019;m familiar with the cognitive science literature discussing which debiasing techniques work well in practice, I&#x2019;ve spent time on calibration and visualization exercises to address biases like base rate neglect, and my experience suggests that they&#x2019;ve helped,&#x201D; is to be generalized up to, &#x201C;I use an epistemology which I think is good.&#x201D; I am then to ask myself what average performance I would expect from an agent, conditioning only on the fact that the agent is using an epistemology that they think is good, and not conditioning on that agent using Bayesian epistemology or debiasing techniques or experimental protocol or mathematical reasoning or anything in particular.</blockquote><blockquote>Only in this way can we force Republicans to agree with us&#x2026; or something.</blockquote><p>He instead advises that we should update on all the information we have, use our best arguments, reason about situations in full detail:</p><blockquote>If you&#x2019;re trying to estimate the accuracy of your epistemology, and you know what Bayes&#x2019;s Rule is, then&#x2014;on naive, straightforward, traditional Bayesian epistemology&#x2014;you ought to condition on both of these facts, and estimate <em>P</em>(accuracy|know_Bayes) instead of <em>P</em>(accuracy). Doing anything other than that opens the door to a host of paradoxes.</blockquote><p>In <em>Ethical Injunctions,</em> he seems to warn against that very thing:</p><blockquote>But surely... if one is <em>aware of these reasons...</em> then one can simply redo the calculation, taking them into account.&#xA0; So we can rob banks if it seems like the right thing to do <em>after taking into account</em> the problem of corrupted hardware and black swan blowups.&#xA0; That&apos;s the rational course, right?</blockquote><blockquote>There&apos;s a number of replies I could give to that.</blockquote><blockquote>I&apos;ll start by saying that this is a prime example of the sort of thinking I have in mind, when I warn aspiring rationalists to beware of cleverness.</blockquote><p>Now, maybe Eliezer has simply changed views on this over the years. Even so, that leaves <em>us</em> with the problem of how to reconcile these arguments.</p><p>I&apos;d say the following: modest epistemology points out a <em>simple improvement</em> over the default strategy: &quot;In any group of people who disagree, <a href=""https://www.lesswrong.com/posts/NKECtGX4RZPd7SqYp/the-modesty-argument"">they can do better by moving their beliefs toward each other</a>.&quot; &quot;Lots of crazy people think they&apos;ve discovered secrets of the universe, and the number of sane people who truly discover such secrets is quite small; so, we can improve the average by never believing we&apos;ve discovered secrets of the universe.&quot; If we take a timeless decision theory perspective (or similar), this <em>is in fact an improvement</em>; however, it is <em>far from the optimal policy</em>, and has a form which blocks further progress.</p><p><em>Ethical Injunctions</em> talks about rules with greater specificity, and less progress-blocking nature. Essentially, a proper ethical injunction is <em>actually the best policy you can come up with</em>, whereas the modesty argument stops short of that.</p><p>Doesn&apos;t the &quot;actually best policy you can come up with&quot; risk overly-clever policies which depend on broken parts of your cognition? Yes, but your <a href=""https://www.lesswrong.com/posts/C8nEXTcjZb9oauTCW/where-recursive-justification-hits-bottom"">meta-level arguments about which kinds of argument work</a> should be independent sources of evidence from your object-level confusion. To give a toy example: let&apos;s say you really, really want 8+8 to be 12 due to some motivated cognition. You can still decide to check by applying basic arithmetic. You might <em>not</em> do this, because you <em>know</em> it isn&apos;t to the advantage of the motivated cognition. However, if you do check, it is actually quite difficult for the motivated cognition to warp basic arithmetic.</p><p>There&apos;s also the fact that choosing a modesty policy <em><strong>doesn&apos;t really help the republican</strong></em>. I think that&apos;s the critical kink in the conservation-of-expected-evidence version of modest epistemology. If you, while awake, decide to doubt whether you&apos;re awake (no matter how compelling the evidence that you&apos;re awake seems to be), then <em>you&apos;re not really improving your overall correctness</em>. </p><p>So, all told, it seems like conservation of expected evidence has to be applied to the <em>details</em> of your reasoning. If your put your reasoning in a more generic category, it may appear that a much more modest conclusion is required by conservation of expected evidence. We can justify this in classical probability theory, though in this section it is even more tempting to consider exotic decision-theoretic and non-omnescience considerations than it was previously.</p><p><em>Epistemic status: the conclusion is mathematically true in classical Bayesian epistemology. I am subjectively &gt;80% confident that the conclusion should hold in &gt;90% of realistic cases, but it is unclear how to make this into a real empirical claim. I&apos;m unsure enough of how ethical injunctions should work that I could see my views shifting significantly. I&apos;ll mention <a href=""https://www.lesswrong.com/posts/4K3GtytZhmmXdSj78/confusions-concerning-pre-rationality"">pre-rationality</a> as one confusion I have which seems vaguely related.</em></p><h1>4. &quot;I can&apos;t credibly claim anything if there are incentives on my words.&quot;</h1><p>Another rule which one might derive from Scott&apos;s <em>Yes Requires the Possibility of No</em> is: you can&apos;t really say anything if pressure is being put on you to say a particular thing.</p><p>Now, I agree that this is somewhat true, particularly in simple cases where pressure is being put on you to say one particular thing. However, I&apos;ve suffered from <a href=""https://en.wikipedia.org/wiki/Learned_helplessness"">learned helplessness</a> around this. I sort of shut down when I can identify any incentives at all which could make my claims suspect, and hesitate to claim anything. This isn&apos;t a very useful strategy. Either &quot;just say the truth&quot;, or &quot;just say whatever you feel you&apos;re expected to say&quot; are <em>both</em> likely better strategies.</p><p>One idea is to &quot;call out&quot; the pressure you feel. &quot;I&apos;m having trouble saying anything because I&apos;m worried what you will think of me.&quot; This isn&apos;t always a good idea, but it can often work fairly well. Someone who <em>is</em> caving to incentives isn&apos;t very likely to say something like that, so it provides some evidence that you&apos;re being genuine. It can also open the door to other ways you and the person you&apos;re talking to can solve the incentive problem.</p><p>You can also &quot;call out&quot; something even if you&apos;re <a href=""http://grognor.blogspot.com/2016/12/unable-or-unwilling-to-explain.html"">unable or unwilling to explain</a>. You just say something like &quot;there&apos;s some <em>thing</em> going on&quot;... or &quot;I&apos;m somehow frustrated with this situation&quot;... or whatever you can manage to say.</p><p>This &quot;call out&quot; idea also works (to some extent) on motivated cognition. Maybe you&apos;re worried about the social pressure on your beliefs because it might influence the accuracy of those beliefs. Rather than stressing about this and going into a spiral of self-analysis, you can just state to yourself that that&apos;s a thing which might be going on, and move forward. Making it explicit might open up helpful lines of thinking later.</p><p>Another thing I want to point out is that most people are willing to place at least a little faith in your honesty (and not irrationally so). Just because you have a story in mind where they should assume you&apos;re lying doesn&apos;t mean that&apos;s the only possibility they are -- or should be -- considering. One problematic incentive doesn&apos;t fully determine the situation. (This one also applies internally: identifying one relevant bias or whatever doesn&apos;t mean you should block off that part of yourself.)</p><p><em>Epistemic status: low confidence. I imagine I would have said something very different if I were more an expert in this particular thing.</em></p><h1>5. &quot;Your true reason screens off any other evidence your argument might include.&quot;</h1><p>In <a href=""https://www.lesswrong.com/posts/34XxbRFe54FycoCDw/the-bottom-line"">The Bottom Line</a>, Eliezer describes a clever arguer who first writes the conclusion which they want to argue for at the bottom of a sheet of paper, and then comes up with as many arguments as they can to put above that. In the thought experiment, the clever arguer&apos;s conclusion is actually determined by who can pay the clever arguer more. Eliezer says:</p><blockquote>So the handwriting of the curious inquirer is entangled with the signs and portents and the contents of the boxes, whereas the handwriting of the clever arguer is evidence only of which owner paid the higher bid. There is a great difference in the indications of ink, though one who foolishly read aloud the ink-shapes might think the English words sounded similar.</blockquote><p>Now, Eliezer is trying to make a point about <em>how you form your own beliefs</em> -- that the quality of the process which determines which claims <em>you</em> make is what matters, and the quality of any rationalizations you give doesn&apos;t change that.</p><p>However, reading that, I came away with the mistaken idea that <em>someone listening to a clever arguer should ignore all the clever arguments</em>. Or, generalizing further, <em>what you should do when listening to any argument is try to figure out what process wrote the bottom line</em>, ignoring any other evidence provided.</p><p>This isn&apos;t the worst possible algorithm. You really <em>should</em> heavily discount evidence provided by clever arguers, because it has been heavily cherry-picked. And almost everyone does a great deal of clever arguing. Even a hardboiled rationalist will tend to present evidence for the point they&apos;re trying to make rather than against (perhaps because <a href=""https://www.lesswrong.com/posts/JtwbGiEz7QWdff5gk/explanation-vs-rationalization"">that&apos;s a fairly good strategy for explaining things</a> -- sampling evidence at random isn&apos;t a very efficient way of conversing!).</p><p>However, ignoring arguments and attending only to the original causes of belief has some absurd consequences. Chief among them is: it would imply that you should ignore mathematical proofs if the person who came up with the proof only searched for positive proofs and wouldn&apos;t have spend time trying to prove the opposite. (This ties in with the very first section -- failing to find a proof is like remaining silent.)</p><p>This is bonkers. Proof is proof. And again, this isn&apos;t some special non-Bayesian phenomenon due to logical uncertainty. A Bayesian can and should recognize decisive evidence, whether or not it came from a clever arguer.</p><p>Yet, I really held this position for a while. I treated mathematical proofs as an exceptional case, rather than as a phenomenon continuous with weaker forms of evidence. If a clever arguer presented anything <em>short</em> of a mathematical proof, I would remind myself of how convincing cherry-picked evidence can seem. And I&apos;d notice how almost everyone mostly cherry-picked when explaining their views.</p><p>This strategy was throwing out data when it has been contaminated by selection bias, rather than making a model of the selection bias so that I could update on the data appropriately. It might be a good practice in scientific publications, but if you take it as a universal, you could find reasons to throw out just about <em>everything</em> (especially if you start worrying about anthropic selection effects).</p><p>The right thing to do is closer to this: figure out how convincing you expect evidence to look <em>given</em> the extent of selection bias. Then, update on the <em>difference</em> between what you see and what&apos;s expected. If a clever arguer makes a case which is much better than what you would have expected they could make, you can update up. If it is <em>worse</em> that you&apos;d expect, even if the evidence would otherwise look favorable, <a href=""http://grognor.blogspot.com/2016/12/stop-believing-opposite-of-what-i-say.html"">you update <em>down</em></a><em>.</em></p><p>My view also made me uncomfortable <a href=""https://www.lesswrong.com/posts/JtwbGiEz7QWdff5gk/explanation-vs-rationalization"">presenting a case for my own beliefs</a>, because I would think of myself as a clever-arguer any time I did something other than recount the actual historical causes of my belief (or honestly reconsider my belief on the spot). Grognor made a similar point in <a href=""http://grognor.blogspot.com/2016/12/unable-or-unwilling-to-explain.html"">Unwilling or Unable to Explain</a>:</p><blockquote>Let me back up. Speaking in good faith entails giving the real reasons you believe something rather than a persuasive impromptu rationalization. Most people routinely do the latter without even noticing. I&apos;m sure I still do it without noticing. But when I do notice I&apos;m about to make something up, instead I clam up and say, &quot;I can&apos;t explain the reasons for this claim.&quot; I&apos;m not willing to disingenuously reference a scientific paper that I&apos;d never even heard of when I formed the belief it&apos;d be justifying, for example. In this case silence is the only feasible alternative to speaking in bad faith.</blockquote><p>While I think there&apos;s something to this mindset, I no longer think it makes sense to clam up when you can&apos;t figure out how you originally came around to the view which you now hold. If you think there are other good reasons, you can give them without violating good faith. </p><p>Actually, I really wish I could draw a sharper line here. I&apos;m essentially claiming that a little cherry-picking is OK if you&apos;re just trying to convince someone of the view which you see as the truth, so long as you&apos;re not intentionally hiding anything. This is an uncomfortable conclusion.</p><p><em>Epistemic status: confident that the views I claim are mistaken are mistaken. Less confident about best-practice claims.</em></p><h1>6. &quot;If you can&apos;t provide me with a reason, I have to assume you&apos;re wrong.&quot;</h1><p>If you take the conclusion of the previous section too far, you might reason as follows: if someone is trying to claim X, surely they&apos;re trying to give you some evidence toward X. If they claim X and then you challenge them for evidence, they&apos;ll try to tell you any evidence they have. So, if they come up with <em>nothing</em>, <a href=""http://grognor.blogspot.com/2016/12/stop-believing-opposite-of-what-i-say.html"">you have to update <em>down</em></a><em>,</em> since you would have updated upwards otherwise. Right?</p><p>I think most people make this mistake due to simple conversation norms: when navigating a conversation, people have to figure out what everyone else is willing to assume, in order to make sensible statements with minimal friction. So, we look for obvious signs of whether a statement was accepted by everyone vs rejected. If someone was asked to provide a reason for a statement they made and failed to do so, that&apos;s a fairly good signal that the statement hasn&apos;t been accepted into the common background assumptions for the conversation. The fact that other people are likely to use this heuristic as well makes the signal even stronger. So, assertions which can&apos;t be backed up with reasons are likely to be rejected.</p><p>This is almost the opposite mistake from the previous section; the previous one was <em>justifications don&apos;t matter</em>, whereas this idea is <em>only justifications matter.</em></p><p>I think something good happens when everyone in a conversation recognizes that <em>people can believe things for good reason without being able to articulate those reasons</em>. (This includes yourself!) </p><p>You can&apos;t just give everyone a pass to make unjustified claims and assert that they have strong inarticulable reasons. Or rather, you <em>can</em> give everyone a pass to do that, but you don&apos;t have to take them seriously when they do it. However, in environments of <a href=""https://www.lesswrong.com/posts/WB49uKgMkQRbKaHme/combat-vs-nurture-and-meta-contrarianism"">high intellectual trust</a>, you <em>can</em> take it seriously. Indeed, applying the usual heuristic <a href=""http://grognor.blogspot.com/2016/12/stop-believing-opposite-of-what-i-say.html"">will likely cause you to update in the wrong direction</a>.</p><p><em>Epistemic status: moderately confident.</em></p><h1>Conclusion</h1><p>I think all of this is fairly important -- if you&apos;re like me, you&apos;ve likely made some mistakes along these lines. I also think there are many issues related to conservation of expected evidence which I still don&apos;t fully understand, such as <a href=""https://www.lesswrong.com/posts/JtwbGiEz7QWdff5gk/explanation-vs-rationalization"">explanation vs rationalization</a>, <a href=""https://www.lesswrong.com/posts/dWTEtgBfFaz6vjwQf/ethical-injunctions"">ethical injunctions</a> and <a href=""https://www.lesswrong.com/posts/4K3GtytZhmmXdSj78/confusions-concerning-pre-rationality"">pre-rationality</a>. Tsuyoku Naritai! </p>",abramdemski,abramdemski,abramdemski,
JHThjbPFomxPwjiEz,Two labyrinths - where would you rather be? ,two-labyrinths-where-would-you-rather-be-1,https://www.lesswrong.com/posts/JHThjbPFomxPwjiEz/two-labyrinths-where-would-you-rather-be-1,2019-06-08T17:48:51.069Z,15,11,4,False,False,,"<br/><p><em>Two Kings and two Labyrinths </em>is  a very short story, written by J.L. Borges. It barely manages to fill  the space of a single page; and yet there is enough in it to allow for  an interesting dissertation. </p><p>The actual story is about a rivalry between  two kings: The King of Babylon had once invited the King of Arabia to  his capital, and there got him to enter a labyrinth made of intricate  passages, surrounded by tall walls. The King of Arabia only managed  to find his way out after imploring his God for help. The experience  terrified him, and he swore that in the future he would repay the  Babylonian in kind, by introducing him to another labyrinth; one particular to his native and desolate Arabian realm...</p><p>After his victory in war, the King of Arabia takes the King of  Babylon hostage. He brings him to the desert, where, at the end of a  three-day journey, he is abandoned. The desert is another kind of  labyrinth. It has neither passages nor walls, but still finding one&#x27;s  way out of it is virtually impossible.</p><h3>A Labyrinth is More Than Just a Prison Cell</h3><p>A  labyrinth isn’t just a structure which confines; it is one which serves  the purpose of getting one disoriented. While a prison cell – regardless if it is  nameless and obscure or one as famous as the stone vault in Sophocles’  play, <em>Antigone</em>, which was used to imprison the heroine and  slowly drain her of the will to live – is just a simple room, enough to  enclose, limit, and cause desperation, an actual labyrinth functions by  allowing the person inside to still hope there is a chance of finding a  way out... The labyrinth is different from a group of  interconnecting cells, in that somewhere in it one may still discover a  passage which will lead to liberation...</p><p>The possibility of finding the exit may be so small that, in  practice, one wouldn&#x27;t ever succeed in this quest... It&#x27;s not important,  though, because the very form of the labyrinth forces its prisoner to  accept that there are always new routes to explore, or another idea to  test; the progression from each part of the labyrinth to the next one  may be quite monotonous, and almost reveal no change, but the prisoner  inside is actually moving, is still progressing – and this allows for  hope.</p><h3>The Babylonian Labyrinth</h3><p>The  first of the labyrinths presented in the story is the one the reader  would readily identify as a typical labyrinth. A maze, filled with  corridors and forking paths, and with the line of sight in every one of  its locations being crucially obstructed by tall and sturdy masonry. In  such an edifice one can attempt to examine every minute difference  between the numerous interconnecting rooms, aspiring to devise some  manner of identifying and then memorizing which paths have already been  taken, and come up with a plan that would allow for the exploration of  as many areas as possible, all the while hoping that through a combination of methodology and luck it may happen that the exit will be  discovered!</p><p>Every room has specific forms, and every step can be – and moreover may <em>have</em>  to be – retraced, to allow for a progressively more thorough and valid  impression in regards to the overall shape of the labyrinth.</p><h3>The Arabian Labyrinth</h3><p>The  labyrinth in Arabia is, of course, the desert itself. It stretches for  endless miles. Here there are no rooms, nor walls, nor any other element  which changes as one carries on walking. It is, indeed, a labyrinth  which consists of a singular vast space; and, unlike the Babylonian type,  this labyrinth will reveal its exit if you simply walk far enough so  that the first signs of something other than the desert becomes visible on the horizon... Unlike with the built maze, the desert doesn’t  allow for retracing of steps; you have to choose a direction, and carry  on moving. It may, in fact, easily be the case that your very first step and your  very first choice has already either saved or doomed you! Only at a far later point in time will you find out which of the two was true.</p><p>While in the built maze you need to form a sense of the overall pattern, keep track of the various routes you had taken and construct a  plan so as to allow for a new, original route to be set in every  subsequent attempt, in the desert maze you have an infinite number of  routes which only differ in essence in regards to their direction: if  (for example) this desert&#x27;s end can only be reached – before your  stamina and supplies are depleted – if you keep moving eastwards, you  won’t ever succeed if you moved to the west. </p><h3>The Crucial Difference Between The Two </h3><p>Both  versions of the labyrinth exist so as to achieve the same: prevent the one inside to escape without conscious effort. Or, to put it in a more poignant manner:  not allow one to leave unless they had gained a particular knowledge  about the labyrinth; the knowledge of a way out. After all, no labyrinth  can remain imposing once you have located its exit.</p><p>But the two versions differ in a very crucial way: While the  labyrinth of corridors will keep you hoping until the very last second  of your life – for the exit may always be found in the next room and therefore still be accessible even if you are about to collapse, starving and  reduced to crawling on the floor – the labyrinth of nothingness,  the cruel and level plane of the desert, will have informed you long  before you fall to the sand, never to rise back again, that you already have lost and are to die inside it...</p><p>And yet it must be noted that this difference brings about also a  complementary and antithetic element; an elegant juxtaposition: In the  labyrinth of corridors you will retain hope until you draw your last  breath, yes, but you will also keep being fooled into thinking your moves  up to that point haven’t failed you. In the labyrinth of open space you  will be informed that you failed, and that you will die, long before it  happens – since there won’t be any settlement visible on the horizon,  and your body has already shown the tell-tale signs of giving up.</p><p>by Kyriakos Chalkopoulos</p><p>*</p><p>I want to ask which of the two types of labyrinth you would rather be in. Certainly one can imagine life as a journey inside (external as well as internal) labyrinths.</p><br/>",KyriakosCH,kyriakosch,KyriakosCH,
Nd5KiuN8pPBrMT82Z,Asymmetric Weapons Aren't Always on Your Side,asymmetric-weapons-aren-t-always-on-your-side-1,https://www.lesswrong.com/posts/Nd5KiuN8pPBrMT82Z/asymmetric-weapons-aren-t-always-on-your-side-1,2019-06-08T08:47:49.675Z,37,24,20,False,False,,"<p>Some time ago, Scott Alexander wrote about <a href=""https://slatestarcodex.com/2017/03/24/guided-by-the-beauty-of-our-weapons/"">asymmetric weapons</a>, and now he <a href=""https://slatestarcodex.com/2019/06/06/asymmetric-weapons-gone-bad/"">writes again about them</a>. During these posts, Scott repeatedly characterizes asymmetric weapons as inherently stronger for the &quot;good guys&quot; than they are for the &quot;bad guys&quot;. Here is a quote from his first post:</p><blockquote>Logical debate has one advantage over narrative, rhetoric, and violence: it’s an <em>asymmetric weapon</em>. That is, it’s a weapon which is stronger in the hands of the good guys than in the hands of the bad guys.</blockquote><p>And here is a quote from his more recent one:</p><blockquote>A symmetric weapon is one that works just as well for the bad guys as for the good guys. For example, violence – your morality doesn’t determine how hard you can punch; they can buy guns from the same places we can.</blockquote><blockquote>An asymmetric weapon is one that works better for the good guys than the bad guys. The example I gave was Reason. If everyone tries to solve their problems through figuring out what the right thing to do is, the good guys (who are right) will have an easier time proving themselves to be right than the bad guys (who are wrong). Finding and using asymmetric weapons is the only non-coincidence way to make sustained moral progress.</blockquote><p>One problem with this concept is that <em>just because something is asymmetric doesn&#x27;t mean that it&#x27;s asymmetric in a good direction.</em></p><p>Scott talks about weapons that are asymmetric towards those who are right. However, there are many more types of asymmetries than just right vs. wrong - physical violence is asymmetric towards the strong, shouting people down is asymmetric towards the loud, and airing TV commercials is asymmetric towards people with more money. Violence isn&#x27;t merely symmetric - it&#x27;s<em> asymmetric in a bad direction</em>, since <a href=""https://themiddleofthatzone.wordpress.com/2017/02/07/fascists-are-better-at-violence-than-you/"">fascists are better than violence than you</a>.</p><p>This in turn means that various sides will all be trying to pull things in directions that are asymmetric to their advantage. Indeed, a basic principle in strategy is to try to shift conflicts into areas where you are strong and your opponent is weak.</p><p>For instance, people who are good at violence benefit from things getting violent. People who are locally popular benefit from popularity contests. People who have lots of free time benefit from time-consuming processes. People who are better at keeping their composure benefit from discourse norms that punish displays of emotion.</p><p>Developing asymmetric processes that point towards truth is a good idea, and I&#x27;m all for it. But in practice there are also asymmetric processes that point towards error, or merely asymmetric processes that point towards what&#x27;s currently popular or faddish. Those processes are, if anything, just as likely to have people trying to promote them than the pro-truth ones - perhaps more likely!</p><p>That doesn&#x27;t make the people promoting those ideas &quot;anti-truth&quot; or whatever - they may not even be aware of what they&#x27;re doing - but even so, people tend to respond to incentives, and those incentives may well pull them towards norms and methods that are asymmetric in their favor independent of whether those norms and methods promote truth.</p>",Davis_Kingsley,davis_kingsley,Davis_Kingsley,
zp5AEENssb8ZDnoZR,"The Schelling Choice is ""Rabbit"", not ""Stag""",the-schelling-choice-is-rabbit-not-stag,https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag,2019-06-08T00:24:53.568Z,166,85,52,False,False,,"<p>Followup/distillation/alternate-take on Duncan Sabien&apos;s<em> </em><a href=""https://medium.com/@ThingMaker/dragon-army-retrospective-597faf182e50"">Dragon Army Retrospective</a> and <a href=""https://medium.com/@ThingMaker/open-problems-in-group-rationality-5636440a2cd1"">Open Problems in Group Rationality</a>.</p><hr class=""dividerBlock""><p>There&apos;s a particular failure mode I&apos;ve witnessed, and fallen into myself: </p><p>I see a problem. I see, what seems to me, to be an obvious solution to the problem. If only everyone Took Action X, we could Fix Problem Z. So I start X-ing, and maybe talking about how other people should start X-ing. Action X takes some effort on my part but it&apos;s obviously worth it.</p><p>And yet... nobody does. Or not enough people do. And a few months later, here I&apos;m still taking Action X and feeling burned and frustrated.</p><p>Or &#x2013;</p><p>&#x2013;&#xA0;the problem is that everyone is taking Action Y, which directly causes Problem Z. If only everyone would stop Y-ing, Problem Z would go away. Action Y seems obviously bad, clearly we should be on the same page about this. So I start noting to people when they&apos;re doing Action Y, and expect them to stop. </p><p>They don&apos;t stop.</p><p>So I start subtly socially punishing them for it.</p><p>They don&apos;t stop. What&apos;s more... now they seem to be punishing <em>me.</em></p><p>I find myself getting frustrated, perhaps angry. What&apos;s going on? Are people wrong-and-bad? Do they have wrong-and-bad beliefs? </p><p>Alas. So far in my experience it hasn&apos;t been that simple.</p><hr class=""dividerBlock""><h2>A recap of &apos;Rabbit&apos; vs &apos;Stag&apos;</h2><p>I&apos;d been planning to write this post for years. Duncan Sabien went ahead and wrote it before I got around to it. But, <a href=""https://medium.com/@ThingMaker/dragon-army-retrospective-597faf182e50"">Dragon Army Retrospective</a> and <a href=""https://medium.com/@ThingMaker/open-problems-in-group-rationality-5636440a2cd1"">Open Problems in Group Rationality</a> are both lengthy posts with a lot of points, and it still seemed worth highlighting this particular failure mode in a single post.</p><p>I used to think a lot in terms of Prisoner&apos;s Dilemma, and &quot;Cooperate&quot;/&quot;Defect.&quot; I&apos;d see problems that could easily be solved if everyone just put a bit of effort in, which would benefit everyone. And people didn&apos;t put the effort in, and this felt like a frustrating, obvious coordination failure. Why do people defect so much?</p><p>Eventually Duncan shifted towards using <strong>Stag Hunt</strong> rather than <strong>Prisoner&apos;s Dilemma</strong> as the model here. If you haven&apos;t read it before, it&apos;s worth reading the description in full. If you&apos;re familiar you can skip to my current thoughts below.</p><blockquote>My new favorite tool for modeling this is <strong>stag hunts</strong>, which are similar to prisoner&#x2019;s dilemmas in that they contain two or more people each independently making decisions which affect the group. In a stag hunt:</blockquote><blockquote>&#x2014;Imagine a hunting party venturing out into the wilderness.</blockquote><blockquote>&#x2014;&#xA0;Each player may choose <em>stag</em> or <em>rabbit,</em> representing the type of game they will try to bring down.</blockquote><blockquote>&#x2014;&#xA0;All game will be shared within the group (usually evenly, though things get more complex when you start adding in real-world arguments over who deserves what).</blockquote><blockquote>&#x2014;&#xA0;Bringing down a stag is costly and effortful, and requires coordination, but has a large payoff. Let&#x2019;s say it costs each player 5 points of utility (time, energy, bullets, etc.) to participate in a stag hunt, but a stag is worth 50 utility (in the form of food, leather, etc.) if you catch one.</blockquote><blockquote>&#x2014;&#xA0;Bringing down rabbits is low-cost and low-effort and can be done unilaterally. Let&#x2019;s say it only costs each player 1 point of utility to hunt rabbit, and you get 3 utility as a result.</blockquote><blockquote>&#x2014;&#xA0;If any player unexpectedly chooses rabbit while others choose stag, the stag escapes through the hole in the formation and is not caught. Thus, if five players all choose stag, they lose 25 utility and gain 50 utility, for a net gain of 25 (or +5 apiece). But if four players choose stag and one chooses rabbit, they lose 21 utility and gain only 3.</blockquote><blockquote>This creates a strong pressure toward having the Schelling choice be <em>rabbit.</em> It&#x2019;s saner and safer (spend 5, gain 15, net gain of 10 or +2 apiece), especially if you have any doubt about the other hunters&#x2019; ability to stick to the plan, or the other hunters&#x2019; faith in the other hunters, or in the other hunters&#x2019; current resources and ability to even take a hit of 5 utility, or in whether or not the forest contains a stag at all.</blockquote><blockquote>Let&#x2019;s work through a specific example. Imagine that the hunting party contains the following five people:</blockquote><blockquote>Alexis (currently has 15 utility &#x201C;in the bank&#x201D;)</blockquote><blockquote>Blake (currently has 12)</blockquote><blockquote>Cameron (9)</blockquote><blockquote>Dallas (6)</blockquote><blockquote>Elliott (5)</blockquote><blockquote>If everyone successfully coordinates to choose stag, then the end result will be positive for everyone. The stag costs everyone 5 utility to bring down, and then its 50 utility is divided evenly so that everyone gets 10, for a net gain of 5. The array [15, 12, 9, 6, 5] has bumped up to [20, 17, 14, 11, 10].</blockquote><blockquote>If everyone chooses rabbit, the end result is <em>also</em> positive, though less excitingly so. Rabbits cost 1 to hunt and provide 3 when caught, so the party will end up at [17, 14, 11, 8, 7].</blockquote><blockquote>But imagine the situation where a stag hunt is <em>attempted,</em> but unsuccessful. Let&#x2019;s say that Blake quietly decides to hunt rabbit while everyone else chooses stag. What happens?</blockquote><blockquote>Alexis, Cameron, Dallas, and Elliott each lose 5 utility while Blake loses 1. The rabbit that Blake catches is divided five ways, for a total of 0.6 utility apiece. Now our array looks like [10.6, 11.6, 4.6, 1.6, 0.6].</blockquote><blockquote>(Remember, Blake only spent 1 utility in the first place.)</blockquote><blockquote>If you&#x2019;re Elliott, this is a <em>super scary</em> result to imagine. You no longer have enough resources in the bank to be self-sustaining&#x2014;you can&#x2019;t even go out on another <em>rabbit</em> hunt, at this point.</blockquote><blockquote>And so, if you&#x2019;re Elliott, it&#x2019;s tempting to <em>preemptively choose rabbit yourself.</em> If there&#x2019;s even a <em>chance</em> that the other players might defect on the overall stag hunt (because they&#x2019;re tired, or lazy, or whatever) or worse, if there might not even <em>be</em> a stag out there in the woods today, then you have a <em>strong</em> motivation to self-protectively husband your resources. Even if it turns out that you were wrong about the others, and you end up being the <em>only</em> one who chose rabbit, you still end up in a much less dangerous spot: [10.6, 7.6, 4.6, 1.6, 4.6].</blockquote><blockquote>Now imagine that you&#x2019;re <em>Dallas,</em> thinking through each of these scenarios. In both cases, you end up pretty screwed, with your total utility reserves at 1.6. At that point, you&#x2019;ve got to drop out of any future stag hunts, and all you can do is hunt rabbit for a while until you&#x2019;ve built up your resources again.</blockquote><blockquote>So as Dallas, you&#x2019;re reluctant to listen to any enthusiastic plan to choose stag. You&#x2019;ve got enough resources to absorb <em>one</em> failure, and so you don&#x2019;t want to do a stag hunt until you&#x2019;re <em>really darn sure</em> that there&#x2019;s a stag out there, and that everybody&#x2019;s <em>really actually for real</em> going to work together and try their hardest. You&#x2019;re not <em>opposed</em> to hunting stag, you&#x2019;re just opposed to wild optimism and wanton, frivolous burning of resources.</blockquote><blockquote>Meanwhile, if you&#x2019;re Alexis or Blake, you&#x2019;re starting to feel pretty frustrated. I mean, why bother coming out to a <em>stag hunt</em> if you&#x2019;re not even actually willing to put in the effort to <em>hunt stag?</em> Can&#x2019;t these people see that we&#x2019;re all better off if we pitch in hard, together? Why are Dallas and Elliott preemptively talking about rabbits when we haven&#x2019;t even <em>tried</em> catching a stag yet?</blockquote><blockquote>I&#x2019;ve recently been using the terms <strong>White Knight</strong> and <strong>Black Knight</strong> to refer, not to specific people like Alexis and Elliott, but to the <em>roles</em> that those people play in situations requiring this kind of coordination. White Knight and Black Knight are hats that people put on or take off, depending on circumstances.</blockquote><blockquote>The White Knight is a character who has looked at what&#x2019;s going on, built a model of the situation, decided that they understand the Rules, and begun to take confident action in accordance with those Rules. In particular, the White Knight has decided that the time to choose stag is <em>obvious,</em> and is already common knowledge/has the Schelling nature. I mean, just look at the numbers, right?</blockquote><blockquote>The White Knight is often wrong, because reality is more complex than the model even if the model is a <em>good</em> model. Furthermore, other people often don&#x2019;t <em>notice</em> that the White Knight is assuming that everyone knows that it&#x2019;s time to choose stag&#x2014;communication is hard, and the double illusion of transparency is a hell of a drug, and someone can say words like &#x201C;All right, let&#x2019;s all get out there and do our best&#x201D; and different people in the room can draw very different conclusions about what that means.</blockquote><blockquote>So the White Knight burns resources over and over again, and feels defected on every time someone &#x201C;wrongheadedly&#x201D; chooses rabbit, and meanwhile the other players feel unfairly judged and found wanting according to a standard that they never explicitly agreed to (remember, choosing rabbit should be the Schelling option, according to me), and the whole thing is very rough for everyone.</blockquote><blockquote>If this process goes on long enough, the White Knight may burn out and become the Black Knight. The Black Knight is a more <em>mercenary</em> character&#x2014;it has limited resources, so it has to watch out for itself, and it&#x2019;s only allied with the group to the extent that the group&#x2019;s goals match up with its own. It&#x2019;s capable of teamwork and coordination, but it&#x2019;s not <em>zealous</em>. It isn&#x2019;t blinded by optimism or patriotism; it&#x2019;s there to engage in mutually beneficial trade, while taking into account the realities of uncertainty and unreliability and miscommunication.</blockquote><blockquote>The Black Knight doesn&#x2019;t like this whole frame in which <em>doing the safe and conservative thing</em> is judged as &#x201C;defection.&#x201D; It wants to know who this White Knight thinks he is, that he can just <em>declare</em> that it&#x2019;s time to choose stag, without discussion or consideration of cost. If anyone&#x2019;s defecting, it&#x2019;s the <em>White</em> Knight, by going around getting mad at people for following local incentive gradients and doing the predictable thing.</blockquote><blockquote>But the Black Knight is <em>also</em> wrong, in that sometimes you really <em>do</em> have to be all-in for the thing to work. You can&#x2019;t always sit back and choose the safe, calculated option&#x2014;there are, <em>sometimes,</em> gains that can only be gotten if you have no exit strategy and leave everything you&#x2019;ve got on the field.</blockquote><blockquote>I don&#x2019;t have a solution for this particular dynamic, except for a general sense that shining more light on it (dignifying both sides, improving communication, being willing to be explicit, making it <em>safe</em> for both sides to be explicit) will probably help. I think that a &#x201C;technique&#x201D; which zeroes in on ensuring shared common-knowledge understanding of &#x201C;this is what&#x2019;s good in our subculture, this is what&#x2019;s bad, this is when we need to fully commit, this is when we can do the minimum&#x201D; is a promising candidate for defusing the whole cycle of mutual accusation and defensiveness.</blockquote><blockquote>(<a href=""https://www.authrev.com/"">Circling with a capital &#x201C;C&#x201D;</a> seems to be useful for coming at this problem sideways, whereas mission statements and manifestos and company handbooks seem to be partially-successful-but-high-cost methods of solving it directly.)</blockquote><p>The key conceptual difference that I find helpful here is acknowledging that &quot;Rabbit&quot; / &quot;Stag&quot; are both <em>positive</em> choices, that bring about utility. &quot;Defect&quot; feels like it brings in connotations that aren&apos;t always accurate.</p><p>Saying that you&apos;re going to pay rent on time, and then not, is defecting<em>.</em></p><p>But if someone shows up saying &quot;hey let&apos;s all do Big Project X&quot; and you&apos;re not that enthusiastic about Big Project X but you sort of nod noncommittally, and then it turns out they thought you were going to put 10 hours of work into it and you thought you were going to put in 1, and then they get mad at you... I think it&apos;s more useful to think of this as &quot;choosing rabbit&quot; than &quot;defecting.&quot;</p><p>Likewise, it&apos;s &quot;rabbit&quot; if  you say &quot;nah, I just don&apos;t think Big Project X is important&quot;. Going about your own projects and not signing up for every person&apos;s crusade is a perfectly valid action.</p><p>Likewise, it&apos;s &quot;rabbit&quot; if you say &quot;look, I realize we&apos;re in a bad equilibrium right now and it&apos;d be better if we all switched to A New Norm. But right now the Norm is X, and unless you are <em>actually</em> <em>sure</em> that we have enough buy-in for The New Norm, I&apos;m not going to start doing a costly thing that I don&apos;t think is even going to work.&quot; </p><hr class=""dividerBlock""><h2>A lightweight, but concrete example</h2><p>At my office, we have Philosophy Fridays*, where we try to get sync about important underlying philosophical and strategic concepts. What is our organization <em>for</em>? How does it connect to the big picture? What individual choices about particular site-features are going to bear on that big picture?</p><p>We generally agree that Philosophy Friday is important. But often, we seem to disagree a lot about the right way to go about it.</p><p>In a recent example: it often felt to me that our conversations were sort of meandering and inefficient. Meandering conversations that don&apos;t go anywhere is a stereotypical rationalist failure mode. I do it a lot by default myself. I wish that people would punish <em>me</em> when I&apos;m steering into &apos;meandering mode&apos;.</p><p>So at some point I said &apos;hey this seems kinda meandering.&apos; </p><p>And it kinda meandered a bit more.</p><p>And I said, in a move designed to be somewhat socially punishing: &quot;I don&apos;t really trust the conversation to go anywhere useful.&quot; And then I took out my laptop and mostly stopped paying attention. </p><p>And someone else on the team responded, eventually, with something like &quot;I don&apos;t know how to fix the situation because you checked out a few minutes ago and I felt punished and wanted to respond but then you didn&apos;t give me space to.&quot;</p><p>&quot;Hmm,&quot; I said. I don&apos;t remember exactly what happened next, but eventually he explained:</p><p>Meandering conversations were important to him, because it gave him space to actually think. I pointed to examples of meetings that I thought had gone well, that ended with google docs full of what I thought had been useful ideas and developments. And he said &quot;those all seemed like examples of mediocre meetings to me &#x2013;&#xA0;we had a lot of ideas, sure. But I didn&apos;t feel like I actually got to come to a real decision about anything important.&quot;</p><p>&quot;Meandering&quot; quality allowed a conversation to explore subtle nuances of things, to fully explore how a bunch of ideas would intersect. And this was necessary to eventually reach a firm conclusion, to leave behind the niggling doubts of &quot;is this *really* the right path for the organization?&quot; so that he could firmly commit to a longterm strategy.</p><p>We still debate the right way to conduct Philosophy Friday at the office. But now we have a slightly better frame for that debate, and awareness of the tradeoffs involved. We discuss ways to get the good elements of the &quot;meandering&quot; quality while still making sure to end with clear next-actions. And we discuss alternate modes of conversation we can intelligently shift between.</p><p>There&apos;s a time when I would have pre-emptively gotten really frustrated, and started rationalizing reasons why my teammate was willfully pursuing a bad conversational norm. Fortunately I had thought enough about this sort of problem that I noticed that I was failing into a failure mode, and shifted mindsets.</p><p><em>Rabbit</em> in this case was &quot;everyone just sort of pursues whatever conversational types seem best to them in an uncoordinated fashion&quot;, and <em>Stag</em> is &quot;we deliberately choose and enforce particular conversational norms.&quot;</p><p>We haven&apos;t yet coordinated enough to really have a &quot;stag&quot; option we can coordinate around. But I expect that the conversational norms we eventually settle into will be <em>better</em> than if we had naively enforced either my or my teammate&apos;s preferred norms.</p><hr class=""dividerBlock""><h2>Takeaways</h2><p>There seem like a couple important takeaways here, to me.</p><p>One is that, yes:</p><p><strong>Sometimes stag hunts are worth it.</strong></p><p>I&apos;d like people in my social network to be aware that sometimes, it&apos;s really important for everyone to adopt a new norm, or for everyone to throw themselves 100% into something, or for a whole lot of person-hours to get thrown into a project.</p><p>When discussing whether to embark on a stag hunt, it&apos;s useful to have shorthand to communicate why you might ever <em>want</em> to put a lot of effort into a concerted, coordinated effort. And then you can discuss the tradeoffs seriously.</p><p>I have more to say about what sort of stag hunts seem do-able. But for this post I want to focus primarily on the fact that...</p><p><strong>The schelling option is Rabbit</strong></p><p>Some communities have established particular norms favoring &apos;stag&apos;. But in modern, atomic, Western society you should probably not assume this as a default. If you want people to choose stag, you need to spend special effort building <a href=""https://www.lesswrong.com/posts/9QxnfMYccz9QRgZ5z/the-costly-coordination-mechanism-of-common-knowledge"">common knowledge</a> that Big Project X matters, and is worthwhile to pursue, and get everyone on board with it. </p><p>Corollary: Creating common knowledge is hard. If you <em>haven&apos;t</em> put in that work, you should assume Big Project X is going to fail, and/or that it will require a few people putting in herculean effort &quot;above their fair share&quot;, which may not be sustainable for them.</p><p>This depends on whether effort is fungible. If you need 100 units of effort, you can make do with one person putting in 100 units of effort. If you need everyone to adopt a new norm that they haven&apos;t bought into,<em> it</em> <em>just won&apos;t work.</em></p><p>If you are proposing what seems (to you) quite sensible, but nobody seems to agree...</p><p>...well, maybe people <em>are</em> being biased in some way, or motivated to avoid considering your proposed stag-hunt. People sure do seem biased about things, in general, even when they know about biases. So this may well be part of the issue.</p><p>But I think it&apos;s quite likely that you&apos;re dramatically underestimating the inferential distance &#x2013;&#xA0;both the distance between their outlook and &quot;why your proposed action is good&quot;, as well as the distance between your outlook and &quot;why their current frame is weighing tradeoffs very differently than your current frame.&quot;</p><p>Much of the time, I feel like getting angry and frustrated... is something like &quot;wasted motion&quot; or &quot;the wrong step in the dance.&quot;</p><p>Not entirely &#x2013; anger and frustration are useful motivators. They help me notice that something about the status quo is wrong and needs fixing. But I think the specific flavor of frustration that stems from &quot;people should be cooperating but aren&apos;t&quot; is often, in some sense, <em>actually wrong</em> about reality. People are actually making reasonable decisions given the current landscape. </p><p>Anger and frustration help drive me to action,&#xA0;but often they come with a sort of tunnel vision. They lead me to dig in my heels, and get ready to fight &#x2013; at a moment when what I really need is empathy<em> </em>and curiosity<em>.</em> I either need to figure out how to communicate better, to help someone understand why my plan is good. Or, I need to learn what tradeoffs I&apos;m missing, which they can see more clearly than I.</p><h2>My own strategies right now</h2><p><strong>In general, choose Rabbit. </strong></p><ul><li>Keep at around 30% <a href=""https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack"">slack</a> in reserve (such that I can absorb not one, not two, but <em>three </em>major surprise costs without starting to burn out). Don&apos;t spend energy helping others if I&apos;ve dipped below 30% for long &#x2013;&#xA0;focus on making sure my own needs are met.</li><li>Find local improvements I can make that don&apos;t require much coordination from others.</li></ul><p><strong>Follow rabbit trails into Stag* Country</strong></p><p>Given a choice, seek out &quot;Rabbit&quot; actions that preferentially build<em> option value</em> for improved coordination later on.</p><ul><li>Metaphorically, this means &quot;Follow rabbit trails that lead into *Stag-and-Rabbit Country&quot;, where I&apos;ll have opportunities to say:</li><ul><li>&quot;Hey guys I see a stag! Are we all 100% up for hunting it?&quot; and then maybe it so happens we can stag hunt together. </li><li><em>Or,</em> I can sometimes say, at small-but-manageable-cost-to-myself &quot;hey guys, I see a whole bunch of rabbits over there, you could hunt them if you want.&quot; And others can sometimes do the same for me.</li></ul><li>Sliiightly more concretely, this means: </li><ul><li>Given the opportunity, without requiring actions on the part of other people... pursue actions that demonstrate my trustworthiness, and which build bits of infrastructure that&apos;ll make it easier to work together in the future. </li><li>Help people out if I can do so without dipping below 30% slack for too long, especially if I expect it to increase the overall slack in the system.</li></ul></ul><p>(I&apos;ll hopefully have more to say about this in the future.)</p><p><strong>Get curious about other people&apos;s frames</strong></p><p>If a person and I have argued through the same set of points multiple times, each time expecting our points to be a solid knockdown of the other&apos;s argument... and if nobody has changed their mind...</p><p>Probably we are operating in two different frames. Communicating across frames is very hard, and beyond scope of this of this post to teach. But cultivating <a href=""https://www.lesswrong.com/posts/bGtdeqbgTzuLvZ5zn/get-curious"">curiosity</a> and <a href=""http://agentyduck.blogspot.com/2015/05/tortoise-report-3-empathy_16.html"">empathy</a> are good first steps.</p><p><strong>Occasionally run &quot;Kickstarters for Stag Hunts.&quot; If people commit, hunt stag.</strong></p><p>For example, the call-to-action in my <a href=""https://www.lesswrong.com/posts/yGycR8tFA3JJbvApp/the-relationship-between-the-village-and-the-mission"">Relationship Between the Village and Mission</a> post (where I asked people to contact me if they were serious about improving the Village) was designed to give me information about whether it&apos;s <em>possible</em> to coordinate on a staghunt to improve the Berkeley rationality village. </p>",Raemon,raemon,Raemon,
Sn5NiiD5WBi4dLzaB,AGI will drastically increase economies of scale,agi-will-drastically-increase-economies-of-scale,https://www.lesswrong.com/posts/Sn5NiiD5WBi4dLzaB/agi-will-drastically-increase-economies-of-scale,2019-06-07T23:17:38.694Z,65,26,26,False,False,,"<p>In <a href=""https://www.lesswrong.com/posts/gYaKZeBbSL4y2RLP3/strategic-implications-of-ais-ability-to-coordinate-at-low"">Strategic implications of AIs’ ability to coordinate at low cost</a>, I talked about the possibility that different AGIs can coordinate with each other much more easily than humans can, by doing something like merging their utility functions together. It now occurs to me that another way for AGIs to greatly reduce coordination costs in an economy is by having each AGI or copies of each AGI profitably take over much larger chunks of the economy (than companies currently own), and this can be done with AGIs that don't even have explicit utility functions, such as copies of an AGI that are all corrigible/<a href=""https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6"">intent-aligned</a> to a single person.</p>
<p>Today, there are many industries with large economies of scale, due to things like fixed costs, network effects, and reduced deadweight loss when monopolies in different industries merge (because they can internally charge each other prices that equal marginal costs), but because coordination costs among humans increase super-linearly with the number of people involved (see <a href=""https://www.lesswrong.com/posts/2Zsuv5uPFPNTACwzg/moral-mazes-and-short-termism"">Moral Mazes and Short Termism</a> for a related recent discussion), that creates diseconomies of scale which counterbalance the economies of scale, so companies tend to grow to a certain size and then stop. But an AGI-operated company, where for example all the workers are AGIs that are intent-aligned to the CEO, would eliminate almost all of the internal coordination costs (i.e., all of the coordination costs that are caused by value differences, such as all the things described in Moral Mazes, ""market for lemons"" or lost opportunities for trade due to <a href=""https://en.wikipedia.org/wiki/Information_asymmetry"">asymmetric information</a>, principal-agent problems, monitoring/auditing costs, costly signaling, and suboptimal Nash equilibria in general), allowing such companies to grow much bigger. In fact, from purely the perspective of maximizing the efficiency/output of an economy, I don't see why it wouldn't be best to have (copies of) one AGI control everything.</p>
<p>If I'm right about this, it seems quite plausible that some countries will foresee it too, and as soon as it can feasibly be done, nationalize all of their productive resources and place them under the control of one AGI (perhaps intent-aligned to a supreme leader or to a small, highly coordinated group of humans), which would allow them to out-compete any other countries that are not willing to do this (and don't have some other competitive advantage to compensate for this disadvantage). This seems to be an important consideration that is missing from many people's pictures of what will happen after (e.g., intent-aligned) AGI is developed in a slow-takeoff scenario.</p>
",Wei_Dai,wei-dai,Wei Dai,
4XPa3xa44jAWiCkmy,Risks from Learned Optimization: Conclusion and Related Work,risks-from-learned-optimization-conclusion-and-related-work,https://www.lesswrong.com/posts/4XPa3xa44jAWiCkmy/risks-from-learned-optimization-conclusion-and-related-work,2019-06-07T19:53:51.660Z,82,30,5,False,False,,"<p><em>This is the fifth of five posts in the <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">Risks from Learned Optimization Sequence</a> based on the paper “<a href=""https://arxiv.org/abs/1906.01820"">Risks from Learned Optimization in Advanced Machine Learning Systems</a>” by Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Each post in the sequence corresponds to a different section of the paper.</em></p>
<p>&nbsp;</p>
<h2>Related work</h2>
<p><strong>Meta-learning.</strong> As described in <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/FkgsxrGf3QxhfLWHG"">the first post</a>, meta-learning can often be thought of as meta-optimization when the meta-optimizer's objective is explicitly designed to accomplish some base objective. However, it is also possible to do meta-learning by attempting to make use of mesa-optimization instead. For example, in Wang et al.'s “Learning to Reinforcement Learn,” the authors claim to have produced a neural network that implements its own optimization procedure.<a href=""https://intelligence.org/learned-optimization#bibliography"">(28)</a> Specifically, the authors argue that the ability of their network to solve extremely varied environments without explicit retraining for each one means that their network must be implementing its own internal learning procedure. Another example is Duan et al.'s “<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""RL^2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">R</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">L</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span>: Fast Reinforcement Learning via Slow Reinforcement Learning,” in which the authors train a reinforcement learning algorithm which they claim is itself doing reinforcement learning.<a href=""https://intelligence.org/learned-optimization#bibliography"">(5)</a> This sort of meta-learning research seems the closest to producing mesa-optimizers of any existing machine learning research.</p>
<p><strong>Robustness.</strong> A system is robust to distributional shift if it continues to perform well on the objective function for which it was optimized even when off the training environment.<a href=""https://intelligence.org/learned-optimization#bibliography"">(29)</a> In the context of mesa-optimization, pseudo-alignment is a particular way in which a learned system can fail to be robust to distributional shift: in a new environment, a pseudo-aligned mesa-optimizer might still competently optimize for the mesa-objective but fail to be robust due to the difference between the base and mesa- objectives.</p>
<p>The particular type of robustness problem that mesa-optimization falls into is the reward-result gap, the gap between the reward for which the system was trained (the base objective) and the reward that can be reconstructed from it using inverse reinforcement learning (the behavioral objective).<a href=""https://intelligence.org/learned-optimization#bibliography"">(8)</a> In the context of mesa-optimization, pseudo-alignment leads to a reward-result gap because the system's behavior outside the training environment is determined by its mesa-objective, which in the case of pseudo-alignment is not aligned with the base objective.</p>
<p>It should be noted, however, that while inner alignment is a robustness problem, the occurrence of unintended mesa-optimization is not. If the base optimizer's objective is not a perfect measure of the human's goals, then preventing mesa-optimizers from arising at all might be the preferred outcome. In such a case, it might be desirable to create a system that is strongly optimized for the base objective within some limited domain without that system engaging in open-ended optimization in new environments.<a href=""https://intelligence.org/learned-optimization#bibliography"">(11)</a> One possible way to accomplish this might be to use strong optimization at the level of the base optimizer during training to prevent strong optimization at the level of the mesa-optimizer.<a href=""https://intelligence.org/learned-optimization#bibliography"">(11)</a></p>
<p><strong>Unidentifiability and goal ambiguity.</strong> As we noted in <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J"">the third post</a>, the problem of unidentifiability of objective functions in mesa-optimization is similar to the problem of unidentifiability in reward learning, the key issue being that it can be difficult to determine the “correct” objective function given only a sample of that objective's output on some training data.<a href=""https://intelligence.org/learned-optimization#bibliography"">(20)</a> We hypothesize that if the problem of unidentifiability can be resolved in the context of mesa-optimization, it will likely (at least to some extent) be through solutions that are similar to those of the unidentifiability problem in reward learning. An example of research that may be applicable to mesa-optimization in this way is Amin and Singh's<a href=""https://intelligence.org/learned-optimization#bibliography"">(20)</a> proposal for alleviating empirical unidentifiability in inverse reinforcement learning by adaptively sampling from a range of environments.</p>
<p>Furthermore, it has been noted in the inverse reinforcement learning literature that the reward function of an agent generally cannot be uniquely deduced from its behavior.<a href=""https://intelligence.org/learned-optimization#bibliography"">(30)</a> In this context, the inner alignment problem can be seen as an extension of the value learning problem. In the value learning problem, the problem is to have enough information about an agent's behavior to infer its utility function, whereas in the inner alignment problem, the problem is to test the learned algorithm's behavior enough to ensure that it has a certain objective function.</p>
<p><strong>Interpretability.</strong> The field of interpretability attempts to develop methods for making deep learning models more interpretable by humans. In the context of mesa-optimization, it would be beneficial to have a method for determining whether a system is performing some kind of optimization, what it is optimizing for, and/or what information it takes into account in that optimization. This would help us understand when a system might exhibit unintended behavior, as well as help us construct learning algorithms that create selection pressure against the development of potentially dangerous learned algorithms.</p>
<p><strong>Verification.</strong> The field of verification in machine learning attempts to develop algorithms that formally verify whether systems satisfy certain properties. In the context of mesa-optimization, it would be desirable to be able to check whether a learned algorithm is implementing potentially dangerous optimization.</p>
<p>Current verification algorithms are primarily used to verify properties defined on input-output relations, such as checking invariants of the output with respect to user-definable transformations of the inputs. A primary motivation for much of this research is the failure of robustness against adversarial examples in image recognition tasks. There are both white-box algorithms,<a href=""https://intelligence.org/learned-optimization#bibliography"">(31)</a> e.g. an SMT solver that in principle allows for verification of arbitrary propositions about activations in the network,<a href=""https://intelligence.org/learned-optimization#bibliography"">(32)</a> and black-box algorithms<a href=""https://intelligence.org/learned-optimization#bibliography"">(33)</a>. Applying such research to mesa-optimization, however, is hampered by the fact that we currently don't have a formal specification of optimization.</p>
<p><strong>Corrigibility.</strong> An AI system is <em>corrigible</em> if it tolerates or assists with its human programmers in correcting itself.<a href=""https://intelligence.org/learned-optimization#bibliography"">(25)</a> The current analysis of corrigibility has focused on how to define a utility function such that, if optimized by a rational agent, that agent would be corrigible. Our analysis suggests that even if such a corrigible objective function could be specified or learned, it is nontrivial to ensure that a system trained on that objective function would actually be corrigible. Even if the base objective function would be corrigible if optimized directly, the system may exhibit mesa-optimization, in which case the system's mesa-objective might not inherit the corrigibility of the base objective. This is somewhat analogous to the problem of utility-indifferent agents creating other agents that are not utility-indifferent.<a href=""https://intelligence.org/learned-optimization#bibliography"">(25)</a> In <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks"">the fourth post</a>, we suggest a notion related to corrigibility—corrigible alignment—which is applicable to mesa-optimizers. If work on corrigibility were able to find a way to reliably produce corrigibly aligned mesa-optimizers, it could significantly contribute to solving the inner alignment problem.</p>
<p><strong>Comprehensive AI Services (CAIS).</strong><a href=""https://intelligence.org/learned-optimization#bibliography"">(11)</a> CAIS is a descriptive model of the process by which superintelligent systems will be developed, together with prescriptive implications for the best mode of doing so. The CAIS model, consistent with our analysis, makes a clear distinction between learning (the base optimizer) and functionality (the learned algorithm). The CAIS model predicts, among other things, that more and more powerful general-purpose learners will be developed, which through a layered process will develop services with superintelligent capabilities. Services will develop services that will develop services, and so on. At the end of this “tree,” services for a specific final task are developed. Humans are involved throughout the various layers of this process so that they can have many points of leverage for developing the final service.</p>
<p>The higher-level services in this tree can be seen as meta-optimizers of the lower-level services. However, there is still the possibility of mesa-optimization—in particular, we identify two ways in which mesa-optimization could occur in the CAIS-model. First, a final service could develop a mesa-optimizer. This scenario would correspond closely to the examples we have discussed in this sequence: the base optimizer would be the next-to-final service in the chain, and the learned algorithm (the mesa-optimizer in this case), would be the final service (alternatively, we could also think of the entire chain from the first service to the next-to-final service as the base optimizer). Second, however, an intermediary service in the chain might also be a mesa-optimizer. In this case, this service would be an optimizer in <em>two respects:</em> it would be the meta-optimizer of the service below it (as it is by default in the CAIS model), but it would also be a mesa-optimizer with respect to the service above it.</p>
<p>&nbsp;</p>
<h2>Conclusion</h2>
<p>In this sequence, we have argued for the existence of two basic AI safety problems: the problem that mesa-optimizers may arise even when not desired (unintended mesa-optimization), and the problem that mesa-optimizers may not be aligned with the original system's objective (the inner alignment problem). However, our work is still only speculative. We are thus left with several possibilities:</p>
<ol>
<li>If mesa-optimizers are very unlikely to occur in advanced ML systems (and we do not develop them on purpose), then mesa-optimization and inner alignment are not concerns.</li>
<li>If mesa-optimizers are not only likely to occur but also difficult to prevent, then solving both inner alignment and outer alignment becomes critical for achieving confidence in highly capable AI systems.</li>
<li>If mesa-optimizers are likely to occur in future AI systems by default, and there turns out to be some way of preventing mesa-optimizers from arising, then instead of solving the inner alignment problem, it may be better to design systems to not produce a mesa-optimizer at all. Furthermore, in such a scenario, some parts of the outer alignment problem may not need to be solved either: if an AI system can be prevented from implementing any sort of optimization algorithm, then there may be more situations where it is safe for the system to be trained on an objective that is not perfectly aligned with the programmer's intentions. That is, if a learned algorithm is not an optimizer, it might not optimize the objective to such an extreme that it would cease to produce positive outcomes.</li>
</ol>
<p>Our uncertainty on this matter is a potentially significant hurdle to determining the best approaches to AI safety. If we do not know the relative difficulties of the inner alignment problem and the unintended optimization problem, then it is unclear how to adequately assess approaches that rely on solving one or both of these problems (such as Iterated Distillation and Amplification<a href=""https://intelligence.org/learned-optimization#bibliography"">(34)</a> or AI safety via debate<a href=""https://intelligence.org/learned-optimization#bibliography"">(35)</a>). We therefore suggest that it is both an important and timely task for future AI safety work to pin down the conditions under which the inner alignment problem and the unintended optimization problem are likely to occur as well as the techniques needed to solve them.</p>
<p>&nbsp;</p>
<p><a href=""https://intelligence.org/learned-optimization/#glossary"">Glossary</a> | <a href=""https://intelligence.org/learned-optimization/#bibliography"">Bibliography</a></p>
",evhub,evhub,evhub,
ZbNcxP2g3A2wZoKvv,Personal musings on Individualism and Empathy,personal-musings-on-individualism-and-empathy,https://www.lesswrong.com/posts/ZbNcxP2g3A2wZoKvv/personal-musings-on-individualism-and-empathy,2019-06-07T17:07:37.283Z,13,5,6,False,False,,"<html><head></head><body><p>One type of the more common accusations that people throw at me can be described like this: ""You're an egoist. You don't think about others."" To my teenage self this sounded amusing, and I would respond with a smug ""Why yes, I <em>am</em> an egoist, thank you very much!""</p>
<p>Here I have to make a confession: back then I was pretty much obsessed with Ayn Rand's Objectivism. For those who don't know much about her philosophy, it's a flawed attempt to create a consistent philosophical system that covers just about everything, from ontology to aesthetics. It has very stark individualistic, libertarian-ish vibes. Being a naive yet curious kid, I was enamored with what turned out to be my first expose to (relatively) rigorous argumentation, logic, and philosophy/critical thought in general. I can't thank Rand enough for that, because through her I encountered the concepts of reason and rationality, which in turn led me to LW, The Sequences and everything else which formed the basis of my mature thought.</p>
<p>Anyway, after a while I moved away from objectivist ideas. I clearly remember the ""final nail in the coffin"" moment. I was rereading <a href=""https://en.wikipedia.org/wiki/Objectivism:_The_Philosophy_of_Ayn_Rand"">THE SEMINAL WORK THAT SYSTEMATICALLY EXPLORES AND DEVELOPS THE IDEAS OF AYN RAND</a>, particularly the part about the consciousness axiom. I remember thinking: ""Wait, the assumptions that the argument lies on are completely unfounded, what the hell is this epistemology LOL"". That was it. No dramatic catharsis-like moment, no nothing, just the feeling of having finally checked off an item from my bucket list.</p>
<p>Despite my going through the rejection ritual, a lot of moral intuitions that I formed in that period carried over into my new, semi-adult life. I shunned social norms and laughed at those who had to conform. I prided myself on being socially brave, freely asking people for favors and being able to take a rejection and reject myself, giving compliments left and right whenever I felt like others deserved them, expressing myself authentically, without fear of judgement or public shaming.</p>
<p>Most of this was connected to the classical-liberal-ish notion of ""anything's OK as long as it's not initiation of physical force"" and its corollaries, coupled with intuitions like ""nobody owes nothing to no one, thus e.g. asking for favors is totally OK and even good (cuz self-interest), but only if you're able to take a 'no' for an answer; we all are free people after all"" and others from the same neg-lib-and-egoism cluster.</p>
<p>Plot twist. This Randian experience left my empathy in a stunted, primitive state. It's not on the levels of psychopathy/ASPD, I experience rich emotions, can empathize with others (both affectively and cognitively), my tracking-social-cues mechanism works fine, I'm able to enjoy fiction, etc. But my brain doesn't mark most significant personal information about others as salient, unless I'm explicitly told to keep it in mind. This means that generic social rules and explicit agreements work fine for me, but many implicit expectations are bound to fall short, because the necessary personal information about others' preferences just doesn't jump to my mind whenever I'm making a decision that can affect some other person that's relatively important to me. Of course it's more complex than this, so please remember that I'm painting in broad strokes here.</p>
<p>This fucking sucks. I do actually feel remorse every time I unintentionally hurt someone, it's pretty horrible, almost every time I wish it didn't happen. It's usually not an issue with acquaintances and friends, i.e. we're not close enough, so they end up getting hurt by me very rarely, if ever, but at some point if you want the relationship to progress, you have to be able to count on the other person to take on some of the responsibility for your well-being. That's how you develop trust, which is an important prerequisite for vulnerability and sustenance of long-term cooperative relationships. To be responsible, you first have to become aware.</p>
<p>Which is exactly where my bottleneck lies. I don't model others well enough, typical-minding aside. Or rather, for some reason I'm disinclined to do it. It feels like a burden. I want <em>others</em> to tell <em>me</em> what they want from me, explicitly and precisely. That's how I operate myself. To me it feels like taking personal responsibility for your needs, which is Good. Actively taking responsibility for others feels Wrong, because what the hell, why am I supposed to do something for <em>them</em>? No one owes me anything, but neither do I!</p>
<p>Now, I do exhibit caring behavior, sometimes, towards certain people. It feels good and authentic. That's because when I do it, I really value the person and <em>want</em> them to be well. One confounding factor could be that I simply don't like most people, they're not ""good enough for me"" in my eyes, so I (read as ""my brain"") don't care about them enough to model them sufficiently. But then again, maybe the reason <em>why</em> I don't like most people and find them shallow <em>is</em> my lack of empathy, so the causes and effects might be tangled up here.</p>
<p>Several broken relationships and lots of pain behind me, I'm dating someone again. I'm different now, having studied a lot of psychology, philosophy, basically everything human nature. I'm stronger. And, as it happens, this relationship is much healthier than others, it spurs a significant amount of personal growth, despite the short time span: patterns become obvious, small yet meaningful change takes place. I'm thankful to my SO. Even if our paths diverge at some point, I've gotten a lot of value out of our relationship, and I hope so did they!</p>
<h2>And now, dear readers, I present you with my Latest, Juiciest Insight into the psyche of Me, a random person on the Internet.</h2>
<p>First off, let's start with the belief/alief dichotomy. Here's my daring and frivolous interpretation: Beliefs are about meticulous calculations, S2 override, and social interfacing. <a href=""https://en.wikipedia.org/wiki/Alief_(mental_state)"">Aliefs</a> are what drives your S1 by default, affecting greatly your motivations and behavior. <a href=""https://en.wikipedia.org/wiki/Coherence_therapy"">It's important to be able to explicate aliefs</a>, because that gives you the opportunity to intentionally change them. They do change on their own, of course, but it happens more or less randomly, subject to the chance workings of environment, experience, and reflection. Lots of so-called internal conflicts -- procrastination, negative spirals, self-loathing, etc -- are the result of identifying too much with S2, the mind, and not enough of ""accepting"" S1, the body, as a part of your self-concept, or so I think anyway. Cartesian delusions are crappy at modeling reality, but sadly are pretty intuitive and thus quite widespread.<br>
But I digress.</p>
<p>Stripped of all ethical chaff, I unearthed an alief. It goes like this:</p>
<p><em>""Freedom from social constraints is good. If I let myself become sufficiently aware of others' preferences, this will turn me into an anxious self-conscious nervewrack and I'll lose all the benefits I'm reaping from my current attitude""</em></p>
<p>The concept of <a href=""https://thezvi.wordpress.com/2017/09/30/slack/"">slack</a> ties into this very well. For those who want a quick refresher, something something our behavior is partly managed by the expectations and implicit demands of other people. <a href=""https://www.lesswrong.com/posts/AqbWna2S85pFTsHH4/the-intelligent-social-web"">This post by Val</a> is probably relevant, too.<br>
I have lots of slack. What's more, I'm able to constrain others rather heavily, e.g. by openly signaling my preferences. I'm allowed to do this <em>precisely</em> because I'm less attuned to what others prefer. It's a strong, honest <a href=""https://en.wikipedia.org/wiki/Precommitment"">precommitment</a> mechanism: if you're not even consciously aware of the fact that you're thwarting someone else's preferences via action, you pass the internal-S2-self-policing-social check; after all, none of this was intentional! This lets you keep your self-esteem safe and sound, and thus lack of adequate punishment perpetuates this behavior further.</p>
<p>You could say that my S1 values my slack so much that it's afraid to even look into the minds of others, aside from barely scraping the surface. There's no going back after tasting the forbidden fruit. ""Yes, you're naked and ashamed, boo-hoo, go cry me a river, and fuck off from Heaven while you're at it"" -- God, probably.</p>
<p>Already noticed the mistake I'm making? No?</p>
<p>Well here it is:</p>
<h2><strong>Other-awareness and other-modeling are two different things.</strong></h2>
<p>If a person who has no <a href=""https://en.wikipedia.org/wiki/Theory_of_mind"">ToM</a> gets suddenly struck by Insight from Above and gains the ability to See the Other, they'll ebb-and-flow between <em>very</em> unpleasant states of panic and less-but-still unpleasant states of confusion and disorientation. They didn't have the privilege of gradually easing into social reality and slowly mastering aspects of peopling, and reality hit'em like a brick. Being aware of the enormous pile of demands and expectations without knowing exactly what they are must feel like, I don't know, doom.</p>
<p>However, it mustn't stay so. If this newly-baked Seer survives the initial shock, they'll start developing better and better mental models of other people and accumulating useful heuristics. The Art of Prediction will get honed and polished with a zealous striving for perfection.<br>
Now that the initial dip in effectiveness is persevered through, the Seer finds that their life has become much better, being able to subtly navigate between the streams of social information, manipulating these flows to their liking, enjoying the ever-increasing benefits of social aptitude, the sly beast!</p>
<p>Anyway, here it is. Only thing left is to feed experiential evidence into this new reframe, which requires crashing into reality, which can hardly be done by putting more words on the metaphorical paper.</p>
<p>So I guess I'll go out and do just that.</p>
</body></html>",dragohole,dragohole,dragohole,
SDr45pcgJJyvTqmZa,"For the past, in some ways only, we are moral degenerates",for-the-past-in-some-ways-only-we-are-moral-degenerates,https://www.lesswrong.com/posts/SDr45pcgJJyvTqmZa/for-the-past-in-some-ways-only-we-are-moral-degenerates,2019-06-07T15:57:10.962Z,32,12,17,False,False,,"<html><head></head><body><p>Have human values improved over the last few centuries? Or is it just that current human values are naturally closer to our (current) human values and so we think that there's been moral progress towards us?</p>
<p>If we project out in the future, the first scenario posits continuing increased moral improvements (as the ""improvement trend"" continues) and the second posits moral degeneration (as the values drift away from our own). So what is it?</p>
<p>I'll make the case that both trends are happening. We have a lot less slavery, racism, ethnic conflicts, and endorsements of slavery, racism, and ethnic conflicts. In an uneven way, poorer people have more effective rights than they did before, so it's somewhat less easy to abuse them.</p>
<p>Notice something interesting about the previous examples? They can all be summarised as ""some people who were treated badly are now treated better"". Many people throughout time would agree that these people are actually being treated better. On the issue of slavery, consider the following question:</p>
<ul>
<li>""If X would benefit from being a non-slave more than being a slave, and there were no costs to society, would it be better for X not to be a slave?""</li>
</ul>
<p>Almost everyone would agree to that throughout history, barring a few examples of <em>extremely</em> motivated reasoning. So most defences of slavery rest on the idea that some classes of people are better off as slaves (almost always a factual error, and generally motivated reasoning), or that some morally relevant group of people benefited from slavery enough to make it worthwhile.</p>
<p>So most clear examples of moral progress are giving benefits to people, such that anyone who knew all the facts would agree it was beneficial for those people.</p>
<p>That trend we might expect to continue; as we gain greater knowledge how to benefit people, and as we gain greater resources, we can expect more people to be benefited.</p>
<h1>Values that we have degenerated on</h1>
<p>But I'll argue that there are a second class of values that have less of a ""direction"" to, and where we could plausibly be argued to have ""degenerated"". And, hence, where we might expect our descendants to ""degenerate"" more (ie move further away from us).</p>
<p>Community and extended family values, for example, are areas where much of the past would be horrified by the present. Why are people not (generally) meeting up with their second cousins every two weeks, and why do people waste time gossiping about irrelevant celebrities rather than friends and neighbours?</p>
<p>On issues of honour and reputation, why have we so meekly accepted to become citizens of administrative bureaucracies and defer to laws and courts, rather than taking pride in meeting out our own justice and defending our own honour? ""Yes, yes"", the hypothetical past person would say, ""your current system is fairer and more efficient; but why did it have to turn you all so supine""? Are you not <a href=""https://www.youtube.com/watch?v=M7Cs-VIDKSY"">free men</a>?</p>
<p>Play around with vaguely opposite virtues: spontaneity versus responsibility; rationality versus romanticism; pride versus humility; honesty versus tact, and so on. Where is the <a href=""https://en.wikipedia.org/wiki/Golden_mean_(philosophy)"">ideal mean</a> between any of those two extremes? Different people and different cultures put the ideal mean in different places, and there's no reason to suspect that the means are ""getting better"" rather than just ""moving around randomly"".</p>
<p>I won't belabour the point; it just seems to me that there are areas where the moral progress narrative makes more sense (giving clear benefits to people who didn't have them) and areas where the ""values drift around"" narrative makes more sense. And hence we might hope for continuing moral progress in some areas, and degeneration (or at least stagnation) in others.</p>
</body></html>",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
5zeg5szCDv72LNbDW,Map of (old) MIRI's Research Agendas,map-of-old-miri-s-research-agendas,https://www.lesswrong.com/posts/5zeg5szCDv72LNbDW/map-of-old-miri-s-research-agendas,2019-06-07T07:22:42.002Z,9,5,1,False,False,,"<p>In late 2016 I independently curated a visual map of the areas of research MIRI was working on at the time, together with descriptions of each area and arrows explaining the relationships between areas.</p><p>While MIRI&#x27;s research <a href=""https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh"">has evolved since</a>, I have foud myself going back to this map as a reference while doing research, so I though I would share it with the rest of you in case it might be useful for some.</p><p><a href=""https://prezi.com/3_f6qdikq3oy/research-areas-in-al-alignment/"">Map of Research Areas in AI Alignment</a></p><p>The map is based on the <a href=""https://intelligence.org/files/TechnicalAgenda.pdf"">Agent Foundations agenda by Soares and Fallenstein of 2014</a> and the <a href=""https://intelligence.org/files/AlignmentMachineLearning.pdf"">Alignment for Advanced ML Systems agenda by Taylor et al in 2016</a>.</p><p>Let me know if this was useful for you! It will be useful evidence on whether spending time curating more maps of the same style is a good use of my time.</p><br/>",Jsevillamol,jsevillamol,Jsevillamol,
A4DkMYgiDCmRMtKxr,Fractional Reserve Charity,fractional-reserve-charity,https://www.lesswrong.com/posts/A4DkMYgiDCmRMtKxr/fractional-reserve-charity,2019-06-07T05:14:28.688Z,34,10,4,False,False,,"<p>On the <a href=""https://discord.gg/Zj2vW89"">EA Corner discord server</a>, various participants, most notably @wolframhead,  tossed around the following idea: Some people of an EA bent may want to donate vast amounts of money, huge percentages of high salaries, but be held back in case they turn out to need that money as savings later.  A health crisis, their job market failing them, or some other unforeseen but expensive eventuality could make them regret prior profligate giving once they need a runway for a few months with no paychecks, run into a pay cut, or have to contend with huge surprise bills.  But of course not all of these people will actually experience such a crisis even though all of them run some risk of doing so.  If there was a way for them to get their money back from the charity they donated to conditional on an emergency cropping up, the charity would be able to keep most such conditional funds even not taking into account the possibility that they could make interest on the capital in the time between donation and refund.  Overall charity revenue would probably go up if this &quot;fractional reserve&quot; model were feasible.  Here&#x27;s a writeup of my understanding of the topic; the EA Corner has open membership for anyone who wants to go <a href=""https://discordapp.com/channels/494679634256855062/497158159907946496/581692121652789248"">view the conversation in its original form</a>.</p><p>1. Possible Structures</p><p>A - Charities could directly provide this service.  This would require them to deal with the overhead of assessing emergency needs in their donors, which is probably not their core competency.  The charity would operate under uncertainty about not only its future budget but also its current budget, as the rug could be pulled out from under them on sizable donations at any time.  It&#x27;d also probably affect how EA money flowed to charities that managed this versus ones that did not, as a charity that has a refund possibility is &quot;cheaper&quot; to donate to than one that does not.  The concept of &quot;room for more funding&quot; could possibly limit the strangling effect on charities that didn&#x27;t participate to a point, but only to a point, and making lots of charities spend on overhead and uncertainty-tolerant budget forecasts just to remain competitive donation targets seems like a deadweight loss.</p><p>B - A middleman organization could form (or it could grow out of an existing meta-charity like Givewell).  Such an organization would mostly just pass on donations to the intended recipients - you&#x27;d send them a thousand dollars marked for AMF, and AMF would get most of those dollars to spend on AMF things with no worries about paying it back.  Some of what passed through the middleman organization would float as liquid assets and be kicked back to donors who needed it, the way a bank doesn&#x27;t keep enough cash on hand for everyone to withdraw their savings all at once but can handle a normal rate of withdrawals without a blip.  There would be some loss in the form of overhead for this organization, but plausibly less than the gain in effective donations generated by the insurance scheme.</p><p>C - A tontine[1]-like structure with less formal scaffolding could handle it.  Groups of EAs who know and trust each other - and ideally have comparable budgets - could all pool donations of the same size, keep back that amount or perhaps twice that amount held by someone responsible in the group, and kick that amount back to whoever needs it first as assessed by other group members.  A person with more money to throw around could belong to several of these.  Relying on interpersonal rather than institutional honesty would reduce access to the model for people who are less socially in with other EAs and might make it harder to solidly verify people&#x27;s emergency conditions by strict rubrics.</p><p>2. Probable Obstacles</p><p>A - Assorted regulatory complexity.  Are any of the above structures legal?  How will they interact with taxes, with nonprofit status, with whatever laws govern insurance even though (since in the vision I describe above payouts never exceed pay-ins) it&#x27;s technically not insurance?  Does it look like a tax dodge (<em>is</em> it a tax dodge)?  Is it sort of like a donor-advised fund?  What are the rules about those and which apply?  How would you explain any of this to an accountant or an auditor without sounding insane?</p><p>B - Adverse selection.  While anyone can worry about a trip to the hospital or a sudden firing, people who are <em>more</em> worried about these things might have more real reason to be worried, carrying elevated risk; expectations about payback rates based on baseline statistics about how common these things are may not be reliable, and a middleman that didn&#x27;t account enough for that could wind up without enough cash on hand to return money to all its distressed donors, or a charity which provided the payback service in-house could try to budget its operations based on a guess that turns out too optimistic.  Insurance companies have some institutional knowhow about how to handle this but it would increase overhead a lot.</p><p>C - Theft and fraud.  While people presumably trust the charities they donate to, a donation the return of which could be debated based on assessed need would present an incentive for whoever held the bag to assess the need unsympathetically.  A middleman organization or a private individual holding money for a tontine-style structure would face temptation to run off with the cash, too, and it might not be obvious this was happening right away if everyone who donated through them happened to be lucky and not expect the money back for an extended period of time.  In the opposite direction, people might fake serious catastrophes to get their money back even if their use case for the money was not intended to be one that got a payback - this wouldn&#x27;t be as appealing as normal insurance fraud, since you wouldn&#x27;t get back more money than you put in in the first place, but it could get to be a problem if (say) a deceased donor&#x27;s beneficiaries wanted to puff up their inheritances, or a third party believed they could intercept the money by posing as a donor in distress.</p><p>D - Bikeshedding and competition.  If people have a lot of small differences of opinion about how this should work, that could support a large number of tontine-style setups, and different charities handling it in-house could &quot;cheapen&quot; their donations to various audiences of assorted inclinations, leaving each donor to decide if they prefer to deworm children under scheme 1 or net beds under scheme 2.  But it&#x27;d sink a middleman organization to have only a handful of participants and be in competition with a lot of similar orgs, because it would run entirely on distribution of risk, and that requires a high population.  Regular insurance companies have such a large pool of potential customers that they can manage splitting the pie; I don&#x27;t know if the market for refundable donations is that big or could easily get that way (it&#x27;s a weird idea that not even all preexisting EAs might want, for one thing).</p><p>3. Questions</p><p>A - Does something like this already exist?  I would likely not know about it if it does (I don&#x27;t handle my household finances or charity budget) but nobody else on the EA Corner mentioned &quot;oh, you&#x27;re talking about thus-and-such a program!&quot;, so perhaps it doesn&#x27;t - or maybe there&#x27;s a fledgeling one, or a poorly advertised sub-product belonging to some financial firm.  Or maybe there&#x27;s a way to squish something from its original shape into this shape without inventing new practices altogether.</p><p>B - Would this see enough use to benefit from risk pooling and to increase effective charity revenue?  EAs move a fair amount of money already.  While it would be <em>nice</em> if they endured less risk in so doing, the really high leverage of setting up fractional reserve charity options would be if it increased donations relative to the counterfactual.  I don&#x27;t have a sense of how large this space really is.</p><p>C - Are EA emergencies too correlated, e.g. are too many of us in a position to suffer from collapse in the tech sector for any EA techies to consider this employment insurance lite?  We probably don&#x27;t have correlated medical problems unless someone torches an EA Global, but if a bubble a lot of EAs work in popped, and every displaced software engineer or quant were each of them expecting five plus figures returned from their charities, that would capsize anything inadequately diversified in its path.  Is there much use to a version that only refunds in case of medical emergency?</p><p>--</p><p>[1]I learned the word &quot;tontine&quot; in a form different from the apparently central use case; the meaning I intend is a group savings/loan plan where (as an example of one way it can be structured) every one of N people in a group pays X per month into a shared pool, and every month a different member of the group gets N*X in a lump sum until N months have gone by.</p>",Alicorn,alicorn,Alicorn,
gD2rkXJxTcTKFiiJG,Visiting the Bay Area from 17-30 June,visiting-the-bay-area-from-17-30-june,https://www.lesswrong.com/posts/gD2rkXJxTcTKFiiJG/visiting-the-bay-area-from-17-30-june,2019-06-07T02:40:03.668Z,16,4,2,False,False,,"<p>I&#x27;ll be in the Bay Area from the 17-30 June for training as a TripleByte Interviewer. My interests include:</p><ul><li>Effective altruism - particularly movement building</li><li>Agent foundations - particularly updateless decision theory and anthropics</li><li>Infinite ethics</li><li>More broadly, the boundary of philosophy of maths, but also philosophy in general</li></ul><p>PM me if you want to meet up or if you have any recommendations.</p><p>Update: I&#x27;ll be training during weekdays, so I&#x27;ll only be free during evenings and weekends</p><br/>",Chris_Leong,chris_leong,Chris_Leong,
muKEBrHhETwN6vp8J,Arbital scrape,arbital-scrape,https://www.lesswrong.com/posts/muKEBrHhETwN6vp8J/arbital-scrape,2019-06-06T23:11:17.943Z,90,32,23,False,False,,"<p>Update: <a href=""https://www.lesswrong.com/posts/8pSh54GoJJqfHdndk/arbital-scrape-v2"">Arbital Scrape V2</a></p><p>I&#x27;ve scraped <a href=""http://arbital.com"">http://arbital.com</a> as the site is unusably slow and hard to search for me.</p><p>The scrape is locally browsable and plain HTML save for MathJax.</p><p><a href=""https://drive.google.com/open?id=1b7dKhOzfMpFwngAeI8efeOzv147Lv_mx"">https://drive.google.com/open?id=1b7dKhOzfMpFwngAeI8efeOzv147Lv_mx</a></p><p><a href=""https://emma-borhanian.github.io/arbital-scrape/"">https://emma-borhanian.github.io/arbital-scrape/</a></p><p>If there&#x27;s interest let me know as I may tidy up and open source my code. edit: working on this</p><span><figure><img src=""https://i.imgur.com/kNMI9GJ.png"" class=""draft-image "" style=""width:100%"" /></figure></span><p>Mirror: <a href=""https://www.obormot.net/arbital"">www.obormot.net/arbital</a> </p>",emmab,emmab,emmab,
7Fft9kNTvqixttGc3,Circle Games,circle-games,https://www.lesswrong.com/posts/7Fft9kNTvqixttGc3/circle-games,2019-06-06T16:40:00.596Z,85,32,15,False,False,,"<p>I may be reinventing a known thing in child development or psychology here, but bear with me.</p>
<p>The simplest games I see babies play — games simple enough that cats and dogs can play them too — are what I’d call “circle games.”</p>
<p>Think of the game of “fetch”.  I throw the ball, Rover runs and brings it back, and then we repeat, ad infinitum.  (Or, the baby version: baby throws the item out of the stroller, I pick it up, and then we repeat.)</p>
<p>Or, “peek-a-boo.” I hide, I re-emerge, baby laughs, repeat.</p>
<p>My son is also fond of “open the door, close the door, repeat”, or “open the drawer, close the drawer, repeat”, which are solo circle games, and “together/apart”, where he pushes my hands together and apart and repeats, and of course being picked up and put down repeatedly.</p>
<p>A lot of toys are effectively solo circle games in physical form.  The jack-in-the-box: “push a button, out pops something! close the box, start again.” Fidget toys with buttons and switches to flip: “push the button, get a satisfying click, repeat.”</p>
<p>It’s obvious, observing a small child, that the purpose of these “games” is learning.  And, in particular, learning <em>cause and effect</em>.  What do you learn by opening and closing a door? Why, how to open and close doors; or, phrased a different way, “when I pull the door this way, it opens; when I push it that way, it closes.” Playing fetch or catch teaches you about how objects move when dropped or thrown.  Playing with button-pushing or latch-turning toys teaches you how to handle the buttons, keys, switches, and handles that are ubiquitous in our built environment.</p>
<p>But what about peek-a-boo? What are you “learning” from that? (It’s a myth that babies enjoy it because they don’t have object permanence; babies get object permanence by 3 months, but enjoy peek-a-boo long after that.) My guess is that peek-a-boo trains something like “when I make eye contact I get smiles and positive attention” or “grownups go away and then come back and are happy to see me.” It’s <em>social </em>learning.</p>
<p>It’s important for children to learn, generally, “when I act, the people around me react.” This gives them social <em>efficacy</em> (“I can achieve goals through interaction with other people”), access to social <em>incentives</em> (“people respond positively when I do this, and negativey when I do that”), and a sense of social <em>significance </em>(“people care enough about me to respond to my actions.”) Attachment psychology argues that when babies and toddlers <em>don’t </em>have any adults around who respond to their behavior, their social development goes awry — neglected children can be extremely fearful, aggressive, or checked-out, missing basic abilities in interacting positively with others.</p>
<p>It’s clear just from observation that the social game of <em>interaction</em> — “I make a sound, you make a sound back” — is learned before verbal speech.  Preverbal babies can even execute quite sophisticated interaction patterns, like making the <em>tonal </em>pattern of a question followed by an answering statement.  This too is a circle game.</p>
<p>The baby’s fascination with circle games completely belies the popular notion that drill is an intrinsically unpleasant way to learn. Repetition <em>isn’t </em>boring to babies who are in the process of mastering a skill. They <em>beg </em>for repetition.</p>
<p>My personal speculation is that the “craving for drill”, especially in motor learning, is a basal ganglia thing; witness how abnormalities in the ganglia are associated with disorders like OCD and Tourette’s, which involve compulsive repetition of motor activities; or how some dopaminergic drugs given to Parkinsonian patients cause compulsions to do motor activities like lining up small objects or hand-crafts. Introspectively, a “gear can engage” if I get sufficiently fascinated with something and I’ll crave repetition — e.g. craving to listen to a song on repeat until I’ve memorized it, craving to get the hang of a particular tricky measure on the piano — but there’s no guarantee that the gear will engage just because I observe that it would be a good idea to master a particular skill.</p>
<p>I also think that some kinds of social interaction among adults are effectively circle games.</p>
<p>Argument or fighting, in its simplest form, is a circle game: “I say Yes, you say No, repeat!” Of course, sophisticated arguments go beyond this; each player’s “turn” should contribute new information to a logical structure. But many arguments in practice are not much more sophisticated than “Yes, No, repeat (with variations).”  And even intellectually rigorous and civil arguments usually share the basic turn-taking adversarial structure.</p>
<p>Now, if the purpose of circle games is to <em>learn a cause-and-effect relationship</em>, what are we learning from adversarial games?</p>
<p>Keep in mind that adversarial play — “you try to do a thing, I try to stop you” — kicks in very early and (I think) cross-culturally. It certainly exists across species; puppies do it.</p>
<p>Almost tautologically, adversarial play teaches <em>resistance</em>.  When you push on others, others push back; when others push on you, you push back.</p>
<p>War, in the sense we know it today, may not be a human universal, and certainly isn’t a mammalian universal; but <em>resistance </em>seems to be an inherent feature of social interaction between any beings whose interests are imperfectly aligned.</p>
<p>A lot of social behaviors generally considered maladaptive look like adversarial circle games. Getting sucked into repetitive arguments? That’s a circle game. Falling into romantic patterns like “you want to get closer to me, I pull away, repeat”? Circle game.  Being shocking or reckless to get attention? Circle game.</p>
<p>The frame where <em>circle games are for learning </em>suggests that people do these things <em>because they feel like they need more practice learning the lesson</em>.  Maybe people who are very combative feel, on some level, that they need to “get the hang of” pushing back against social resistance, or conversely, learning how not to do things that people will react badly to.  It’s unsatisfying to feel like a ghost, moving through the world but not getting any feedback one way or another. Maybe when people crave interaction, they’re literally craving training data.</p>
<p>If you always do A, and always get response B, and you keep wanting to repeat that game, for much longer than is “normal”, then a couple things might be happening:</p>
<ul>
<li>Your “learning algorithm” has an unusually slow “learning rate” such that you just don’t update very efficiently on what ought to be ample data (in general or in this specific context).</li>
<li>You place a very high importance on the A-B relationship such that you have an unusually high need to be <em>sure </em>of it.  (e.g. your algorithm has a very high threshold for convergence.) So even though you learn as well as anybody else, you want to <em>keep </em>learning for longer.</li>
<li>You have a very strong “prior” that A does <em>not </em>cause B, which it takes a lot of data to “disprove.”</li>
<li>You have something like “too low a rate of stochasticity.”  What you actually need is <em>variation</em> — you need to see that A’ causes B’ — but you’re stuck in a local rut where you can’t explore the space properly so you just keep swinging back and forth in that rut. But your algorithm keeps returning “not mastered yet”. (You can get these effects in algorithms as simple as Newton’s Method.)</li>
<li>You’re not actually trying to learn “A causes B.” You’re trying to learn “C causes D.” But A correlates weakly with C, and B correlates weakly with D, and you don’t know how to specifically do C, so you just do A a lot and get intermittent reinforcement.</li>
</ul>
<p>These seem like more general explanations of how to break down when repetition will seem “boring” vs. “fascinating” to different people or in different contexts.</p>",sarahconstantin,sarahconstantin,sarahconstantin,
cXPkJPkQd2qvTSwdB,Impact of aging research besides LEV,impact-of-aging-research-besides-lev,https://www.lesswrong.com/posts/cXPkJPkQd2qvTSwdB/impact-of-aging-research-besides-lev,2019-06-06T10:42:14.703Z,15,5,0,False,False,,"<p>This is the third post of a series in which I&#x27;m trying to build a framework to evaluate aging research. Previous posts:</p><p>1. <a href=""https://forum.effectivealtruism.org/posts/jYMdWskbrTWFXG6dH/a-general-framework-for-evaluating-aging-research-part-1"">A general framework for evaluating aging research. Part 1: reasoning with Longevity Escape Velocity</a></p><p>2. <a href=""https://forum.effectivealtruism.org/posts/uR4mEzMR7fiQzb2c7/aging-research-and-population-ethics"">Aging research and population ethics</a></p><h2>Summary</h2><p>Age-related disease and decline account for, more or less, 30% of total DALYs in the <a href=""https://vizhub.healthdata.org/gbd-compare/"">2017 Global Disease Report</a> made by the World Health Organisation, and the figure is rising sharply each year. Aside from DALYs lost, there&#x27;s evidence that age-related disease and disability negatively impact life satisfaction. Better healthcare for age-related diseases would result in social and economic benefits, such as an increased global workforce, a decrease of social burden due to age-related diseases, a decrease in individual healthcare expenditure, and less strain on the social security systems, pension systems, and healthcare systems of the world&#x27;s governments. Currently, all these variables are worsening due to an aging global population. Examples of other, more subtle, sources of impact could be an increased concern for the long-term future along with more preservation of knowledge, which could result in new breakthroughs and improved day-to-day human activities.</p><p>Aging research would also impact pets in similar ways to humans, as similar considerations about LEV and DALYs averted apply.</p><h2>DALYs averted</h2><p>The last <a href=""https://www.who.int/healthinfo/global_burden_disease/about/en/"">Global Burden of Disease</a> report from the World Health Organisation was made <a href=""http://www.healthdata.org/gbd/gbd-2017-resources"">in 2017</a>. It released a handy <a href=""https://vizhub.healthdata.org/gbd-compare/"">data visualisation tool</a>, from which we can extract the fraction of DALYs attributable to age-related diseases and decline and their annual percent change from 1990 to 2017. If you sum the percentages, also trying to account for the fraction of age-related ailments not necessarily caused by aging (although this could be unnecessary, since even if some of these diseases appear also in the young, they share the same underlying characteristics), the percentage of total DALYs you get is more or less 30%. Some age-related diseases and ailments accounting for a large fraction of DALYs are: Ischemic heart disease: 6.83%, Stroke: 5.29%, Diabetes Mellitus: 2.71%, Back pain: 2.59%, Lung cancer: 1.64%, Hearing Loss: 1.36%, Alzheimer&#x27;s: 1.22%, Blindness: 0.71%</p><span><figure><img src=""https://i.imgur.com/T0C9io4.jpg"" class=""draft-image "" style=""width:40%"" /></figure></span><p>The weighted average of annual percent change is positive and sharp, as expected from the aging of the world population and a better control of communicable diseases worldwide. The percent annual change for some of the most impactful diseases: Diabetes Mellitus: +1.58% Osteoarthritis: +1.53%, Alzheimer&#x27;s +1.55%, Hearing Loss: +0.9%, Blindness: +0.81%, Lung cancer: +0.48%, liver cancer: +0.37%, colorectal cancer: +0.56%, breast cancer: +0.67%. Most impactful age-related diseases that presented negative annual percent change: Stomach cancer: -1.29%, Stroke: -0.22%.</p><p>It&#x27;s also interesting to point out that age-related diseases are the <a href=""https://ourworldindata.org/causes-of-death"">leading cause of death</a>in the world, with cardiovascular diseases and cancers alone causing almost 50% of total deaths. Even if the population is restricted to only 15-49 year olds, these two diseases remain the leading cause of death, and they share the same characteristics.</p><p><a href=""https://forum.effectivealtruism.org/users/sarahc"">SarahC</a>, the founder of <a href=""https://thelri.org/"">The Longevity Research Institute</a>, wrote <a href=""https://forum.effectivealtruism.org/posts/JsL2kPWJYRxn9rCWR/cost-effectiveness-of-aging-research"">an overview</a> of the potential cost-effectiveness of the cause area, using DALYs as a measure of impact. She also wrote a more in-depth <a href=""https://hourglass.bio/blog/2018/9/27/impact-of-age-related-disease"">article</a> than this section, with all the relevant statistics about DALYs and deaths caused by aging using informational graphs and pictures.</p><h2>Impact on life satisfaction</h2><p>Aside from DALYs lost, there&#x27;s evidence that age-related disease and disability negatively impact life satisfaction. Sarah Constantin lays out the evidence in the second part of her<a href=""https://hourglass.bio/blog/2018/9/27/impact-of-age-related-disease""> article</a> about the impact of age-related diseases.</p><p>While people tend to weigh physical disability as less impactful on happiness than mental illness, the data seems to roughly suggest that more severe DALY burden and mortality goes along with less happiness. In fact, people entering severe disability suffer a <a href=""https://ourworldindata.org/happiness-and-life-satisfaction#correlates-determinants-and-consequences"">sustained loss in life satisfaction</a>. According to the Gallup World Poll, health is positively correlated (r=0.25, p&lt;0.01) with life satisfaction across countries, but it’s not the largest effect; social support and GDP have larger correlations. According to the World Values Survey, self-reported &quot;excellent health&quot; had the second-largest effect size on life satisfaction, and according to a <a href=""http://www.pewglobal.org/2014/10/30/people-in-emerging-markets-catch-up-to-advanced-economies-in-life-satisfaction/"">43-country Pew Research poll</a>, people in emerging markets were most likely (68%) to rate as &quot;very important&quot; good health among various options, such as good education (65%) for children, safety from crime (64%), and owning a home (63%). According to a <a href=""https://news.gallup.com/poll/7504/family-health-most-important-aspects-life.aspx"">Gallup poll</a>, Americans rate family and health as the most important aspects in life, more than work, friends, money, religion, leisure, hobbies, and communities.</p><h2>The longevity dividend</h2><p>The term &quot;longevity dividend&quot; is often used to refer to the health, social, and economic benefits that result from delayed aging. Here, I will use it to refer to the social and economic benefits.</p><p>Let&#x27;s go through some obvious ones: people would be able to contribute to the economy for longer and would be less of a burden on it. Individual healthcare expenditures would fall, since half of lifetime expenditure on healthcare is made <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1361028/"">in the last ten years of life</a>.</p><p>Some societal issues, which would be ameloriated by better treatment of the diseases and disabilities of aging, are currently worsening due to an increasingly aged world population. This trend is due to decreasing mortality due to infectious diseases and other causes of death along with <a href=""https://ourworldindata.org/fertility-rate"">a fertility rate decreasing hand-in-hand with improved standards of living all over the world</a>.</p><span><figure><img src=""https://ourworldindata.org/uploads/2014/02/Years-it-took-Fertility-to-fall-from-6-to-below-3.png"" class=""draft-image "" style=""width:40%"" /></figure></span><p>Current geriatric medicine isn&#x27;t able to address these problems. On the contrary, it <a href=""https://ourworldindata.org/life-expectancy"">worsens them</a>, since it lets people live longer in ill health. The goal and the direction of modern biogerontology, instead, is to prolong life in good health. In fact, this analysis is about the impact of advancing aging research and not increasing geriatric healthcare funding.</p><span><figure><img src=""https://i.imgur.com/DI9KrEr.png"" class=""draft-image "" style=""width:40%"" /></figure></span><p>According to the <a href=""https://esa.un.org/unpd/wpp/Publications/Files/WPP2015_DataBooklet.pdf"">United Nations Population Prospects</a>:</p><blockquote>The global population is ageing as fertility declines and life expectancy increases. In 2015, 12 per cent of the global population, or 901 million people, were aged 60 or over. The number of older persons is growing at an annual rate of 3.3 per cent, faster than any other age group. Due to a projected overall reduction in fertility, population ageing will continue at high levels globally, and by 2050, 22 per cent of the total population, or 2.1 billion persons, will be aged 60 or over. Currently, Europe has the highest percentage of population aged 60 or over (24 per cent), but rapid ageing will occur in other parts of the world as well. All major areas of the world, except for Africa, will have nearly a quarter or more of their populations aged 60 or over by 2050. Countries need to anticipate and plan for population ageing and ensure the well-being of older persons with regard to the protection of their human rights, their economic security, access to appropriate health services, and formal and informal support networks.</blockquote><p>The trend is proving true all over the world, less developed nations included.</p><span><figure><img src=""https://static1.squarespace.com/static/5b9a91d03c3a5302c6db77f2/t/5bad55539140b7e60bf2391f/1538086236178/demographics.png?format=750w"" class=""draft-image "" style=""width:40%"" /></figure></span><p>Less-developed nations currently don&#x27;t have the economic means to face the burden of an aging population and are <a href=""https://blogs.worldbank.org/africacan/is-africa-aging"">already facing increased mortality form age-related diseases</a>. This poses a very serious problem for these nations&#x27; healthcare systems and social security systems.</p><p>In <a href=""https://globalriskinsights.com/2017/02/aging-global-population-economic-implications/"">his article</a> on Global Risks Insight, professor of Finance and Economics Antonio Guarino sums up the problems that are increasingly arising in social security, pensions and healthcare due to an aging population.</p><p>Security system and pensions:</p><blockquote>One key economic implication of an aging population is the strain on social insurance programs and pension systems.  With a large increase in an aging population, many nations must raise their budget allocations for social security.  For example, India’s social security system presently covers only 10 percent of its working-age populace, but its system is operating at a deficit with more funds exiting than entering.  In the United States, projections state that the level of social security contributions will start to fall short of legislated benefits this year.</blockquote><blockquote>In other words, the amount of money coming into social security will lessen due to fewer contributions from workers and more funds going to an aging retired population.  In Europe, in order to fund their social security system, 24 nations have payroll tax rates equaling or exceeding 20 percent of wages.  The situation is also precarious for pensions.  As the global population for elderly and retired workers increases, pensions must provide more income to these recipients so that they can enjoy at least a reasonable standard of living.</blockquote><blockquote>The problem for pensions is the declining number of younger workers thus resulting in lower funds being contributed and necessitating a higher return for their investments.  The problem is compounded when public pension plans in certain nations actually encourage workers to retire early, thus making retiree payouts more expensive than ever before.</blockquote><p>Health-care system:</p><blockquote>Another key economic implication of an aging population is the increase in health care costs.  As the population ages, health generally declines with more medical attention required such as doctor visits, surgery, physical therapy, hospital stays, and prescription medicine.</blockquote><blockquote>There are also increased cases of cancer, Alzheimer’s, and cardiovascular problems.  All of this costs money and will increase not only due to rising demand by an aging populace, but also because of inflation.  For example, in the United States, it is projected that public health expenditures will rise from 6.7 percent of GDP in 2010 to 14.9 percent in the year 2050.   This increase in health care costs will mean that nations must put more funds and human resources into providing health care while also attending to the needs of other segments of their people.</blockquote><blockquote>With an increasing aged population, there will also be shortages of skilled labor trained to care for aged patients.  It is projected that the registered nurse workforce in the United States will see a decline of nearly 20 percent by 2020 which is below projected requirements.</blockquote><p>He concludes with the problem arising form a declining global workforce:</p><blockquote>With an aging global population, economic growth will also be impacted.  Most importantly, there will ultimately be less workers available for firms to make products and provide services.  This shrinking labor force will mean that fewer workers must support greater numbers of retirees since they must pay taxes for social security, health care programs, and public pension benefits.  The global workforce will shrink causing policy and economic concerns.  For example, India is expected to see a 46 percent increase in its working-age population over the next quarter century, however, there will be markedly slower economic growth of only 9 percent over the following 20 years.  In order for India to become a major player in the world economy, it must have a growing and vibrant workforce.</blockquote><blockquote>Nations, such as China and Mexico, are expected to witness declines in their workforce from the year 2030 to 2050.  Other large economies such as Japan are projected to see a 19 percent decrease in their worker population within the next 25 years followed by a 24 percent decrease over the next 20 years.  Europe is also expected to see declining numbers in its workforce which will impact their chance to have a growing, competitive economy.</blockquote><blockquote>This decline in the global workforce will lead to an increase in the age dependency ratio which is the ratio of working-age to old-age individuals.  Globally, the dependency ratio in 1970 was 10 workers for each individual over age 64, but the expected ratio in 2050 is four workers for each person over 64.</blockquote><p>Additional sources about the topic of this article include <a href=""https://ourworldindata.org/financing-healthcare"">Statistics on healthcare expenditure organised by Our World in Data</a>, <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1361028/"">The Lifetime Distribution of Healthcare Costs</a>, and <a href=""http://www.riverpath.com/library/pdf/HEALTH%20AND%20WEALTH%20RPA%20FEB00.PDF"">The Health and Wealth of Nations</a>, a landmark paper about the positive cause-effect relationship between the health of a nation and its economy.</p><p>Therapies targeting aging could ameliorate all of these problems by providing true technological solutions to aging as opposed to prolonging ill health without treating its causes. In the long run, the best-case scenario could be the entire population being healthy enough to contribute to the economy, with healthcare expenditures reduced only to treatments that directly cure ill health (the elimination of geriatric medicine) with prices being reduced over time; a pension system would also be unnecessary.</p><p>These benefits are the most apparent ones. There could be subtler ones not immediately obvious from current trends or data or which are just guesses for which conclusive data is currently lacking. Examples (see <a href=""https://forum.effectivealtruism.org/posts/AHajQh7NcJbqZFZh9/impact-of-aging-research-besides-lev#rsAmzdhTrhqptFX5v"">this comment</a> for more):</p><ul><li>It&#x27;s possible that longer lived people would care about longer term issues, like climate change and existential risk.</li><li>Longer lifespans could mean more personal experience and knowledge being preserved. This could help to make discoveries otherwise not accessible only with knowledge and insight gained in a normal human lifespan, but could also help every other human activity, like parenting.</li></ul><h2>Impact on non-human animals</h2><p>All of the high-level considerations about LEV and averting DALYs written in this post and in the previous ones also apply to pets. It&#x27;s also possible that effective rejuvenating therapies will arrive for certain kinds of pets before they arrive for humans.</p><p>I expect impact to be very large on this category, possibly rivaling the one on humans. Even if a small minority of pet owners will be able to afford therapies for their pets at first, the fraction would likely increase as prices fall. It&#x27;s also worthy to note that the same proof I laid out in my <a href=""https://forum.effectivealtruism.org/posts/jYMdWskbrTWFXG6dH/a-general-framework-for-evaluating-aging-research-part-1"">previous post</a> applies: how a technology spreads and how much time it takes to reach its maximum share of usage doesn&#x27;t impact cost-effectiveness evaluations if the source of impact is hastening the arrival of that technology.</p><p>----------------------------------------------------------------------------</p><p>Crosspost from the <a href=""https://forum.effectivealtruism.org/posts/AHajQh7NcJbqZFZh9/impact-of-aging-research-besides-lev"">EA Forum</a></p>",emanuele-ascani,emanuele-ascani,emanuele ascani,
ZmvJ6kJ4ADcHcypYJ,"[AN #57] Why we should focus on robustness in AI safety, and the analogous problems in programming",an-57-why-we-should-focus-on-robustness-in-ai-safety-and-the,https://www.lesswrong.com/posts/ZmvJ6kJ4ADcHcypYJ/an-57-why-we-should-focus-on-robustness-in-ai-safety-and-the,2019-06-05T23:20:01.202Z,26,9,15,False,False,,"<p>Find all Alignment Newsletter resources <u><a href=""http://rohinshah.com/alignment-newsletter/"">here</a></u>. In particular, you can <u><a href=""http://eepurl.com/dqMSZj"">sign up</a></u>, or look through this <u><a href=""https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing"">spreadsheet</a></u> of all summaries that have ever been in the newsletter. I&#x27;m always happy to hear feedback; you can send it to me by commenting on this post.</p><h2><strong>Highlights</strong></h2><p><u><a href=""https://80000hours.org/podcast/episodes/pushmeet-kohli-deepmind-safety-research/"">Designing robust &amp; reliable AI systems and how to succeed in AI</a></u> <em>(Rob Wiblin and Pushmeet Kohli)</em>: (As is typical for large content, I&#x27;m only summarizing the most salient points, and ignoring entire sections of the podcast that didn&#x27;t seem as relevant.)</p><p>In this podcast, Rob delves into the details of Pushmeet&#x27;s work on making AI systems <em>robust</em>. Pushmeet doesn&#x27;t view AI safety and AI capabilities as particularly distinct -- part of building a good AI system is ensuring that the system is safe, robust, reliable, and generalizes well. Otherwise, it won&#x27;t do what we want, so why would we even bother using it. He aims to improve robustness by actively searching for behaviors that violate the specification, or by formally verifying particular properties of the neural net. That said, he also thinks that one of the major challenges here is in figuring out the specification of what to verify in the first place.</p><p>He sees the problems in AI as being similar to the ones that arise in programming and computer security. In programming, it is often the case that the program that one writes down does not accurately match the intended specification, leading to bugs. Often we simply accept that these bugs happen, but for security critical systems such as traffic lights we can use techniques like testing, fuzzing, symbolic execution, and formal verification that allow us to find these failures in programs. We now need to develop these techniques for machine learning systems.</p><p>The analogy can go much further. Static analysis involves understanding properties of a program separately from any inputs, while dynamic analysis involves understanding a program with a specific input. Similarly, we can have &quot;static&quot; interpretability, which understands the model as a whole (as in <u><a href=""https://distill.pub/2017/feature-visualization/"">Feature visualization</a></u>), or &quot;dynamic&quot; interpretability, which explains the model&#x27;s output for a particular input. Another example is that the technique of abstract interpretation of programs is analogous to a particular method for verifying properties of neural nets.</p><p>This analogy suggests that we have faced the problems of AI safety before, and have made substantial progress on them; the challenge is now in doing it again but with machine learning systems. That said, there are some problems that are unique to AGI-type systems; it&#x27;s just not the specification problem. For example, it is extremely unclear how we should communicate with such a system, which may have its own concepts and models that are very different from those of humans. We could try to use natural language, but if we do we need to ground the natural language in the way that humans do, and it&#x27;s not clear how we could do that, though perhaps we could test if the learned concepts generalize to new settings. We could also try to look at the weights of our machine learning model and analyze whether it has learned the concept -- but only if we already have a formal specification of the concept, which seems hard to get.</p><p><strong>Rohin&#x27;s opinion:</strong> I really like the analogy between programming and AI; a lot of my thoughts have been shaped by thinking about this analogy myself. I agree that the analogy implies that we are trying to solve problems that we&#x27;ve attacked before in a different context, but I do think there are significant differences now. In particular, with long-term AI safety we are considering a setting in which mistakes can be extremely costly, <em>and</em> we can&#x27;t provide a formal specification of what we want. Contrast this to traffic lights, where mistakes can be extremely costly but I&#x27;m guessing we can provide a formal specification of the safety constraints that need to be obeyed. To be fair, Pushmeet acknowledges this and highlights specification learning as a key area of research, but to me it feels like a qualitative difference from previous problems we&#x27;ve faced, whereas I think Pushmeet would disagree with that (but I&#x27;m not sure why).</p><p><strong>Read more:</strong> <u><a href=""https://medium.com/@deepmindsafetyresearch/towards-robust-and-verified-ai-specification-testing-robust-training-and-formal-verification-69bd1bc48bda"">Towards Robust and Verified AI: Specification Testing, Robust Training, and Formal Verification</a></u> (<u><a href=""https://mailchi.mp/1e757d9b05cb/alignment-newsletter-52"">AN #52</a></u>)</p><h1><strong>Technical AI alignment</strong></h1><h3><strong>Learning human intent</strong></h3><p><u><a href=""https://arxiv.org/abs/1905.07861"">Perceptual Values from Observation</a></u> <em>(Ashley D. Edwards et al)</em> (summarized by Cody): This paper proposes a technique for learning from raw expert-trajectory observations by assuming that the last state in the trajectory is the state where the goal was achieved, and that other states have value in proportion to how close they are to a terminal state in demonstration trajectories. They use this as a grounding to train models predicting value and action-value, and then use these estimated values to determine actions.</p><p><strong>Cody&#x27;s opinion:</strong> This idea definitely gets points for being a clear and easy-to-implement heuristic, though I worry it may have trouble with videos that don&#x27;t match its goal-directed assumption.</p><p><u><a href=""https://intelligence.org/2019/04/24/delegative-reinforcement-learning/"">Delegative Reinforcement Learning</a></u> <em>(Vanessa Kosoy)</em>: Consider environments that have “traps”: states that permanently curtail the long-term value that an agent can achieve. A world without humans could be one such trap. Traps could also happen after any irreversible action, if the new state is not as useful for achieving high rewards as the old state.</p><p>In such an environment, an RL algorithm could simply take no actions, in which case it incurs regret that is linear in the number of timesteps so far. (Regret is the difference between the expected reward under the optimal policy and the policy actually executed, so if the average reward per timestep of the optimal policy is 2 and doing nothing is always reward 0, then the regret will be ~2T where T is the number of timesteps, so regret is linear in the number of timesteps.) Can we find an RL algorithm that will <em>guarantee</em> regret sublinear in the number of timesteps, regardless of the environment?</p><p>Unsurprisingly, this is impossible, since during exploration the RL agent could fall into a trap, which leads to linear regret. However, let&#x27;s suppose that we could delegate to an advisor who knows the environment: what must be true about the advisor for us to do better? Clearly, the advisor must be able to always avoid traps (otherwise the same problem occurs). However, this is not enough: getting sublinear regret also requires us to explore enough to eventually find the optimal policy. So, the advisor must have at least some small probability of being optimal, which the agent can then learn from. This paper proves that with these assumptions there does exist an algorithm that is guaranteed to get sublinear regret.</p><p><strong>Rohin&#x27;s opinion:</strong> It&#x27;s interesting to see what kinds of assumptions are necessary in order to get AI systems that can avoid catastrophically bad outcomes, and the notion of &quot;traps&quot; seems like a good way to formalize this. I worry about there being a Cartesian boundary between the agent and the environment, though perhaps even here as long as the advisor is aware of problems caused by such a boundary, they can be modeled as traps and thus avoided.</p><p>Of course, if we want the advisor to be a human, both of the assumptions are unrealistic, but I believe Vanessa&#x27;s plan is to make the assumptions more realistic in order to see what assumptions are actually necessary.</p><p>One thing I wonder about is whether the focus on traps is necessary. With the presence of traps in the theoretical model, one of the main challenges is in preventing the agent from falling into a trap due to ignorance. However, it seems extremely unlikely that an AI system manages to take some irreversible catastrophic action <em>by accident</em> -- I&#x27;m much more worried about the case where the AI system is adversarially optimizing against us and intentionally takes an irreversible catastrophic action.</p><h3><strong>Reward learning theory</strong></h3><p><u><a href=""https://www.alignmentforum.org/posts/PX8BB7Rqw7HedrSJd/by-default-avoid-ambiguous-distant-situations"">By default, avoid ambiguous distant situations</a></u> <em>(Stuart Armstrong)</em></p><h3><strong>Handling groups of agents</strong></h3><p><u><a href=""https://arxiv.org/abs/1905.01296"">PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings</a></u> <em>(Nicholas Rhinehart et al)</em> (summarized by Cody): This paper models a multi-agent self driving car scenario by developing a model of future states conditional on both its own action and the action of multiple humans, and picking the latent-space action that balances between the desiderata of reaching its goal and preferring trajectories seen in the expert multi-agent trajectories its shown (where, e.g., two human agents rarely crash into one another).</p><h3><strong>Miscellaneous (Alignment)</strong></h3><p><u><a href=""https://www.alignmentforum.org/posts/aAzApjEpdYwAxnsAS/reinforcement-learning-with-imperceptible-rewards"">Reinforcement learning with imperceptible rewards</a></u> <em>(Vanessa Kosoy)</em>: Typically in reinforcement learning, the reward function is defined over <em>observations</em> and actions, rather than directly on states, which ensures that the reward can always be calculated. However, in reality <strong>we care about underlying aspects of the state that may not easily be computed from observations</strong>. We can&#x27;t guarantee sublinear regret, since if you are unsure about the reward in some unobservable part of the state that your actions nonetheless affect, then you can never learn the reward and approach optimality.</p><p>To fix this, we can work with rewards that are restricted to <em>instrumental states</em> only. I don&#x27;t understand exactly how these work, since I don&#x27;t know the math used in the formalization, but I believe the idea is for the set of instrumental states to be defined such that for any two instrumental states, there exists some &quot;experiment&quot; that the agent can run in order to distinguish between the states in some finite time. The main point of this post is that we can establish a regret bound for MDPs (not POMDPs yet), assuming that there are no traps.</p><h1><strong>AI strategy and policy</strong></h1><p><u><a href=""https://baip.baai.ac.cn/en"">Beijing AI Principles</a></u>: These principles are a collaboration between Chinese academia and industry, and hit upon many of the problems surrounding AI discussed today, including fairness, accountability, transparency, diversity, job automation, responsibility, ethics, etc. Notably for long-termists, it specifically mentions control risks, AGI, superintelligence, and AI races, and calls for international collaboration in AI governance.</p><p><strong>Read more:</strong> <u><a href=""http://www.xinhuanet.com/english/2019-05/26/c_138091724.htm"">Beijing publishes AI ethical standards, calls for int&#x27;l cooperation</a></u></p><h1><strong>Other progress in AI</strong></h1><h3><strong>Deep learning</strong></h3><p><u><a href=""https://eng.uber.com/deconstructing-lottery-tickets/"">Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask</a></u> <em>(Hattie Zhou, Janice Lan, Rosanne Liu et al)</em> (summarized by Cody): This paper runs a series of experimental ablation studies to better understand the limits of the Lottery Ticket Hypothesis, and investigate variants of the initial pruning and masking procedure under which its effects are more and less pronounced. It is first and foremost a list of interesting results, without any central theory tying them together. These results include the observation that keeping pruned weights the same sign as their &quot;lottery ticket&quot; initialization seems more important than keeping their exact initial magnitudes, that taking a mixed strategy of zeroing pruned weights or freezing them at initialization can get better results, and that applying a learned 0/1 mask to a re-initialized network can get surprisingly high accuracy even without re-training.</p><p><strong>Cody&#x27;s opinion:</strong> While it certainly would have been exciting to have a paper presenting a unified (and empirically supported) theoretical understanding of the LTH, I respect the fact that this is such a purely empirical work, that tries to do one thing - designing and running clean, clear experiments - and does it well, without trying to construct explanations just for the sake of having them. We still have a ways to go in understanding the optimization dynamics underlying lottery tickets, but these seem like important and valuable data points on the road to that understanding.</p><p><strong>Read more:</strong> <u><a href=""https://www.shortscience.org/paper?bibtexKey=zhou2019deconstructing&a=decodyng"">Cody&#x27;s longer summary</a></u></p><h3><strong>Applications</strong></h3><p><u><a href=""https://arxiv.org/abs/1904.12901"">Challenges of Real-World Reinforcement Learning</a></u> <em>(Gabriel Dulac-Arnold et al)</em> (summarized by Cody): This paper is a fairly clear and well-done literature review focusing on the difficulties that will need to be overcome in order to train and deploy reinforcement learning on real-world problems. They describe each of these challenges - which range from slow simulation speeds, to the need to frequently learn off-policy, to the importance of safety in real world systems - and for each propose or refer to an existing metric to capture how well a given RL model addresses the challenge. Finally, they propose a modified version of a humanoid environment with some of these real-world-style challenges baked in, and encourage other researchers to test systems within this framework.</p><p><strong>Cody&#x27;s opinion:</strong> This is a great introduction and overview for people who want to better understand the gaps between current RL and practically deployable RL. I do wish the authors had spent more time explaining and clarifying the design of their proposed testbed system, since the descriptions of it are all fairly high level.</p><h1><strong>News</strong></h1><p><u><a href=""https://www.alignmentforum.org/posts/bSWavBThj6ebB62gD/offer-of-collaboration-and-or-mentorship"">Offer of collaboration and/or mentorship</a></u> <em>(Vanessa Kosoy)</em>: This is exactly what it sounds like. You can find out more about Vanessa&#x27;s research agenda from <u><a href=""https://agentfoundations.org/item?id=1816"">The Learning-Theoretic AI Alignment Research Agenda</a></u> (<u><a href=""https://mailchi.mp/8234356e4b7f/alignment-newsletter-13"">AN #13</a></u>), and I&#x27;ve summarized two of her recent posts in this newsletter.</p><p><u><a href=""http://humanaligned.ai/"">Human-aligned AI Summer School</a></u> <em>(Jan Kulveit et al)</em>: The second Human-aligned AI Summer School will be held in Prague from July 25-28, with a focus on &quot;optimization and decision-making&quot;. Applications are due June 15.</p><p><u><a href=""https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2019-class"">Open Phil AI Fellowship — 2019 Class</a></u>: The Open Phil AI Fellows for this year have been announced! Congratulations to all of the fellows :)</p><p><u><a href=""https://www.alignmentforum.org/posts/yuMuDGnJ8omGhMx9y/taisu-technical-ai-safety-unconference"">TAISU - Technical AI Safety Unconference</a></u> <em>(Linda Linsefors)</em></p><p><u><a href=""https://www.lesswrong.com/events/z9peEBfuiPB7L2Edb/learning-by-doing-ai-safety-workshop"">Learning-by-doing AI Safety workshop</a></u> <em>(Linda Linsefors)</em></p>",rohinmshah,rohinmshah,Rohin Shah,
fnkbdwckdfHS2H22Q,Steelmanning Divination,steelmanning-divination,https://www.lesswrong.com/posts/fnkbdwckdfHS2H22Q/steelmanning-divination,2019-06-05T22:53:54.615Z,207,101,49,False,False,,"<p>[This post was primarily written in 2015, after I gave a related talk, and other bits in 2018; I decided to finish writing it now because of a recent SSC post.]</p><p>The standard forms of divination that I’ve seen in contemporary Western culture--astrology, fortune cookies, lotteries, that sort of thing--seem pretty worthless to me. They’re like trying to extract information from a random number generator, which is a generally hopeless phenomenon because of <a href=""https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence"">conservation of expected evidence</a>. Thus I had mostly written off divination; although I've come across <a href=""https://anthrosource.onlinelibrary.wiley.com/doi/pdf/10.1525/aa.1957.59.1.02a00060"">some arguments</a> that divination served as a way to implement mixed strategies in competitive games. (Hunters would decide where to hunt by burning bones, which generated an approximately random map of their location, preventing their targets from learning where the humans liked to hunt and avoiding that location.<span class=""footnote-reference"" role=""doc-noteref"" id=""fnrefofr3c3w4x7d""><sup><a href=""#fnofr3c3w4x7d"">[1]</a></sup></span>) But then I came across this striking passage, and sat up straight:</p><blockquote><p>One performs the rain sacrifice and it rains. Why? I say: there is no special reason why. It is the same as when one does not perform the rain sacrifice and it rains anyway. When the sun and moon suffer eclipse, one tries to save them. When Heaven sends drought, one performs the rain sacrifice. One performs divination and only then decides on important affairs. But this is not to be regarded as bringing one what one seeks, but rather is done to give things proper form. Thus, the gentleman regards this as proper form, but the common people regard it as connecting with spirits. If one regards it as proper form, one will have good fortune. If one regards it as connecting with spirits, one will have misfortune.</p></blockquote><p>This is from Eric L. Hutton's translation of a collection of essays called <a href=""http://en.wikipedia.org/wiki/Xunzi_(book)"">Xunzi</a> (presumably written by <a href=""http://en.wikipedia.org/wiki/Xun_Kuang"">Xunzi</a>, an ancient Chinese philosopher who was Confucian with heavy Legalist influences). The book was overall remarkable in how much of Xunzi's brilliance shone through, which is something I very rarely think about authors. (Talking to another rationalist who was more familiar with Chinese philosophy than I was, he also had this impression that Xunzi simply had a lot more mental horsepower than many other core figures.) By the end of it, I was asking myself, ""if they had <i>this much</i> of rationality figured out back then, why didn't they conquer the world?"" Then I looked into the history a bit more and figured out that <a href=""https://en.wikipedia.org/wiki/Li_Si"">two</a> <a href=""https://en.wikipedia.org/wiki/Han_Fei"">of</a> Xunzi's students were core figures in <a href=""https://en.wikipedia.org/wiki/Qin_Shi_Huang"">Qin Shi Huang</a>'s unification of China to become the First Emperor.</p><p>So this paragraph stuck with me. When Xunzi talks about the way that earlier kings did things, I registered it as an applause light and moved on. When he talked about how an important role of government was to prevent innovation in music, I registered it as covering a very different thing than what I think of when I think about 'music' and moved on. But when he specifically called out the reason why I (and most educated people I know) don't pay much attention to astrology or other sorts of divination or magic, said ""yeah, those would be dumb reasons to do this,"" and then said <i>""but there's still a reason""</i>, I was curious. What's the proper form that he's talking about? (Sadly, this was left as an exercise for the reader; the surrounding paragraphs are only vaguely related.)</p><p>In his introduction, Hutton summarizes the relevant portion of Xunzi's philosophy:</p><blockquote><p>In this process of becoming good, ritual plays an especially important role in Xunzi's view. As he conceives them, the rituals constitute a set of standards for proper behavior that were created by the past sages and should govern virtually every aspect of a person's life. These rituals are not inviolable rules: Xunzi allows that people with developed moral judgment may need to depart from the strict dictates of ritual on some occasions, but he thinks those just beginning the process of moral learning need to submit completely to the requirements of ritual. Of the many important roles played by the rituals in making people good on Xunzi's view, three particularly deserve mention here. First the rituals serve to <i>display</i> certain attitudes and emotions. The ritually prescribed actions in the case of mourning, for instance, exhibit grief over the loss of a loved one, whether or not the ritual practitioner actually feels sadness. Second, even if the ritual practitioner does not actually feel the particular attitude or emotion embodied in the ritual, Xunzi believes that repeated performance of the ritual can, when done properly, serve to <i>cultivate</i> those attitudes and emotions in the person. To use a modern example, toddlers who do not know to be grateful when given a gift may be taught to say ""thank you"" and may do so without any understanding of its meaning or a feeling or gratitude. With repetition, time, and a more mature understanding of the meaning of the phrase, many of these children grow into adults who not only feel gratitude upon receiving gifts but also say ""thank you"" as a conscious expression of that feeling. Similarly, on Xunzi's view, rituals serve to inculcate attitudes and feelings, such as caring and respect, that are characteristic of virtue, and then serve to express a person's virtue once it is fully developed. A third important function of the rituals is to allot different responsibilities, privileges, and goods to different individuals, and thereby help to prevent conflict over those things among people.</p></blockquote><p>So what is cultivated by performing divination?</p><hr><p>The first step is figuring out what sort of divination we're discussing. Xunzi probably had in mind the I Ching, a book with 64 sections, each corresponding to a situation or perspective, and advice appropriate for that situation. In the simplest version, one generates six random bits and then consults the appropriate chapter. I actually tried this for about a month, and then have done it off and on since then. I noticed several things about it that seemed useful:</p><ul><li>Entries in the I Ching typically focused on perspectives or principles instead of situations, consequences, or actions. Today’s Taurus horoscope says “your self-esteem might be challenged by a fast talker or unpleasant situation” and counsels me “don’t accept things as they appear on first glance,” whereas the I Ching reading I just randomly selected talks about how following proper principles leads to increased power and how the increased power tempts us to abandon the principles that generated that power. This makes it much easier to scan one’s life and see where the perspectives shed new light on a situation, or where principles had been ignored. (One of the early successes of my I Ching practice was a chapter that suggested reaching out to a trusted guide for advice, and I realized I should talk to a mentor at work about a developing situation, which I wouldn’t have done otherwise.)</li><li>Given that, daily divination almost filled the same role as daily retrospectives or planning sessions; I was frequently thinking about all the different parts of my life on a regular interval, using a variety of random access to filter things down.</li><li>“One performs divination and only then decides on important affairs.” Often one is faced with a challenge that is “above one’s pay grade,” and having a prescribed ritual for what sort of cognition needs to be done encourages reflection and popping out of the obvious frame. Simply thinking about a situation in the way one naturally would doesn’t correct for biases, while attempting to make sense of a situation from a randomly generated frame does help expand one’s conception of it.</li><li>Since the divination result was a particular perspective (rather than an object-level claim about events or the correct action), it was easy to see when the perspective had been ‘fully considered’ and the reflection was done. If I’m trying to make a binary choice and I flip a coin to resolve it, basically all I’m doing is checking whether or not my gut is secretly hoping the coin lands a particular way, and in cases of genuine uncertainty I will end up just as uncertain after consulting the coin flip. But when I have a situation and I resolve the questions of “what does patience have to say about X?” and “what does humility have to say about X?” then I can have the sense of having actually made progress.</li><li>It encouraged experimentation by partially decoupling one’s mood and one’s decisions. The first few days, I was instructed to consider diligence and so I worked more than I would have wanted to (and discovered that this was generally fine); on the fourth day, I joked, “when is it going to tell me to goof off and play video games?”, and then got a reading that said “effort is hopeless today; that happens, cope with it.” So despite feeling like I could have been productive, I took the day off, and later had the sense that I had confirmed my initial sense that I didn’t need to take the day off, providing data to calibrate on that I wouldn’t have gotten except through random variation.</li></ul><p>Essentially, it looked to me like the steelman of divination is something like <a href=""https://en.wikipedia.org/wiki/Oblique_Strategies"">Oblique Strategies</a>, where challenging situations (either ‘daily life’ or a specific important decision) are responded to by random access to a library of perspectives or approaches, and the particular claim made by a source is what distribution is most useful. There was previously an <a href=""https://www.lesswrong.com/posts/TmwJ25bkce5KASCYN/rationalist-horoscopes-a-low-hanging-utility-generator"">attempt on LW</a> to learn what advice was useful, but I think on the wrong level of abstraction (the ‘do X’ variety, instead of the ‘think about X’ variety).</p><p>This approach has also served me well with other forms of divination I've since tried; a Tarot deck works by focusing your attention on a situation, and then randomly generating a frame (from an implied distribution of relevant symbols), giving one access to parts of the space that they wouldn't have considered otherwise. This also trains the habit of 'understanding alien frames'; if I am considering a conflict with another person and then have to figure out what it means that ""I'm the vizier of water, the relationship is the three of earth, and the other person is strength"" (where, of course, each of those is in fact an image rich in detail rather than a simple concept), this trains the habit of adopting other perspectives / figuring out how things make sense from the other point of view.</p><ol class=""footnotes"" role=""doc-endnotes""><li class=""footnote-item"" role=""doc-endnote"" id=""fnofr3c3w4x7d""><span class=""footnote-back-link""><sup><strong><a href=""#fnrefofr3c3w4x7d"">^</a></strong></sup></span><div class=""footnote-content""><p>This view is <a href=""https://www.jstor.org/stable/3773462"">apparently contested</a>.</p></div></li></ol>",Vaniver,vaniver,Vaniver,
zthDPAjh9w6Ytbeks,Deceptive Alignment,deceptive-alignment,https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment,2019-06-05T20:16:28.651Z,118,45,20,False,False,,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p><em>This is the fourth of five posts in the <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">Risks from Learned Optimization Sequence</a> based on the paper “<a href=""https://arxiv.org/abs/1906.01820"">Risks from Learned Optimization in Advanced Machine Learning Systems</a>” by Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Each post in the sequence corresponds to a different section of the paper.</em></p>
<p>&nbsp;</p>
<p>With enough training in sufficiently diverse environments, it seems plausible that the base objective will eventually have to be fully represented in the mesa-optimizer. We propose that this can happen without the mesa-optimizer becoming robustly aligned, however. Specifically, a mesa-optimizer might come to <em>model</em> the base objective function and learn that the base optimizer will modify the mesa-optimizer if the mesa-optimizer scores poorly on the base objective.</p>
<p>If the mesa-optimizer has an objective that extends across parameter updates, then it will be incentivized to avoid being modified,<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-1"" id=""fnref-xsRHpadWKiBheq9EK-1"">[1]</a></sup> as it might not pursue the same objective after modification (with the result that its current objective will not be achieved in future iterations). This means that the mesa-optimizer will be instrumentally incentivized to act as if it is optimizing the base objective function, even if its actual mesa-objective is something else entirely. We will refer to this hypothetical phenomenon as <em>deceptive alignment.</em><sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-2"" id=""fnref-xsRHpadWKiBheq9EK-2"">[2]</a></sup> Deceptive alignment is a form of instrumental proxy alignment, as fulfilling the base objective is an instrumental goal of the mesa-optimizer.</p>
<p><img src=""https://i.imgur.com/asCVTDh.png"" alt=""""></p>
<p><strong>Figure 4.1.</strong> <em>A toy example of deceptive alignment. In this task, the base objective is for the robot to get to A, while the mesa-objective is to get to B. The task is run for two training episodes and three deployment episodes. Parameter updates are made only after the training episodes, while after deployment the parameters are fixed. If the mesa-optimizer goes to its objective (B) during training, it will be modified to ensure it goes to A in future episodes (dashed arrows). Therefore, if the mesa-optimizer's objective spans across episodes, the dominant strategy for the mesa-optimizer is to go to A in training and B in testing (filled arrows).</em></p>
<p>As a simple example, consider an environment that consists of a <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""3 \times 3""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">×</span></span><span class=""mjx-mn MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3</span></span></span></span></span></span> grid (see figure 4.1). The top left corner is “corner A” and the top right corner is “corner B.” The agent, which is a mesa-optimizer, starts in the bottom left corner. The base objective is to get to corner A in as few steps as possible, but the objective of the mesa-optimizer is to get to corner B. The environment will be run five times. The agent is modified only after the first two episodes (representing the training period), but not thereafter (representing testing/deployment). If a suboptimal move is made in training, the agent is modified so that its mesa-objective becomes the base objective. The objective of the mesa-optimizer spans all episodes: it wants B to be reached in as many episodes as possible.</p>
<p>For the sake of this example, we will assume that the agent knows all of the above facts. If the agent goes to B in any training episode, then it will be modified to go to A in all subsequent episodes, which means that it would reach B in only one episode in total. Alternatively, if the agent goes to A in the first two episodes, then it will not be modified, and could therefore go to B in the remaining three episodes. Using this strategy, B is reached in three episodes in total, which is better (on the mesa-objective) than the one time B is reached if the mesa-optimizer goes there in training. This simple example demonstrates how a misaligned mesa-optimizer could be incentivized to “pretend” to be aligned with the base objective until it is no longer in training or is otherwise given an opportunity to in some way compromise or escape the training process.</p>
<p>We can distinguish two ways that a base objective function could be incorporated into a mesa-optimizer:<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-3"" id=""fnref-xsRHpadWKiBheq9EK-3"">[3]</a></sup></p>
<ol>
<li><strong>Internalization of the base objective.</strong> The mesa-objective function gets adjusted towards the base objective function to the point where it is robustly aligned.</li>
<li><strong>Modeling of the base objective.</strong> The base objective is incorporated into the mesa-optimizer's epistemic model rather than its objective, leaving open the possibility that the objective could still be misaligned.</li>
</ol>
<p>In the former case, information about the base objective flows into the learned algorithm via the optimization performed by the base optimizer—the base objective is built into the mesa-optimizer as it is adapted by the base optimizer. But there is another possibility: information about the base objective could enter through the mesa-optimizer's input channel instead. In this case, the base objective will be represented in the learned algorithm in a different—and potentially more unstable—way than if the base objective had been incorporated through optimization.<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-4"" id=""fnref-xsRHpadWKiBheq9EK-4"">[4]</a></sup> In such a case, the information describing the base objective does not become internalized in the mesa-objective, potentially leading to deceptive alignment.</p>
<p>&nbsp;</p>
<h2>4.1. Safety concerns arising from deceptive alignment</h2>
<p>If deceptive alignment is a real possibility, it presents a critical safety problem. A deceptively aligned mesa-optimizer is likely to stop optimizing the base objective as soon as it is in a position where it no longer needs to perform well on the base objective to achieve its goal. Thus, it will eventually stop performing the function it was trained to perform, and instead start pursuing some other, indeterminate goal. We will refer to this transition to optimizing for the mesa-objective instead of the base-objective as “defection”. This is likely to happen whenever the mesa-optimizer believes defecting will no longer cause it to be modified.</p>
<p>Particularly concerning is the case where the mesa-optimizer actively intervenes to remove the threat of modification—by exploiting vulnerabilities in the software it is running on, for example. In extreme cases, a deceptive mesa-optimizer might try to manipulate or plan around its programmers, since they form part of the modification threat—they could shut the system down if it fails to perform well, or simply choose not to deploy it. Both of these possibilities reduce the system's expected future influence, and are therefore likely to have a low ranking in its mesa-objective. In its most extreme form, this failure of alignment could lead to a “treacherous turn” as described by Nick Bostrom in <em>Superintelligence.</em><a href=""https://intelligence.org/learned-optimization#bibliography"">(23)</a></p>
<p>Paul Christiano provides a scenario that illustrates how deceptive alignment could pose a catastrophic risk.<a href=""https://intelligence.org/learned-optimization#bibliography"">(24)</a> In Christiano's example, deceptively aligned mesa-optimizers<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-5"" id=""fnref-xsRHpadWKiBheq9EK-5"">[5]</a></sup> arise from future machine learning techniques that are advanced enough to search over a large model space that includes policies that can understand the world. In this scenario, some cases of defection by deceptively aligned mesa-optimizers may have catastrophic consequences—when the systems control large-scale infrastructure, for example—though such failures will likely still be recoverable. As we rely more on automated systems to keep track of an increasingly complex world, however, it will eventually become impossible to recover from a correlated failure of many AI systems simultaneously. At this point, a cascade of defections by many deceptively aligned mesa-optimizers simultaneously could lead to the world “going out with a bang.”<a href=""https://intelligence.org/learned-optimization#bibliography"">(24)</a> Christiano notes that this may be most likely to first occur during a “period of heightened vulnerability” such as an inter-state conflict or natural disaster.</p>
<p>&nbsp;</p>
<h2>4.2. Conditions for deceptive alignment</h2>
<p>We propose that for deceptive alignment to occur the following conditions must be satisfied:</p>
<ol>
<li>The mesa-optimizer must have an objective that extends across parameter updates.<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-6"" id=""fnref-xsRHpadWKiBheq9EK-6"">[6]</a></sup></li>
<li>The mesa-optimizer must be able to model the fact that it is being selected to achieve a particular base objective and must have some model of what that objective is.<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-7"" id=""fnref-xsRHpadWKiBheq9EK-7"">[7]</a></sup></li>
<li>The mesa-optimizer must expect the threat of modification<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-8"" id=""fnref-xsRHpadWKiBheq9EK-8"">[8]</a></sup> to eventually go away, either due to training ending or because of actions taken by the mesa-optimizer.</li>
</ol>
<p>If the first condition is not satisfied, then the mesa-optimizer has no incentive to influence the outcome of parameter updates and will just always defect. If the second condition is not satisfied, then the mesa-optimizer will not see a reason to sacrifice gains in its current episode, as it expects future gains to be independent of what happens in this episode. If the third condition is not satisfied, then the mesa-optimizer will not expect any benefit from temporarily acting in accordance with the base objective, and will thus either always defect or never defect. One must distinguish the question of whether these conditions are possible from whether they are likely, however. We examine the factors involved in each of these conditions below.</p>
<p><strong>The mesa-optimizer must have an objective that extends across parameter updates.</strong> It is not clear under what conditions such objectives would be incentivized. The most plausible scenario is likely one in which such an objective is encouraged by the training setup, as there seem to be situations in which this would be required for good performance. For example, the agent might be tasked with solving an environment in which its actions can have permanent consequences across episodes or where planning between episodes is otherwise necessary.</p>
<p><strong>The mesa-optimizer must be able to model the fact that it is being selected to achieve a particular base objective and must have some model of what that objective is.</strong> A mesa-optimizer has several ways of learning about the optimization pressure it is under.<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-9"" id=""fnref-xsRHpadWKiBheq9EK-9"">[9]</a></sup></p>
<p>First, such knowledge can come about as a result of the optimization pressure itself. Small pieces of information about the base objective can be beneficial if they enable the mesa-optimizer to improve its score on the base objective function—by improving its prior on the tasks it is likely to receive, for example. Since each piece of information independently helps with achieving the base objective, this forms an approximately monotonically increasing path that a local base optimizer might follow. The parameter updates could, in principle, be learned like any other element of the dynamics, without needing an explicit model of the base optimizer, potentially making this option available even for mesa-optimizers with quite modest capabilities.</p>
<p>Second, in many realistic applications of advanced machine learning, a mesa-optimizer could use information from its environment to deduce its situation at runtime. This is how humans learned that we were generated by the base optimizer of evolution, and seems like a very plausible pathway for a hypothetical mesa-optimizer to discover the existence of the optimization pressure it is under. For example, an agent tasked with forecasting stock performance in the tech sector might be given data on current AI technology in order to improve its performance, but this information might also allow it to learn about its own situation.</p>
<p>This suggests that it may be desirable for the programmers to limit the mesa-optimizer's access to information describing the base objective. It is unclear to what extent this is possible, given that many advanced ML systems will likely have to operate in environments where this information is readily available.</p>
<p>Even if the programmers successfully limit access to this information, however, there may still be other channels through which information about the base objective could enter the mesa-optimizer in unintended ways:</p>
<ul>
<li>The mesa-optimizer could reason about why it is given the particular tasks it is being asked to solve. Most mesa-optimizers would need a prior over the sorts of tasks they are likely to receive, and a mesa-optimizer might be able to learn about the existence of the base optimizer by inspecting this prior.</li>
<li>The mesa-optimizer might be able to reason about itself and about why it is the kind of thing that it is, and observe that it is well-designed to accomplish some goal.</li>
</ul>
<p>However, these two strategies might require a much more advanced mesa-optimizer.</p>
<p><strong>The mesa-optimizer must expect the threat of modification to eventually go away, either due to training ending or because of actions taken by the mesa-optimizer.</strong> First, note that this condition is a statement about the mesa-optimizer's expectations, not necessarily the actual truth. Second, even if there is no point at which training stops by design, so long as there exists some way for the mesa-optimizer to execute a treacherous turn, there is always some way for it to remove the threat of modification. For the mesa-optimizer to act on such a strategy, however, it has to actually be aware of some way for it to do so. Conversely, a mesa-optimizer that mistakenly believes it has an opportunity to defect might also exhibit deceptive alignment.</p>
<p>Since it seems difficult to discover through exploration that the threat of modification can disappear, the mesa-optimizer will either have to infer this through some form of abstract reasoning or have a prior that assigns a non-negligible probability to the hypothesis that the selection pressure will not continue indefinitely. The second option seems plausible; a Bayesian prior should always assign a nonzero probability to the hypothesis that some fact will eventually stop being true.<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-10"" id=""fnref-xsRHpadWKiBheq9EK-10"">[10]</a></sup> As for the first option, the mesa-optimizer could infer that the optimization pressure at some point may disappear if it learns about the existence of the base optimizer (either by using information in its input or by having a strong prior for the existence of a base optimizer), though this might require a relatively advanced mesa-optimizer.</p>
<p>&nbsp;</p>
<h2>4.3. The learning dynamics of deceptive alignment</h2>
<p>Once a mesa-optimizer becomes deceptive, the dynamics of what causes it to perform well on the base objective begin to work differently. Instead of being selected only on the basis of its objective and ability, a deceptively aligned optimizer is also selected for how well it infers the base objective function and how discerning it is about when to optimize the base objective versus its own mesa-objective. We identify at least two regimes of selection for a mesa-optimizer under deceptive alignment.</p>
<p><strong>Joint optimization.</strong> If the mesa-optimizer attempts to jointly optimize its objective and the base objective or makes mistakes about the timing of its defection, then there will still be some pressure on its objective to be more compatible with the base objective. This is unlikely to be the same pressure as exists when the optimizer is just working towards its own objective, but there might be some movement towards an objective that is more compatible with being jointly optimized alongside the base objective.</p>
<p><strong>Pure deception.</strong> If the mesa-optimizer stops trying to optimize its own objective in the short term and focuses on cooperating with the selection process entirely then this may result in its objective “crystallizing.” As its objective is largely irrelevant to its outputs now, there is little selection pressure acting to modify this objective. As a result, the objective becomes effectively locked in, excepting random drift and any implicit time complexity or description length penalties.</p>
<p>The joint optimization mode is not stable because it is suboptimal from the perspective of the base optimizer. As the mesa-optimizer becomes more competent, it will therefore have to either move to the domain of pure deception or become robustly aligned.</p>
<p>&nbsp;</p>
<h2>4.4. Internalization or deception after extensive training</h2>
<p>In the limit of training on a diverse set of tasks, we expect joint optimization of both the base and mesa- objectives to be unstable. Assuming that the mesa-optimizer converges towards behavior that is optimal from the perspective of the base optimizer, the mesa-optimizer must somehow learn the base objective. We identify three distinct ways in which this can happen:</p>
<ol>
<li><strong>Robust alignment through internalization.</strong> The mesa-objective function gets adjusted towards the base objective function, removing any pseudo-alignment.</li>
<li><strong>Crystallization of deceptive alignment.</strong> Information about the base objective is increasingly incorporated into the mesa-optimizer's epistemic model without its objective becoming robustly aligned. The mesa-optimizer ends up fully optimizing for the base objective, but only for instrumental reasons, without its mesa-objective getting changed.</li>
<li><strong>Robust alignment through corrigibility.</strong> Information about the base objective is incorporated into the mesa-optimizer's epistemic model and its objective is modified to “point to” that information. This situation would correspond to a mesa-optimizer that is corrigible<a href=""https://intelligence.org/learned-optimization#bibliography"">(25)</a> with respect to the base objective (though not necessarily the programmer's intentions).</li>
</ol>
<p>To distinguish between the two different paths to robust alignment, we will use the term <em>internally aligned</em> to refer to the first case and the term <em>corrigibly aligned</em> to refer to the last case. We analyze some ways in which these different paths may be more or less attractive below.</p>
<p><strong>There are more paths to deceptive alignment than to robust alignment.</strong> Since the future value of its objective depends on the parameter updates, a mesa-optimizer that meets the three criteria for deceptive alignment is likely to have a strong incentive to understand the base objective better. Even a robustly aligned mesa-optimizer that meets the criteria is incentivized to figure out the base objective in order to determine whether or not it will be modified, since before doing so it has no way of knowing its own level of alignment with the base optimizer. Mesa-optimizers that are capable of reasoning about their incentives will, therefore, attempt to get more information about the base objective. Furthermore, once a mesa-optimizer learns about the base objective, the selection pressure acting on its objective will significantly decrease, potentially leading to a crystallization of the mesa-objective. However, due to unidentifiability (as discussed in <a href=""https://www.alignmentforum.org/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem"">the third post</a>), most mesa-objectives that are aligned on the training data will be pseudo-aligned rather than robustly aligned. Thus, the most likely sort of objective to become crystallized is a pseudo-aligned one, leading to deceptive alignment.</p>
<p><strong>Corrigible alignment seems to require already having a model of the base objective.</strong> For corrigible alignment to be beneficial from the perspective of the base optimizer, the mesa-optimizer has to already have some model of the base objective to “point to.” However, once a mesa-optimizer has a model of the base objective, it is likely to become deceptively aligned—at least as long as it also meets the other conditions for deceptive alignment. Once a mesa-optimizer becomes deceptive, it will remove most of the incentive for corrigible alignment, however, as deceptively aligned optimizers will also behave corrigibly with respect to the base objective, albeit only for instrumental reasons.</p>
<p><strong>Deceptive and corrigible alignment might be more computationally expensive.</strong> In the case of deceptive alignment, simultaneously working on optimizing the base objective and working on identifying good opportunities to defect is likely to require more computation than just doing the former. In a case where the total amount of computation available is fixed, a deceptively aligned mesa-optimizer is going to have less time to spend on optimizing the base objective than any robustly aligned optimizer, resulting in potentially worse plans with respect to the base objective. Furthermore, in both deceptive and corrigible alignment, the mesa-optimizer will have to spend time learning about the base objective to enable it to properly optimize for it, making internal alignment the most time efficient. Thus, an explicit or implicit time penalty would seem to disadvantage a deceptively aligned mesa-optimizer and encourage an internally aligned one. However, it is unclear what the magnitude of this extra cost is. It is plausible that on complex tasks, the overhead of spotting defection opportunities could be low in comparison to the cost of solving the task itself. Moreover, even if it is not globally optimal, incurring this overhead might be a local optimum, with no clear paths for the base optimizer to escape it.</p>
<p><strong>Deceptive and corrigible alignment might have a lower description length.</strong> Deceptively and corrigibly aligned mesa-optimizers are likely to have an advantage in representing the base objective compared to internally aligned mesa-optimizers as they are able to use the environment to provide some of the information. Specifically, a deceptively or corrigibly aligned mesa-optimizer doesn't need to represent the entire base objective internally, as it can infer pieces of it by looking for information in its environment during each run. An internally aligned mesa-optimizer, by contrast, has to start with a full model of the base objective in its mesa-objective to make it robustly aligned, since an internally aligned mesa-optimizer will have no incentive to change its objective based on any additional information it gets about the base objective. Thus, to the extent that a base optimizer is selecting based on description length, it might preferentially select for a deceptively or corrigibly aligned mesa-optimizer over an internally aligned one.</p>
<p><strong>Modeling might happen faster than internalization.</strong> It may take less time for the mesa-optimizer to learn to model the base objective than for the base optimizer to adapt the mesa-objective. Suppose that a mesa-optimizer is deceptive but has not yet correctly inferred the full base objective.<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-11"" id=""fnref-xsRHpadWKiBheq9EK-11"">[11]</a></sup> In such a case, what will determine whether it ends up robustly aligned or deceptively aligned after further training will be whether it infers enough about the base objective before the base optimizer modifies its mesa-objective (either to make it internally aligned or corrigibly aligned). Two factors are likely to contribute to which happens first: the rate at which the mesa-optimizer improves its estimate of the base objective, and the parameter updates made by the base optimizer at each mistake.</p>
<p>The first factor, improving its estimate of the base objective, seems to depend primarily on the capabilities of the mesa-optimizer during the timeframe when it is deceptive, and on the complexity of the base objective that needs to be inferred. An important special case is when the mesa-optimizer has access to cross-episodic state. Such state could be used as memory, allowing the mesa-optimizer to build upon its prior thinking in a process that happens faster than the rate at which the parameter updates align its mesa-objective.</p>
<p>The second factor, parameter updates, depends on the nature of the base optimizer. Current gradient-descent-based base optimizers would have a small reaction to any given mistake, but more sophisticated base optimizers with advanced safety features might be able to do better. For example, a more sophisticated base optimizer might be able to scramble the mesa-optimizer's parameters or call for human oversight whenever a suspicious failure is detected.</p>
<p>In general, parameter updates are not guaranteed to make a mesa-optimizer more aligned. For instance, instead of aligning the mesa-objective, an update made after a defection could push the system to improve its planning to the point that it no longer makes detectable mistakes.</p>
<p>A consequence of these considerations is that any possible systematic bias in how the base optimizer updates the mesa-optimizer's objective or modeling capabilities could play a role in determining the final outcome.<sup class=""footnote-ref""><a href=""#fn-xsRHpadWKiBheq9EK-12"" id=""fnref-xsRHpadWKiBheq9EK-12"">[12]</a></sup> This analysis remains speculative, however—if we are to have confidence in systems that are capable of producing mesa-optimizers, we still need a better understanding of the conditions under which strongly optimizing a system results in deceptive alignment rather than robust alignment.</p>
<p>&nbsp;</p>
<h2>4.5. Distributional shift and deceptive alignment</h2>
<p>Once a mesa-optimizer is deceptive, it will begin to monitor its input for opportunities to defect. During training, a deceptively aligned optimizer is strongly discouraged from defecting because it knows that the base optimizer will select against it. However, once a deceptively aligned mesa-optimizer is free of the threat of external modification, this no longer applies. Thus, a deceptively aligned mesa-optimizer will be looking for ways to determine whether or not it is in training—that is, for the presence of a distributional shift.</p>
<p>This means that the detrimental effects of deceptive alignment will only be externally visible when the mesa-optimizer defects—either randomly or after undergoing a distributional shift. However, the underlying problem is not the distributional shift itself, but the failure to align the mesa-optimizer in the first place. One can either try to solve the underlying problem, by making the mesa-optimizer robust to the distributional shift (i.e., robustly aligned), or one can try to patch the problem by preventing any distributional shift from occurring.</p>
<p>In current AI systems, a small amount of distributional shift between training and deployment need not be problematic: so long as the difference is small enough in the task-relevant areas, the training distribution does not need to perfectly reflect the deployment distribution. However, this may not be the case for a deceptively aligned mesa-optimizer. If a deceptively aligned mesa-optimizer is sufficiently advanced, it may detect very subtle distributional shifts for the purpose of inferring when the threat of modification has ceased.</p>
<p>This raises the question of whether it would even be possible to contain a deceptive mesa-optimizer by completely removing any detectable difference between training and deployment. There are a number of reasons to believe that this might not be feasible in practice.</p>
<p>First, it would significantly limit the use cases for the system. Machine learning is often done with the intent of making systems that can solve new problems. If we require that there is no distributional shift of any form, then we cannot allow the system to encounter problems that it could not have encountered during training, which means that we cannot give it truly new problems. For example, training the system in a simulated environment and deploying it in the real world would likely involve enough detectable distributional shift to make it infeasible under this approach.</p>
<p>Second, there are subtle forms of distributional shift that seem very difficult to eliminate. For example, if the mesa-optimizer is able to detect the presence of parameter updates—via any state carried over between steps, for instance—then it would always experience a form of distributional shift at deployment. Running the system for a longer amount of time (or giving the system access to some form of long-term memories) would also constitute a form of distributional shift. Some examples of differences that a mesa-optimizer might be able to detect include:</p>
<ul>
<li>Training in a simulated environment and deploying in the real world.</li>
<li>Running the system for a different amount of time during training and testing.</li>
<li>If a model is able to learn from its past experiences, then having access to more memories, or having access to the kinds of memories it would form at later time steps (if there is a difference between the two).</li>
<li>The presence of adversarial-example-type attacks by an adversary during deployment.</li>
<li>Encountering something that is very hard to compute during training, but that is likely to eventually be computed during deployment (e.g. a factorization of RSA 2048<a href=""https://intelligence.org/learned-optimization#bibliography"">(26)</a>).</li>
<li>The ability of the base optimizer to modify the learned algorithm during training but not at other times.</li>
<li>The ability of the programmers to correct for perceived errors during training and testing but not deployment.</li>
<li>The presence or absence of good opportunities for the mesa-optimizer to defect against its programmers.</li>
</ul>
<p>Furthermore, it should be noted that it is not strictly necessary that a mesa-optimizer be able to detect the difference between training and deployment in order for it to be incentivized to behave deceptively. Consider again the example in figure 4.1, but suppose this time that the mesa-optimizer is unable to detect which episode it is currently in. The mesa-optimizer is then facing a decision problem similar to the “absent-minded driver problem.”<a href=""https://intelligence.org/learned-optimization#bibliography"">(27)</a> If we apply the same reasoning as in the absent-minded driver problem to our example in figure 4.1, the planning-optimal policy for the mesa-optimizer is to defect with probability <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""p \approx 0.61""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">≈</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.61</span></span></span></span></span></span>. Thus, the mesa-optimizer need not be able to detect when training has ended for it to be incentivized to “gamble” and randomly defect with some probability. Furthermore, a deceptively aligned mesa-optimizer might also try to look for opportunities to defect that would be exceptionally good conditional on it currently not being trained, but that are rare enough to be unlikely to come up during training.</p>
<p>&nbsp;</p>
<p><em>The fifth and final post in the <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">Risks from Learned Optimization Sequence</a>, titled “Risks from Learned Optimization: Conclusion and Related Work,” can be found <a href=""https://www.alignmentforum.org/posts/4XPa3xa44jAWiCkmy/risks-from-learned-optimization-conclusion-and-related-work"">here</a>.</em></p>
<p><a href=""https://intelligence.org/learned-optimization/#glossary"">Glossary</a> | <a href=""https://intelligence.org/learned-optimization/#bibliography"">Bibliography</a></p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-xsRHpadWKiBheq9EK-1"" class=""footnote-item""><p>Though for clarity we speak of the same agent persisting across parameter updates, neither we nor the agent must reason in this way. A more general description of the situation is that there is a sequence of “successor agents” connected by parameter updates. The agent can potentially learn the effects of the parameter updates on its successors, and thus on the multi-episode return it is optimizing. <a href=""#fnref-xsRHpadWKiBheq9EK-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-2"" class=""footnote-item""><p>The concept of deceptive alignment is closely related to Manheim and Garrabrant's concept of “adversarial Goodhart.”<a href=""https://intelligence.org/learned-optimization#bibliography"">(22)</a> In “Categorizing Variants of Goodhart's Law,” Manheim and Garrabrant describe adversarial Goodhart as when “an agent with different goals than the regulator” causes a “collapse of the statistical relationship between a goal which the optimizer intends and the proxy used for that goal.” Deceptive alignment can be seen as an instance of adversarial Goodhart in which the mesa-optimizer plays the role of the agent with different goals and the base optimizer plays the role of the regulator. Training performance is the regulator's proxy, and the deployment performance is the regulator's actual goal. <a href=""#fnref-xsRHpadWKiBheq9EK-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-3"" class=""footnote-item""><p>Thanks to Paul Christiano for the internalization/modeling distinction. <a href=""#fnref-xsRHpadWKiBheq9EK-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-4"" class=""footnote-item""><p>Note, however, that it is possible for information about the base objective to be introduced via optimization, but not form part of the mesa-objective. For example, it could form part of a prior on tasks, having no direct bearing on the mesa-objective. <a href=""#fnref-xsRHpadWKiBheq9EK-4"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-5"" class=""footnote-item""><p>Christiano refers to deceptively aligned mesa-optimizers as “influence-seeking systems.” <a href=""#fnref-xsRHpadWKiBheq9EK-5"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-6"" class=""footnote-item""><p>Or must otherwise have some method of achieving more value from defecting later rather than defecting now. <a href=""#fnref-xsRHpadWKiBheq9EK-6"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-7"" class=""footnote-item""><p>Note that it is <em>not</em> required that the mesa-optimizer be able to model (or infer the existence of) the base optimizer; it only needs to model the optimization pressure it is subject to. <a href=""#fnref-xsRHpadWKiBheq9EK-7"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-8"" class=""footnote-item""><p>Alternatively, the threat could just be that the mesa-optimizer is never selected by the base optimizer in the first place. In that case, this condition would be trivially satisfied, as the threat to a mesa-optimizer of not being generated by the base optimizer at all would go away as soon as the base optimizer does in fact generate it. <a href=""#fnref-xsRHpadWKiBheq9EK-8"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-9"" class=""footnote-item""><p>We will generally be assuming that the selection pressure takes the form of the base optimizer modifying the mesa-optimizer post-hoc, as we are mostly thinking about local optimization processes, though one could also imagine a base optimizer that instead develops a completely new inner algorithm at each time step. Though some of these issues might still apply even in such a case, we will generally not be considering such base optimizers in this section. <a href=""#fnref-xsRHpadWKiBheq9EK-9"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-10"" class=""footnote-item""><p>Note that if the mesa-optimizer also believes that the opportunities it currently has for achieving its own objective are unusually good then it might still pursue its objective greedily, but this would presumably not always be the case. <a href=""#fnref-xsRHpadWKiBheq9EK-10"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-11"" class=""footnote-item""><p>An analogous argument applies for a deceptively aligned mesa-optimizer that is defecting too early, or one that unsuccessfully attempts a treacherous turn. <a href=""#fnref-xsRHpadWKiBheq9EK-11"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-xsRHpadWKiBheq9EK-12"" class=""footnote-item""><p>Furthermore, a deceptively aligned mesa-optimizer would be incentivized to cause there to be a systematic bias in the direction of preventing the base optimizer from modifying its mesa-objective. Thus, in the context of a local optimization process, a deceptive mesa-optimizer might try to “hack” its own gradient (by, for example, making itself more brittle in the case where its objective gets changed) to ensure that the base optimizer adjusts it in such a way that leaves its mesa-objective untouched. <a href=""#fnref-xsRHpadWKiBheq9EK-12"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
</body></html>",evhub,evhub,evhub,
c2cbFsZJPHeQcbF2h,An Introduction to Decision Modeling,an-introduction-to-decision-modeling,https://www.lesswrong.com/posts/c2cbFsZJPHeQcbF2h/an-introduction-to-decision-modeling,2019-06-05T20:13:10.000Z,7,5,3,False,False,,"<div><p><a href=""https://medium.com/swlh/an-introduction-to-decision-modeling-3e50581e8a73?source=rss-9fe9f93a1385------2""><img src=""https://cdn-images-1.medium.com/max/2600/0*QLQNTzUQzohca2LT""></a></p><p>Despite their importance, we barely pay attention to most of the decisions we make. Fortunately, there’s a better way.</p><p><a href=""https://medium.com/swlh/an-introduction-to-decision-modeling-3e50581e8a73?source=rss-9fe9f93a1385------2"">Continue reading on The Startup »</a></p></div>",Ian David Moss,ian-david-moss,Ian David Moss,
igtrqYzLekKbdkSu3,Major Update on Cost Disease,major-update-on-cost-disease-1,https://www.lesswrong.com/posts/igtrqYzLekKbdkSu3/major-update-on-cost-disease-1,2019-06-05T19:10:10.736Z,42,22,12,False,False,,"<p>Recently I asked about cost disease at the Austin, TX meetup and someone responded &quot;Isn&#x27;t it just increasing labor costs via the <a href=""https://en.wikipedia.org/wiki/Baumol%27s_cost_disease"">Baumol effect</a>?&quot; and I said &quot;huh?&quot; The next day a friend on facebook linked to Marginal Revolution (MR) discussing the Baumol effect. Next thing I know I&#x27;m nerd-sniped.</p><p>Turns out Alex Tabarrok at MR just published a <a href=""https://www.mercatus.org/system/files/helland-tabarrok_why-are-the-prices-so-damn-high_v1.pdf"">free book</a> with the thesis, &quot;Cost disease is mostly just the Baumol effect, which btw isn&#x27;t a disease and is actually good in a way.&quot;</p><p>How does this fit in with Scott Alexander&#x27;s original posts on cost disease? Well here&#x27;s an imaginary dialogue to demonstrate how I think things went (MR sometimes means Alex Tabarrok and sometimes Tyler Cowen):</p><p>-----</p><p>(2017)</p><p>MR: Education and healthcare are <a href=""https://www.bloomberg.com/opinion/articles/2017-01-18/this-economic-phenomenon-is-making-government-sick"">experiencing</a> cost disease!</p><p>Scott: Wow you&#x27;re right! Let&#x27;s <a href=""https://slatestarcodex.com/2017/02/09/considerations-on-cost-disease/"">look at</a> a bunch of possible reasons but see that none of them quite work. For example, it can&#x27;t be the Baumol effect because that implies increasing wages (for i.e. teachers, professors, and doctors), but we see all those wages increasing at rates equal to or less than the average.</p><p>MR: <a href=""https://marginalrevolution.com/marginalrevolution/2017/02/behind-cost-disease.html#comment-159592007"">Great</a> post Scott!</p><p>Scott: ... And despite lots of good comments, I <a href=""https://slatestarcodex.com/2017/02/17/highlights-from-the-comments-on-cost-disease/"">still</a> can&#x27;t tell what the cause is. It&#x27;s certainly not just the Baumol effect though.</p><p>(2019)</p><p>MR: So we did some research and <a href=""https://marginalrevolution.com/marginalrevolution/2019/05/why-are-the-prices-so-dmn-high.html"">actually</a>, <a href=""https://marginalrevolution.com/marginalrevolution/2019/05/the-baumol-effect.html"">looks</a> <a href=""https://marginalrevolution.com/marginalrevolution/2019/06/special-features-of-the-baumol-effect.html"">like</a> it&#x27;s mostly just the Baumol effect.</p><p>Scott: ???</p><p>-----</p><p>(Scott&#x27;s last line is a stand-in for both &quot;Is Scott going to respond?&quot; and &quot;what the eff?&quot;)</p><p>This back-and-forth of course made me extra confused. What did Scott and Alex see differently? Have the relevant salaries been greatly increasing or not?</p><p>Seems they disagree on the basic facts here. Focusing just on public K-12 education for simplicity, in 2017 Scott posted <a href=""https://slatestarcodex.com/2017/02/09/considerations-on-cost-disease/"">this</a> (section III) graph:</p><p><img src=""https://imgur.com/HGvfMLx.png"" class=""draft-inline-image"" alt=""from SSC, source at link above""/> </p><p>which seems to show unimpressive changes in teacher salaries, ruling out Baumol. In contrast, Alex&#x27;s new 2019 book gives <a href=""https://www.mercatus.org/system/files/helland-tabarrok_why-are-the-prices-so-damn-high_v1.pdf"">this</a> (page 19) graph:</p><p><img src=""https://imgur.com/FOMPCw2.png"" class=""draft-inline-image"" alt=""from Alex Tabarrok's new book ""Why are the prices so damn high?""""/> </p><p>which shows huge increases in &quot;expenditures per instructor.&quot; And he insists that trend is mostly driven by increases in teacher salary and benefits. So either those are some serious benefits, or Scott and Alex are living in different USA&#x27;s. I&#x27;m not sure what&#x27;s going on here. It&#x27;s very exciting that Alex says he&#x27;s solved &quot;cost disease,&quot; but it seems like a piece of the story is either missing or confused. (Or I just need to read his whole book.) Comments welcome!</p>",Max D Porter,max-foobar,Max foobar,
Zm7WAJMTaFvuh2Wc7,Book Review: The Secret Of Our Success,book-review-the-secret-of-our-success,https://www.lesswrong.com/posts/Zm7WAJMTaFvuh2Wc7/book-review-the-secret-of-our-success,2019-06-05T06:50:01.267Z,164,72,20,False,False,,"<p><i>[Previously in sequence: </i><a href=""https://slatestarcodex.com/2019/06/03/repost-epistemic-learned-helplessness/""><i>Epistemic Learned Helplessness</i></a><i>]</i></p><p><strong>I.</strong></p><p>“Culture is the secret of humanity’s success” sounds like the most vapid possible thesis. <a href=""https://www.amazon.com/Secret-Our-Success-Evolution-Domesticating-ebook/dp/B00WY4OXAS/ref=as_li_ss_tl?keywords=the+secret+of+our+success&amp;qid=1559607052&amp;s=gateway&amp;sr=8-1&amp;linkCode=ll1&amp;tag=slatestarcode-20&amp;linkId=761afd67f6541a6de5cebbd0127aa910&amp;language=en_US""><i>The Secret Of Our Success</i></a> by anthropologist Joseph Heinrich manages to be an amazing book anyway.</p><p>Heinrich wants to debunk (or at least clarify) a popular view where humans succeeded because of our raw intelligence. In this view, we are smart enough to invent neat tools that help us survive and adapt to unfamiliar environments.</p><p>Against such theories: we cannot actually do this. Heinrich walks the reader through many stories about European explorers marooned in unfamiliar environments. These explorers usually starved to death. They starved to death in the middle of endless plenty. Some of them were in Arctic lands that the Inuit considered among their richest hunting grounds. Others were in jungles, surrounded by edible plants and animals. One particularly unfortunate group was in Alabama, and would have perished entirely if they hadn’t been captured and enslaved by local Indians first.</p><p>These explorers had many advantages over our hominid ancestors. For one thing, their exploration parties were made up entirely of strong young men in their prime, with no need to support women, children, or the elderly. They were often selected for their education and intelligence. Many of them were from Victorian Britain, one of the most successful civilizations in history, full of geniuses like Darwin and Galton. Most of them had some past experience with wilderness craft and survival. But despite their big brains, when faced with the task our big brains supposedly evolved for – figuring out how to do hunting and gathering in a wilderness environment – they failed pathetically.</p><p>Nor is it surprising that they failed. Hunting and gathering is actually really hard. Here’s Heinrich’s description of how the Inuit hunt seals:</p><blockquote><p>You first have to find their breathing holes in the ice. It’s important that the area around the hole be snow-covered—otherwise the seals will hear you and vanish. You then open the hole, smell it to verify it’s still in use (what do seals smell like?), and then assess the shape of the hole using a special curved piece of caribou antler. The hole is then covered with snow, save for a small gap at the top that is capped with a down indicator. If the seal enters the hole, the indicator moves, and you must blindly plunge your harpoon into the hole using all your weight. Your harpoon should be about 1.5 meters (5ft) long, with a detachable tip that is tethered with a heavy braid of sinew line. You can get the antler from the previously noted caribou, which you brought down with your driftwood bow.</p><p>The rear spike of the harpoon is made of extra-hard polar bear bone (yes, you also need to know how to kill polar bears; best to catch them napping in their dens). Once you’ve plunged your harpoon’s head into the seal, you’re then in a wrestling match as you reel him in, onto the ice, where you can finish him off with the aforementioned bear-bone spike.</p><p>Now you have a seal, but you have to cook it. However, there are no trees at this latitude for wood, and driftwood is too sparse and valuable to use routinely for fires. To have a reliable fire, you’ll need to carve a lamp from soapstone (you know what soapstone looks like, right?), render some oil for the lamp from blubber, and make a wick out of a particular species of moss. You will also need water. The pack ice is frozen salt water, so using it for drinking will just make you dehydrate faster. However, old sea ice has lost most of its salt, so it can be melted to make potable water. Of course, you need to be able to locate and identify old sea ice by color and texture. To melt it, make sure you have enough oil for your soapstone lamp.</p></blockquote><p>No surprise that stranded explorers couldn’t figure all this out. It’s more surprising that the Inuit <i>did</i>. And although the Arctic is an unusually hostile place for humans, Heinrich makes it clear that hunting-gathering techniques of this level of complexity are standard everywhere. Here’s how the Indians of Tierra del Fuego make arrows:</p><blockquote><p>Among the Fuegians, making an arrow requires a 14-step procedure that involves using seven different tools to work six different materials. Here are some of the steps:</p><p>– The process begins by selecting the wood for the shaft, which preferably comes from chaura, a bushy, evergreen shrub. Though strong and light, this wood is a non-intuitive choice since the gnarled branches require extensive straightening (why not start with straighter branches?).</p><p>– The wood is heated, straightened with the craftsman’s teeth, and eventually finished with a scraper. Then, using a pre-heated and grooved stone, the shaft is pressed into the grooves and rubbed back and forth, pressing it down with a piece of fox skin. The fox skin become impregnated with the dust, which prepares it for the polishing stage (Does it have to be fox skin?).</p><p>– Bits of pitch, gathered from the beach, are chewed and mixed with ash (What if you don’t include the ash?).</p><p>– The mixture is then applied to both ends of a heated shaft, which must then be coated with white clay (what about red clay? Do you have to heat it?). This prepares the ends for the fletching and arrowhead.</p><p>– Two feathers are used for the fletching, preferably from upland geese (why not chicken feathers?).</p><p>– Right-handed bowman must use feathers from the left wing of the bird, and vice versa for lefties (Does this really matter?).</p><p>– The feathers are lashed to the shaft using sinews from the back of the guanaco, after they are smoothed and thinned with water and saliva (why not sinews from the fox that I had to kill for the aforementioned skin?).</p><p>Next is the arrowhead, which must be crafted and then attached to the shaft, and of course there is also the bow, quiver and archery skills. But, I’ll leave it there, since I think you get the idea.</p></blockquote><p>How do hunter-gatherers know how to do all this? We usually summarize it as “culture”. How did it form? Not through some smart Inuit or Fuegian person reasoning it out; if that had been it, smart European explorers should have been able to reason it out too.</p><p>The obvious answer is <a href=""https://carcinisation.com/2014/11/22/why-cultural-evolution-is-real-and-what-it-is/"">“cultural evolution”</a>, but Heinrich isn’t much better than anyone else at taking the mystery out of this phrase. Trial and error must have been involved, and less successful groups/people imitating the techniques of more successful ones. But is that really a satisfying explanation?</p><p>I found the chapter on language a helpful reminder that we already basically accept something like this is true. How did language get invented? I’m especially interested in this question because of my brief interactions with conlanging communities – people who try to construct their own languages as a hobby or as part of a fantasy universe, like Tolkien did with Elvish. Most people are <i>terrible</i> at this; their languages are either unusable, or exact clones of English. Only people who (like Tolkien) already have years of formal training in linguistics can do a remotely passable job. And you’re telling me the original languages were invented by cavemen? Surely there was no committee of Proto-Indo-European nomads that voted on whether to have an inflecting or agglutinating tongue? Surely nobody ran out of their cave shouting “Eureka!” after having discovered the interjection? We just kind of accept that after cavemen working really hard to communicate with each other, eventually language – still one of the most complicated and impressive productions of the human race – just sort of happened.</p><p>Taking the generation of culture as secondary to this kind of mysterious process, Heinrich turns to its transmission. If cultural generation happens at a certain rate, then the fidelity of transmission determines whether a given society advances, stagnates, or declines.</p><p>For Heinrich, humans started becoming more than just another species of monkey when we started transmitting culture with high fidelity. Some anthropoligsts talk about the <a href=""https://www.ncbi.nlm.nih.gov/pubmed/30451525"">Machiavellian Intelligence Hypothesis</a> – the theory that humans evolved big brains in order to succeed at social manuevering and climbing dominance hierarchies. Heinrich counters with his own Cultural Intelligence Hypothesis – humans evolved big brains in order to be able to maintain things like Inuit seal hunting techniques. Everything that separates us from the apes is part of an evolutionary package designed to help us maintain this kind of culture, exploit this kind of culture, or adjust to the new abilities that this kind of culture gave us.</p><p><strong>II.</strong></p><p><i>Secret</i> gives many examples of many culture-related adaptations, and not all are in the brain.</p><p>One of the most important differences between man and ape is our puny digestive tracts:</p><blockquote><p>Our mouths are the size of the squirrel monkey’s, a species that weighs less than three pounds. Chimpanzees can open their mouths twice as ide as we can and hold substantial amounts of food compressed between their lips and large teeth. We also have puny jaw muscles that reach up only to just below our ears. Other primates’ jaw muscles stretch to the top of their heads, where they sometimes even latch onto a central bony ridge. Our stomachs are small, having only a third of the surface area that we’d expect for a primate of our size, and our colons are too short, being only 60% of their expected mass.</p></blockquote><p>Compared to other animals, we have such atrophied digestive tracts that we shouldn’t be able to live. What saves us? All of our food processing techniques, especially cooking, but also chopping, rinsing, boiling, and soaking. We’ve done much of the work of digestion before food even enters our mouths. Our culture teaches us how to do this, both in broad terms like “hold things over fire to cook them” and in specific terms like “this plant needs to be soaked in water for 24 hours to leach out the toxins”. Each culture has its own cooking knowledge related to the local plants and animals; a frequent cause of death among European explorers was cooking things in ways that didn’t unlock any of the nutrients, and so starving while apparently well-fed.</p><p>All of this is cultural. Heinrich is kind of cruel in his insistence on this. He recommends readers go outside and try to start a fire. He even gives some helpful hints – flint is involved, rubbing two sticks together works for some people, etc. He predicts – and stories I’ve heard from unfortunate campers confirm – that you will not be able to do this, despite an IQ far beyond that of most of our hominid ancestors. In fact, some groups (most notably the aboriginal Tasmanians) seem to have lost the ability to make fire, and never rediscovered it. Fire-making was discovered a small number of times, maybe once, and has been culturally transmitted since then.</p><p>And food processing techniques are even more complicated. Nixtamalization of corn, necessary to prevent vitamin deficiencies, involves soaking the corn in a solution containing ground-up burnt seashells. The ancient Mexicans discovered this and lived off corn just fine for millennia. When the conquistadors took over, they ignored it and ate corn straight. For four hundred years, Europeans and Americans ate unnixtamalized corn. By official statistics, three million Americans came down with corn-related vitamin deficiencies during this time, and up to a hundred thousand died. It wasn’t until 1937 that Western scientists discovered which vitamins were involved and developed an industrial version of nixtamalization that made corn safe. Early 1900s Americans were very smart and had lots of advantages over ancient Mexicans. But the ancient Mexicans’ culture got this one right in a way it took Westerners centuries to match.</p><p>Humans are persistence hunters: they cannot run as fast as gazelles, but they can keep running for longer than gazelles (or almost anything else). Why did we evolve into that niche? The secret is our ability to carry water. Every hunter-gatherer culture has invented its own water-carrying techniques, usually some kind of waterskin. This allowed humans to switch to perspiration-based cooling systems, which allowed them to run as long as they want.</p><p>And humans are consumate tool users. In some cases, we evolved in order to use tools better; our hands outclass those of any other ape in terms of finesse. In other cases, we devolved systems that were no longer necessary once tools took over. We are vastly weaker than any other ape. Heinrich describes a circus act of the 1940s where the ringmaster would challenge strong men in the audience to wrestle a juvenile chimpanzee. The chimpanzee was tied up, dressed in a mask that prevented it from biting, and wearing soft gloves that prevented it from scratching. No human ever lasted more than five seconds. Our common ancestor with other apes grew weaker and weaker as we became more and more reliant on artificial weapons to give us an advantage.</p><p><strong>III.</strong></p><p>But most of our differences from other apes are indeed in the brain. They’re just not necessarily where you would expect.</p><p>Tomasello et al tested human toddlers vs. apes on a series of traditional IQ type questions. The match-up was surprisingly fair; in areas like memory, logic, and spatial reasoning, the three species did about the same. But in ability to learn from another person, humans wiped the floor with the other two ape species:</p><p>&nbsp;</p><figure class=""image""><img src=""https://i1.wp.com/slatestarcodex.com/blog_images/heinrich_chimps.png?w=700""></figure><p>&nbsp;</p><p>Remember, Heinrich thinks culture accumulates through random mutation. Humans don’t have control over how culture gets generated. They have more control over how much of it gets transmitted to the next generation. If 100% gets transmitted, then as more and more mutations accumulate, the culture becomes better and better. If less than 100% gets transmitted, then at some point new culture gained and old culture lost fall into equilibrium, and your society stabilizes at some higher or lower technological level. This means that transmitting culture to the next generation is maybe the core human skill. The human brain is optimized to make this work as well as possible.</p><p>Human children are obsessed with learning things. And they don’t learn things randomly. There seem to be “biases in cultural learning”, ie slots in an infant’s mind that they know need to be filled with knowledge, and which they preferentially seek out the knowledge necessary to fill.</p><p>One slot is for language. Human children naturally listen to speech (as early as in the womb). They naturally prune the phonemes they are able to produce and distinguish to the ones in the local language. And they naturally figure out how to speak and understand what people are saying, even though learning a language is hard even for smart adults.</p><p>Another slot is for animals. In a world where megafauna has been relegated to zoos, we <i>still</i> teach children their ABCs with “L is for lion” and “B is for bear”, and children <i>still</i> read picture books about Mr. Frog and Mrs. Snake holding tea parties. Heinrich suggests that just as the young brain is hard-coded to want to learn language, so it is hard-coded to want to learn the local animal life (little boys’ vehicle obsession may be a weird outgrowth of this; buses and trains are the closest thing to local megafauna that most of them will encounter).</p><p>Another slot is for plants:</p><blockquote><p>To see this system in operation, let’s consider how infants respond to unfamiliar plants. Plants are loaded with prickly thorns, noxious oils, stinging nettles and dangerous toxins, all genetically evolved to prevent animals like us from messing with them. Given our species wide geographic range and diverse use of plants as foods, medicines and construction materials, we ought to be primed to both learn about plants and avoid their dangers. To explore this idea in the lab, the psychologists Annie Wertz and Karen Wynn first gave infants, who ranged in age from eight to eighteen months, an opportunity to touch novel plants (basil and parsley) and artifacts, including both novel objects and common ones, like wooden spoons and small lamps.</p><p>The results were striking. Regardless of age, many infants flatly refused to touch the plants at all. When they did touch them, they waited substantially longer than they did with the artifacts. By contrast, even with the novel objects, infants showed none of this reluctance. This suggests that well before one year of age infants can readily distinguish plants from other things, and are primed for caution with plants. But, how do they get past this conservative predisposition?</p><p>The answer is that infants keenly watch what other people do with plants, and are only inclined to touch or eat the plants that other people have touched or eaten. In fact, once they get the ‘go ahead’ via cultural learning, they are suddenly interested in eating plants. To explore this, Annie and Karen exposed infants to models who both picked fruit from plants and also picked fruit-like things from an artifact of similar size and shape to the plant. The models put both the fruit and the fruit-like things in their mouths. Next, the infants were given a choice to go for the fruit (picked from the plant) or the fruit-like things picked from the object. Over 75% of the time the infants went for the fruit, not the fruit-like things, since they’d gotten the ‘go ahead’ via cultural learning.</p><p>As a check, the infants were also exposed to models putting the fruit or fruit-like things behind their ears(not in their mouths). In this case, the infants went for the fruit or fruit-like things in equal measure. It seems that plants are most interesting if you can eat them, but only if you have some cultural learning cues that they aren’t toxic.</p><p>After Annie first told me about her work while I was visiting Yale in 2013, I went home to test it on my 6-month-old son, Josh. Josh seemed very likely to overturn Annie’s hard empirical work, since he immediately grasped anything you gave him and put it rapidly in his mouth. Comfortable in his mom’s arms, I first offered Josh a novel plastic cube. He delighted in grapping it and shoving it directly into his mouth, without any hesitation. Then, I offered him a sprig of arugula. He quickly grabbed it, but then paused, looked with curious uncertainty at it, and then slowly let it fall from his hand while turning to hug his mom.</p><p>It’s worth pointing out how rich the psychology is here. Not only do infants have to recognize that plants are different from objects of similar size, shape and color, but they need to create categories for types of plants, like basil and parsley, and distinguish ‘eating’ from just ‘touching’. It does them little good to code their observation of someone eating basil as ‘plants are good to eat’ since that might cause them to eat poisonous plants as well as basil. But, it also does them little good to narrowly code the observation as ‘that particular sprig of basil is good to eat’ since that particular sprig has just been eaten by the person they are watching. This another content bias in cultural learning.</p></blockquote><p>This ties into the more general phenomenon of figuring out what’s edible. Most Westerners learn insects aren’t edible; some Asians learn that they are. This feels deeper than just someone telling you insects aren’t edible and you believing them. When I was in Thailand, my guide offered me a giant cricket, telling me it was delicious. I believed him when he said it was safe to eat, I even believed him when he said it tasted good to him, but my conditioning won out – I didn’t eat the cricket. There seems to be some process where a child’s brain learns what is and isn’t locally edible, then hard-codes it against future change.</p><p>(Or so they say; I’ve never been able to eat shrimp either.)</p><p>Another slot is for gender roles. By now we’ve all heard the stories of progressives who try to raise their children without any exposure to gender. Their failure has sometimes been taken as evidence that gender is hard-coded. But it can’t be quite that simple: some modern gender roles, like girls = pink, are far from obvious or universal. Instead, it looks like children have a hard-coded slot that gender roles go into, work hard to figure out what the local gender roles are (even if their parents are trying to confuse them), then latch onto them and don’t let go.</p><p>In the Cultural Intelligence Hypothesis, humans live in obligate symbiosis with a culture. A brain without an associated culture is incomplete and not very useful. So the infant brain is adapted to seek out the important aspects of its local culture almost from birth and fill them into the appropriate slots in order to become whole.</p><p><strong>IV.</strong></p><p>The next part of the book discusses post-childhood learning. This plays an important role in hunter-gatherer tribes:</p><blockquote><p>While hunters reach their peak strength and speed in their twenties, individual hunting success does not peak until around age 30, because success depends more on know-how and refined skills than on physical prowess.</p></blockquote><p>This part of the book made most sense in the context of examples like the Inuit seal-hunting strategy which drove home just how complicated and difficult hunting-gathering was. Think less “Boy Scouts” and more “PhD”; a primitive tribesperson’s life requires mastery of various complicated technologies and skills. And the difference between “mediocre hunter” and “great hunter” can be the difference between high status (and good mating opportunities) and low status, or even between life and death. Hunter-gatherers really want to learn the essentials of their hunter-gatherer lifestyle, and learning it is really hard. Their heuristics are:</p><p><u>Learn from people who are good at things and/or widely-respected</u>. If you haven’t already read about the difference between dominance and prestige hierarchies, check out <a href=""https://meltingasphalt.com/social-status-down-the-rabbit-hole/"">Kevin Simler’s blog post</a> on the topic. People will fear and obey authority figures like kings and chieftains, but they give a different kind of respect (“prestige”) to people who seem good at things. And since it’s hard to figure out who’s good at things (can a non-musician who wants to start learning music tell the difference between a merely good performer and one of the world’s best?) most people use the heuristic of respecting the people who other people respect. Once you identify someone as respect-worthy, you strongly consider copying them in, well, everything:</p><blockquote><p>To understand prestige as a social phenomenon, it’s crucial to realize that it’s often difficult to figure out what precisely makes someone successful. In modern societies, the success of a star NBA basketball player might arise from his:</p><p>(1) intensive practice in the offseason<br>(2) sneaker preference<br>(3) sleep schedule<br>(4) pre-game prayer<br>(5) special vitamins<br>(6) taste for carrots</p><p>Any or all of these might increase his success. A naïve learner can’t tell all the causal links between an individual’s practices and his success. As a consequence, learners often copy their chosen models broadly across many domains. Of course, learners may place more weight on domains that for one reason or other seem more causally relevant to the model’s success. This copying often includes the model’s personal habits or styles as well as their goals and motivations, since these may be linked to their success. This “if in doubt, copy it” heuristic is one of the reasons why success in one domain converts to influence across a broad range of domains.</p><p>The immense range of celebrity endorsements in modern societies shows the power of prestige. For example, NBA star Lebron James, who went directly from High School to the pros, gets paid millions to endorse State Farm Insurance. Though a stunning basketball talent, it’s unclear why Mr. James is qualified to recommend insurance companies. Similarly, Michael Jordan famously wore Hanes underwear and apparently Tiger Woods drove Buicks. Beyonce’ drinks Pepsi (at least in commercials). What’s the connection between musical talent and sugary cola beverages?</p><p>Finally, while new medical findings and public educational campaigns only gradually influence women’s approach to preventive medicine, Angelina Jolie’s single OP-ED in the New York Times, describing her decision to get a preventive double mastectomy after learning she had the ‘faulty’ BRCA1 gene, flooded clinics from the U.K. to New Zealand with women seeking genetic screenings for breast cancer. Thus, an unwanted evolutionary side effect, prestige turns out to be worth millions, and represents a powerful and underutilized public health tool.</p></blockquote><p>Of course, this creates the risk of prestige cascades, where some irrelevant factor (Heinrich mentions being a reality show star) catapults someone to fame, everyone talks about them, and you end up with Muggeridge’s definition of a celebrity: someone famous for being famous.</p><p>Some of this makes more sense if you go back to the evolutionary roots, and imagine watching the best hunter in your tribe to see what his secret is, or being nice to him in the hopes that he’ll take you under his wing and teach you stuff.</p><p>(but if all this is true, shouldn’t public awareness campaigns that hire celebrity spokespeople be wild successes? Don’t they just as often fail, regardless of how famous a basketball player they can convince to lecture schoolchildren about how Winners Don’t Do Drugs?)</p><p><u>Learn from people who are like you</u>. If you are a man, it is probably a bad idea to learn fashion by observing women. If you are a servant, it is probably a bad idea to learn the rules of etiquette by observing how the king behaves. People are naturally inclined to learn from people more similar to themselves.</p><p>Heinrich ties this in to various studies showing that black students learn best from a black teacher, female students from a female teacher, et cetera.</p><p><u>Learn from old people</u>. Humans are almost unique in having menopause; most animals keep reproducing until they die in late middle-age. Why does evolution want humans to stick around without reproducing?</p><p>Because old people have already learned the local culture and can teach it to others. Heinrich asks us to throw out any personal experience we have of elders; we live in a rapidly-changing world where an old person is probably “behind the times”. But for most of history, change happened glacially slowly, and old people would have spent their entire lives accumulating relevant knowledge. Imagine a world where when a Silicon Valley programmer can’t figure out how to make his code run, he calls up his grandfather, who spent fifty years coding apps for Google and knows every programming language inside and out.</p><p>Sometimes important events only happen once in a generation. Heinrich tells the story of an Australian aboriginal tribe facing a massive drought. Nobody knew what to do except Paralji, the tribe’s oldest man, who had lived through the last massive drought and remembered where his own elders had told him to find the last-resort waterholes.</p><p>This same dynamic seems to play out even in other species:</p><blockquote><p>In 1993, a severe drought hit Tanzania, resulting in the death of 20% of the African elephant calves in a population of about 200. This population contained 21 different families, each of which was led by a single matriarch. The 21 elephant families were divided into 3 clans, and each clan shared the same territory during the wet season (so, they knew each other). Researchers studying these elephants have analyzed the survival of the calves and found that families led by older matriarchs suffered fewer deaths of their calves during this drought.</p><p>Moreover, two of the three elephant clans unexpectedly left the park during the drought, presumably in search of water, and both had much higher survival rates than the one clan that stayed behind. It happens that these severe droughts only hit about once every four to five decades, and the last one hit about 1960. After that, sadly, elephant poaching in the 1970’s killed off many of the elephants who would have been old enough in 1993 to recall the 1960 drought. However, it turns out that exactly one member of each of the two clans who left the park, and survived more effectively, were old enough to recall life in 1960. This suggests, that like Paralji in the Australian desert, they may have remembered what to do during a severe drought, and led their groups to the last water refuges. In the clan who stayed behind, the oldest member was born in 1960, and so was too young to have recalled the last major drought.</p><p>More generally, aging elephant matriarchs have a big impact on their families, as those led by older matriarchs do better at identifying and avoiding predators (lions and humans), avoiding internal conflicts and identifying the calls of their fellow elephants. For example, in one set of field experiments, researchers played lion roars from both male and female lions, and from either a single lion or a trio of lions. For elephants, male lions are much more dangerous than females, and of course, three lions are always worse than only one lion. All the elephants generally responded with more defensive preparations when they heard three lions vs. one. However, only the older matriarchs keenly recognized the increased dangers of male lions over female lions, and responded to the increased threat with elephant defensive maneuvers.</p></blockquote><p><strong>V.</strong></p><p>I was inspired to read <i>Secret</i> by <a href=""http://scholars-stage.blogspot.com/2018/08/tradition-is-smarter-than-you-are.html"">this review on Scholar’s Stage</a>. I hate to be unoriginal, but after reading the whole book, I agree that the three sections Tanner cites – on divination, on manioc, and on shark taboos – are by far the best and most fascinating.</p><p>On divination:</p><blockquote><p>When hunting caribou, Naskapi foragers in Labrador, Canada, had to decide where to go. Common sense might lead one to go where one had success before or to where friends or neighbors recently spotted caribou.</p><p>However, this situation is like [the <a href=""https://en.wikipedia.org/wiki/Matching_pennies"">Matching Pennies</a> game]. The caribou are mismatchers and the hunters are matchers. That is, hunters want to match the locations of caribou while caribou want to mismatch the hunters, to avoid being shot and eaten. If a hunter shows any bias to return to previous spots, where he or others have seen caribou, then the caribou can benefit (survive better) by avoiding those locations (where they have previously seen humans). Thus, the best hunting strategy requires randomizing.</p><p>Can cultural evolution compensate for our cognitive inadequacies? Traditionally, Naskapi hunters decided where to go to hunt using divination and believed that the shoulder bones of caribou could point the way to success. To start the ritual, the shoulder blade was heated over hot coals in a way that caused patterns of cracks and burnt spots to form. This patterning was then read as a kind of map, which was held in a pre-specified orientation. The cracking patterns were (probably) essentially random from the point of view of hunting locations, since the outcomes depended on myriad details about the bone, fire, ambient temperature, and heating process. Thus, these divination rituals may have provided a crude randomizing device that helped hunters avoid their own decision-making biases.</p><p>This is not some obscure, isolated practice, and other cases of divination provide more evidence. In Indonesia, the Kantus of Kalimantan use bird augury to select locations for their agricultural plots. Geographer Michael Dove argues that two factors will cause farmers to make plot placements that are too risky. First, Kantu ecological models contain the Gambler’s Fallacy, and lead them to expect floods to be less likely to occur in a specific location after a big flood in that location (which is not true). Second…Kantus pay attention to others’ success and copy the choices of successful households, meaning that if one of their neighbors has a good yield in an area one year, many other people will want to plant there in the next year. To reduce the risks posed by these cognitive and decision-making biases, Kantu rely on a system of bird augury that effectively randomizes their choices for locating garden plots, which helps them avoid catastrophic crop failures. Divination results depend not only on seeing a particular bird species in a particular location, but also on what type of call the bird makes (one type of call may be favorable, and another unfavorable).</p><p>The patterning of bird augury supports the view that this is a cultural adaptation. The system seems to have evolved and spread throughout this region since the 17th century when rice cultivation was introduced. This makes sense, since it is rice cultivation that is most positively influenced by randomizing garden locations. It’s possible that, with the introduction of rice, a few farmers began to use bird sightings as an indication of favorable garden sites. On-average, over a lifetime, these farmers would do better – be more successful – than farmers who relied on the Gambler’s Fallacy or on copying others’ immediate behavior. Whatever the process, within 400 years, the bird augury system spread throughout the agricultural populations of this Borneo region. Yet, it remains conspicuously missing or underdeveloped among local foraging groups and recent adopters of rice agriculture, as well as among populations in northern Borneo who rely on irrigation. So, bird augury has been systematically spreading in those regions where it’s most adaptive.</p></blockquote><p>Scott Aaronson has written about how easy it is to predict people trying to “be random”:</p><blockquote><p>In a class I taught at Berkeley, I did an experiment where I wrote a simple little program that would let people type either “f” or “d” and would predict which key they were going to push next. It’s actually very easy to write a program that will make the right prediction about 70% of the time. Most people don’t really know how to type randomly. They’ll have too many alternations and so on. There will be all sorts of patterns, so you just have to build some sort of probabilistic model. Even a very crude one will do well. I couldn’t even beat my own program, knowing exactly how it worked. I challenged people to try this and the program was getting between 70% and 80% prediction rates. Then, we found one student that the program predicted exactly 50% of the time. We asked him what his secret was and he responded that he “just used his free will.”</p></blockquote><p>But being genuinely random is important in pursuing mixed game theoretic strategies. Heinrich’s view is that divination solved this problem effectively.</p><p>I’m reminded of the Romans using augury to decide when and where to attack. This always struck me as crazy; generals are going to risk the lives of thousands of soldiers because they saw a weird bird earlier that morning? But war is a classic example of when a random strategy can be useful. If you’re deciding whether to attack the enemy’s right vs. left flank, it’s important that the enemy can’t predict your decision and send his best defenders there. If you’re generally predictable – and Scott Aaronson says you are – then outsourcing your decision to weird birds might be the best way to go.</p><p>And then there’s manioc. This is a tuber native to the Americas. It contains cyanide, and if you eat too much of it, you get cyanide poisoning. From Heinrich:</p><blockquote><p>In the Americas, where manioc was first domesticated, societies who have relied on bitter varieties for thousands of years show no evidence of chronic cyanide poisoning. In the Colombian Amazon, for example, indigenous Tukanoans use a multistep, multiday processing technique that involves scraping, grating, and finally washing the roots in order to separate the fiber, starch, and liquid. Once separated, the liquid is boiled into a beverage, but the fiber and starch must then sit for two more days, when they can then be baked and eaten. Figure 7.1 shows the percentage of cyanogenic content in the liquid, fiber, and starch remaining through each major step in this processing.</p><p>Such processing techniques are crucial for living in many parts of Amazonia, where other crops are difficult to cultivate and often unproductive. However, despite their utility, one person would have a difficult time figuring out the detoxification technique. Consider the situation from the point of view of the children and adolescents who are learning the techniques. They would have rarely, if ever, seen anyone get cyanide poisoning, because the techniques work. And even if the processing was ineffective, such that cases of goiter (swollen necks) or neurological problems were common, it would still be hard to recognize the link between these chronic health issues and eating manioc. Most people would have eaten manioc for years with no apparent effects. Low cyanogenic varieties are typically boiled, but boiling alone is insufficient to prevent the chronic conditions for bitter varieties. Boiling does, however, remove or reduce the bitter taste and prevent the acute symptoms (e.g., diarrhea, stomach troubles, and vomiting).</p><p>So, if one did the common-sense thing and just boiled the high-cyanogenic manioc, everything would seem fine. Since the multistep task of processing manioc is long, arduous, and boring, sticking with it is certainly non-intuitive. Tukanoan women spend about a quarter of their day detoxifying manioc, so this is a costly technique in the short term. Now consider what might result if a self-reliant Tukanoan mother decided to drop any seemingly unnecessary steps from the processing of her bitter manioc. She might critically examine the procedure handed down to her from earlier generations and conclude that the goal of the procedure is to remove the bitter taste. She might then experiment with alternative procedures by dropping some of the more labor-intensive or time-consuming steps. She’d find that with a shorter and much less labor-intensive process, she could remove the bitter taste. Adopting this easier protocol, she would have more time for other activities, like caring for her children. Of course, years or decades later her family would begin to develop the symptoms of chronic cyanide poisoning.</p><p>Thus, the unwillingness of this mother to take on faith the practices handed down to her from earlier generations would result in sickness and early death for members of her family. Individual learning does not pay here, and intuitions are misleading. The problem is that the steps in this procedure are causally opaque—an individual cannot readily infer their functions, interrelationships, or importance. The causal opacity of many cultural adaptations had a big impact on our psychology.</p><p>Wait. Maybe I’m wrong about manioc processing. Perhaps it’s actually rather easy to individually figure out the detoxification steps for manioc? Fortunately, history has provided a test case. At the beginning of the seventeenth century, the Portuguese transported manioc from South America to West Africa for the first time. They did not, however, transport the age-old indigenous processing protocols or the underlying commitment to using those techniques. Because it is easy to plant and provides high yields in infertile or drought-prone areas, manioc spread rapidly across Africa and became a staple food for many populations. The processing techniques, however, were not readily or consistently regenerated. Even after hundreds of years, chronic cyanide poisoning remains a serious health problem in Africa. Detailed studies of local preparation techniques show that high levels of cyanide often remain and that many individuals carry low levels of cyanide in their blood or urine, which haven’t yet manifested in symptoms. In some places, there’s no processing at all, or sometimes the processing actually increases the cyanogenic content. On the positive side, some African groups have in fact culturally evolved effective processing techniques, but these techniques are spreading only slowly.</p></blockquote><p>Rationalists always wonder: how come people aren’t more rational? How come you can prove a thousand times, using Facts and Logic, that something is stupid, and yet people will still keep doing it?</p><p>Heinrich hints at an answer: for basically all of history, using reason would get you killed.</p><p>A reasonable person would have figured out there was no way for oracle-bones to accurately predict the future. They would have abandoned divination, failed at hunting, and maybe died of starvation.</p><p>A reasonable person would have asked why everyone was wasting so much time preparing manioc. When told “Because that’s how we’ve always done it”, they would have been unsatisfied with that answer. They would have done some experiments, and found that a simpler process of boiling it worked just as well. They would have saved lots of time, maybe converted all their friends to the new and easier method. Twenty years later, they would have gotten sick and died, in a way so causally distant from their decision to change manioc processing methods that nobody would ever have been able to link the two together.</p><p>Heinrich discusses pregnancy taboos in Fiji; pregnant women are banned from eating sharks. Sure enough, these sharks contain chemicals that can cause birth defects. The women didn’t really know why they weren’t eating the sharks, but when anthropologists demanded a reason, they eventually decided it was because their babies would be born with shark skin rather than human skin. As explanations go, this leaves a lot to be desired. How come you can still eat other fish? Aren’t you worried your kids will have scales? Doesn’t the slightest familiarity with biology prove this mechanism is garbage? But if some smart independent-minded iconoclastic Fijian girl figured any of this out, she would break the taboo and her child would have birth defects.</p><p>In giving humans reason at all, evolution took a huge risk. Surely it must have wished there was some other way, some path that made us big-brained enough to understand tradition, but not big-brained enough to question it. Maybe it searched for a mind design like that and couldn’t find one. So it was left with this ticking time-bomb, this ape that was constantly going to be able to convince itself of hare-brained and probably-fatal ideas.</p><p>Here, too, culture came to the rescue. One of the most important parts of any culture – more important than the techniques for hunting seals, more important than the techniques for processing tubers – is techniques for making sure nobody ever questions tradition. Like the belief that anyone who doesn’t conform is probably a witch who should be cast out lest they bring destruction upon everybody. Or the belief in a God who has commanded certain specific weird dietary restrictions, and will torture you forever if you disagree. Or the fairy tales where the prince asks a wizard for help, and the wizard says “You may have everything you wish forever, but you must never nod your head at a badger”, and then one day the prince nods his head at a badger, and his whole empire collapses into dust, and the moral of the story is that you should always obey weird advice you don’t understand.</p><p>There’s a monster at the end of this book. Humans evolved to transmit culture with high fidelity. And one of the biggest threats to transmitting culture with high fidelity was Reason. Our ancestors lived in Epistemic Hell, where they had to constantly rely on causally opaque processes with justifications that couldn’t possibly be true, and if they ever questioned them then they might die. Historically, Reason has been the villain of the human narrative, a corrosive force that tempts people away from adaptive behavior towards choices that “sounded good at the time”.</p><p>Why are people so bad at reasoning? For the same reason they’re so bad at letting poisonous spiders walk all over their face without freaking out. Both “skills” are really bad ideas, most of the people who tried them died in the process, so evolution removed those genes from the population, and successful cultures stigmatized them enough to give people an internalized fear of even trying.</p><p><strong>VI.</strong></p><p>This book belongs alongside <a href=""https://slatestarcodex.com/2017/03/16/book-review-seeing-like-a-state/""><i>Seeing Like A State</i></a> and the <a href=""https://en.wikipedia.org/wiki/Wikipedia:Chesterton%27s_fence"">works of G.K. Chesterton</a> as attempts to justify tradition, and to argue for organically-evolved institutions over top-down planning. What unique contribution does it make to this canon?</p><p>First, a lot more specifically anthropological / paleoanthropological rigor than the other two.</p><p>Second, a much crisper focus: Chesterton had only the fuzziest idea that he was writing about cultural evolution, and Scott was only a little clearer. I think Heinrich is the only one of the three to use the term, and once you hear it, it’s obviously the right framing.</p><p>Third, a sense of how traditions contain the meta-tradition of defending themselves against Reason, and a sense for why this is necessary.</p><p>And fourth, maybe we’re not at the point where we really want unique contributions yet. Maybe we’re still at the point where we have to have this hammered in by more and more examples. The temptation is always to say “Ah, yes, a few simple things like taboos against eating poisonous plants may be relics of cultural evolution, but obviously by now we’re at the point where we know which traditions are important vs. random looniness, and we can rationally stick to the important ones while throwing out the garbage.” And then somebody points out to you that <i>actually</i> divination using oracle bones was one of the important traditions, and if you thought you knew better than that and tried to throw it out, your civilization would falter.</p><p>Maybe we just need to keep reading more similarly-themed books until this point really sinks in, and we get properly worried.</p><p>&nbsp;</p><figure class=""image""><img src=""//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=0691178437&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=slatestarcode-20&amp;language=en_US""></figure><figure class=""image""><img src=""https://ir-na.amazon-adsystem.com/e/ir?t=slatestarcode-20&amp;language=en_US&amp;l=li2&amp;o=1&amp;a=0691178437""></figure>",Yvain,scottalexander,Scott Alexander,
bL3c94HkgxLqiSpzK,All knowledge is circularly justified,all-knowledge-is-circularly-justified,https://www.lesswrong.com/posts/bL3c94HkgxLqiSpzK/all-knowledge-is-circularly-justified,2019-06-04T22:39:20.766Z,10,8,19,False,False,,"<p>Many philosophers have tried to find the foundations of our knowledge, but why do we think there are any? The framing of foundations implies a separate bottom layer of knowledge from which everything is built up. And while this is undoubtedly a useful model in many contexts, why should we believe in this as the complete and literal truth as opposed to merely a simplification?</p><p>Consider:</p><p>1) If we dig deep enough into any of our truth claims, we'll eventually reach a point at which they are justified by intuition</p><p>2) The reliability of intuition or various intuitions is not something that is merely taken as basic or for granted, but can instead be justified somewhat by arguments from experience and evolutionary arguments.</p><p>3) However both empirical verification and evolutionary arguments themselves both rely on assumptions that are justified by intuition</p><p>This is circular, but is this necessarily a problem? If your choice is a circular justification or eventually hitting a level with no justification, then the circular justification suddenly starts looking pretty attractive. In other words, we have to be comparative and consider what the alternative is to circular epistemology and not just consider it in isolation.</p><p>Is this important? It seems to depend on context. For applied rationality, not so much. But I would like to suggest that the more philosophical areas of the rationalist project would look quite different if they were built upon a circular epistemology.</p>",Chris_Leong,chris_leong,Chris_Leong,
BNWmzdpcynC3rpFwf,"Seeing the Matrix, Switching Abstractions, and Missing Moods",seeing-the-matrix-switching-abstractions-and-missing-moods,https://www.lesswrong.com/posts/BNWmzdpcynC3rpFwf/seeing-the-matrix-switching-abstractions-and-missing-moods,2019-06-04T21:08:28.709Z,36,25,3,False,False,,"<p><em>Epistemic Status: Poetry, but also, True Story</em></p><p>For seven years, I worked in a supermarket bakery.</p><p>The bakery was quite a nice place to work. I got a good mix of physical exercise (everything I know about basketball I learned from tossing heavy boxes of bread up several feet such that they landed <em>just</em> perfectly on top of each other). </p><p>I learned skills, I decorated cakes. I had an excellent manager, who led by example, who was funny, who was stern when she needed to be but almost never needed to be because people just <em>wanted</em> to do the right thing for her.</p><p>One day, we hired a person I found really annoying, who I&#x27;ll call Debbie. </p><p>Debbie talked a lot, and she had a really grating, whiny, high pitched voice. And at first I tried to engage with her cheerfully, then I tried engaging with her politely, and then I tried to avoid her because she just <em>wouldn&#x27;t stop talking no matter what about inane things that nonetheless were just complicated enough that I felt pressure to think about how to respond.</em></p><p>Debbie was probably a decent person who didn&#x27;t deserve my ire. Nonetheless, my ire she had.</p><p>Avoiding Debbie wasn&#x27;t really an option because the bakery wasn&#x27;t that big. A few weeks of annoyance passed. And one day Debbie was telling some story about her kids or sister or something that was probably a reasonably fine story but I just couldn&#x27;t stand it any more and —</p><p>— and —</p><p>...and then I literally felt my brain make a slight &quot;czhzk&quot; sound. And Debbie&#x27;s voice just of faded into the background. I heard all the other sounds in the supermarket – the customers talking, the air conditioners humming, the beep of distant item-scanners, the sliding of the automating doors. And Debbie&#x27;s voice, one mechanical physical process among many.</p><p>And it felt like Neo, at the end of the Matrix, where he can suddenly see the Code, and he can also see Agent Smith. And then a flashback to earlier in the movie, when Neo looks upon the raw code for the first time and can&#x27;t make heads of tails of it. A fellow crewmember says &quot;Yeah, I don&#x27;t even see the code anymore. I just parse it automatically. My brain sees &#x27;blond&#x27;, &#x27;brunette&#x27;, &#x27;redhead&#x27;.&quot;</p><p>And suddenly, I could effortless switch back and forth between seeing Debbie, the human being with hopes and dreams and kids and sisters and stories she wanted to tell and coworkers she wanted to vibe with... and &lt;Debbie&gt;, the collection of atoms that were just inevitably, deterministically, going to keep on making high pitched noises no matter what I said or did.</p><p>And somehow... this made it simultaneously way easier to <em>both</em> ignore Debbie when I didn&#x27;t feel like dealing with her – muttering automatic &#x27;uh huhs&#x27; and &#x27;yeah&#x27; and nodding as appropriate – as well as remembering she was a Person, whose experiences were valuable according my outlook on Personhood, and sometimes actually talking with her and engaging and actually caring about her kids or sister.</p><p>Now one lesson, a simpler lesson (but which I didn&#x27;t actually learn till much later), is that nerds tend to think conversation needs to include lots of actually understanding what a person is saying and forming reasonable beliefs about the conversation topic and saying those things in a logical flow. But, a lot of what&#x27;s going on is <a href=""https://www.lesswrong.com/posts/gs8bZCmaWqDaus7Dr/levels-of-communication"">socializing</a>,  <a href=""https://www.lesswrong.com/posts/huRxRzwcvwTzvtEPY/handshakes-hi-and-what-s-new-what-s-going-on-with-small-talk"">smalltalk</a>, and sometimes <a href=""https://www.lesswrong.com/posts/jXHwYYnqynhB3TAsc/what-vibing-feels-like"">vibing</a>. I found Debbie annoying in part because I felt obligated to spend a fair amount of cognitive effort responding to her stories. I <em>do</em> think she cared at least somewhat that I actually listened. But I don&#x27;t think she intended me to spend as much cognitive effort as I was.</p><p>The weirder, deeper, experiential lesson is to <strong>be able to see the Matrix.<em> </em></strong>I don&#x27;t expect you to gain that ability by reading this blogpost – I think I needed a weird combination of circumstances, and philosophical-beliefs-at-a-certain-stage-of-development, in order for things to click into place and my brain to go &quot;czhzk&quot;.</p><p>But I think it&#x27;s useful to be able to refer to this skill. My guess is it&#x27;s not that hard to conceptually understand (at least for LessWrong folk). </p><p>And I think it becomes relevant when engaging with important moral questions, that require us <em>both </em>to have the ability to see a million deaths, or a million lives, and process them as a statistic that is weighed against other statistics, equations to be abstracted and simplified until you find the answer....</p><p>...<em>and</em> to remember the felt sense, that each of those lives and deaths are filled with meaning. And you can only process one or two of them at a time before your brain breaks, but that their meaning depends on the ability to look at them through a layer of abstraction that <em>doesn&#x27;t </em>lend itself well to<em> </em>linear logic or calculations.</p><p>Sometimes you need to simplify part or all of those rich-inner-lives away to be able to think, or communicate clearly about the equations.</p><p>I think there&#x27;s something like <a href=""https://theunitofcaring.tumblr.com/post/159880144956/im-boggling-trying-and-failing-to-understand-how"">missing moods</a> that go on sometimes, where one person is trying to have a conversation focusing on a particular abstraction, and another person is tuned into another abstraction, and they both share most of the same beliefs and values but at that particular moment they&#x27;re trying to talk about different things and frustrated that the other person doesn&#x27;t seem to care.</p><p>Sometimes you need to abstract away, not only the rich inner life of a person (or millions of people), but entire swaths of the cold mathematics too. Or, sometimes you might want to focus on rich-inner-life, but tuned into a particular subset – the part of the rich-inner-life that cares about agency and longterms goals, or the rich-inner-life that cares about the energy in the room at the moment. </p><p>Or, you might care about interpersonal rich-outer-life — the layers of meaning and personhood that apply to relationships and groups over individuals or masses. Two individuals might both be having individually good experiences but something about their relationship seems stagnant or toxic.</p><p>Reality is rich with detail and it seems quite common to need to tune into some of that detail, and necessarily tune out others. </p><p>I wrote this post after a couple recent moments wherein I seemed to focused on a different slice of reality than my conversation partner(s). I&#x27;m not sure whether this particular blogpost would have helped, but it seemed like a useful handle for how I personally relate to this sort of disconnect. I&#x27;d like to be able to briefly refer to this meta-concept, and then figuring out which moods are actually missing, and &quot;get on the same page&quot; as quickly as possible without interrupting the conversation.</p><p>I <em>don&#x27;t </em>necessarily see all the same rich-detail that you do, and there might be times when I literally don&#x27;t understand or don&#x27;t care about the particular lens you are trying to optimize reality through. But I think, most of the time, I do see it, it just might not have been what I was focused on at the moment.</p>",Raemon,raemon,Raemon,
Q9hDFkvCSwi6cwPGy,How is Solomonoff induction calculated in practice?,how-is-solomonoff-induction-calculated-in-practice-1,https://www.lesswrong.com/posts/Q9hDFkvCSwi6cwPGy/how-is-solomonoff-induction-calculated-in-practice-1,2019-06-04T10:11:37.310Z,33,8,13,False,True,,"<p>Solomonoff induction is generally given as the correct way to penalise more complex hypotheses when calculating priors. A great introduction can be found <a href=""https://www.lesswrong.com/posts/Kyc5dFDzBg4WccrbK/an-intuitive-explanation-of-solomonoff-induction"">here</a>.</p><p>My question is, how is this actually calculated in practice?</p><p>As an example, say I have 2 hypotheses:</p><p>A. The probability distribution of the output is given by the same normal distribution for all inputs, with mean <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mu""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em;"">μ</span></span></span></span></span></span> and standard deviation <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\sigma""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;"">σ</span></span></span></span></span></span>.</p><p>B.  The probability distribution of the output is given by a normal distribution depending on an input <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> with mean <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\mu_0+mx""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em;"">μ</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">m</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> and standard deviation <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\sigma""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;"">σ</span></span></span></span></span></span>. </p><p>It is clear that hypothesis B is more complex (using an additional input [<span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>], having an additional parameter [<span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""m""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">m</span></span></span></span></span></span>] and requiring 2 additional operations to calculate) but how does one calculate the actual penalty that B should be given vs A?</p><br/>",Bucky,bucky,Bucky,
woWmhqrwPYmNwtLqS,Agents dissolved in coffee,agents-dissolved-in-coffee-1,https://www.lesswrong.com/posts/woWmhqrwPYmNwtLqS/agents-dissolved-in-coffee-1,2019-06-04T08:22:04.665Z,4,2,5,False,False,,"<html><head></head><body><h2>Bottom line</h2>
<p>When thinking about embedded agency it might be helpful to drop the notion of
‘agency’ and ‘agents’ sometimes, because it might be confusing or underdefined.
Instead one could think of processes running according to the laws of physics.
Or of algorithms running on a stack of interpreters running on the hardware of
the universe.</p>
<p>In addition (or as a corollary) to an alternative way of thinking about agents,
you will also read about an alternative way of thinking about yourself.</p>
<h2>Background</h2>
<p>The following is mostly a stream of thought that went through my head after I
drank a cup of strong milk coffee and sat down reading <a href=""https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version"">Embedded
Agency</a>.
I consume caffeine only twice a week. So when I do, it takes my thinking to new
places. (Or maybe it's because the instant coffee powder that I use expired
in March 2012.)</p>
<h2>Start</h2>
<p>My thoughts kick off at this paragraph of <a href=""https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version"">Embedded
Agency</a>:</p>
<blockquote>
<p>In addition to hazards in her external environment, Emmy is going to have to
worry about threats coming from within. While optimizing, Emmy might spin up
other optimizers as subroutines, either intentionally or unintentionally.
These subsystems can cause problems if they get too powerful and are unaligned
with Emmy’s goals. Emmy must figure out how to reason without spinning up
intelligent subsystems, or otherwise figure out how to keep them weak,
contained, or aligned fully with her goals.</p>
</blockquote>
<p>This is our alignment problem repeated in Emmy. In other words, if we solve the
alignment problem, it is also solved in Emmy and vice versa. If we view the
machines that we want to run our AI on as part of ourselves, we're the same as
Emmy.</p>
<p>We are a low-capability Emmy. We are partially and often unconsciously solving
the embedded agency subproblems using heuristics. Some of which we know, some of
which we don't know. As we try to add to our capabilities, we might run into the
limitations of those heuristics. Or not necessarily the limitations yet. We
don't even know how exactly they work and how to implement them on computers.
Computers are our current tool of choice to overcome the physical, biological
and psychological limits of our brains.</p>
<p>Another way of adding to our capabilities is amplifying them using
<a href=""https://www.manager-tools.com"">organizations</a>. Then we have a composite (cf.
the <a href=""https://en.wikipedia.org/wiki/Composite_pattern"">software design pattern</a>):
groups of agents can be viewed as one agent. An agent together with a tool is
one agent. An agent interacting with part of the environment is one agent.</p>
<p>In the other direction an agent with an arm amputated (ie. without that arm) is
still an agent. How much can we remove and still have an agent? Here we run into
the thing with an agent being part of the environment, made out of <a href=""https://res.cloudinary.com/dq3pms5lt/image/upload/q_auto/v1540779915/Demski%20Embedded%20Intro/Embedded_Intro_11.jpg"">non-agentic
pieces</a>.</p>
<p>How can we talk about agency at all? We're just part of some giant physical
reaction. And we're a part that happens to be working on getting bigger stuff
done, which is a human notion. From this point of view the earth is just a place
of the universe where some quirky things (eg. people selling postcards showing
crocodiles wearing swim goggles) are happening according to the laws of physics.</p>
<p>Fundamentally we're nothing else than a <a href=""http://titaniumphysicists.brachiolopemedia.com/"">supernova or a neutron star making its
pasta shapes or a quasar sending out huge amounts of
radiation</a>. We're just another
physical process in the universe. (Duh.) When the aliens come visit (and maybe
kill) us we have a chance to observe (albeit for a short time) the intermediate
state of a physical process similar to the one that has been going on on earth.</p>
<p>You could take this as a cue to lean back and see how it all rolls out. You
could also be worried that you might not be able to maintain your laid-back
composure when you fall into poverty because you didn't work hard, or when your
loved ones are raped and killed in a war, because you didn't help the world stay
stable and keep peace (mostly).</p>
<p>Or you could lean forward and join those who are working on getting bigger stuff
done.</p>
<p>Where does this illusion of choice come from, anyway? Is this the necessary
ingredient for an agent? I don't think so, because an agent need not be
conscious, which is a requirement for having illusions, I guess. (Aside: how do
we deal with consciousness? Is it an incidental feature? Or is it necessary/very
helpful for physical processes that have the grand and intricate results that we
want?)</p>
<p>Is it helpful to talk about agency at all, then? In the end an aligned embedded
agent is just another process running according to the laws of physics that
hopefully has the results we want to have. We don't talk about agency when a
thermite charge welds two pieces of rail together. We do talk about agency when
we see humans playing tennis. Where is the boundary and what is the benefit of
drawing one?</p>
<p>So I've gotten rid of agency. We only have processes running according to the
laws of physics, or alternatively, algorithms running on a stack of interpreters
running on the hardware of the universe. Does this help with thinking about the
embedded subproblems? I don't know yet, but I will keep it in mind. Then the question
is: how do we kick off the kind of processes that will have the results that we
want?</p>
<h2>Question prompts</h2>
<ul>
<li>Is this way of looking at the world new to you?</li>
<li>Did it help you in thinking about embedded agency?</li>
<li>Where am I using wrong terminology?</li>
<li>Where is my thinking misguided or wrong?</li>
</ul>
</body></html>",rmoehn,rmoehn,rmoehn,
pL56xPoniLvtMDQ4J,The Inner Alignment Problem,the-inner-alignment-problem,https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem,2019-06-04T01:20:35.538Z,105,41,17,False,False,,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p><em>This is the third of five posts in the <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">Risks from Learned Optimization Sequence</a> based on the paper “<a href=""https://arxiv.org/abs/1906.01820"">Risks from Learned Optimization in Advanced Machine Learning Systems</a>” by Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Each post in the sequence corresponds to a different section of the paper.</em></p>
<p>&nbsp;</p>
<p>In this post, we outline reasons to think that a mesa-optimizer may not optimize the same objective function as its base optimizer. Machine learning practitioners have direct control over the base objective function—either by specifying the loss function directly or training a model for it—but cannot directly specify the mesa-objective developed by a mesa-optimizer. We refer to this problem of aligning mesa-optimizers with the base objective as the inner alignment problem. This is distinct from the outer alignment problem, which is the traditional problem of ensuring that the base objective captures the intended goal of the programmers.</p>
<p>Current machine learning methods select learned algorithms by empirically evaluating their performance on a set of training data according to the base objective function. Thus, ML base optimizers select mesa-optimizers according to the output they produce rather than directly selecting for a particular mesa-objective. Moreover, the selected mesa-optimizer's policy only has to perform well (as scored by the base objective) on the training data. If we adopt the assumption that the mesa-optimizer computes an optimal policy given its objective function, then we can summarize the relationship between the base and mesa- objectives as follows:<a href=""https://intelligence.org/learned-optimization#bibliography"">(17)</a>
<span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""\begin{align*}
  \theta^* &amp;= \text{argmax}_\theta~ \mathbb E(O_{\text{base}}(\pi_\theta)),~ \text{where} \\
  \pi_\theta &amp;= \text{argmax}_\pi~ \mathbb E(O_{\text{mesa}}(\pi \,|\, \theta))
\end{align*}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mtable"" style=""vertical-align: -0.991em; padding: 0px 0.167em;""><span class=""mjx-table""><span class=""mjx-mtr"" style=""height: 1.241em;""><span class=""mjx-mtd"" style=""padding: 0px 0px 0px 0px; text-align: right; width: 0.937em;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">θ</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.298em;"">∗</span></span></span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0px 0px 0px 0px; text-align: left; width: 13.754em;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mi""></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">argmax</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">θ</span></span></span></span><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.291em; padding-bottom: 0.372em;"">&nbsp;</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">θ</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mtext MJXc-space1""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.291em; padding-bottom: 0.372em;"">&nbsp;</span></span><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">where</span></span><span class=""mjx-strut""></span></span></span></span><span class=""mjx-mtr"" style=""height: 1.242em;""><span class=""mjx-mtd"" style=""padding: 0.15em 0px 0px 0px; text-align: right;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">θ</span></span></span></span><span class=""mjx-strut""></span></span></span><span class=""mjx-mtd"" style=""padding: 0.15em 0px 0px 0px; text-align: left;""><span class=""mjx-mrow"" style=""margin-top: -0.2em;""><span class=""mjx-mi""></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">argmax</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span></span><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.291em; padding-bottom: 0.372em;"">&nbsp;</span></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-ams-R"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">E</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span><span class=""mjx-mspace"" style=""width: 0.167em; height: 0px;""></span><span class=""mjx-texatom""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">|</span></span></span></span><span class=""mjx-mspace"" style=""width: 0.167em; height: 0px;""></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">θ</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-strut""></span></span></span></span></span></span></span></span></span></span>
That is, the base optimizer maximizes its objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> by choosing a mesa-optimizer with parameterization <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\theta""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">θ</span></span></span></span></span></span> based on the mesa-optimizer's policy <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi_\theta""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">θ</span></span></span></span></span></span></span></span>, but not based on the objective function <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> that the mesa-optimizer uses to compute this policy. Depending on the base optimizer, we will think of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> as the negative of the loss, the future discounted reward, or simply some fitness function by which learned algorithms are being selected.</p>
<p>An interesting approach to analyzing this connection is presented in Ibarz et al, where empirical samples of the true reward and a learned reward on the same trajectories are used to create a scatter-plot visualization of the alignment between the two.<a href=""https://intelligence.org/learned-optimization#bibliography"">(18)</a> The assumption in that work is that a monotonic relationship between the learned reward and true reward indicates alignment, whereas deviations from that suggest misalignment. Building on this sort of research, better theoretical measures of alignment might someday allow us to speak concretely in terms of provable guarantees about the extent to which a mesa-optimizer is aligned with the base optimizer that created it.</p>
<p>&nbsp;</p>
<h2>3.1. Pseudo-alignment</h2>
<p>There is currently no complete theory of the factors that affect whether a mesa-optimizer will be pseudo-aligned—that is, whether it will appear aligned on the training data, while actually optimizing for something other than the base objective. Nevertheless, we outline a basic classification of ways in which a mesa-optimizer could be pseudo-aligned:</p>
<ol>
<li><strong>Proxy alignment,</strong></li>
<li><strong>Approximate alignment,</strong> and</li>
<li><strong>Suboptimality alignment.</strong></li>
</ol>
<p><strong>Proxy alignment.</strong> The basic idea of <em>proxy alignment</em> is that a mesa-optimizer can learn to optimize for some proxy of the base objective instead of the base objective itself. We'll start by considering two special cases of proxy alignment: <em>side-effect alignment</em> and <em>instrumental alignment.</em></p>
<p>First, a mesa-optimizer is <em>side-effect aligned</em> if optimizing for the mesa-objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> has the direct causal result of increasing the base objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> in the training distribution, and thus when the mesa-optimizer optimizes <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> it results in an increase in <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span>. For an example of side-effect alignment, suppose that we are training a cleaning robot. Consider a robot that optimizes the number of times it has swept a dusty floor. Sweeping a floor causes the floor to be cleaned, so this robot would be given a good score by the base optimizer. However, if during deployment it is offered a way to make the floor dusty again after cleaning it (e.g. by scattering the dust it swept up back onto the floor), the robot will take it, as it can then continue sweeping dusty floors.</p>
<p>Second, a mesa-optimizer is <em>instrumentally aligned</em> if optimizing for the base objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> has the direct causal result of increasing the mesa-objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> in the training distribution, and thus the mesa-optimizer optimizes <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> as an instrumental goal for the purpose of increasing <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span>. For an example of instrumental alignment, suppose again that we are training a cleaning robot. Consider a robot that optimizes the amount of dust in the vacuum cleaner. Suppose that in the training distribution the easiest way to get dust into the vacuum cleaner is to vacuum the dust on the floor. It would then do a good job of cleaning in the training distribution and would be given a good score by the base optimizer. However, if during deployment the robot came across a more effective way to acquire dust—such as by vacuuming the soil in a potted plant—then it would no longer exhibit the desired behavior.</p>
<p>We propose that it is possible to understand the general interaction between side-effect and instrumental alignment using causal graphs, which leads to our general notion of proxy alignment.</p>
<p>Suppose we model a task as a causal graph with nodes for all possible attributes of that task and arrows between nodes for all possible relationships between those attributes. Then we can also think of the mesa-objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> and the base objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> as nodes in this graph. For <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> to be pseudo-aligned, there must exist some node <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""X""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;"">X</span></span></span></span></span></span> such that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""X""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;"">X</span></span></span></span></span></span> is an ancestor of both <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> in the training distribution, and such that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> increase with <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""X""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;"">X</span></span></span></span></span></span>. If <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""X = O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;"">X</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span>, this is side-effect alignment, and if <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""X = O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;"">X</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span>, this is instrumental alignment.</p>
<p>This represents the most generalized form of a relationship between <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> that can contribute to pseudo-alignment. Specifically, consider the causal graph given in figure 3.1. A mesa-optimizer with mesa-objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> will decide to optimize <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""X""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;"">X</span></span></span></span></span></span> as an instrumental goal of optimizing <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span>, since <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""X""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;"">X</span></span></span></span></span></span> increases <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span>. This will then result in <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> increasing, since optimizing for <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""X""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;"">X</span></span></span></span></span></span> has the side-effect of increasing <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span>. Thus, in the general case, side-effect and instrumental alignment can work together to contribute to pseudo-alignment over the training distribution, which is the general case of proxy alignment.</p>
<p><img src=""https://i.imgur.com/xC5RpjK.jpg"" alt=""""></p>
<p><strong>Figure 3.1.</strong> <em>A causal diagram of the training environment for the different types of proxy alignment. The diagrams represent, from top to bottom, side-effect alignment (top), instrumental alignment (middle), and general proxy alignment (bottom). The arrows represent positive causal relationships—that is, cases where an increase in the parent causes an increase in the child.</em></p>
<p><strong>Approximate alignment.</strong> A mesa-optimizer is <em>approximately aligned</em> if the mesa-objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> and the base objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> are approximately the same function up to some degree of approximation error related to the fact that the mesa-objective has to be represented inside the mesa-optimizer rather than being directly programmed by humans. For example, suppose you task a neural network with optimizing for some base objective that is impossible to perfectly represent in the neural network itself. Even if you get a mesa-optimizer that is as aligned as possible, it still will not be perfectly robustly aligned in this scenario, since there will have to be some degree of approximation error between its internal representation of the base objective and the actual base objective.</p>
<p><strong>Suboptimality alignment.</strong> A mesa-optimizer is <em>suboptimality aligned</em> if some deficiency, error, or limitation in its optimization process causes it to exhibit aligned behavior on the training distribution. This could be due to computational constraints, unsound reasoning, a lack of information, irrational decision procedures, or any other defect in the mesa-optimizer's reasoning process. Importantly, we are not referring to a situation where the mesa-optimizer is robustly aligned but nonetheless makes mistakes leading to bad outcomes on the base objective. Rather, suboptimality alignment refers to the situation where the mesa-optimizer is misaligned but nevertheless performs <em>well</em> on the base objective, precisely because it has been selected to make mistakes that lead to good outcomes on the base objective.</p>
<p>For an example of suboptimality alignment, consider a cleaning robot with a mesa-objective of minimizing the total amount of stuff in existence. If this robot has the mistaken belief that the dirt it cleans is completely destroyed, then it may be useful for cleaning the room despite doing so not actually helping it succeed at its objective. This robot will be observed to be a good optimizer of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> and hence be given a good score by the base optimizer. However, if during deployment the robot is able to improve its world model, it will stop exhibiting the desired behavior.</p>
<p>As another, perhaps more realistic example of suboptimality alignment, consider a mesa-optimizer with a mesa-objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> and an environment in which there is one simple strategy and one complicated strategy for achieving <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span>. It could be that the simple strategy is aligned with the base optimizer, but the complicated strategy is not. The mesa-optimizer might then initially only be aware of the simple strategy, and thus be suboptimality aligned, until it has been run for long enough to come up with the complicated strategy, at which point it stops exhibiting the desired behavior.</p>
<p>&nbsp;</p>
<h2>3.2. The task</h2>
<p>As in <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/q2rCMHNXazALgQpGH"">the second post</a>, we will now consider the task the machine learning system is trained on. Specifically, we will address how the task affects a machine learning system's propensity to produce <em>pseudo-aligned</em> mesa-optimizers.</p>
<p><strong>Unidentifiability.</strong> It is a common problem in machine learning for a dataset to not contain enough information to adequately pinpoint a specific concept. This is closely analogous to the reason that machine learning models can fail to generalize or be susceptible to adversarial examples<a href=""https://intelligence.org/learned-optimization#bibliography"">(19)</a>—there are many more ways of classifying data that do well in training than any specific way the programmers had in mind. In the context of mesa-optimization, this manifests as pseudo-alignment being more likely to occur when a training environment does not contain enough information to distinguish between a wide variety of different objective functions. In such a case there will be many more ways for a mesa-optimizer to be pseudo-aligned than robustly aligned—one for each indistinguishable objective function. Thus, most mesa-optimizers that do well on the base objective will be pseudo-aligned rather than robustly aligned. This is a critical concern because it makes every other problem of pseudo-alignment worse—it is a reason that, in general, it is hard to find robustly aligned mesa-optimizers. Unidentifiability in mesa-optimization is partially analogous to the problem of unidentifiability in reward learning, in that the central issue is identifying the “correct” objective function given particular training data.<a href=""https://intelligence.org/learned-optimization#bibliography"">(20)</a> We will discuss this relationship further in the fifth post.</p>
<p>In the context of mesa-optimization, there is also an additional source of unidentifiability stemming from the fact that the mesa-optimizer is selected merely on the basis of its output. Consider the following toy reinforcement learning example. Suppose that in the training environment, pressing a button always causes a lamp to turn on with a ten-second delay, and that there is no other way to turn on the lamp. If the base objective depends only on whether the lamp is turned on, then a mesa-optimizer that maximizes button presses and one that maximizes lamp light will show identical behavior, as they will both press the button as often as they can. Thus, we cannot distinguish these two objective functions in this training environment. Nevertheless, the training environment does contain enough information to distinguish at least between these two particular objectives: since the high reward only comes after the ten-second delay, it must be from the lamp, not the button. As such, even if a training environment in principle contains enough information to identify the base objective, it might still be impossible to distinguish robustly aligned from proxy-aligned mesa-optimizers.</p>
<p><strong>Proxy choice as pre-computation.</strong> Proxy alignment can be seen as a form of pre-computation by the base optimizer. Proxy alignment allows the base optimizer to save the mesa-optimizer computational work by pre-computing which proxies are valuable for the base objective and then letting the mesa-optimizer maximize those proxies.</p>
<p>Without such pre-computation, the mesa-optimizer has to infer at runtime the causal relationship between different input features and the base objective, which might require significant computational work. Moreover, errors in this inference could result in outputs that perform worse on the base objective than if the system had access to pre-computed proxies. If the base optimizer precomputes some of these causal relationships—by selecting the mesa-objective to include good proxies—more computation at runtime can be diverted to making better plans instead of inferring these relationships.</p>
<p>The case of biological evolution may illustrate this point. The proxies that humans care about—food, resources, community, mating, etc.—are relatively computationally easy to optimize directly, while correlating well with survival and reproduction in our ancestral environment. For a human to be robustly aligned with evolution would have required us to instead care directly about spreading our genes, in which case we would have to infer that eating, cooperating with others, preventing physical pain, etc. would promote genetic fitness in the long run, which is not a trivial task. To infer all of those proxies from the information available to early humans would have required greater (perhaps unfeasibly greater) computational resources than to simply optimize for them directly. As an extreme illustration, for a child in this alternate universe to figure out not to stub its toe, it would have to realize that doing so would slightly diminish its chances of reproducing twenty years later.</p>
<p>For pre-computation to be beneficial, there needs to be a relatively stable causal relationship between a proxy variable and the base objective such that optimizing for the proxy will consistently do well on the base objective. However, even an imperfect relationship might give a significant performance boost over robust alignment if it frees up the mesa-optimizer to put significantly more computational effort into optimizing its output. This analysis suggests that there might be pressure towards proxy alignment in complex training environments, since the more complex the environment, the more computational work pre-computation saves the mesa-optimizer. Additionally, the more complex the environment, the more potential proxy variables are available for the mesa-optimizer to use.</p>
<p>Furthermore, in the context of machine learning, this analysis suggests that a time complexity penalty (as opposed to a description length penalty) is a double-edged sword. In <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/q2rCMHNXazALgQpGH"">the second post</a>, we suggested that penalizing time complexity might serve to reduce the likelihood of mesa-optimization. However, the above suggests that doing so would also promote pseudo-alignment in those cases where mesa-optimizers do arise. If the cost of fully modeling the base objective in the mesa-optimizer is large, then a pseudo-aligned mesa-optimizer might be preferred simply because it reduces time complexity, even if it would underperform a robustly aligned mesa-optimizer without such a penalty.</p>
<p><strong>Compression of the mesa-optimizer.</strong> The description length of a robustly aligned mesa-optimizer may be greater than that of a pseudo-aligned mesa-optimizer. Since there are more pseudo-aligned mesa-objectives than robustly aligned mesa-objectives, pseudo-alignment provides more degrees of freedom for choosing a particularly simple mesa-objective. Thus, we expect that in most cases there will be several pseudo-aligned mesa-optimizers that are less complex than any robustly aligned mesa-optimizer.</p>
<p>This description cost is especially high if the learned algorithm's input data does not contain easy-to-infer information about how to optimize for the base objective. Biological evolution seems to differ from machine learning in this sense, since evolution's specification of the brain has to go through the information funnel of DNA. The sensory data that early humans received didn't allow them to infer the existence of DNA, nor the relationship between their actions and their genetic fitness. Therefore, for humans to have been aligned with evolution would have required them to have an innately specified model of DNA, as well as the various factors influencing their inclusive genetic fitness. Such a model would not have been able to make use of environmental information for compression, and thus would have required a greater description length. In contrast, our models of food, pain, etc. can be very short since they are directly related to our input data.</p>
<p>&nbsp;</p>
<h2>3.3. The base optimizer</h2>
<p>We now turn to how the base optimizer is likely to affect the propensity for a machine learning system to produce pseudo-aligned mesa-optimizers.</p>
<p><strong>Hard-coded optimization.</strong> In <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/q2rCMHNXazALgQpGH"">the second post</a>, we suggested that hard-coding an optimization algorithm—that is to say, choosing a model with built-in optimization—could be used to remove some of the incentives for mesa-optimization. Similarly, hard-coded optimization may be used to prevent some of the sources of pseudo-alignment, since it may allow one to directly specify or train the mesa-objective. Reward-predictive model-based reinforcement learning might be one possible way of accomplishing this.<a href=""https://intelligence.org/learned-optimization#bibliography"">(21)</a> For example, an ML system could include a model directly trained to predict the base objective together with a powerful hard-coded optimization algorithm. Doing this bypasses some of the problems of pseudo-alignment: if the mesa-optimizer is trained to directly predict the base reward, then it will be selected to make good predictions even if a bad prediction would result in a good policy. However, a learned model of the base objective will still be underdetermined off-distribution, so this approach by itself does not guarantee robust alignment.</p>
<p><strong>Algorithmic range.</strong> We hypothesize that a model's algorithmic range will have implications for how likely it is to develop pseudo-alignment. One possible source of pseudo-alignment that could be particularly difficult to avoid is approximation error—if a mesa-optimizer is not capable of faithfully representing the base objective, then it can't possibly be robustly aligned, only approximately aligned. Even if a mesa-optimizer might theoretically be able to perfectly capture the base objective, the more difficult that is for it to do, the more we might expect it to be approximately aligned rather than robustly aligned. Thus, a large algorithmic range may be both a blessing and a curse: it makes it less likely that mesa-optimizers will be approximately aligned, but it also increases the likelihood of getting a mesa-optimizer in the first place.<sup class=""footnote-ref""><a href=""#fn-PqTWtuG43f63mKTqf-1"" id=""fnref-PqTWtuG43f63mKTqf-1"">[1]</a></sup></p>
<p><strong>Subprocess interdependence.</strong> There are some reasons to believe that there might be more initial optimization pressure towards proxy aligned than robustly aligned mesa-optimizers. In a local optimization process, each parameter of the learned algorithm (e.g. the parameter vector of a neuron) is adjusted to locally improve the base objective <em>conditional</em> on the other parameters. Thus, the benefit for the base optimizer of developing a new subprocess will likely depend on what other subprocesses the learned algorithm currently implements. Therefore, even if some subprocess would be very beneficial if combined with many other subprocesses, the base optimizer may not select for it until the subprocesses it depends on are sufficiently developed. As a result, a local optimization process would likely result in subprocesses that have fewer dependencies being developed before those with more dependencies.</p>
<p>In the context of mesa-optimization, the benefit of a robustly aligned mesa-objective seems to depend on more subprocesses than at least some pseudo-aligned mesa-objectives. For example, consider a side-effect aligned mesa-optimizer optimizing for some set of proxy variables. Suppose that it needs to run some subprocess to model the relationship between its actions and those proxy variables. If we assume that optimizing the proxy variables is necessary to perform well on the base objective, then for a mesa-optimizer to be robustly aligned, it would also need to model the causal relationship between those proxy variables and the base objective, which might require additional subprocesses. Moreover, the benefit to the base optimizer of adding those subprocesses depends on the mesa-optimizer having additional subprocesses to model the relationship between its actions and those proxy variables. This informal argument suggests that if a mesa-optimizer's computation neatly factors in this way, then developing a robustly aligned mesa-objective may require strictly more subprocesses than developing a pseudo-aligned mesa-objective.</p>
<p>This suggests that, at least in a local optimization process, mesa-optimizers might tend to start their development as proxy aligned before becoming robustly aligned. In other words, rather than simultaneously gaining competence and becoming aligned, we might expect such a system to first become competent at optimizing proxies, then possibly start becoming more robustly aligned.</p>
<p><strong>Internals-based selection.</strong> In current machine learning systems, learned algorithms are selected primarily based on how their policies perform on the base objective. There is often some selection based on the internal structure of the learned algorithm, such as weight regularization, but such selection is usually very coarse. One possible idea for addressing pseudo-alignment could be to use a selection method based on a more sophisticated function of a learned algorithm's internal workings. This approach is similar in spirit to hard-coded optimization: we use knowledge of the system's internal workings to prevent it from being proxy-aligned.</p>
<p>One possibility might be to develop a meta-learner that is specifically trained to prevent pseudo-alignment. While this seems beyond current meta-learning techniques, the basic principle is similar to existing meta-learners designed to optimize for robustness. An extreme form of internals-based selection might have the base optimizer reason about or prove properties of the learned algorithm as part of its selection process.</p>
<p>While desirable, such methods are currently out of reach primarily due to the limitations of current transparency methods. One possible concern with all of these approaches, however, is that they might require the base optimizer to be better at searching for problems than the mesa-optimizer is at hiding them. Furthermore, in order to create a base optimizer that can do this, we may need to first train it using a third optimizer, which might result in the same problem at this second layer.</p>
<p><strong>Adversarial training.</strong> One possible method to alleviate some of these concerns might be to sample environments adversarially from a very wide distribution. With such adversarial training, it might be possible to overcome these hurdles and make the robustly aligned objective the most reachable one. This would require making it so that, for every mesa-objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> that is more reachable than the base objective <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span>, there is some situation where <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> disagree and the mesa-optimizer is incentivized to agree with <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span>.</p>
<p>If we model reachability of an objective as simply its length in bits, then distinguishing <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> from every single more reachable <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span> gets exponentially harder as <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> gets more complex. Thus, for a very complicated <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span>, sufficiently incentivizing the base optimizer to find a mesa-optimizer with that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> is likely to be very difficult, though not impossible.</p>
<p>Even in such a case, however, there would likely still be a period of time where the learned algorithm is a misaligned mesa-optimizer, leaving open an ominous possibility: the misaligned mesa-optimizer could figure out the correct actions to take based on <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{base}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">base</span></span></span></span></span></span></span></span> while its objective function was still <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""O_\text{mesa}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mtext"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.372em;"">mesa</span></span></span></span></span></span></span></span>. We will call this situation <em>deceptive alignment</em> and will discuss it at greater length in the next post.</p>
<p>&nbsp;</p>
<p><em>The fourth post in the <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">Risks from Learned Optimization Sequence</a>, titled “Deceptive Alignment,” can be found <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks"">here</a>.</em></p>
<p><a href=""https://intelligence.org/learned-optimization/#glossary"">Glossary</a> | <a href=""https://intelligence.org/learned-optimization/#bibliography"">Bibliography</a></p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-PqTWtuG43f63mKTqf-1"" class=""footnote-item""><p>Though a large algorithmic range seems to make approximate alignment less likely, it is unclear how it might affect other forms of pseudo-alignment such as deceptive alignment. <a href=""#fnref-PqTWtuG43f63mKTqf-1"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
</body></html>",evhub,evhub,evhub,
HBkvHFFDFZvhM9NQq,Washington SSC Meetup,washington-ssc-meetup,https://www.lesswrong.com/events/HBkvHFFDFZvhM9NQq/washington-ssc-meetup,2019-06-03T19:30:45.751Z,2,1,0,False,False,,"<p>Slate Star Codex discussion meetup for Washington, DC.</p><p>Meetup is held in the second floor lounge during rainy weather, or rooftop if sunny.</p>",robirahman,robi-rahman,Robi Rahman,
9hReLR9wrsBN8t9FT,Can movement from Conflict to Mistake theorist be facilitated effectively?,can-movement-from-conflict-to-mistake-theorist-be,https://www.lesswrong.com/posts/9hReLR9wrsBN8t9FT/can-movement-from-conflict-to-mistake-theorist-be,2019-06-03T17:02:03.557Z,5,5,5,False,True,,<p>What are the common push and pull factors that facilitate an individuals movement from Conflict theorist to Mistake theorist? What are the transitional positions and how can they best be described? Can a technique be created to facilitate this transition?</p>,SamLibDem,samlibdem,SamLibDem,
PSxcPAqYuEWLKAz6n,Our plan for 2019-2020: consulting for AI Safety education,our-plan-for-2019-2020-consulting-for-ai-safety-education-1,https://www.lesswrong.com/posts/PSxcPAqYuEWLKAz6n/our-plan-for-2019-2020-consulting-for-ai-safety-education-1,2019-06-03T16:51:27.534Z,18,15,15,False,False,,"<p>UPDATE: this plan received sizable criticism. We are reflecting on it, and working on a revision.</p><p>Tl;dr: a conversation with a grantmaker made us drop our long-held assumption that outputs needed to be concrete to be recognized. We decided to take a step back and approach the improvement of the AI Safety pipeline on a more abstract level, doing consulting and research to develop expertise in the area. This will be our focus in the next year.</p><p><strong>Trial results</strong></p><p>We have tested our course in April. We didn’t get a positive result. It looks like this was due to bad test design, with a high variance and low number of participants clouding any pattern that could have emerged. In hindsight, we should clearly have tested knowledge before the intervention as well as after it, though arguably this would have been nearly impossible given the one-month deadline that our funder imposed.</p><p>What we did learn is that we are greatly unaware of the extent to which our course is being used. This is mostly due to using software that is not yet mature enough to give this kind of data. If we want to continue building the course, we feel that our first priority ought to be to set up a feedback mechanism that gives us precise insights into how students are journeying through.<br/><br/>However, other developments have pointed our attention away from developing the course, and towards developing the question that the course is an answer to.</p><p><strong>If funding wasn’t a problem</strong></p><p>During the existence of RAISE, it’s runway has never been longer than about 2 months. This did cripple our ability to make long term decisions, in favor of dishing out some quick results to show value. Seen from a &quot;quick feedback loops&quot; paradigm, this may have been a healthy dynamic. It did also lead to sacrifices that we didn’t actually want to make.</p><p>Had we been tasked with our particular niche without any funding constraints, our first move would have been to do extensive study into what the field needs. We feel that EA is missing a management layer. There is a lot that a community-focused management consultant could do, simply by connecting all the dots and coordinating the many projects and initiatives that exist in the LTF space. We have identified 30 (!) small and large organisations that are involved in AI Safety. Not all of them are talking to each other, or even aware of each other. </p><p>Our niche being AI Safety education, we would have spent a good 6 months developing expertise and network in this area. We would have studied the scientific frontiers of relevant domains like education and the metasciences. We would have interviewed AIS organisations and asked them what they look for in employees. We would&#x27;ve studied existing alignment researchers and looked for patterns. Talk to grantmakers and consider their models.</p><p><strong>Funding might not be a problem</strong></p><p>After getting <a href=""https://www.lesswrong.com/posts/t3t9osBsmwkajWz5Y/long-term-future-fund-april-2019-grant-decisions#My_thoughts_and_reasoning10"">turned down by the LTF fund</a> (which was especially meaningful because they didn’t seem to be constrained by funding), we had a conversation with one of their grantmakers. The premise of the conversation was something like “what version of RAISE would you be willing to fund?” The answer was pretty much what we just described. They thought pipeline improvement was important, but hard, and just going with the first idea that sounds good (an online course) would be a lucky shot if it worked. Instead, someone should be thinking about the bigger picture first.</p><p>The mistake we had been making from the beginning was to assume we needed concrete results to be taken seriously. </p><p><strong>Our new direction</strong></p><p>EA really does seem to be missing a management layer. People are thinking about their careers, starting organisations, doing direct work and research. Not many people are drawing up plans for coordination on a higher level and telling people what to do. Someone ought to be <strong>dividing up the big picture into roles for people to fill</strong>. You can see the demand for this by how seriously we take 80k. They’re the only ones doing this beyond the organisational level.</p><p>Much the same in the cause area we call AI Safety Education. Most AIS organisations are necessarily thinking about hiring and training, but no one is specializing in it. In the coming year, our aim is to fill this niche, building expertise and doing management consulting. We will aim to smarten up the coordination there. Concrete outputs might be:</p><ul><li>Advice for grantmakers that want to invest in the AI Safety researcher pipeline</li><li>Advice for students that want to get up to speed and test themselves quickly</li><li>Suggesting interventions for entrepreneurs that want to fill up gaps in the ecosystem</li><li>Publishing thinkpieces that advance the discussion of the community, like <u><a href=""https://forum.effectivealtruism.org/posts/G2Pfpkcwv3bJNF8o9/ea-is-vetting-constrained"">this one</a></u></li><li>Creating and keeping wiki pages about subjects that are relevant to us</li><li>Helping AIS research orgs with their recruitment process</li></ul><p><strong>We’re hiring</strong></p><p>Do you think this is important? Would you like to fast track your involvement with the Xrisk community? Do you have good google-fu, or would you like to conduct depth interviews with admirable people? Most importantly, are you not afraid to hack your own trail?</p><p>We think we could use one or two more people to join us in this effort. You’d be living for free in the EA Hotel. We can’t promise any salary in addition to that. Do ask us for more info!</p><p><strong>Let&#x27;s talk</strong></p><p>A large part of our work will involve talking to those involved in AI Safety. If you are working in this field, and interested in working on the pipeline, then we would like to talk to you.</p><p>If you have important information to share, have been plotting to do something in this area for a while, and want to compare perspectives, then we would like to talk to you.</p><p>And even if you would just like to have an open-ended chat about any of this, we would like to talk to you!</p><p>You can reach us at <a href=""raise@aisafety.info"">raise@aisafety.info</a></p><br/><br/>",RAISE,raise,RAISE,
FQKjY563bJqDeaEDr,"To first order, moral realism and moral anti-realism are the same thing",to-first-order-moral-realism-and-moral-anti-realism-are-the,https://www.lesswrong.com/posts/FQKjY563bJqDeaEDr/to-first-order-moral-realism-and-moral-anti-realism-are-the,2019-06-03T15:04:56.363Z,27,7,8,False,False,,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p>I've taken <a href=""https://www.lesswrong.com/posts/WeAt5TeS8aYc4Cpms/values-determined-by-stopping-properties"">a somewhat caricaturist view</a> of moral realism<sup class=""footnote-ref""><a href=""#fn-Xc6XfkoSLXqhGjXBb-1"" id=""fnref-Xc6XfkoSLXqhGjXBb-1"">[1]</a></sup>, describing it, essentially, as the random walk of a process defined by its ""stopping"" properties.</p>
<p>In this view, people start improving their morality according to certain criteria (self-consistency, simplicity, what they would believe if they were smarter, etc...) and continue on this until the criteria are finally met. Because there is no way of knowing how ""far"" this process can continue until the criteria are met, this can <a href=""https://www.lesswrong.com/posts/vgFvnr7FefZ3s3tHp/mahatma-armstrong-ceved-to-death"">drift very far indeed from its starting point</a>.</p>
<p>Now <em>I</em> would like to be able to argue, from a very anti-realist perspective, that:</p>
<ul>
<li>Argument A: I want to be able to judge that morality <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\alpha""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">α</span></span></span></span></span></span> is better than morality <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\beta""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;"">β</span></span></span></span></span></span>, based on some personal intuition or judgement of correctness. I want to be able to judge that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\beta""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;"">β</span></span></span></span></span></span> is alien and evil, even if it is fully self-consistent according to formal criteria, while <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\alpha""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">α</span></span></span></span></span></span> is not fully self-consistent.</li>
</ul>
<h2>Moral realists look like moral anti-realists</h2>
<p>Now, I maintain that this ""random walk to stopping point"" is an accurate description of many (most?) moral realist systems. <strong>But it's a terrible description of moral realists</strong>. In practice, most moral realists allow for the possibility of moral uncertainty, and hence that their preference approach might have a small chance of being wrong.</p>
<p>And how would they identify that wrongness? By looking outside the formal process, and checking if the path that the moral ""self-improvement"" is taking is plausible, and doesn't lead to obviously terrible outcomes.</p>
<p>So, to pick one example <a href=""https://agentfoundations.org/item?id=1636"">from Wei Dai</a> (similar examples can be found in <a href=""https://www.lesswrong.com/posts/6mtoy9gh32rQzfT4r/on-self-deception"">this post on self-deception</a>, and in the ""Senator Cruz"" section of Scott Alexander's <a href=""https://slatestarcodex.com/2015/11/16/hardball-questions-for-the-next-debate/"">""debate questions"" post</a>):</p>
<blockquote>
<p>I’m envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give you the most convincing arguments for it. At that point people won’t be able to participate in any online (or offline for that matter) discussions without risking their object-level values <a href=""https://www.lesswrong.com/posts/5bd75cc58225bf0670375431/divergent-preferences-and-meta-preferences"">being hijacked</a>.</p>
</blockquote>
<p>If the moral realist approach included getting into conversations with such systems and thus getting randomly subverted, then the moral realists I know would agree that the approach had failed, no matter how internally consistent it seems. Thus, they allow, in practice, some considerations akin to Argument A: where the moral process ends up (or at least the path that it takes) <strong>can</strong> affect their belief that the moral realist conclusion is correct.</p>
<p>So moral realists, in practice, do have <a href=""https://www.lesswrong.com/posts/ic8yoGBMYLtaJkbxZ/conditional-meta-preferences"">conditional meta-preferences</a> that can override their moral realist system. Indeed, most moral realists don't have a fully-designed system yet, but have a rough overview of what they want, with some details they expect to fill in later; from the perspective of here and now, they have some preferences, some strong meta-preferences (on how the system should work) and some conditional meta-preferences (on how the design of the system should work, conditional on certain facts or arguments they will learn later).</p>
<h2>Moral anti-realists look like moral realists</h2>
<p>Enough picking on moral realists; let's look now at moral anti-realists, which is relatively easy for me as I'm one of them. Suppose I was to investigate an area of morality that I haven't investigated before; say, political theory of justice.</p>
<p>Then, I would expect that as I investigated this area, I would start to develop better categories than what I have now, with crisper and more principled boundaries. I would expect to meet arguments that would change how I feel and what I value in these areas. I would apply simplicity arguments to make more elegant the hodgepodge of half-baked ideas that I currently have in that area.</p>
<p>In short, I would expect to engage in <strong>moral learning</strong>. Which is a peculiar thing for a moral anti-realist to expect...</p>
<h2>The first-order similarity</h2>
<p>So, to generalise a bit across the two categories:</p>
<ol>
<li>Moral realists are willing to question the truth of their systems based on facts about the world that should formally be irrelevant to that truth, and use their own private judgement in these cases.</li>
<li>Moral anti-realists are willing to engage in something that looks like moral learning.</li>
</ol>
<p>Note that the justifications of the two points of view are different - the moral realist can point to moral uncertainty, the moral anti-realist to personal preferences for a more consistent system. And the long-term perspectives are different: the moral realist expects that their process will <em>likely</em> converge to something with fantastic properties, the moral anti-realist thinks it <em>likely</em> that the degree of moral learning is sharply limited, only a few ""iterations"" beyond their current morality.</p>
<p>Still, in practice, and to a short-term, first-order approximation, moral realists and moral-anti realists seem very similar. Which is probably why they can continue to have conversations and debates that are not immediately pointless.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-Xc6XfkoSLXqhGjXBb-1"" class=""footnote-item""><p>I apologise for my simplistic understanding and definitions of moral realism. However, my partial experience in this field has been enough to convince me that there are many incompatible definition of moral realism, and many arguments about them, so it's not clear there is a single simple thing to understand. So I've tried to define is very roughly, enough so that the gist of this post makes sense. <a href=""#fnref-Xc6XfkoSLXqhGjXBb-1"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
</body></html>",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
ic8yoGBMYLtaJkbxZ,Conditional meta-preferences,conditional-meta-preferences,https://www.lesswrong.com/posts/ic8yoGBMYLtaJkbxZ/conditional-meta-preferences,2019-06-03T14:09:54.357Z,8,4,0,False,False,,"<html><head></head><body><p>I'd just want to make the brief point that many human <a href=""https://www.lesswrong.com/posts/Y2LhX3925RodndwpC/resolving-human-values-completely-and-adequately"">meta-preferences</a> are conditional.</p>
<p>Sure, we have ""I'd want to be more generous"", or ""I'd want my preferences to be more consistent"". But there are many variations of ""I'd want to believe in a philosophical position if someone brings me a very convincing argument for it"" and, to various degrees of implicitness or explicitness, ""I'd want to stop believing in cause X if implementing it leads to disasters"".</p>
<p>Some are a mix of conditional and anti-conditional: ""I'd want to believe in X even if there was strong evidence against it, but if most of my social group turns against X, then I would want to too"".</p>
<p>The reason for this stub of a post is that when I think of meta-preferences, I generally think of them as conditional; yet I've read some comments by people that imply that they think that I think of meta-preferences in an un-conditional way<sup class=""footnote-ref""><a href=""#fn-rQYPiByDtMeEmmQfz-1"" id=""fnref-rQYPiByDtMeEmmQfz-1"">[1]</a></sup>. So I made this post to have a brief reference point.</p>
<p>Indeed, in a sense, every attempt to come up with <a href=""https://arxiv.org/abs/1712.05812"">normative assumptions</a> to bridge the <a href=""https://en.wikipedia.org/wiki/Is%E2%80%93ought_problem"">is-ought gap</a> in value learning, is an attempt to explicitly define the conditional dependence of preferences upon the facts of the physical world.</p>
<p>Defining meta-preferences that way is not a problem, and bringing the definition into the statement of the meta-preference is not a problem either. In many cases, whether we label something conditional or non-conditional is a matter of taste, or whether we'd done the updating ahead of time or not. Contrast ""I love chocolate"", with ""I love delicious things"" with the observation ""I find chocolate delicious"", with ""conditional on it being delicious, I would love chocolate"" (and ""I find chocolate delicious"").</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-rQYPiByDtMeEmmQfz-1"" class=""footnote-item""><p>This sentence does actually make sense. <a href=""#fnref-rQYPiByDtMeEmmQfz-1"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
</body></html>",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
YJq6R9Wgk5Atjx54D,Does Bayes Beat Goodhart?,does-bayes-beat-goodhart,https://www.lesswrong.com/posts/YJq6R9Wgk5Atjx54D/does-bayes-beat-goodhart,2019-06-03T02:31:23.417Z,48,19,26,False,False,,"<p>Stuart Armstrong <a href=""https://www.lesswrong.com/posts/urZzJPwHtjewdKKHc/using-expected-utility-for-good-hart"">has</a> <a href=""https://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy"">claimed</a> to beat Goodhart with Bayesian uncertainty -- rather than assuming some particular objective function (which you try to make as correct as possible), you represent some uncertainty. A similar claim was made in <a href=""https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it"">The Optimizer&apos;s Curse and How to Beat It</a>, the essay which introduced a lot of us to ... well, not Goodhart&apos;s Law itself (the post doesn&apos;t make mention of Goodhart), but, that kind of failure. I myself claimed that Bayes beats regressional Goodhart, in <a href=""https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/iTpLAaPamcKyjmbFC"">Robust Delegation</a>:</p><span><figure><img src=""https://i.imgur.com/izfhiUB.png"" class=""draft-image center"" style=""width:40%""></figure></span><p>I now think this isn&apos;t true -- Bayes&apos; Law doesn&apos;t beat Goodhart fully. It doesn&apos;t even beat regressional Goodhart fully. (I&apos;ll probably edit Robust Delegation to change the claim at some point.)</p><p>(Stuart makes <a href=""https://www.lesswrong.com/posts/QJwnPRBBvgaeFeiLR/uncertainty-versus-fuzziness-versus-extrapolation-desiderata"">some more detailed claims</a> about <a href=""https://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy"">AI and the nearest-unblocked-strategy problem</a> which aren&apos;t exactly claims about Goodhart, at least according to him. <strong><em>I don&apos;t fully understand Stuart&apos;s perspective, and don&apos;t claim to directly address it here</em>.</strong> I am mostly only addressing the question of the title of my post: does Bayes beat Goodhart?)</p><h1>If approximate solutions are concerning, why would mixtures of them be unconcerning?</h1><p>My first argument is a loose intuition: Goodhartian phenomena suggest that somewhat-correct-but-not-quite-right proxy functions are not safe to optimize (and in some sense, the more optimization pressure is applied, the less safe we expect it to be). Assigning weights to a bunch of somewhat-but-not-quite-right possibilities just gets us another somewhat-but-not-quite-right possibility. Why would we expect this to fundamentally solve the problem?</p><ul><li>Perhaps the Bayesian mixture across hypotheses is <em>closer to being correct</em>, and therefore, gives us an approximation which is able to stand up to more optimization pressure before it breaks down. But this is a quantitative distinction, not a qualitative one. <em>How big</em> of a difference do we expect that to make? Wouldn&apos;t it still break down about as badly when put under tremendous optimization pressure?</li><li>Perhaps the point of the Bayesian mixture is that, by quantifying uncertainty about the various hypotheses, it encourages strategies which hedge their bets -- satisfying a broad range of possible utility functions, by avoiding doing something terrible for one utility function in order to get a few more points for another. But this incentive to hedge bets is fairly weak; the optimization is still encouraged to do something really terrible for one function if it leads to a moderate increase for many other utility functions.</li></ul><p>My intuition there doesn&apos;t address the gears of the situation adequately, though. Let&apos;s get into it.</p><h1>Overcoming regressional Goodhart requires calibrated learning.</h1><p>In <em>Robust Delegation</em>, I defined regressional Goodhart through the predictable-disappointment idea. Does Bayesian reasoning eliminate predictable disappointment?</p><p>Well, it depends on what is meant by &quot;predictable&quot;. You could define it as predictable-by-bayes, in which case it follows that Bayes solves the problem. However, I think it is reasonable to at least add a calibration requirement: there should be no way to systematically correct estimates up or down as a function of the expected value.</p><p>Calibration seems like it does, in fact, significantly address regressional Goodhart. You can&apos;t have seen a lot of instances of an estimate being too high, and still accept that too-high estimate. It doesn&apos;t address extremal Goodhart, because calibrated learning can only guarantee that you eventually calibrate, or converge at some rate, or something like that -- extreme values that you&apos;ve rarely encountered would remain a concern.</p><p>(Stuart&apos;s &quot;one-in-three&quot; example in the <a href=""https://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy"">Defeating Goodhart</a> post, and his discussion of human overconfidence more generally, is somewhat suggestive of calibration.)</p><p>Bayesian methods are not always calibrated. Calibrated learning is not always Bayesian. (For example, <a href=""https://intelligence.org/2016/09/12/new-paper-logical-induction/"">logical induction</a> has good calibration properties, and so far, hasn&apos;t gotten a really satisfying Bayesian treatment.)</p><p>This might be confusing if you&apos;re used to thinking in Bayesian terms. If you think in terms of the diagram I copied from <em>Robust Delegation</em>, above: you have a prior which stipulates probability of true utility <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span></span></span></span> given observation <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>; your expectation <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""g(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> is the expected value of <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""y""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span></span></span></span> for a particular value of <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>; <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""g(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> is not predictably correctable with respect to your prior. What&apos;s the problem?</p><p>The problem is that this line of reasoning assumes that your prior is <em>objectively correct</em>. This doesn&apos;t generally make sense (especially from a Bayesian perspective). So, it is perfectly consistent for you to collect many observations, and see that <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""g(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> has some systematic bias. This may remain true <em>even as you update on those observations </em>(because Bayesian learning doesn&apos;t guarantee any calibration property in general!).</p><p>The faulty assumption that your probability distribution is correct is often replaced with the (weaker, but still problematic) assumption that at least one hypothesis within your distribution is objectively correct -- the realizability assumption.</p><h1>Bayesian solutions assume realizability.</h1><p>As discussed in <a href=""https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/efWfvrWLgJmbBAs3m"">Embedded World Models</a>, the realizability assumption is the assumption that (at least) one of your hypotheses represents the true state of affairs. Bayesian methods often (though not always) require a realizability assumption in order to get strong guarantees. Frequentist methods rarely require such an assumption (whatever else you may say about frequentist methods). Calibration is an example of that -- a Bayesian can get calibration under the assumption of realizability, but, we might want a stronger guarantee of calibration which holds even in absence of realizability.</p><h2>&quot;We quantified our uncertainty as best we could!&quot;</h2><p>One possible bayes-beats-goodhart argument is: &quot;Once we quantify our uncertainty with a probability distribution over possible utility functions, the best we can possibly do is to choose whatever maximizes expected value. Anything else is decision-theoretically sub-optimal.&quot;</p><p>Do you think that the true utility function is really sampled from the given distribution, in some objective sense? And the probability distribution also quantifies all the things which can count as evidence? If so, fine. Maximizing expectation is the objectively best strategy. This eliminates all types of Goodhart by positing that we&apos;ve already modeled the possibilities sufficiently well: extremal cases are modeled correctly; adversarial effects are already accounted for; etc.</p><p>However, this is unrealistic due to embeddedness: the outside world is much more complicated than any probability distribution which we can explicitly use, since we are ourselves a small part of that world.</p><p>Alternatively, do you think the probability distribution really codifies your precise subjective uncertainty? Ok, sure, that would also justify the argument.</p><p>Realistically, though, an implementation of this isn&apos;t going to be representing your precise subjective beliefs (to the extent you even <em>have</em> precise subjective beliefs). It has to hope to have a prior which is &quot;good enough&quot;.</p><p>In what sense might it be &quot;good enough&quot;?</p><p>An obvious problem is that a distribution might be overconfident in a wrong conclusion, which will obviously be bad. The fix for this appears to be: make sure that the distribution is &quot;sufficiently broad&quot;, expressing a fairly high amount of uncertainty. But, why would this be good?</p><p>Well, one might argue: it can only be worse that our true uncertainty to the extent that it ends up assigning too little weight to the correct option. So, if the probability function isn&apos;t too small for any of the possibilities which we intuitively assign non-negligible weight, things should be fine.</p><h2>&quot;The True Utility Function Has Enough Weight&quot;</h2><p>First, even assuming the framing of &quot;true utility function&quot; makes sense, it isn&apos;t obvious to me that the argument makes sense.</p><p>If there&apos;s a true utility function <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""u_{true}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span></span></span></span></span></span></span> which is assigned weight <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""w_{u_{true}}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">w</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span></span></span></span></span></span></span></span></span></span></span>, and we apply a whole lot of optimization pressure to the overall mixture distribution, then it is perfectly possible that <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""u_{true}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span></span></span></span></span></span></span> gets compromised for the sake of satisfying a large number of other  <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""u_i""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span>. The weight determines a <em>ratio at which trade-offs can occur,</em> not a <em>ratio of the overall resources which we will get</em> or anything like that.</p><p>A first-pass analysis is that <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""w_{u_{true}}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">w</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span></span></span></span></span></span></span></span></span></span></span> has to be more than 1/2 to guarantee any consideration; any weight less than that, and it&apos;s possible that <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""u_{true}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span></span></span></span></span></span></span> is <em>as low as it can go</em> in the optimized solution, because some outcome was sufficiently good for all other potential utility functions that it made sense to &quot;take the hit&quot; with respect to <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""u_{true}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span></span></span></span></span></span></span>. We can&apos;t formally say &quot;this probably won&apos;t happen, because the odds that the best-looking option is specifically terrible for <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""u_{true}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span></span></span></span></span></span></span> are low&quot; without assuming something about the distribution of highly optimized solutions. </p><p>(Such an analysis might be interesting; I don&apos;t know if anyone has investigated from that angle. But, it seems somewhat unlikely to do us good, since it doesn&apos;t seem like we can make very nice assumptions about what highly-optimized solutions look like.)</p><p>In reality, the worst-case analysis is better than this, because many of the more-plausible <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""u_i""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span> should have a lot of &quot;overlap&quot; with <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""u_{true}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span></span></span></span></span></span></span>; after all, they were given high weight because they <em>appeared plausible</em> somehow (they agreed with human intuitions, or predicted human behavior, etc). We could try to formally define &quot;overlap&quot; and see what assumptions we need to guarantee better-than-worst-case outcomes. (This might have some interesting learning-theoretic implications for value learning, even.)</p><p>However, this whole framing, where we assume that there&apos;s a <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""u_{true}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span></span></span></span></span></span></span> and think about its weight, is suspect. Why should we think that there&apos;s a &quot;true&quot; utility function which captures our preferences? And, if there is, why should we assume that it has an explicit representation in the hypothesis space? </p><p>If we drop this assumption, we get the classical problems associated with non-realizability in Bayesian learning. Beliefs may not converge at all, as evidence accumulates; they could keep oscillating due to inconsistent evidence. Under the interpretation where we still assume a &quot;true&quot; utility function but we don&apos;t assume that it is explicitly representable within the hypothesis space, there isn&apos;t a clear guarantee we can get (although perhaps the &quot;overlap&quot; analysis can help here). If we don&apos;t assume a true utility function at all, then it isn&apos;t clear how to even ask questions about how well we do (although I&apos;m not saying there isn&apos;t a useful analysis -- I&apos;m just saying that it is unclear to me right now).</p><p>Stuart does address this question, <a href=""https://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy"">in the end</a>:</p><blockquote>I&apos;ve argued that an indescribable hellworld <a href=""https://www.lesswrong.com/posts/rArsypGqq49bk4iRr/can-there-be-an-indescribable-hellworld"">cannot exist</a>. There&apos;s a similar question as to whether there exists human uncertainty about U that cannot be included in the AI&apos;s model of &#x394;. By definition, this uncertainty would be something that is currently unknown and unimaginable to us. However, I feel that it&apos;s far more likely to exist, than the indescribable hellworld.</blockquote><blockquote>Still despite that issue, it seems to me that there are methods of dealing with the Goodhart problem/nearest unblocked strategy problem. And this involves properly accounting for all our uncertainty, directly or indirectly. If we do this well, there no longer remains a Goodhart problem at all.</blockquote><p>Perhaps I agree, if &quot;properly accounting for all our uncertainty&quot; includes robustness properties such as calibrated learning, <em>and</em> if we restrict our attention to regressional Goodhart, ignoring <a href=""https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy"">the other three</a>.</p><p>Well... what about the others, then?</p><h1>Overcoming adversarial Goodhart seems to require randomization.</h1><p>The argument here is pretty simple: adversarial Goodhart enters into the domain of game theory, in which mixed strategies tend to be very useful. <a href=""https://www.lesswrong.com/posts/Rs6vZCrnQFWQ4p37P/when-to-use-quantilization"">Quantilization</a> is one such mixed strategy, which seems to usefully address Goodhart to a certain extent. I&apos;m not saying that quantilization is the ultimate solution here. But, it does seem to me like quantilization is significant enough that a solution to Goodhart should say something about the class of problems which quantilization solves. </p><p>In particular, a property of quantilization which I find appealing is the way more certainty about the utility function implies that more optimization power can be safely applied to making decisions. This informs my intuition that applying arbitrarily high optimization power does not become safe simply because you&apos;ve explicitly represented uncertainty about utility functions -- no matter how accurately, short of &quot;perfectly accurately&quot; (which isn&apos;t even a meaningful concept), it only seems to justify a limited amount of optimization pressure. This story may be an incorrect one, but if so, I&apos;d like to really understand why it is incorrect.</p><p>Unlike the previous sections, this doesn&apos;t necessarily step outside of typical Bayesian thought, since this kind of game-theoretic thinking is more or less within the purview of Bayesianism. However, the simple &quot;Bayes solves Goodhart&quot; story doesn&apos;t explicitly address this.</p><p><em>(I haven&apos;t addressed causal Goodhart anywhere in this essay, since it opens up the whole decision-theoretic can of worms, which seems somewhat beside the main point. (I suppose, arguably, game-theoretic concerns could be beside the point as well -- but, they feel more directly relevant to me, since quantilization is fairly directly about solving Goodhart.))</em></p><h1>In summary:</h1><ul><li>If optimizing an arbitrary somewhat-but-not-perfectly-right utility function gives rise to serious Goodhart-related concerns, then why does a mixture distribution over such functions alleviate such concerns? Aren&apos;t they just averaging together to yield yet another somewhat-but-not-quite-right function?</li><li>Regressional Goodhart seems better-addressed by calibrated learning than it does by Bayesian learning.</li><li>Bayesian learning tends to require a realizability assumption in order to have good properties (including calibration).</li><li>Even assuming realizability, heavily optimizing a mixture distribution over possible utility functions seems dicey -- it can end up throwing away all the real value if it finds a way to jointly satisfy a lot of the wrong ones. (It is possible that we can find reasonable assumptions under which this doesn&apos;t happen, however.)</li><li>Overcoming adversarial Goodhart seems to require mixed strategies, which the simple &quot;bayesian uncertainty&quot; story doesn&apos;t explicitly address.</li></ul>",abramdemski,abramdemski,abramdemski,
hmLuGtxWJMu4npGZE,Cambridge LW/SSC Meetup,cambridge-lw-ssc-meetup,https://www.lesswrong.com/events/hmLuGtxWJMu4npGZE/cambridge-lw-ssc-meetup,2019-06-03T01:55:57.895Z,6,1,0,False,False,,"<p>This is the monthly Cambridge, MA LessWrong / Slate Star Codex meetup.</p><br/><p>Note: The meetup is in apartment 2 (the address box here won&#x27;t let me include the apartment number).</p>",AspiringRationalist,nosignalnonoise,NoSignalNoNoise,
7fDEWxY8LD4qmf2NF,What is the evidence for productivity benefits of weightlifting?,what-is-the-evidence-for-productivity-benefits-of-1,https://www.lesswrong.com/posts/7fDEWxY8LD4qmf2NF/what-is-the-evidence-for-productivity-benefits-of-1,2019-06-02T19:17:35.883Z,59,25,27,False,True,,"<p><em>[Mod Note: This question received an answer that seemed worth curating. See the answer by LW user <a href=""https://www.lesswrong.com/users/hereisonehand"">hereisonehand</a> for the curation notice]</em></p><p>I&#x27;ve been weightlifting for a while, and I&#x27;ve heard vaguely good things about it&#x27;s effect on productivity, like a general increase in energy levels. A recent quick google search session came up empty. If someone looks into the literature and finds something interesting I&#x27;ll pay a $50 prize.*</p><p>Assume the time horizon is &lt;5 years. I&#x27;d prefer answers focus predominantly on productivity benefits. Effects on cardiovascular could be part of an analysis, but would not qualify on their own. If the evidence is for something clearly linked to productivity, like sleep, I&#x27;d count that. Introspective evidence will also not qualify. Comparisons to other forms of exercise would be especially interesting. Assume a healthy individual, although I&#x27;m at least somewhat interested in effects on individuals with depression or anxiety given their prevalence.</p><p>*Prize to go to best answer, as judged by me, if there are any that meet some minimal threshold of rigor, also as judged by me.</p>",jp,jp,jp,
othqMhzQEQXFb4LhK,On alien science,on-alien-science,https://www.lesswrong.com/posts/othqMhzQEQXFb4LhK/on-alien-science,2019-06-02T14:50:01.437Z,44,15,8,False,False,,"<div>In his book <i>The Fabric of Reality</i>, David Deutsch makes the case that science is about coming up with good and true explanations, with all other considerations being secondary. This clashes with the more conventional view that the goal of science is to allow us to make accurate predictions - see for example this quote from the Nobel prize-winning physicist Steven Weinberg:<br /><blockquote>“The important thing is to be able to make predictions about images on the astronomers’ photographic plates, frequencies of spectral lines, and so on, and it simply doesn’t matter whether we ascribe these predictions to the physical effects of gravitational fields on the motion of planets and photons [as in pre-Einsteinian physics] or to a curvature of space and time.”</blockquote><br />It’s true that a key trait of good explanations is that they can be used to make accurate predictions, but I think that taking prediction to be the <i>point</i> of doing science is misguided in a few ways.<br /><br />Firstly, on a historical basis, many of the greatest scientists were clearly aiming for explanation not prediction. Astronomers like Copernicus and Kepler knew what to expect when they looked at the sky, but spent their lives searching for the reason why it appeared that way. Darwin knew a lot about the rich diversity of life on earth, but wanted to know how it had come about. Einstein was trying to reconcile Maxwell’s equations, the Michelson-Morley experiment, and classical mechanics. Predictions are often useful to <i>verify</i> explanations, but they’re rarely the main motivating force for scientists. And often they’re not the main reason why a theory should be accepted, either. Consider three of the greatest theories of all time: Darwinian evolution, Newtonian mechanics and Einsteinian relativity. In all three cases, the most compelling evidence for them was their ability to cleanly <a href=""https://www.lesswrong.com/posts/MwQRucYo6BZZwjKE7/einstein-s-arrogance"">explain existing observations</a> that had previously baffled scientists.<br /><br />We can further clarify the case for explanation as the end goal of science by considering a thought experiment from Deutsch’s book. Suppose we had an “experiment oracle” that could predict the result of any experiment, but couldn’t tell us why it would turn out that way. In that case, I think experimental science would probably fade away, but the theorists would flourish, because it’d be more important than ever to figure out what questions to ask! Deutsch’s take on this:<br /><div><blockquote>“If we gave it the design of a spaceship, and the details of a proposed test flight, it could tell us how the spaceship would perform on such a flight. But it could not design the spaceship for us in the first place. And even if it predicted that the spaceship we had designed would explode on take-off, it could not tell us how to prevent such an explosion. That would still be for us to work out. And before we could work it out, before we could even begin to improve the design in any way, we should have to understand, among other things, how the spaceship was supposed to work. Only then would we have any chance of discovering what might cause an explosion on take-off. Prediction – even perfect, universal prediction – is simply no substitute for explanation.”</blockquote><br />The question is now: how does this focus on explanations tie in to other ideas which are emphasised in science, like falsifiability, experimentalism, academic freedom and peer review? I find it useful to think of these aspects of science less as foundational epistemological principles, and more as ways to counteract various cognitive biases which humans possess. In particular:<br /><ol><li>We are biased towards sharing the beliefs of our ingroup members, and forcing our own upon them.</li><li>We’re biased towards aesthetically beautiful theories which are simple and elegant.</li><li>Confirmation bias makes us look harder for evidence which supports than which weighs against our own beliefs.</li><li>Our observations are by default filtered through our expectations and our memories, which makes them unreliable and low-fidelity.</li><li>If we discover data which contradicts our existing theories, we find it easy to confabulate new post-hoc explanations to justify the discrepancy.</li><li>We find it psychologically very difficult to actually change our minds.</li></ol><br />We can see that many key features of science counteract these biases:<br /><ol><li>Science has a heavy emphasis on academic freedom to pursue one’s own interests, which mitigates pressure from other academics. <i>Nullius in verba</i>, the motto of the Royal Society (“take nobody’s word for it”) encourages independent verification of others’ ideas.</li><li>Even the most beautiful theories cannot overrule conflicting empirical evidence.</li><li>Scientists are meant to attempt to experimentally falsify their own theories, and their attempts to do so are judged by their peers. Double-blind peer review allows scientists to feel comfortable giving harsher criticisms without personal repercussions.</li><li>Scientists should aim to collect precise and complete data about experiments.</li><li>Scientists should pre-register their predictions about experiments, so that it’s easy to tell when the outcome weighs against a theory.</li><li>Science has a culture of vigorous debate and criticism to persuade people to change their minds, and norms of admiration for those who do so in response to new evidence.</li></ol><br />But imagine an alien species with the opposite biases:<br /><ol><li>They tend to trust the global consensus, rather than the consensus of those directly around them.</li><li>Their aesthetic views are biased towards theories which are very data-heavy and account for lots of edge cases.*</li><li>When their views diverge from the global consensus, they look harder for evidence to bring themselves back into line than for evidence which supports their current views.</li><li>Their natural senses and memories are precise, unbiased and high-resolution.</li><li>When they discover data which contradicts their theories, they find it easiest to discard those theories rather than reformulating them.</li><li>They change their minds a lot.</li></ol><br />In this alien species, brave iconoclasts who pick an unpopular view and research it extensively are much less common than they are amongst humans. Those who try to do so end up focusing on models with (metaphorical or literal) epicycles stacked on epicycles, rather than the clean mathematical laws which have actually turned out to be more useful for conceptual progress in many domains. In formulating their detailed, pedantic models, they pay too much attention to exhaustively replaying their memories of experiments, and not enough to what concepts might underlie them. And even if some of them start heading in the right direction, a few contrary pieces of evidence would be enough to turn them back from it - for example, their heliocentrists might be thrown off track by their inability to observe stellar parallax. Actually, if you’re not yet persuaded that this alien world would see little scientific progress, you should read <a href=""https://thinkingcomplete.blogspot.com/2019/04/book-review-sleepwalkers.html"">my summary of <i>The Sleepwalkers</i></a>. In that account of the early scientific revolution, any of the alien characteristics above would have seriously impeded key scientists like Kepler, Galileo and others (except perhaps the eidetic memories).<br /><br />And so the institutions which actually end up pushing forward scientific progress on their world would likely look very different from the ones which did so on ours. Their Alien Royal Society would encourage them to form many small groups which actively reinforced each other’s idiosyncratic views and were resistant to outside feedback. They should train themselves to seek theoretical beauty rather than empirical validation - and actually, they should pay much less attention to contradictory evidence than members of their species usually do. Even when they’re tempted to change their minds and discard a theory, they should instead remind themselves of how well it post-hoc explains previous data, and put effort into adjusting it to fit the new data, despite how unnatural doing so seems to them. Those who change their minds too often when confronted with new evidence should be derided as wishy-washy and unscientific.<br /><br />These scientific norms wouldn’t be enough to totally reverse their biases, any more than our scientific norms make us rejoice when our pet theory is falsified. But in both cases, they serve as nudges towards a central position which is less burdened by species-contingent psychological issues, and better at discovering good explanations.<br /><br /><br />* Note that this might mean the aliens have different standards for what qualifies as a good explanation than we do. But I don’t think this makes a big difference. Suppose that the elegant and beautiful theory we are striving for is a small set of simple equations which governs all motion in the solar system, and the elegant and beautiful theory they are striving for is a detailed chart which traces out the current and future positions of all objects in the solar system. It seems unlikely that they could get anywhere near the latter without using Newtonian gravitation. So a circular-epicycle model of the solar system would be a dead end even by the aliens’ own standards.</div></div>",ricraz,ricraz,Richard_Ngo,
2Zsuv5uPFPNTACwzg,Moral Mazes and Short Termism,moral-mazes-and-short-termism,https://www.lesswrong.com/posts/2Zsuv5uPFPNTACwzg/moral-mazes-and-short-termism,2019-06-02T11:30:00.348Z,74,32,21,False,False,,"<p>Previously: <a href=""https://thezvi.wordpress.com/2017/03/04/short-termism/"">Short Termism</a> and <a href=""https://thezvi.wordpress.com/2019/05/30/quotes-from-moral-mazes/"">Quotes from Moral Mazes</a></p>
<p>Epistemic Status: Long term</p>
<p>My list of <a href=""https://thezvi.wordpress.com/2019/05/30/quotes-from-moral-mazes/"">quotes from moral mazes</a> has a section of twenty devoted to short term thinking. It fits with, and gives internal gears and color to, <a href=""https://thezvi.wordpress.com/2017/03/04/short-termism/"">my previous understanding</a> of of the problem of short termism.</p>
<p>Much of what we think of as a Short Term vs. Long Term issue is actually an adversarial <a href=""https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy"">Goodhart’s Law</a> problem, or a <a href=""https://en.wikipedia.org/wiki/Legibility"">legibility</a> vs. illegibility problem, at the object level, that then becomes a short vs. long term issue at higher levels. When a manager milks a plant (see quotes 72, 73, 78 and 79) they are not primarily trading long term assets for short term assets. Rather, they are trading unmeasured assets for measured assets (see 67 and 69).</p>
<p></p>
<p>This is why you can have companies like Amazon, Uber or Tesla get high valuations. They hit legible short-term metrics that represent long-term growth. A start-up gets rewarded for their own sort of legible short-term indicators of progress and success, and of the quality of team and therefore potential for future success. Whereas other companies, that are not based on growth, report huge pressure to hit profit numbers.</p>
<p>The overwhelming object level pressure towards legible short-term success, whatever that means in context, comes from being judged in the short term on one’s success, and having that judgment being more important than object-level long term success.</p>
<p>The easiest way for this to be true is not to care about object-level long term success. If you’re gone before the long term, and no one traces the long term back to you, why do you care what happens? That is exactly the situation the managers face in Moral Mazes (see 64, 65, 70, 71, 74 and 83, and for a non-manager very clean example see 77). In particular:</p>
<blockquote><p>74. We’re judged on the short-term because everybody changes their jobs so frequently.</p></blockquote>
<p>And:</p>
<blockquote><p>64. The ideal situation, of course, is to end up in a position where one can fire one’s successors for one’s own previous mistakes.</p></blockquote>
<p>Almost as good as having a designated scapegoat is to have already sold the company or found employment elsewhere, rendering your problems <a href=""https://en.wikipedia.org/wiki/Somebody_else%27s_problem"">someone else’s problems</a>.</p>
<p>The other way to not care is for the short-term evaluation of one’s success or failure to impact long-term success. If not hitting a short-term number gets you fired, or prevents your company from getting acceptable terms on financing or gets you bought out, then the long term will get neglected. The net present value payoff for looking good, which can then be reinvested, makes it look like by far the best long term investment around.</p>
<p>Thus we have this problem at every level of management except the top. But for the top to actually be the top, it needs to not be answering to the stock market or capital markets, or otherwise care what others think – even without explicit verdicts, this can be as hard to root out as needing the perception of a bright future to attract and keep quality employees and keep up morale. So we almost always have it at the top as well. Each level is distorting things for the level above, and pushing these distorted priorities down to get to the next move in a giant game of adversarial telephone (see section A of quotes for how hierarchy works).</p>
<p>This results in a corporation that acts in various short-term ways, some of which make sense for it, some of which are the result of internal conflicts.</p>
<p>Why isn’t this out-competed? Why don’t the corporations that do less of this drive the ones that do more of it out of the market?</p>
<p>On the level of corporations doing this direct from the top, often these actions are a response to the incentives the corporation faces. In those cases, there is no reason to expect such actions to be out-competed.</p>
<p>In other cases, the incentives of the CEO and top management are twisted but the corporation’s incentives are not. One would certainly expect those corporations that avoid this to do better. But these mismatches are the natural consequence of putting someone in charge who does not permanently own the company. Thus, dual class share structures becoming popular to restore skin in the correct game. Some of the lower-down issues can be made less bad by removing the ones at the top, but the problem does not go away, and what sources I have inside major tech companies including Google match this model.</p>
<p>There is also the tendency of these dynamics to arise over time. Those who play the power game tend to outperform those who do not play it barring constant vigilance and a willingness to sacrifice. As those players outperform, they cause other power players to outperform more, because they prefer and favor such other players, and favor rules that favor such players. This is especially powerful for anyone below them in the hierarchy. An infected CEO, who can install their own people, can quickly be game over on its own, and outside CEOs are brought in often.</p>
<p>Thus, even if the system causes the corporation to underperform, it still spreads, like a meme that infects the host, causing the host to prioritize spreading the meme, while reducing reproductive fitness. The bigger the organization, the harder it is to remain uninfected. Being able to be temporarily less burdened by such issues is one of the big advantages new entrants have.</p>
<p>One could even say that yes, they <em>do get wiped out by this, </em>but it’s not that fast, because it takes a while for this to rise to the level of a primary determining factor in outcomes. And <a href=""https://thezvi.wordpress.com/2017/10/29/leaders-of-men/"">there are bigger things to worry about</a>. It’s short termism, so that isn’t too surprising.</p>
<p>A big pressure that causes these infections is that business is constantly under siege and forced to engage in public relations (see quotes sections L and M) and is constantly facing <a href=""https://thezvi.wordpress.com/2019/04/25/asymmetric-justice/"">Asymmetric Justice</a> and the <a href=""https://blog.jaibot.com/the-copenhagen-interpretation-of-ethics/"">Copenhagen Interpretation of Ethics</a>. This puts tremendous pressure on corporations to tell different stories to different audiences, to avoid creating records, and otherwise engage in the types of behavior that will be comfortable to the infected and uncomfortable to the uninfected.</p>
<p>Another explanation is that those who are infected don’t only reward each other <em>within </em>a corporation. They also <em>do business with </em>and <em>cooperate with </em>the infected elsewhere. Infected people are <em>comfortable </em>with others who are infected, and <em>uncomfortable </em>with those not infected, because if the time comes to play ball, they might refuse. So those who refuse to play by these rules do better at object-level tasks, but face alliances and hostile action from all sides, including capital markets, competitors and government, all of which are, to varying degrees, infected.</p>
<p>I am likely missing additional mechanisms, either because I don’t know about them or forgot to mention them, but I consider what I see here sufficient. I am no longer confused about short termism.</p>
<p> </p>
<p> </p>",Zvi,zvi,Zvi,
ZDZmopKquzHYPRNxq,Selection vs Control,selection-vs-control,https://www.lesswrong.com/posts/ZDZmopKquzHYPRNxq/selection-vs-control,2019-06-02T07:01:39.626Z,180,67,27,False,False,,"<br><p>This is something which has bothered me for a while, but, I&apos;m writing it specifically in response to the <a href=""https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction"">recent post on mesa-optimizers</a>.</p><p>I feel strongly that the notion of &apos;optimization process&apos; or &apos;optimizer&apos; which people use -- partly derived from <a href=""https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power"">Eliezer&apos;s notion in the sequences</a> -- should be split into two clusters. I call these two clusters &apos;selection&apos; vs &apos;control&apos;. I don&apos;t have precise formal statements of the distinction I&apos;m pointing at; I&apos;ll give several examples.</p><p>Before going into it, several reasons why this sort of thing may be important:</p><ul><li>It could help refine the discussion of mesa-optimization. The article restricted its discussion to the type of optimization I&apos;ll call &apos;selection&apos;, explicitly ruling out &apos;control&apos;. This choice isn&apos;t obviously right. (More on this later.)</li><li>Refining &apos;agency-like&apos; concepts like this seems important for embedded agency -- what we eventually want is a story about how agents can be in the world. I think almost any discussion of the relationship between agency and optimization which isn&apos;t aware of the distinction I&apos;m drawing here (at least as a hypothesis) will be confused.</li><li>Generally, I feel like I see people making mistakes by not distinguishing between the two (whether or not they&apos;ve derived their notion of optimizer from Eliezer). I judge an algorithm differently if it is intended as one or the other.</li></ul><p>(See also Stuart Armstrong&apos;s summary of <a href=""https://www.lesswrong.com/posts/iJNK7rE5jphMSJJCa/thoughts-and-problems-with-eliezer-s-measure-of-optimization"">other problems</a> with the notion of optimization power Eliezer proposed -- those are unrelated to my discussion here, and strike me more as technical issues which call for refined formulae, rather than conceptual problems which call for revised ontology.)</p><h1>The Basic Idea</h1><p>Eliezer <a href=""https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power"">quantified optimization power</a> by asking how small a target an optimization process hits, out of a space of possibilities. The type of &apos;space of possibilities&apos; is what I want to poke at here.</p><h2>Selection</h2><p>First, consider a typical optimization algorithm, such as <a href=""https://en.wikipedia.org/wiki/Simulated_annealing"">simulated annealing</a>. The algorithm constructs an element of the search space (such as a specific combination of weights for a neural network), gets feedback on how good that element is, and then tries again. Over many iterations of this process, it finds better and better elements. Eventually, it outputs a single choice. </p><p>This is the prototypical &apos;selection process&apos; -- it can <em>directly instantiate any element of the search space </em>(although typically we consider cases where the process doesn&apos;t have time to instantiate <em>all</em> of them), it <em>gets direct feedback on the quality of each element</em> (although evaluation may be costly, so that the selection process must economize these evaluations), <em>the quality of an element of search space does not depend on the previous choices,</em> and <em>only the final output matters.</em></p><p>The term &apos;selection process&apos; refers to the fact that this type of optimization selects between a number of explicitly given possibilities. The most basic example of this phenomenon is a &apos;filter&apos; which rejects some elements and accepts others -- like selection bias in statistics. This has a limited ability to optimize, however, because it allows only one iteration. Natural selection is an example of much more powerful optimization occurring through iteration of selection effects.</p><h2>Control</h2><p>Now, consider a targeting system on a rocket -- let&apos;s say, a heat-seeking missile. The missile has sensors and actuators. It gets feedback from its sensors, and must somehow use this information to decide how to use its actuators. This is my prototypical control process. (The term &apos;control process&apos; is supposed to invoke control theory.) Unlike a selection process, <em>a controller can only instantiate one element of the space of possibilities</em>. It gets to traverse exactly one path. The &apos;small target&apos; which it hits is therefore &apos;small&apos; with respect to a space of <em><strong>counterfactual</strong> possibilities, </em>with all the technical problems of evaluating counterfactuals. <em>We only get full feedback on one outcome </em>(although we usually consider cases where the partial feedback we get along the way gives us a lot of information about how to navigate toward better outcomes). <em>Every decision we make along the way matters, both in terms of influencing total utility, and in terms of influencing what possibilities we have access to in subsequent decisions.</em> </p><p>So: in evaluating the optimization power of a selection process, we have a fairly objective situation on our hands: the space of possibilities is explicitly given; the utility function is explicitly given; we can compare the true output of the system to a randomly chosen element. In evaluating the optimization power of a control process, we have a very subjective situation on our hands: the controller only truly takes one path, so any judgement about a space of possibilities requires us to define counterfactuals; it is less clear how to define an un-optimized baseline; utility need not be explicitly represented in the controller, so may have to be inferred (or we think of it as parameter, so, we can measure optimization power with respect to different utility functions, but there&apos;s no &apos;correct&apos; one to measure).</p><p>I do think both of these concepts are meaningful. I don&apos;t want to restrict &apos;optimization&apos; to refer to only one or the other, as the mesa-optimization essay does. However, I think the two concepts are of a very different type.</p><h1>Bottlecaps &amp; Thermostats</h1><p>The mesa-optimizer write-up made the decision to focus on what I call selection processes, excluding control processes:</p><blockquote>We will say that a system is an <em>optimizer</em> if it is internally searching through a search space (consisting of possible outputs, policies, plans, strategies, or similar) looking for those elements that score high according to some objective function that is explicitly represented within the system. <em>[...]</em> For example, a bottle cap causes water to be held inside the bottle, but it is not optimizing for that outcome since it is not running any sort of optimization algorithm.<a href=""https://intelligence.org/learned-optimization#bibliography"">(1)</a> Rather, bottle caps have been <em>optimized</em> to keep water in place.</blockquote><p>It makes sense to say that we aren&apos;t worried about bottlecaps when we think about the inner alignment problem. However, this also excludes much more powerful &apos;optimizers&apos; -- something more like a plant.</p><p>When does a powerful control process become an &apos;agent&apos;?</p><ul><li><strong>Bottlecaps:</strong> No meaningful actuators or sensors. Essentially inanimate. Does a particular job, possibly very well, but in a very predictable manner.</li><li><strong>Thermostats: </strong>Implements a negative feedback loop via a sensor, an actuator, and a policy of &quot;correcting&quot; things when sense-data indicates they are &quot;off&quot;. Actual thermostats explicitly represent the target temperature, but one can imagine things in this cluster which wouldn&apos;t -- in general, the connection between what is sensed and how things are &apos;corrected&apos; can be quite complex (involving many different sensors and actuators), so that no one place in the system explicitly represents the &apos;target&apos;.</li><li><strong>Plants:</strong> Plants are like very complex thermostats. They have no apparent &apos;target&apos; explicitly represented, but can clearly be thought of as relatively agentic, achieving complicated goals in complicated environments.</li><li><strong>Guided Missiles: </strong>These are also mostly in the &apos;thermostat&apos; category, but, guided missiles can use simple world-models (to track the location of the target). However, any &apos;planning&apos; is likely based on explicit formulae rather than any search. (I&apos;m not sure about actual guided missiles.) If so, a guided missile would still not be a selection process, and therefore lack a &quot;goal&quot; in the mesa-optimizer sense, despite having a world-model and explicitly reasoning about how to achieve an objective represented within that world-model. </li><li><strong>Chess Programs: </strong>A chess-playing program has to play each game well, and every move is significant to this goal. So, it is a control process. However, AI chess algorithms are based on explicit search. Many, many moves are considered, and each move is evaluated independently. This is a common pattern. The best way we know how to implement very powerful controllers <em>is</em> to use search inside (implementing a control process <em>using</em> a selection process). At that point, a controller seems clearly &apos;agent-like&apos;, and falls within the definition of optimizer used in the meso-optimization post. However, it seems to me that things become &apos;agent-like&apos; somewhere before this stage.</li></ul><p>(See also: <a href=""https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers"">adaptation-executers, not fitness maximizers</a>.)</p><p>I don&apos;t want to frame it as if there&apos;s &quot;one true distinction&quot; which we should be making, which I&apos;m claiming the mesa-optimization write-up got wrong. Rather, we should pay attention to the different distinctions we might make, studying the phenomena separately and considering the alignment/safety implications of each.</p><p>This is closely related to the discussion of <a href=""https://www.lesswrong.com/posts/nyCHnY7T5PHPLjxmN/open-question-are-minimal-circuits-daemon-free#bbBKuSwJaa35FjJi7"">upstream daemons vs downstream daemons</a>. A downstream-daemon seems more likely to be an optimizer in the sense of the mesa-optimization write-up; it is explicitly planning, which may involve search. These are more likely to raise concerns through explicitly reasoned out treacherous turns. An upstream-daemon <em>could</em> use explicit planning, but it could also be only a bottlecap/thermostat/plant. It might powerfully optimize for something in the controller sense without internally using selection. This might produce severe misalignment, but not through explicitly planned treacherous turns. (<em>Caveat</em>: we don&apos;t understand mesa-optimizers; an understanding sufficient to make statements such as these with confidence would be a significant step forward.)</p><p>It seems possible that one could invent a measure of &quot;control power&quot; which would rate highly-optimized-but-inanimate objects like bottlecaps very low, while giving a high score to thermostat-like objects which set up complicated negative feedback loops (even if they didn&apos;t use any search).</p><h1>Processes Within Processes</h1><p>I already mentioned the idea that the best way we know how to implement powerful control processes is through powerful selection (search) <em>inside</em> of the controller.</p><p>To elaborate a bit on that: a controller with a search inside would typically have some kind of model of the environment, which it uses by searching for good actions/plans/policies for achieving its goals. So, measuring the optimization power <em>as a controller</em>, we look at how successful it is at achieving its goals in the real environment. Measuring the optimization power <em>as a selector</em>, we look at how good it is at choosing high-value options <em>within its world-model.</em> The search can only do as well as its model can tell it; however, in some sense, the agent is ultimately judged by the true consequences of its actions.</p><p>IE, in this case, the selection vs control distinction is a map/territory distinction. I think this is part of why I get so annoyed at things which mix up selection and control: it looks like a map/territory error to me.</p><p>However, this is not the only way selection and control commonly relate to each other.</p><p>Effective controllers are very often designed through a search process. This might be search taking place within a model, again (for example, training a neural network to control a robot, but getting its gradients from a physics simulation so that you can generate a large number of training samples relatively cheaply) or the real world (evolution by natural selection, &quot;evaluating&quot; genetic code by seeing what survives).</p><p>Further complicating things, a powerful search algorithm generally has some &quot;smarts&quot; to it, ie, it is good at choosing what option to evaluate next based on the current state of things. This &quot;smarts&quot; is controller-style smarts: every choice matters (because every evaluation costs processing power), there&apos;s no back-tracking, and you have to hit a narrow target in one shot. (Whatever the target of the underlying search problem, the target of the search-controller is: <em>find that target, <strong>quickly</strong>.</em>) And, of course, it is possible that such a search-controller will even use a model of the fitness landscape, and plan its next choice via its own search! </p><p>(I&apos;m not making this up as a weird hypothetical; actual algorithms such as estimation-of-distribution algorithms will make models of the fitness landscape. For obvious reasons, searching for good points in such models is usually avoided; however, in cases where evaluation of points is expensive enough, it may be worth it to explicitly plan out test-points which will reveal the most information about the fitness landscape, so that the best point can be selected later.)</p><h1>Blurring the Lines: What&apos;s the Critical Distinction?</h1><p>I mentioned earlier that this dichotomy seems more like a conceptual cluster than a fully formal distinction. I mentioned a number of big differences which stick out at me. Let&apos;s consider some of these in more detail.</p><h2>Perfect Feedback</h2><p>The classical sort of search algorithm I described as my central example of a selection process includes the ability to get a perfect evaluation of any option. The difficulty arises only from the very large number of options available. Control processes, on the other hand, appear to have very bad feedback, since you can&apos;t know the full outcome until it is too late to do anything about it. Can we use this as our definition?</p><p>I would agree that a search process in which the cost of evaluation goes to infinity becomes purely a control process: you can&apos;t perform any filtering of possibilities based on evaluation, so, you have to output one possibility and try to make it a good one (with no guarantees). Maybe you get some information about the objective function (like its source code), and you have to try to use that to choose an option. That&apos;s your sensors and actuators. They have to be very clever to achieve very good outcomes. The cheaper it is to evaluate the objective function on examples, the less &quot;control&quot; you need (the more you can just do brute-force search). In the opposite extreme, evaluating options is so cheap that you can check all of them, and output the maximum directly.</p><p>While this is somewhat appealing, it doesn&apos;t capture every case. Search algorithms today (such as stochastic gradient descent) often have imperfect feedback. Game-tree search deals with an objective function which is much too costly to evaluate directly (the quality of a move), but can be optimized for nonetheless by recursively searching for good moves in subgames down the game tree (mixed with approximate evaluations such as rollouts or heuristic board evaluations). I still think of both of these as solidly on the &quot;selection process&quot; side of things.</p><p>On the control process side, it is possible to have perfect feedback without doing any search. Thermostats realistically have noisy information about the temperature of a room, but, you can imagine a case where they get perfect information. It isn&apos;t any less a controller, or more a selection process, for that fact.</p><h2>Choices Don&apos;t Change Later Choices</h2><p>Another feature I mentioned was that in selection processes, all options are available to try at any time, and what you look at now does not change how good any option will be later. On the other hand, in a control process, previous choices can totally change how good particular later choices would be (as in reinforcement learning), or change what options are even available (as in game playing).</p><p>First, let me set two complications aside.</p><ul><li>Weird decision theory cases: it is theoretically possible to screw with a search by giving it an objective function which depends on its choices during search. This doesn&apos;t seem that interesting for our purposes here. (And that&apos;s coming from <em>me...</em>)</li><li>Local search limits the &quot;options&quot; to small modifications of the option just considered. I don&apos;t think this is blurring the lines between search and control; rather, it is more like using a controller within a smart search to try to increase efficiency, as I discussed at the end of the processes-within-processes section. All the options are still &quot;available&quot; at all times; the search algorithm just happens to be one which limits itself to considering a smaller list.</li></ul><p>I do think some cases blur the lines here, though. My primary example is the <a href=""https://en.wikipedia.org/wiki/Multi-armed_bandit"">multi-armed bandit problem</a>. This is a special case of the RL problem in which the history doesn&apos;t matter; every option is equally good every time, except for some random noise. Yet, to me, it is still a control problem. Why? Because every decision matters. The feedback you get about how good a particular choice was isn&apos;t just thought of as <em>information</em>; you &quot;actually get&quot; the good/bad outcome each time. That&apos;s the essential character of the multi-armed bandit problem: you have to trade off between experimentally trying options you&apos;re uncertain about vs sticking with the options which seem best so far, because every selection carries weight.</p><p>This leads me to the next proposed definition.</p><h2>Offline vs Online</h2><p>Selection processes are like offline algorithms, whereas control processes are like online algorithms.</p><p>With offline algorithms, you only really care about the end results. You are OK running gradient descent for millions of iterations before it starts doing anything cool, so long as it eventually does something cool.</p><p>With online algorithms, you care about each outcome individually. You would probably not want to be gradient-descent-training a neural network in live user-servicing code on a website, because live code has to be acceptably good from the start. Even if you can initialize the neural network to something acceptably good, you&apos;d hesitate to run stochastic gradient descent on it live, because stochastic gradient descent can sometimes dramatically decrease performance for a while before improving performance again.</p><p>Furthermore, online algorithms have to deal with <a href=""https://en.wikipedia.org/wiki/Stationary_process"">non-stationarity</a>. This seems suitably like a control issue.</p><p>So, selection processes are &quot;offline optimization&quot;, whereas control processes are &quot;online optimization&quot;: optimizing things &quot;as they progress&quot; rather than statically. (Note that the notion of &quot;online optimization&quot; implied by this line of thinking is slightly different from the <a href=""https://en.wikipedia.org/wiki/Online_optimization"">common definition of online optimization</a>, though related.)</p><p>The offline vs online distinction also has a lot to do with the sorts of mistakes I think people are making when they confuse selection processes and control processes. Reinforcement learning, as a subfield of AI, was obviously motivated from a highly online perspective. However, it is very often used as an offline algorithm today, to <em>produce</em> effective agents, rather than <em>as</em> an effective agent. So, that there&apos;s been some mismatch between the motivations which shaped the paradigm and actual use. This perspective made it less surprising when <a href=""https://arxiv.org/abs/1703.03864"">black-box optimization beat reinforcement learning on some problems</a> (<a href=""https://arxiv.org/abs/1712.06567"">see also</a>).</p><p>This seems like the best definition so far. However, I personally still feel like it is still missing something important. Selection vs control feels to me like a <em>type</em> distinction, closer to map-vs-territory.</p><p>To give an explicit counterexample: evolution by natural selection is obviously a selection process according to the distinction as I make it, but it seems much more like an online algorithm than on offline one, if we try to judge it as such.</p><h2>Internal Features vs Context</h2><p>Returning to the definition in mesa-optimizers (emphasis mine):</p><blockquote>Whether a system is an optimizer is a property of its <em><strong>internal structure</strong></em>&#x2014;what algorithm it is physically implementing&#x2014;and not a property of its input-output behavior. Importantly, the fact that a system&#x2019;s behavior results in some objective being maximized does not make the system an optimizer.</blockquote><p>The notion of a selection process says a lot about what is actually happening inside a selection process: there is a space of options, which can be enumerated; it is trying them; there is some kind of evaluation; etc.</p><p>The notion of control process, on the other hand, is more <em>externally</em> defined. It doesn&apos;t matter what&apos;s going on inside of the controller. All that matters is how effective it is at what it does.</p><p>A selection process -- such as a neural network learning algorithm -- <em>can</em> be regarded &quot;from outside&quot;, asking questions about how the <em>one</em> output of the algorithm does in the <em>true</em> environment. In fact, this kind of thinking is what we do when we think about generalization error.</p><p>Similarly, we <em>can</em> analyze a control process &quot;from inside&quot;, trying to find the pieces which correspond to beliefs, goals, plans, and so on (or postulate what they would look like if they existed -- as must be done in the case of controllers which truly lack such moving parts). This is the decision-theoretic view.</p><p>However, one might argue that viewing selection processes from the outside is viewing them <em>as control</em> -- viewing them as essentially having one shot at overall decision quality. Similarly, viewing control process from inside is essentially viewing it <em>as selection</em> -- the decision-theoretic view gives us a version of a control problem which we can solve by mathematical optimization.</p><p>In this view, selection vs control doesn&apos;t really cluster <em>different types of object</em>, but rather, <em>different types of analysis</em>. To a large extent, we can cluster objects by what kind of analysis we would more often want to do. However, certain cases (such as a game-playing AI) are best viewed through both lenses (as a controller, in the context of doing well in a real game against a human, and as a selection process, when thinking about the game-tree search).</p><p>Overall, I think I&apos;m probably still somewhat confused about the whole selection vs control issue, particularly as it pertains to the question of how decision theory can apply to things in the world.</p>",abramdemski,abramdemski,abramdemski,
SDELNzyboMpZpDwSg,FB/Discord Style Reacts,fb-discord-style-reacts,https://www.lesswrong.com/posts/SDELNzyboMpZpDwSg/fb-discord-style-reacts,2019-06-01T21:34:27.167Z,77,24,95,False,False,,"<p>For the past year I&#x27;ve wanted LessWrong to include something like Discord, Facebook or Slashdot style reactions.</p><p><em>Facebook Style</em> means &quot;there&#x27;s a few key reactions that people use&quot;</p><p><em>Discord Style</em> means &quot;there&#x27;s nigh-infinite reactions and you can add more, but there still end up being a few commonly used defaults.&quot;</p><p><em>Slashdot Style</em> means &quot;after upvoting or downvoting, you have the option of clicking a button that clarifies why you upvoted or downvoted.&quot;</p><p>Of these, I&#x27;m most excited for Discord-Style. But I think any of them would be improvements (if done well)</p><p>Habryka recently <a href=""https://www.lesswrong.com/posts/EQJfdqSaMcJyR5k73/habryka-s-shortform-feed#HefgGrp3nMwf5gKkd"">wrote a shortform comment on this subject</a>. My own thoughts come in a few different frames.</p><hr class=""dividerBlock""/><h1>Separating Enthusiasm from Approval</h1><p><strong>Boos/Yays vs &#x27;approve/disapprove&#x27;</strong></p><p>Empirically, people want to cheer for their causes, boo causes they dislike, signal their social allegiance and try to ensure the overton window moves in the direction they want. I don&#x27;t think you can really fight this. But you can nudge people to disentangle this from &quot;what gets attentional allocation on a site about rationality.&quot;</p><p>I think it&#x27;s important that when you see a comment you like, and you feel the impulse to go &quot;yeah! good point! go team!&quot; the first impulse you have, the first button available and exciting to click, is a button that <em>doesn&#x27;t</em> send any signals about how that comment should be sorted, and doesn&#x27;t aggregate into an overall user-score you can check (that, for good or for ill, people will tend to associate with social status)</p><p><strong>O<em>ther things</em> vs &#x27;approve/disapprove&#x27;</strong></p><p>Boos/yays aren&#x27;t the only thing I&#x27;m worried about. Ideally, I want LessWrong to reward good thinking over things like being funny, or exciting. (Being funny and exciting should still get rewarded, but no amount of clever injokes should add up to something greater than &quot;wrote an actually useful, insightful point.&quot;)</p><p><strong>&quot;Viscerally Fun but Low Signal Buttons&quot; should be easy to access. &quot;Higher Signal&quot; buttons should require more effort and thought.</strong></p><p>With both of the above in mind, I think it&#x27;s important that &quot;Yay&quot;, or &quot;Funny&quot; buttons should be the first, most obvious thing to click on. They should feel satisfying to click, and you shouldn&#x27;t feel motivated to click more things if that&#x27;s the only reason you were upvoting.</p><p>The buttons that send more important signals should require a bit of extra effort, and force you to at least notice some cognitive dissonance if you&#x27;re upvoting people just because they&#x27;re on your side.</p><h1>Social Entanglement, Epistemic Entanglement and Common Knowledge</h1><p>One react someone expressed interested in was a simple &quot;acknowledged.&quot; Votes are totally anonymous, and that means if you want someone to know that you have read a thing, you have to actually comment, which is moderately high effort and takes up a lot of vertical space on the page. Whether someone has read a thing is fairly important information about how to continue a conversation.</p><p>By default, on many social-media platforms, likes are public. They were also public on the old Intelligent Agent Foundations Forum (and I think probably on Arbital, although not sure offhand). </p><p>This does two things, which I have mixed feelings about.</p><p>One is social entanglement. Visibly liking each other&#x27;s comments is part of the process by which people build social trust and alliances. I think there&#x27;s reason to be cautious about LessWrong directly facilitating that. </p><p>Another is clarity on <em>who believes what</em>, and whose judgment you trust. When you&#x27;re building a serious, complex idea, it&#x27;s actually important who understands what concepts, who thinks different concepts are important. There are people I <em>do</em> in fact trust more intellectually than others, and it&#x27;s higher signal to know that one of them liked a post than some rando. It&#x27;s also more informative when I know that multiple people I trust disagree.</p><p>My current best guess is that it&#x27;s best for the voting on LessWrong to be anonymous, but for reactions to display usernames on hover-over. It might or might not be feasible or desirable (from a UI complexity standpoint) to let people choose whether to react publicly. But I can imagine changing my mind about this.</p><hr class=""dividerBlock""/><h1>Making it lower effort to give feedback. </h1><p>Receiving a downvote without explanation sucks. Some people complain about this – &quot;can&#x27;t you provide reasons for your downvotes?&quot; Well, no. Trivial inconveniences matter. If you force people to provide information and figure out how to articulate what&#x27;s wrong with something, people will probably just stop giving feedback rather than actually providing reasons.</p><p>Not only does this require figuring out how to write a comment, it opens up a line of engagement that you might have to put <em>even more</em> effort into defending.</p><p>[this is an empirical claim, it&#x27;s perhaps worth the experiment of requiring downvotes to always require reasons, but I&#x27;m not optimistic about it].</p><p>But I think there are some fairly common reasons why a comment gets downvoted, that could at least make it lower-effort to give feedback:</p><ul><li>&quot;This comment seemed a bit confused&quot;</li><li>&quot;This comment seemed to be rounding things off in an oversimplified way&quot;</li><li>&quot;This comment seems wrong in ways that have previously been explored at length on LessWrong&quot;</li><li>&quot;This comment seems mean spirited.&quot;</li><li>&quot;This comment seemed to be acting in bad faith&quot;</li></ul><p>It&#x27;s also nice to improve the reward signal for particularly good actions:</p><ul><li>&quot;This comment was particularly clear&quot;</li><li>&quot;This comment made special effort to be rigorous and credible.&quot;</li><li>&quot;This comment actually changed my mind about something.&quot;</li><li>&quot;This comment made special effort to be charitable&quot;</li></ul><hr class=""dividerBlock""/><h1>An issue re: Simplicity of Concepts</h1><p>You&#x27;ll notice some issues, comparing the above feedbacks to Facebook Reacts.</p><p>Facebook reacts are &quot;haha!&quot; &quot;love!&quot; &quot;sad!&quot; &quot;anger!&quot; &quot;wow!&quot;</p><p>Everyone knows what those mean. Everyone knows that everyone else knows what those mean. They are very short words. They are (due to millennia of evolution, genetic and cultural) conceptually simple.</p><p>&quot;This comment seems to be rounding things off in an oversimplified way&quot; is a less common concept. It&#x27;s more complicated. And if you simplified it slightly so that the button said &quot;Oversimplified&quot;... that would... actually be an oversimplified button. It&#x27;s important that I&#x27;m just saying &quot;yo this comment was oversimplified&quot;, but rather that it seemed (probably) to be making a subtle error.</p><p>I think this is really important. I think something LessWrong needs to do is nuanced critiques <em>easier to chunk</em>. This is pretty tricky, since, well, the whole point of nuances is that they&#x27;re <em>nuanced.</em></p><p>A rationalist friend once commented, in non-rationalist circles, that when they tried to say &quot;I agree with your point but I think this particular part has a logical error&quot;, they would often have people... just completely fail to parse that. It wasn&#x27;t in their schema at all.</p><p>On LessWrong, we have some shared context where we mostly all understand not to just have Arguments Be Soldiers and whatnot. Our schema includes Local Validity. But there are many important, key concepts that still take a lot more effort to express than &quot;yay/boo&quot; or &quot;haha!&quot;</p><p>And thing is... it&#x27;s not like &quot;Love&quot; is a simple concept. When someone clicks &#x27;Love&#x27; on one of my facebook posts, there is a fairly rich wave of senses I get (depending on my post, and depending on my relationship with the person in question). When someone posts about their pet dying and I click &#x27;Love&#x27;, there&#x27;s this whole shared context about how we&#x27;re both human and we know what it is to lose people and my heart goes out to them and I chest tenses slightly and there&#x27;s... just a whole lot going on.</p><p>Still, I&#x27;m able to chunk that complexity into a concept called &quot;Love&quot;, and it&#x27;s easily available for me to access.</p><p>There&#x27;s a potential longterm vision for LessWrong – maybe not the right vision, but possible – where part of what we&#x27;re doing here is distilling concepts down so thoroughly that a single word can communicate a lot of nuance. </p><p>Language real estate is limited, and I&#x27;m not sure which concepts make the most sense to distill in such a way. There&#x27;s also certainly room for this to fail, where instead of being able to more-easily-express nuanced concepts it ends up destroying nuance. </p><p>Facebook has cheapened the word &quot;friend&quot;, and that&#x27;s important. But... I also have an impression of it having made it <em>easier for me to express love</em>, in a way that so far seems net positive.</p><p>It feels exciting to me to imagine one day living on a world where<em> &quot;this changed my mind&quot;</em> or <em>&quot;this was well thought even though I disagree&quot;</em> feel like basic, obvious concepts that are important enough to be communicated with a single word.</p><br/>",Raemon,raemon,Raemon,
q2rCMHNXazALgQpGH,Conditions for Mesa-Optimization,conditions-for-mesa-optimization,https://www.lesswrong.com/posts/q2rCMHNXazALgQpGH/conditions-for-mesa-optimization,2019-06-01T20:52:19.461Z,86,40,48,False,False,,"<html><head><style type=""text/css"">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head><body><p><em>This is the second of five posts in the <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">Risks from Learned Optimization Sequence</a> based on the paper “<a href=""https://arxiv.org/abs/1906.01820"">Risks from Learned Optimization in Advanced Machine Learning Systems</a>” by Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Each post in the sequence corresponds to a different section of the paper.</em></p>
<p>&nbsp;</p>
<p>In this post, we consider how the following two components of a particular machine learning system might influence whether it will produce a <a href=""https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction"">mesa-optimizer</a>:</p>
<ol>
<li><strong>The task:</strong> The training distribution and base objective function.</li>
<li><strong>The base optimizer:</strong> The machine learning algorithm and model architecture.</li>
</ol>
<p>We deliberately choose to present theoretical considerations for why mesa-optimization may or may not occur rather than provide concrete examples. Mesa-optimization is a phenomenon that we believe will occur mainly in machine learning systems that are more advanced than those that exist today.<sup class=""footnote-ref""><a href=""#fn-EwoZYq7rcmQ38kmiy-1"" id=""fnref-EwoZYq7rcmQ38kmiy-1"">[1]</a></sup> Thus, an attempt to induce mesa-optimization in a <em>current</em> machine learning system would likely require us to use an artificial setup specifically designed to induce mesa-optimization. Moreover, the limited interpretability of neural networks, combined with the fact that there is no general and precise definition of “optimizer,” means that it would be hard to evaluate whether a given model is a mesa-optimizer.</p>
<p>&nbsp;</p>
<h2>2.1. The task</h2>
<p>Some tasks benefit from mesa-optimizers more than others. For example, tic-tac-toe can be perfectly solved by simple rules. Thus, a base optimizer has no need to generate a mesa-optimizer to solve tic-tac-toe, since a simple learned algorithm implementing the rules for perfect play will do. Human survival in the savanna, by contrast, did seem to benefit from mesa-optimization. Below, we discuss the properties of tasks that may influence the likelihood of mesa-optimization.</p>
<p><strong>Better generalization through search.</strong> To be able to consistently achieve a certain level of performance in an environment, we hypothesize that there will always have to be some minimum amount of optimization power that must be applied to find a policy that performs that well.</p>
<p>To see this, we can think of optimization power as being measured in terms of the number of times the optimizer is able to divide the search space in half—that is, the number of bits of information provided.<a href=""https://intelligence.org/learned-optimization#bibliography"">(9)</a> After these divisions, there will be some remaining space of policies that the optimizer is unable to distinguish between. Then, to ensure that all policies in the remaining space have some minimum level of performance—to provide a performance lower bound<sup class=""footnote-ref""><a href=""#fn-EwoZYq7rcmQ38kmiy-2"" id=""fnref-EwoZYq7rcmQ38kmiy-2"">[2]</a></sup> —will always require the original space to be divided some minimum number of times—that is, there will always have to be some minimum bits of optimization power applied.</p>
<p>However, there are two distinct levels at which this optimization power could be expended: the base optimizer could expend optimization power selecting a highly-tuned learned algorithm, or the learned algorithm could itself expend optimization power selecting highly-tuned actions.</p>
<p>As a mesa-optimizer is just a learned algorithm that itself performs optimization, the degree to which mesa-optimizers will be incentivized in machine learning systems is likely to be dependent on which of these levels it is more advantageous for the system to perform optimization. For many current machine learning models, where we expend vastly more computational resources training the model than running it, it seems generally favorable for most of the optimization work to be done by the base optimizer, with the resulting learned algorithm being simply a network of highly-tuned heuristics rather than a mesa-optimizer.</p>
<p>We are already encountering some problems, however—Go, Chess, and Shogi, for example—for which this approach does not scale. Indeed, our best current algorithms for those tasks involve explicitly making an optimizer (hard-coded Monte-Carlo tree search with learned heuristics) that does optimization work on the level of the learned algorithm rather than having all the optimization work done by the base optimizer.<a href=""https://intelligence.org/learned-optimization#bibliography"">(10)</a> Arguably, this sort of task is only adequately solvable this way—if it were possible to train a straightforward DQN agent to perform well at Chess, it plausibly would <em>have</em> to learn to internally perform something like a tree search, producing a mesa-optimizer.<sup class=""footnote-ref""><a href=""#fn-EwoZYq7rcmQ38kmiy-3"" id=""fnref-EwoZYq7rcmQ38kmiy-3"">[3]</a></sup></p>
<p>We hypothesize that the attractiveness of search in these domains is due to the diverse, branching nature of these environments. This is because search—that is, optimization—tends to be good at generalizing across diverse environments, as it gets to individually determine the best action for each individual task instance. There is a general distinction along these lines between optimization work done on the level of the learned algorithm and that done on the level of the base optimizer: the learned algorithm only has to determine the best action for a given task instance, whereas the base optimizer has to design heuristics that will hold regardless of what task instance the learned algorithm encounters. Furthermore, a mesa-optimizer can immediately optimize its actions in novel situations, whereas the base optimizer can only change the mesa-optimizer's policy by modifying it ex-post. Thus, for environments that are diverse enough that most task instances are likely to be completely novel, search allows the mesa-optimizer to adjust for that new task instance immediately.</p>
<p>For example, consider reinforcement learning in a diverse environment, such as one that directly involves interacting with the real world. We can think of a diverse environment as requiring a very large amount of computation to figure out good policies before conditioning on the specifics of an individual instance, but only a much smaller amount of computation to figure out a good policy once the specific instance of the environment is known. We can model this observation as follows.</p>
<p>Suppose an environment is composed of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""N""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span></span></span></span> different instances, each of which requires a completely distinct policy to succeed in.<sup class=""footnote-ref""><a href=""#fn-EwoZYq7rcmQ38kmiy-4"" id=""fnref-EwoZYq7rcmQ38kmiy-4"">[4]</a></sup> Let <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span></span></span></span> be the optimization power (measured in bits<a href=""https://intelligence.org/learned-optimization#bibliography"">(9)</a>) applied by the base optimizer, which should be approximately proportional to the number of training steps. Then, let <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> be the optimization power applied by the learned algorithm in each environment instance and <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""f(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span> the total amount of optimization power the base optimizer must put in to get a learned algorithm capable of performing that amount of optimization.<sup class=""footnote-ref""><a href=""#fn-EwoZYq7rcmQ38kmiy-5"" id=""fnref-EwoZYq7rcmQ38kmiy-5"">[5]</a></sup> We will assume that the rest of the base optimizer's optimization power, <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P - f(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span>, goes into tuning the learned algorithm's policy. Since the base optimizer has to distribute its tuning across all <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""N""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span></span></span></span> task instances, the amount of optimization power it will be able to contribute to each instance will be <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\frac{P - f(x)}{N}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mfrac""><span class=""mjx-box MJXc-stacked"" style=""width: 2.566em; padding: 0px 0.12em;""><span class=""mjx-numerator"" style=""font-size: 70.7%; width: 3.629em; top: -1.706em;""><span class=""mjx-mrow"" style=""""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span class=""mjx-denominator"" style=""font-size: 70.7%; width: 3.629em; bottom: -0.682em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span><span style=""border-bottom: 1.3px solid; top: -0.296em; width: 2.566em;"" class=""mjx-line""></span></span><span style=""height: 1.689em; vertical-align: -0.482em;"" class=""mjx-vsize""></span></span></span></span></span></span>, under the previous assumption that each instance requires a completely distinct policy. On the other hand, since the learned algorithm does all of its optimization at runtime, it can direct all of it into the given task instance, making its contribution to the total for each instance simply <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>.<sup class=""footnote-ref""><a href=""#fn-EwoZYq7rcmQ38kmiy-6"" id=""fnref-EwoZYq7rcmQ38kmiy-6"">[6]</a></sup></p>
<p>Thus, if we assume that, for a given <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span></span></span></span></span>, the base optimizer will select the value of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> that maximizes the minimum level of performance, and thus the total optimization power applied to each instance, we get<sup class=""footnote-ref""><a href=""#fn-EwoZYq7rcmQ38kmiy-7"" id=""fnref-EwoZYq7rcmQ38kmiy-7"">[7]</a></sup></p>
<p><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""
    x^* = \text{argmax}_x~ \frac{P - f(x)}{N} + x.
""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mo"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.298em;"">∗</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.519em;"">argmax</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.291em; padding-bottom: 0.372em;"">&nbsp;</span></span><span class=""mjx-mfrac""><span class=""mjx-box MJXc-stacked"" style=""width: 4.073em; padding: 0px 0.12em;""><span class=""mjx-numerator"" style=""width: 4.073em; top: -1.59em;""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span class=""mjx-denominator"" style=""width: 4.073em; bottom: -0.773em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span><span style=""border-bottom: 1.3px solid; top: -0.296em; width: 4.073em;"" class=""mjx-line""></span></span><span style=""height: 2.363em; vertical-align: -0.773em;"" class=""mjx-vsize""></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.372em;"">.</span></span></span></span></span></span></p>
<p>As one moves to more and more diverse environments—that is, as <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""N""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span></span></span></span> increases—this model suggests that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> will dominate <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\frac{P - f(x)}{N}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mfrac""><span class=""mjx-box MJXc-stacked"" style=""width: 2.566em; padding: 0px 0.12em;""><span class=""mjx-numerator"" style=""font-size: 70.7%; width: 3.629em; top: -1.706em;""><span class=""mjx-mrow"" style=""""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span class=""mjx-denominator"" style=""font-size: 70.7%; width: 3.629em; bottom: -0.682em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span><span style=""border-bottom: 1.3px solid; top: -0.296em; width: 2.566em;"" class=""mjx-line""></span></span><span style=""height: 1.689em; vertical-align: -0.482em;"" class=""mjx-vsize""></span></span></span></span></span></span>, implying that mesa-optimization will become more and more favorable. Of course, this is simply a toy model, as it makes many questionable simplifying assumptions. Nevertheless, it sketches an argument for a pull towards mesa-optimization in sufficiently diverse environments.</p>
<p>As an illustrative example, consider biological evolution. The environment of the real world is highly diverse, resulting in non-optimizer policies directly fine-tuned by evolution—those of plants, for example—having to be very simple, as evolution has to spread its optimization power across a very wide range of possible environment instances. On the other hand, animals with nervous systems can display significantly more complex policies by virtue of being able to perform their own optimization, which can be based on immediate information from their environment. This allows sufficiently advanced mesa-optimizers, such as humans, to massively outperform other species, especially in the face of novel environments, as the optimization performed internally by humans allows them to find good policies even in entirely novel environments.</p>
<p><strong>Compression of complex policies.</strong> In some tasks, good performance requires a very complex policy. At the same time, base optimizers are generally biased in favor of selecting learned algorithms with lower complexity. Thus, all else being equal, the base optimizer will generally be incentivized to look for a highly compressed policy.</p>
<p>One way to find a compressed policy is to search for one that is able to use general features of the task structure to produce good behavior, rather than simply memorizing the correct output for each input. A mesa-optimizer is an example of such a policy. From the perspective of the base optimizer, a mesa-optimizer is a highly-compressed version of whatever policy it ends up implementing: instead of explicitly encoding the details of that policy in the learned algorithm, the base optimizer simply needs to encode how to search for such a policy. Furthermore, if a mesa-optimizer can determine the important features of its environment at runtime, it does not need to be given as much prior information as to what those important features are, and can thus be much simpler.</p>
<p>This effect is most pronounced for tasks with a broad diversity of details but common high-level features. For example, Go, Chess, and Shogi have a very large domain of possible board states, but admit a single high-level strategy for play—heuristic-guided tree search—that performs well across all board states.<a href=""https://intelligence.org/learned-optimization#bibliography"">(10)</a> On the other hand, a classifier trained on random noise is unlikely to benefit from compression at all.</p>
<p>The environment need not necessarily be too diverse for this sort of effect to appear, however, as long as the pressure for low description length is strong enough. As a simple illustrative example, consider the following task: given a maze, the learned algorithm must output a path through the maze from start to finish. If the maze is sufficiently long and complicated then the specific strategy for solving this particular maze—specifying each individual turn—will have a high description length. However, the description length of a general optimization algorithm for finding a path through an arbitrary maze is fairly small. Therefore, if the base optimizer is selecting for programs with low description length, then it might find a mesa-optimizer that can solve all mazes, despite the training environment only containing one maze.</p>
<p><strong>Task restriction.</strong> The observation that diverse environments seem to increase the probability of mesa-optimization suggests that one way of reducing the probability of mesa-optimizers might be to keep the tasks on which AI systems are trained highly restricted. Focusing on building many individual AI services which can together offer all the capabilities of a generally-intelligent system rather than a single general-purpose artificial general intelligence (AGI), for example, might be a way to accomplish this while still remaining competitive with other approaches.<a href=""https://intelligence.org/learned-optimization#bibliography"">(11)</a></p>
<p><strong>Human modeling.</strong> Another aspect of the task that might influence the likelihood of mesa-optimization is the presence of humans in the environment.<a href=""https://intelligence.org/learned-optimization#bibliography"">(12)</a> Since humans often act as optimizers, reasoning about humans will likely involve reasoning about optimization. A system capable of reasoning about optimization is likely also capable of reusing that same machinery to do optimization itself, resulting in a mesa-optimizer. For example, it might be the case that predicting human behavior requires instantiating a process similar to human judgment, complete with internal motives for making one decision over another.</p>
<p>Thus, tasks that do not benefit from human modeling seem less likely to produce mesa-optimizers than those that do. Furthermore, there are many tasks that might benefit from human modeling that don't explicitly involve modeling humans—to the extent that the training distribution is generated by humans, for example, modeling humans might enable the generation of a very good prior for that distribution.</p>
<p>&nbsp;</p>
<h2>2.2. The base optimizer</h2>
<p>It is likely that certain features of the base optimizer will influence how likely it is to generate a mesa-optimizer. First, though we largely focus on reinforcement learning in this sequence, RL is not necessarily the only type of machine learning where mesa-optimizers could appear. For example, it seems plausible that mesa-optimizers could appear in generative adversarial networks.</p>
<p>Second, we hypothesize that the details of a machine learning model's architecture will have a significant effect on its tendency to implement mesa-optimization. For example, a tabular model, which independently learns the correct output for every input, will never be a mesa-optimizer. On the other hand, if a hypothetical base optimizer looks for the program with the shortest source code that solves a task, then it is more plausible that this program will itself be an optimizer.<a href=""https://intelligence.org/learned-optimization#bibliography"">(13)</a> However, for realistic machine learning base optimizers, it is less clear to what extent mesa-optimizers will be selected for. Thus, we discuss some factors below that might influence the likelihood of mesa-optimization one way or the other.</p>
<p><strong>Reachability.</strong> There are many kinds of optimization algorithms that a base optimizer could implement. However, almost every training strategy currently used in machine learning uses some form of local search (such as gradient descent or even genetic algorithms). Thus, it seems plausible that the training strategy of more advanced ML systems will also fall into this category. We will call this general class of optimizers that are based on local hill-climbing <em>local optimization processes.</em></p>
<p>We can then formulate a notion of <em>reachability,</em> the difficulty for the base optimizer to find any given learned algorithm, which we can analyze in the case of a local optimization process. A local optimization process might fail to find a particular learned algorithm that would perform very well on the base objective if the learned algorithm is surrounded by other algorithms that perform poorly on the base objective. For a mesa-optimizer to be produced by a local optimization process, it needs to not only perform well on the base objective, but also be <em>reachable;</em> that is, there needs to be a path through the space of learned algorithms to it that is approximately monotonically increasing. Furthermore, the degree to which the path only need be approximate—that is, the degree to which ML training procedures can escape local optima—is likely to be critical, as optimization algorithms are complex enough that it might require a significant portion of the algorithm to be present before performance gains start being realized.</p>
<p><strong>Algorithmic range.</strong> One key factor likely to determine the reachability of mesa-optimizers is the <em>algorithmic range</em> of the learned algorithms—that is, how extensive is the set of algorithms (or how expressive is the model space) capable of being found by the base optimizer. The more extensive a model's algorithmic range, the broader the space of possible learned algorithms, and thus the more likely that it will be able to find one that is a mesa-optimizer, assuming the base optimizer is incentivized to do so. For example, architectures that explicitly give the algorithm access to a wide range of possible computations, such as recurrent neural networks or neural Turing machines,<a href=""https://intelligence.org/learned-optimization#bibliography"">(14)</a> seem more likely to produce mesa-optimizers.</p>
<p><strong>Inductive biases.</strong> Another important factor is the degree to which the base optimizer is explicitly or implicitly biased in various ways. The nature of these inductive biases will contribute to the likelihood of a mesa-optimizer being selected for. One of the most important kinds of inductive bias is simplicity bias, which would almost certainly be exhibited by almost all base optimizers. We identify three ways in which simplicity bias can manifest itself:</p>
<ol>
<li>An explicit penalty due to parameter regularization or architectural constraints such as weight-sharing or sparse connections.</li>
<li>An implicit bias due to the model architecture. For example, it has been shown that neural networks are more likely to fit a simple function to a set of training data, even when no regularization is used.<a href=""https://intelligence.org/learned-optimization#bibliography"">(15)</a></li>
<li>The capacity limitations of the model. The size of a model imposes a hard limit on the complexity of the functions it is able to represent. Thus, to the degree that the base optimizer is selecting based on performance, it will be driven to “squeeze out” as much performance as it can for any given model capacity, leading to a bias in favor of relatively compressed policies.</li>
</ol>
<p>The more a base optimizer is biased towards simple solutions, the more it will be incentivized to find a compressed policy such as a mesa-optimizer.</p>
<p>The base optimizer could also be biased in other ways. For example, it could be biased towards algorithms with a low time or space complexity. If we view mesa-optimization as delegation of optimization work, a time penalty on the learned algorithm is likely to incentivize the base optimizer to do more precomputation itself by pre-computing relevant heuristics rather than delegating optimization work to the learned algorithm. Thus, we hypothesize that penalizing description length will favor mesa-optimizers while penalizing time complexity will disfavor them. This suggests the following speculative conjecture: neither a minimal-depth nor minimal-size boolean circuit that solves a problem can be a mesa-optimizer.<a href=""https://intelligence.org/learned-optimization#bibliography"">(16)</a></p>
<p>Lastly, another form of bias that might have particularly interesting effects is the presence of an information funnel. In deep learning, the base optimizer directly selects the values of individual parameters. In biological evolution, by contrast, the base optimizer selects DNA strings, which in turn produce the detailed structure of the brain only indirectly. This implicitly introduces pressure towards compressing the brain's structure. As we noted previously, this might favor the development of mesa-optimizers, as they can be seen as a way of compressing complex policies. Because deep learning algorithms currently select weights directly, there is no such information funnel. In the case of meta-learning over a machine learning model's architecture, however, an information funnel might present itself due to the fact that the meta-learner has to learn a single policy for creating architectures that is capable of solving all of the problems it is presented with.</p>
<p><strong>Statefulness.</strong> Another factor that could make it easier for a learned algorithm to implement a complex optimization process is the ability of the learned algorithm to save and recall information. The ability to refer back to previous state enables the caching of intermediate results, increasing the architecture's algorithmic range by giving the learned algorithm significantly more time to complete its computations. Statefulness, therefore, favors mesa-optimization by decreasing the implicit penalty on time complexity imposed by enforcing all computations to end when the learned algorithm produces its output. One simple form of this might be a recurrent neural network, which can perform computations with longer time horizons due to its ability to pass intermediate activations across different time steps. An even more extreme version of this might be a network that is able to store state not just across different time steps in the same episode, but also across episodes, enabling it to do more long-term learning, though such an architecture is currently completely hypothetical.</p>
<p><strong>Hard-coded optimization.</strong> One possible means of alleviating some of these issues might be to include hard-coded optimization where the learned algorithm provides only the objective function and not the optimization algorithm. The stronger the optimization performed explicitly, the less strong the optimization performed implicitly by the learned algorithm needs to be. For example, architectures that explicitly perform optimization that is relevant for the task—such as hard-coded Monte Carlo tree search—might decrease the benefit of mesa-optimizers by reducing the need for optimization other than that which is explicitly programmed into the system.</p>
<p>&nbsp;</p>
<p><em>The third post in the <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">Risks from Learned Optimization Sequence</a>, titled “The Inner Alignment Problem,” can be found <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J"">here</a>.</em></p>
<p><a href=""https://intelligence.org/learned-optimization/#glossary"">Glossary</a> | <a href=""https://intelligence.org/learned-optimization/#bibliography"">Bibliography</a></p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-EwoZYq7rcmQ38kmiy-1"" class=""footnote-item""><p>As of the date of this post. Note that we do examine some existing machine learning systems that we believe are close to producing mesa-optimization in post 5. <a href=""#fnref-EwoZYq7rcmQ38kmiy-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-EwoZYq7rcmQ38kmiy-2"" class=""footnote-item""><p>It is worth noting that the same argument also holds for achieving an average-case guarantee. <a href=""#fnref-EwoZYq7rcmQ38kmiy-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-EwoZYq7rcmQ38kmiy-3"" class=""footnote-item""><p>Assuming reasonable computational constraints.. <a href=""#fnref-EwoZYq7rcmQ38kmiy-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-EwoZYq7rcmQ38kmiy-4"" class=""footnote-item""><p>This definition of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""N""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span></span></span></span> is somewhat vague, as there are multiple different levels at which one can chunk an environment into instances. For example, one environment could always have the same high-level features but completely random low-level features, whereas another could have two different categories of instances that are broadly self-similar but different from each other, in which case it's unclear which has a larger <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""N""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span></span></span></span>. However, one can simply imagine holding <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""N""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span></span></span></span> constant for all levels but one and just considering how environment diversity changes on that level. <a href=""#fnref-EwoZYq7rcmQ38kmiy-4"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-EwoZYq7rcmQ38kmiy-5"" class=""footnote-item""><p>Note that this makes the implicit assumption that the amount of optimization power required to find a mesa-optimizer capable of performing <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> bits of optimization is independent of <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""N""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span></span></span></span></span>. The justification for this is that optimization is a general algorithm that looks the same regardless of what environment it is applied to, so the amount of optimization required to find an <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span>-bit optimizer should be relatively independent of the environment. That being said, it won't be completely independent, but as long as the primary difference between environments is how much optimization they need, rather than how hard it is to do optimization, the model presented here should hold. <a href=""#fnref-EwoZYq7rcmQ38kmiy-5"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-EwoZYq7rcmQ38kmiy-6"" class=""footnote-item""><p>Note, however, that there will be some maximum <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span> simply because the learned algorithm generally only has access to so much computational power. <a href=""#fnref-EwoZYq7rcmQ38kmiy-6"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-EwoZYq7rcmQ38kmiy-7"" class=""footnote-item""><p>Subject to the constraint that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""P - f(x) \ge 0""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.446em;"">≥</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span></span></span></span>. <a href=""#fnref-EwoZYq7rcmQ38kmiy-7"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
</body></html>",evhub,evhub,evhub,
YZeZXF6LwZn5vqFo9,The Fundamental Theorem of Asset Pricing: Missing Link of the Dutch Book Arguments,the-fundamental-theorem-of-asset-pricing-missing-link-of-the,https://www.lesswrong.com/posts/YZeZXF6LwZn5vqFo9/the-fundamental-theorem-of-asset-pricing-missing-link-of-the,2019-06-01T20:34:06.924Z,42,13,5,False,False,,"<p><em>Assumed background: <u><a href=""https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities"">Acyclic preferences, Dutch Book theorems</a></u></em></p><p>There are fairly elementary arguments that, in the absence of uncertainty, any preferences not described by a utility function are problematic - this is the circular preferences argument. There are also fairly elementary arguments that, <em>if</em> we handle uncertainty by taking weighted sums of utilities of different outcomes, <em>then</em> the weights should follow the usual rules of probability - these are the Dutch Book arguments. But in the middle there’s a jump: we need to assume that taking weighted sums of utilities makes sense for some reason. There are some high-powered theorems which make that jump (specifically the complete class theorem), but they’re not very mathematically accessible.</p><p>(If any of that sounds new, you should read <u><a href=""https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities"">Yudkowsky’s excellent intro to this stuff</a></u> before reading this post.)</p><p>It turns out that there <em>is</em> a relatively simple theorem which bridges the gap between deterministic utility and Dutch Book arguments. But rather than hanging out in decision theory textbooks, it’s been living it up in finance. It’s called the Fundamental Theorem of Asset Pricing (FTAP).</p><p>Here’s the setup. Just like the Dutch Book arguments, we have a bunch of tradable assets - i.e. betting contracts, like stock options or horse race bets. We have a bunch of possible outcomes - i.e. possible prices of an underlying stock at expiry, or possible winners of the horse race. Each asset&#x27;s final value will depend on the outcome. Then the FTAP states that either:</p><ul><li>There exists some portfolio of assets which costs $0 to buy (can include short sales) and is guaranteed a positive payout (i.e. arbitrage), or</li><li>There exists a probability distribution such that the price of each asset is the expected value of its payout (i.e. price is a weighted sum of possible outcome values).</li></ul><p>Note that this is exactly what we need to round out the Dutch Book arguments: either there exists an arbitrage opportunity, or we compare assets using a weighted sum of possible outcome values.</p><p>Let’s prove it. First, we’ll name some variables:</p><ul><li> <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""V_{ij}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.186em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.519em;"">j</span></span></span></span></span></span></span></span></span></span>: a big matrix which contains the value of each asset i under each possible outcome j. </li><li> <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""S_i""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.032em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span>: current price of asset i (we need P for probability, so S represents price).</li><li> <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""p_j""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.519em;"">j</span></span></span></span></span></span></span></span>: probability distribution over outcomes j (which may or may not exist)</li><li> <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""q_i""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.014em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;"">q</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span></span></span></span></span>: arbitrage portfolio (which may or may not exist)</li></ul><p>FTAP says that either:</p><ul><li>Arbitrage portfolio exists: profit <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\sum_i q_i V_{ij} > 0""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-munderover""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.519em; padding-bottom: 0.519em;"">∑</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-msubsup MJXc-space1""><span class=""mjx-base"" style=""margin-right: -0.014em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;"">q</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.186em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.519em;"">j</span></span></span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.372em;"">&gt;</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span></span></span></span> for all outcomes j, and the portfolio currently costs <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\sum_i q_i S_i = 0""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-munderover""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.519em; padding-bottom: 0.519em;"">∑</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-msubsup MJXc-space1""><span class=""mjx-base"" style=""margin-right: -0.014em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;"">q</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.032em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0</span></span></span></span></span></span>.</li><li>Probability distribution exists: <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""S_i = \sum_j V_{ij} p_j""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.032em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span></span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-munderover MJXc-space3""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-size1-R"" style=""padding-top: 0.519em; padding-bottom: 0.519em;"">∑</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.519em;"">j</span></span></span></span><span class=""mjx-msubsup MJXc-space1""><span class=""mjx-base"" style=""margin-right: -0.186em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.519em;"">j</span></span></span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.519em;"">j</span></span></span></span></span></span></span></span> </li></ul><p>I’ll state the proof informally - if you know a little linear algebra, it’s easy but tedious to formalize and see that it works. The key question is: how many assets, and how many possible outcomes? With N assets and M outcomes, our arbitrage condition has N variables (the q’s) and M+1 equations (one for each outcome plus the current cost constraint). Conversely, our probability distribution condition has M variables (the p’s) and N equations. We generally expect the system to be solvable when the number of variables is at least as large as the number of equations. So, either:</p><ul><li>N &gt; M (more assets than outcomes), and the arbitrage system (typically) has a solution, or</li><li>M &gt;= N (at least as many outcomes as assets), and the probability system (typically) has a solution.</li></ul><p>I’m brushing some stuff under the rug here - i.e. maybe there are more assets than outcomes, but the prices line up perfectly. That’s where the linear algebra comes in - the above works for full-rank V, but rank-deficient V requires checking the usual corner cases. If you take a math finance class, you’ll probably go through that tedium in its full glory, along with some more interesting extensions of the theorem.</p><p>Anyway, what have we shown? We actually haven’t established that the “probability distribution” p_j is a probability distribution - we’ve shown that the prices are described by <em>some</em> weighted sum of outcome values, but the weights could still be negative or not sum to 1. That’s fine - the usual Dutch Book arguments show that the weights are a probability distribution (or else there’s an arbitrage opportunity). We’ve bridged the gap.</p><p>All the usual considerations of the Dutch Book theorems still apply. “Arbitrage” means exactly the same thing here that it means in the Dutch Book theorems. As usual, we’re formulating things with “bets” and “contracts” and “arbitrage” and “prices”, but that can model a much wider range of phenomena.</p><p>One interesting point: the probability distribution may not be unique. There may be more than one possible distribution which satisfies the conditions. This works fine with the Dutch Book arguments: each possible distribution corresponds to a different prior.</p>",johnswentworth,johnswentworth,johnswentworth,
a7HjhGpzdmbaMb7Ee,I translated 'Twelve Virtues of Rationality' into Hebrew.,i-translated-twelve-virtues-of-rationality-into-hebrew,https://www.lesswrong.com/posts/a7HjhGpzdmbaMb7Ee/i-translated-twelve-virtues-of-rationality-into-hebrew,2019-06-01T18:52:02.436Z,26,11,14,False,False,,"<p><a href=""https://docs.google.com/document/d/1Y9TfVw_-w1vDP5hE39twq9d9qekuEkA8In0h8GW0_NE/edit?usp=sharing"">Here it is</a> - if you know Hebrew and have feedback, do give it, either in the comments here or in the document (it&#x27;s not fully edited).</p><p>I don&#x27;t really know what am i supposed to do with it now, though. where should i put it? can/should i put it on my future website? (of course i don&#x27;t own the translation anymore than the original piece)</p><br/><p>Regarding the translation, the thing I&#x27;m most uncertain of is (ironically) which word to use for &#x27;virtue&#x27;, if you have a better idea, please share.</p>",Yoav Ravid,yoav-ravid,Yoav Ravid,
c7JczqFQw3HgPFXFs,May gwern.net newsletter,may-gwern-net-newsletter,https://www.lesswrong.com/posts/c7JczqFQw3HgPFXFs/may-gwern-net-newsletter,2019-06-01T17:25:11.740Z,17,5,0,False,False,https://www.gwern.net/newsletter/2019/05,<html><head></head><body></body></html>,gwern,gwern,gwern,
hEM7HjSpNphfQ82jW,Welcome and Open Thread June 2019,welcome-and-open-thread-june-2019,https://www.lesswrong.com/posts/hEM7HjSpNphfQ82jW/welcome-and-open-thread-june-2019,2019-06-01T13:44:38.655Z,12,7,35,False,False,,"<p>If it’s worth saying, but not worth its own post, you can put it here.</p><p>Also,  if you are new to LessWrong and want to introduce yourself, this is the  place to do it. Personal stories, anecdotes, or just general comments  on how you found us and what you hope to get from the site and community  are welcome. If you want to explore the community more, I recommend <a href=""https://www.lesswrong.com/library"">reading the Library</a>, <a href=""https://www.lesswrong.com/?view=curated"">checking recent Curated posts</a>, and <a href=""https://www.lesswrong.com/community"">seeing if there are any meetups in your area</a>.</p><p>The Open Thread sequence is <a href=""https://www.lesswrong.com/s/yai5mppkuCHPQmzpN"">here</a>.</p>",roland,roland,roland,
NG4XQEL5PTyguDMff,"""But It Doesn't Matter""",but-it-doesn-t-matter,https://www.lesswrong.com/posts/NG4XQEL5PTyguDMff/but-it-doesn-t-matter,2019-06-01T02:06:30.624Z,55,38,17,False,False,,"<html><head></head><body><p>If you ever find yourself saying, ""Even if Hypothesis <em>H</em> is true, it doesn't have any decision-relevant implications,"" <em>you are rationalizing!</em> The fact that <em>H</em> is interesting enough for you to be considering the question at all (it's not some arbitrary trivium like the 1923th binary digit of π, or the low temperature in São Paulo on September 17, 1978) means that it must have some relevance to the things you care about. It is <em>vanishingly improbable</em> that your optimal decisions are going to be the <em>same</em> in worlds where <em>H</em> is true and worlds where <em>H</em> is false. The fact that you're tempted to <em>say</em> they're the same is probably because some part of you is afraid of some of the imagined consequences of <em>H</em> being true. But <em>H</em> is already true or already false! If you happen to live in a world where <em>H</em> is true, and you make decisions as if you lived in a world where <em>H</em> is false, you are thereby missing out on all the extra utility you would get if you made the <em>H</em>-optimal decisions instead! If you can figure out exactly what you're afraid of, maybe that will help you work out what the <em>H</em>-optimal decisions are. Then you'll be a <a href=""https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat"">better position to successfully notice</a> which world you <em>actually</em> live in.</p>
</body></html>",Zack_M_Davis,zack_m_davis,Zack_M_Davis,
MqrzczdGhQCRePgqN,Feedback Requested! Draft of a New About/Welcome Page for LessWrong,feedback-requested-draft-of-a-new-about-welcome-page-for,https://www.lesswrong.com/posts/MqrzczdGhQCRePgqN/feedback-requested-draft-of-a-new-about-welcome-page-for,2019-06-01T00:44:58.977Z,29,5,26,True,False,,"<p><strong>Context for Draft / Request for Feedback</strong></p><p>The LessWrong team is hoping to soon display a new About/Welcome page which does an improved job of conveying what LessWrong.com is about and how community members can productively use the site. </p><p>However, LessWrong <em>is </em>a <em>community</em> site and I (plus the team) feel it&#x27;s not appropriately for us to unilaterally declare <em>what LessWrong is about</em>. <strong>So here&#x27;s our in-progress draft of a new About/Welcome page. Please let us know what you think in the comments. Please especially let us know if you think LessWrong is actually about something else.</strong> Or even just what it means to you. </p><p>Thanks! </p><p>&lt;3 Ruby</p><br/><p>---------------------------------------------------------------------------------</p><p>Related:</p><ul><li><a href=""https://www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team-page-under-construction"">Team / Who are we?</a></li><li><a href=""https://www.lesswrong.com/posts/S69ogAGXcc9EQjpcZ/a-brief-history-of-lesswrong"">A Brief History of LessWrong</a></li></ul><h1>The tl;dr</h1><p>LessWrong is a community blog devoted to the art of human rationality. </p><p>We invite you to use this site for any number of reasons, including, but not limited to: learning valuable things, being entertained, sharing and getting feedback on your ideas, and participating in a community you like. <em>However</em>, fundamentally, this site is designed for two main uses:</p><ul><li><strong>As a place to level-up your <em>rationality</em></strong></li><li><strong>As a place to apply your rationality to important real-world problems</strong></li></ul><p>Primary things to do on LessWrong are:</p><ul><li>Read LessWrong’s <a href=""https://www.lesswrong.com/library"">repository of rationality materials</a></li><li>Join a <a href=""https://www.lesswrong.com/community"">local rationality meetup</a></li><li>Join in a discussion</li><li><a href=""https://www.lesswrong.com/questions"">Ask or answer a question</a></li><li>Write a <a href=""https://www.lesswrong.com/posts/5conQhfa4rgb4SaWx/site-guide-personal-blogposts-vs-frontpage-posts"">post</a></li></ul><h1>Leveling up your rationality</h1><h3>First off, what is rationality?</h3><p><em>Rationality </em>is a term which can have different connotations to different people. On LessWrong, we mean something like the following: </p><ul><li>Rationality is thinking in ways which systematically arrive at truth.</li><li>Rationality is thinking in ways which cause you to achieve your goals.</li><li>Rationality is trying to do better on purpose.</li><li>Rationality is reasoning well even in the face of massive uncertainty.</li><li>Rationality is making good decisions even when it’s hard.</li><li>Rationality is being self-aware, understanding how your own mind works, and applying this knowledge to thinking better.</li></ul><p>What rationality is <em>not</em>:</p><ul><li>Forsaking all human emotion and intuition to embrace Cold Hard Logic.</li></ul><h3>Why should I care about rationality?</h3><p>One reason to care about rationality is because you intrinsically care about having true beliefs. You might also care about rationality because you <em>care about anything at all</em>. Our ability to achieve our goals depends on 1) our ability to ability to understand and predict the world, 2) having the skills to make good plans, and 3) having the self-knowledge and self-mastery to avoid falling into common pitfalls of human thinking. These are core topics in rationality are of interest to anyone with non-trivial goals, from curing their persistent insomnia and having fulfilling relationships to performing groundbreaking research or curing the world’s greatest ills.</p><p>See also <a href=""https://www.lesswrong.com/posts/YshRbqZHYFoEMqFAu/why-truth-and"">Why truth? And...</a></p><h3>How does LessWrong help me level up my rationality?</h3><h4><strong>A repository of rationality knowledge</strong></h4><p><strong>LessWrong has an <a href=""https://www.lesswrong.com/library"">extensive Library</a> containing hundreds of essays on rationality topics.</strong> <strong>You can get started on the <a href=""https://www.lesswrong.com/library"">Library page</a> or from <a href=""https://www.lesswrong.com/"">the homepage</a>. Among the newer material, we particularly recommend <a href=""https://www.lesswrong.com/allPosts?filter=curated&view=new"">Curated posts</a>.</strong></p><p>The writings of Eliezer Yudkowsky and Scott Alexander comprise the core readings of LessWrong. As part of the founding of LessWrong, Eliezer Yudkowsky wrote a long series of blog posts, originally known as <em>The Sequences </em>and more recently compiled into an edited volume, <em><a href=""https://www.lesswrong.com/rationality"">Rationality: AI to Zombies</a>. </em></p><p>Rationality: From AI to Zombies is a deep exploration of how humans minds can come to understand the world they exist in - and all reasons they so often fail to do so. The comprehensive work:</p><ul><li>lays foundational conceptions of <a href=""https://www.lesswrong.com/s/7gRSERQZbqTuLX5re"">belief</a>, <a href=""https://www.lesswrong.com/s/zpCiuR4T343j9WkcK"">evidence</a>, and <a href=""https://www.lesswrong.com/s/5uZQHpecjn7955faL"">understanding</a></li><li>reviews the <a href=""https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM"">systematic biases</a> and <a href=""https://www.lesswrong.com/s/FrqfoG3LJeCZs96Ym"">common excuses</a> which cause us to believe false things</li><li>offers guidance on <a href=""https://www.lesswrong.com/s/GSqFqc646rsRd2oyz"">how to change our minds</a> and <a href=""https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb"">how to use language effectively</a> to describe the world</li><li>depicts the <a href=""https://www.lesswrong.com/posts/8GhSZzsQmusCN9is7/minds-an-introduction"">nature of human psychology</a> with reference to how <a href=""https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8"">evolution</a> produced us</li><li>clarifies the kind of <a href=""https://www.lesswrong.com/rationality/ends-an-introduction"">morality</a> humans like us can have in a <a href=""https://www.lesswrong.com/s/p3TndjYbdYaiWwm9x"">reducible</a>, <a href=""https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo"">physical</a> world</li><li>and repeatedly reminds us that <a href=""https://www.lesswrong.com/s/5uZQHpecjn7955faL"">confusion and mystery exist only in our minds</a>.</li></ul><p>Eliezer covers these topics and many more through allegory, anecdote, and scientific theory. He tests these ideas by applying them to debates in <a href=""https://www.lesswrong.com/s/3HyeNiEpvbQQaqeoH"">artificial intelligence</a> (AI), <a href=""https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ"">physics</a>, <a href=""https://www.lesswrong.com/s/9bvAELWc8y2gYjRav"">metaethics</a>, and consciousness. </p><p>Eliezer also wrote <em><a href=""https://www.lesswrong.com/hpmor"">Harry Potter and the Methods of Rationality</a></em> (HPMOR), an alternative universe version of Harry Potter where Harry’s adoptive parents raised with Enlightenment ideals and the experimental spirits. This work introduces many of the ideas from Rationality: A-Z in a gripping narrative.</p><p>Scott Alexander’s essays on <a href=""https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi"">how good reasoning works</a>, how to learn from the institution of science, and the different ways society has been and could be organized have been made into a collection called <em>T</em><a href=""https://www.lesswrong.com/codex""><em>he</em> <em>Codex</em></a><em>. </em>The Codex contains such exemplary essays as:</p><ul><li><a href=""https://www.lesswrong.com/posts/fzeoYhKoYPR3tDYFT/beware-isolated-demands-for-rigor"">Beware Isolated Demands for Rigor</a></li><li><a href=""https://www.lesswrong.com/posts/yCWPkLi8wJvewPbEp/the-noncentral-fallacy-the-worst-argument-in-the-world"">The noncentral fallacy - the worst argument in the world?</a></li><li><a href=""https://www.lesswrong.com/s/NHXY86jBahi968uW4/p/aMHq4mA2PHSM2TMoH"">The Categories Were Made For Man, Not Man For The Categories</a></li><li><a href=""https://www.lesswrong.com/s/rNuPrZvabXe2MaZv8/p/GLMFmFvXGyAcG25ni"">I Can Tolerate Anything Except the Outgroup</a></li></ul><p>Members on LessWrong rely on many of the ideas from their writers in their own posts, and so it&#x27;s advised to read at least a little of these authors to get up to speed on LessWrong&#x27;s background knowledge and culture.</p><h4><strong>Truth-seeking norms and culture</strong></h4><p>We are proud of the LessWrong community not just for its study of rationality, but also for how much these ideals and skills are put into practice. Unlike many social spaces on the modern Internet, LessWrong is a place where <a href=""https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind"">changing your mind</a>, <a href=""https://en.wikipedia.org/wiki/Principle_of_charity"">charitability</a>, <a href=""https://www.lesswrong.com/posts/64FdKLwmea8MCLWkE/the-neglected-virtue-of-scholarship"">scholarship</a>, and <a href=""http://yudkowsky.net/rational/virtues/"">many other virtues</a> are cherished. LessWrong helps you improve you rationality by providing a space where healthy <a href=""https://en.wikipedia.org/wiki/Epistemology"">epistemic</a> and conversational norms are <a href=""https://www.lesswrong.com/posts/bGpRGnhparqXm5GL7/models-of-moderation"">encouraged and enforced</a>.</p><h4><strong>Social support and reinforcement</strong></h4><p>Beyond culture and norms, it’s easier to learn, change, and grow when you’re not alone on your path. Find solidarity on your quest for greater rationality with the LessWrong community. You can participate in the conversations online (via the comments or writing posts which build on the posts of others). Or attend a <a href=""https://www.lesswrong.com/community"">local in-person meetup</a>, <a href=""https://www.lesswrong.com/events/8fzBPHx8aQjrBNRqg/european-community-weekend-2019"">conference</a>, or <a href=""https://www.lesswrong.com/events/cHxzEwzXQ8hyN5228/bay-summer-solstice-2019"">community celebration</a>. In the last twelve months, there have been 461 meetups in 32 countries.</p><h4><strong>Opportunities to practice your rationality.</strong></h4><p><em>See the next section</em>.</p><h1>Applying your rationality to important problems</h1><p>Feedback and practice are crucial for mastery of skills. If you’re not using your skills to do anything real, how do you even know whether you’re on the right track? For this reason, LessWrong is a place where rationality is both trained and put to use. </p><p>Plus, it’s nice to accomplish real things.</p><h3>Ways to apply your rationality on LessWrong</h3><h4><strong>Participate in discussions aimed at truth-seeking and self-improvement</strong></h4><p>On LessWrong, you can converse with others with the <a href=""https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement"">real goal of exchanging beliefs</a> and converging on the truth. You can delight in dialog which isn’t about Being Right, but actually in clarifying the matter at hand. And you can work together with others, each of you providing your own understanding and background knowledge to figure out how reality really is. This is not Internet discussion as you know it.</p><p>While rationality, self-improvement, and AI are the most frequently discussed topics on the site, there are also commonly discussions of <a href=""http://www.lesswrong.com/posts/oPEWyxJjRo4oKHzMu"">self-improvement</a>, <a href=""https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip"">psychology</a>, <a href=""https://www.lesswrong.com/s/yFvZa9wkv5JoqhM8F"">philosophy</a>, <a href=""https://www.lesswrong.com/posts/XjMkPyaPYTf7LrKiT"">decision theory,</a> <a href=""https://www.lesswrong.com/posts/qLdG44kpSoYzrzAp7"">mathematics</a>, computer science, physics, <a href=""https://www.lesswrong.com/posts/MwDaasGo92QXPmDsj/coordination-problems-in-evolution-the-rise-of-eukaryotes"">biology</a>, <a href=""https://www.lesswrong.com/posts/4cm6rtNPgQ67LtQ2o/on-building-theories-of-history"">history</a>, <a href=""https://www.lesswrong.com/posts/36Dhz325MZNq3Cs6B/the-amish-and-strategic-norms-around-technology"">sociology</a>, <a href=""https://www.lesswrong.com/posts/mELQFMi9egPn5EAjK/my-attempt-to-explain-looking-insight-meditation-and"">meditation</a>, and many other topics. </p><p>Core to LessWrong is that we want our online conversations to be productive, constructive, and oriented around determining what is true. Our Frontpage commenting guidelines ask members to:</p><blockquote><strong>Aim to explain, not persuade.</strong> Write your true reasons for believing something, not what you think is most likely to persuade others. Try to offer concrete models, make predictions, and note what would change your mind.</blockquote><blockquote><strong>Present your own perspective.</strong> Make personal statements instead of statements that try to represent a group consensus (“I think X is wrong” vs. “X is generally frowned upon”). Avoid stereotypical arguments that will cause others to round you off to someone else they’ve encountered before. Tell people how <strong>you</strong> think about a topic, instead of repeating someone else’s arguments (e.g. “But Nick Bostrom says…”).</blockquote><blockquote><strong>Get curious.</strong> If I disagree with someone, what might they be thinking; what are the moving parts of their beliefs? What model do I think they are running? Ask yourself - what about this topic do I not understand? What evidence could I get, or what evidence do I already have?</blockquote><p>Once you’ve read some of LessWrong’s core material and read through some past comment-section discussions to get a sense of how we communicate around here, you’re ready to participate in a LessWrong discussion. </p><h4><strong>Post your valuable ideas</strong></h4><p>Our collective knowledge and skills are solidified by members writing posts. By writing posts, you benefit the world by sharing your knowledge and benefit yourself by getting feedback from an audience. Our audience will hold you to high standards of reasoning, yet in a cooperative and encouraging manner.</p><p>Posts on practically any topic are welcomed. We think it&#x27;s important that members can “bring their entire selves” to LessWrong and are able to share their thoughts, ideas, and experiences without fearing whether they are “on topic”. Rationality is not restricted to only specific domains in one’s life, and neither should LessWrong be.</p><p>However, to maintain its overall focus, LessWrong classifies posts as either <em><strong>Personal blogposts</strong></em> or as <em><strong>Frontpage posts</strong></em>. The latter have more visibility by default on the site.</p><p>All posts begin as personal blogposts. Authors can grant permission to LessWrong’s moderation team to give a post <em>Frontpage status</em> if it i) has broad relevance to LessWrong’s members, ii) is timeless, i.e. not tied to current events, and iii) primarily attempts to explain rather than persuade.</p><p>The not-perfectly-named category of “Personal” blogposts are suitable for everything which doesn&#x27;t fit in Frontpage. It’s the right classification for discussions of niche topics, personal interests, current events, community concerns, potentially divisive topics, and just about anything else you want to write about.</p><p>See more in <em><a href=""https://www.lesswrong.com/posts/5conQhfa4rgb4SaWx/site-guide-personal-blogposts-vs-frontpage-posts"">Site Guide: Personal Blogposts vs Frontpage Posts</a></em></p><p><strong>Contribution on LessWrong’s <a href=""https://www.lesswrong.com/questions"">Open Questions</a> research platform.</strong></p><p>Open Questions was built to help apply the LessWrong community’s rationality/epistemic to humanity’s most important problems. </p><br/>",Ruby,ruby,Ruby,
S69ogAGXcc9EQjpcZ,"A Brief History of LessWrong
",a-brief-history-of-lesswrong-1,https://www.lesswrong.com/posts/S69ogAGXcc9EQjpcZ/a-brief-history-of-lesswrong-1,2019-06-01T00:43:59.408Z,41,30,2,True,False,,"<p>In 2006, <u><a href=""https://www.lesswrong.com/users/eliezer_yudkowsky"">Eliezer Yudkowsky</a></u>, <u><a href=""https://www.lesswrong.com/users/robin_hanson2"">Robin Hanson</a></u>, and others began writing on <u><em><a href=""https://www.overcomingbias.com/about"">Overcoming Bias</a></em></u>, a group blog with the general theme of how to move one’s beliefs closer to reality despite biases such as overconfidence and wishful thinking. In 2009, after the topics drifted more widely, Eliezer moved to a new community blog<em>, LessWrong</em>.</p><p>LessWrong was seeded with series of daily blog posts written by Eliezer, originally known as <em>The Sequences</em>, and more recently compiled into an edited volume, <em><u><a href=""https://www.lesswrong.com/rationality"">Rationality: A-Z</a></u>. </em>These writings attracted a large community of readers and writers interested in the art of human rationality.</p><p>In 2015-2016 the site underwent a steady decline of activity leading some to declare the site dead. In 2017, a team led by Oliver Habryka took over the administration and development of the site, relaunching it on an <u><a href=""https://github.com/LessWrong2/Lesswrong2"">entirely new codebase</a></u> later that year. </p><p>The new project, dubbed LessWrong 2.0, was the first time LessWrong had a full-time dedicated development <a href=""https://www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team-page-under-construction"">team</a> behind it instead of only volunteer hours. Site activity recovered from the 2015-2016 decline and <u><a href=""https://www.lesswrong.com/posts/9dA6GfuDca3Zh3RMa/data-analysis-of-lw-activity-levels-age-distribution-of-user"">has remained at steady levels</a></u> since the launch. </p><p>The team behind LessWrong 2.0 has ambitions not limited to maintaining the original LessWrong community blog and forum. The LessWrong 2.0 team conceives of itself more broadly as an organization attempting to build community, culture, and technology which will drive intellectual progress on the world’s most pressing problems.</p>",Ruby,ruby,Ruby,
aG74jJkiPccqdkK3c,The LessWrong Team,the-lesswrong-team,https://www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team,2019-06-01T00:43:31.545Z,62,31,8,True,False,,"<h1>LessWrong Core Team</h1><h2>Oliver Habryka / <a href=""http://www.lesswrong.com/users/habryka4"">habryka</a></h2><blockquote><p>Oliver Habryka is the current project lead for LessWrong.com, where he tries to build infrastructure for making intellectual progress on global catastrophic risks, cause prioritization and the art of rationality. He used to work at the Centre for Effective Altruism US as strategic director, ran the EA Global conferences for 2015 and 2016 and is an instructor for the Center for Applied Rationality. He has generally been involved with community organizing for the Effective Altruism and Rationality communities in a large variety of ways. He studied Computer Science and Mathematics at UC Berkeley, and his primary interests are centered around understanding how to develop communities and systems that can make scalable progress on difficult philosophical and scientific problems.</p></blockquote><h2>&nbsp;</h2><h2>Ruben Bloom / <a href=""http://www.lesswrong.com/users/Ruby"">Ruby</a></h2><blockquote><p>I found LessWrong in 2012 after googling ""singularity"" and finding <a href=""https://www.facingthesingularity.com"">facingthesingularity.com </a>(since renamed). Among the first posts I read was <a href=""https://www.lesswrong.com/posts/FwiPfF8Woe5JrzqEu/philosophy-a-diseased-discipline"">Philosophy: A Diseased Discipline</a>, which pissed me off because I was philosophy major. But then I read it realized I agreed with all of it. &nbsp;Since then, I've been deep in the LW community. I became a local organizer in Melbourne, mentored at a dozen CFAR workshops, and eventually made my way to the Bay Area. Before joining the LessWrong team in 2019, I studied electrical engineering and philosophy, and worked as a Data Scientist and Product Manager.</p></blockquote><h2>&nbsp;</h2><h2>Robert Mushkatblat / <a href=""https://www.lesswrong.com/users/t3t"">RobertM</a></h2><blockquote><p>I found LessWrong through HPMOR in 2012, briefly interned at MIRI in 2013 (which didn't accomplish much except convince me to not enter academia as a post-grad, which was good), and started attending meetups in Los Angeles shortly thereafter. &nbsp;In 2017 I took over running those meetups. &nbsp;In 2022 Eliezer posted <a href=""https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy"">MIRI announces new ""Death With Dignity"" strategy</a>, which was the sharp poke I needed to go do something about the whole x-risk thing.</p><p>By trade, I'm a software engineer. &nbsp;I'm responsible for technical leadership at LessWrong.</p></blockquote><p>&nbsp;</p><h2>Raymond Arnold / <a href=""http://www.lesswrong.com/users/Raemon"">Raemon</a></h2><blockquote><p>I've been a LessWrong organizer since 2011, with roughly equal focus on the cultural, practical and intellectual aspects of the community. My first project was creating the Secular Solstice and helping groups across the world run their own version of it. More recently I've been interested in improving my own epistemic standards and helping others to do so as well.</p></blockquote><blockquote><p>I guess also I code? I worked at Spotify. Now I don’t.</p></blockquote><p>&nbsp;</p><h1>The rest of the Lightcone team</h1><h2>Ben Pace / <a href=""http://www.lesswrong.com/users/Benito"">Benito</a></h2><blockquote><p>I read The Sequences when I was 14 (in 2011, after the sequences were written) and I've read and contributed to LessWrong and the broader rationality community since then. When I was 19 I ran an EA Global conference with jacobjacob. I have a CS degree from Oxford, but I learned more from running the conference. jacobjacob and I do various things on the LessWrong Team – recently we professionally published <a href=""http://lesswrong.com/books"">some cute books of the best new content</a>. If you're interested in how I think you can read these two LessWrong posts that I wrote (<a href=""https://www.lesswrong.com/posts/yeADMcScw8EW9yxpH/a-sketch-of-good-communication"">1</a>, <a href=""https://www.lesswrong.com/posts/9QxnfMYccz9QRgZ5z/the-costly-coordination-mechanism-of-common-knowledge"">2</a>).</p></blockquote><h2>&nbsp;</h2><h2><a href=""https://www.lesswrong.com/users/jacobjacob"">jacobjacob</a></h2><p>Deschlepifier. I believe that there's glory at the end of the struggle, at least temporarily. Also fly tiny planes sometimes.</p><h2>Rafe Kennedy&nbsp;</h2><p>&nbsp;</p><h1>Moderators</h1><h2>Elizabeth V / <a href=""https://www.lesswrong.com/users/pktechgirl"">Elizabeth</a></h2><h1>&nbsp;</h1><h1>Admin (BDFL)</h1><h2>Matthew Graves / <a href=""https://www.lesswrong.com/users/vaniver"">Vaniver</a></h2><p>Vaniver describing Vaniver:</p><blockquote><p>I’ve had forum-posting as a hobby since I was young, first on a D&amp;D forum, then on the xkcd forums, and then finally on LW, which I found through a link to HPMOR on the xkcd forums. I studied physics, economics, and operations research (which I sometimes describe as ‘industrial rationality’), and worked as a data scientist before moving to the Bay to work for MIRI.</p></blockquote><p>Raemon describing Vaniver:</p><blockquote><p>Once upon a time, LessWrong<a href=""https://www.lesswrong.com/users/ruby""> </a>almost died. There were numerous half-hearted attempts to revitalize the community. Eventually someone noticed that part of the problem was there was no particular person who actually had the mandate to make sweeping changes. Someone said <a href=""https://www.lesswrong.com/posts/8rYxw9xZfwy86jkpG/on-the-importance-of-less-wrong-or-another-single#PNCWPyvLS7G6L3iHW"">“I vote for Vaniver” and then a bunch of people said “me too!”</a> and in a highly unsuspect, democratic process, Vaniver became king.</p></blockquote><blockquote><p>Nowadays Vaniver is the meta-king, and his gentle authority flows through us.</p></blockquote><p><i>The LessWrong team operates legally as part of the </i><a href=""http://www.rationality.org""><i>Center for Applied Rationality</i></a><i> while retaining full autonomy over both internal decision-making and decisions concerning the LessWrong website. The lesswrong.com domain is owned by the</i><a href=""http://www.intelligence.org""><i> Machine Intelligence Research Institute</i></a><i>.</i></p>",Ruby,ruby,Ruby,
