_id,title,slug,pageUrl,postedAt,baseScore,voteCount,commentCount,meta,question,url,htmlBody,user.username,user.slug,user.displayName,user
C6aRQ27hFCgW6u4GG,How to not be an alarmist,how-to-not-be-an-alarmist,https://www.lesswrong.com/posts/C6aRQ27hFCgW6u4GG/how-to-not-be-an-alarmist,2020-09-30T21:35:59.285Z,8,4,2,False,False,,"<blockquote><p><i>""I apologize. I didn't mean-- may I express my concern as calmly and respectfully as I can?""</i></p><p><i>- Prof. Legasov on the consequences of an RBMK reactor meltdown (HBO's Chernobyl)</i></p></blockquote><p>&nbsp;</p><blockquote><p><i>""This is not going to end well.""</i></p><p><i>""There is no evidence of that-""</i></p><p><i>""This is not going to end well.""</i></p><p><i>- Donald Trump and Joe Biden on voter fraud (first presidential debate)</i></p></blockquote><p>Winning an argument often requires <a href=""https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer"">making a case</a> that some X is a problem. X is bad. Here's how an avid practitioner of the dark arts can gather soldiers for their cause, no matter what X is or how bad the issue is in context. I think this is how most people argue for their cause. Craft the strongest argument in each category, and if that argument gets defeated, move to another one.</p><ol><li>Current levels of X. No matter how small X is, it could always be smaller.</li><li>Projected levels of X. Sometimes, it's not the current, but the projected level of X at some point in the future that's bad.</li><li>The rate of change in X. If X is low and getting lower, it might not be declining fast enough. Or in the context of an overall good trend, there might be a tiny recent bad turn.</li><li>Past levels of X. Even if X no longer exists, it can be blamed for current problems.</li><li>Uncertainty in X. There might be uncertainty whether a low level, declining trend, planned solution, or potential technological breakthrough will happen or continue. Alternatively, some unpredictable circumstance or tail risk might cause a spike in X or a reversal of the declining trend in X.</li><li>Side effects of solutions to X. If the reason we're beating X is Y, then focus on bad aspects of Y.<ul><li><i>Corollary: if the reason we're beating X is that we're afraid of X, then admitting that X is low and/or declining reduces to case (5).</i></li></ul></li></ol><p>Stated in the abstract, it's easy to see how arbitrary this process is. It's a recipe for looking like an alarmist, <i>even if we're telling the truth about a serious issue. </i>And, of course, you might not just look like, but actually <i>be </i>an alarmist.</p><p>But we do have to confront the problems in our world. Bad things are, in fact, bad.</p><p>The answer is to have a sense of priorities. Your own, as well as those of your debate partner and audience. Show that you respect them as people. Make a clear statement of what you think their priorities are. Look for fair, win/win solutions.</p><p>If you can't reach that state of mutual understanding and reciprocity no matter how hard you try - and you have to <a href=""https://www.lesswrong.com/posts/WLJwTJ7uGPA5Qphbp/trying-to-try""><i>actually try</i></a> - look for a different audience.</p>",AllAmericanBreakfast,directedevolution,DirectedEvolution,
9GC35E9JkkcLtBi7Y,Competence vs Alignment,competence-vs-alignment,https://www.lesswrong.com/posts/9GC35E9JkkcLtBi7Y/competence-vs-alignment,2020-09-30T21:03:01.826Z,7,4,4,False,True,,"<p>Is there a reliable way of determining whether an agent is misaligned, as compared to just incompetent? And can we talk of such distinction at all? (I suppose the orthogonality thesis implies that we can)</p><p>Let's say I give you an agent with suboptimal performance. Without having insights inside its ""brain"", and only observing its behavior, can we determine whether it's trying to optimize the correct value function but failing, or maybe it's just misaligned?</p>",ariel-kwiatkowski,kwiat-dev,kwiat.dev,
D8ds9idKWbwzCseCh,"""Zero Sum"" is a misnomer.",zero-sum-is-a-misnomer,https://www.lesswrong.com/posts/D8ds9idKWbwzCseCh/zero-sum-is-a-misnomer,2020-09-30T18:25:30.603Z,120,45,34,False,False,,"<p><i>This could have been a relatively short note about why ""zero sum"" is a misnomer, but I decided to elaborate some consequences. This post benefited from discussion with Sam Eisenstat.</i></p><h1>""Zero Sum"" is a misnomer.</h1><p>The term intuitively suggests that an interaction is transferring resources from one person to another. For example, theft is zero-sum in the sense that it cannot create resources only transfer them. Elections are zero-sum in the sense that they only transfer power. And so on.</p><p>But this is far from the technical meaning of the term.</p><p>In order for the standard rationality assumptions used in game theory to apply, the payouts of a game must be <i>utilities</i>, not resources such as money, power, or personal property. Zero-sum transfer of resources is often far from zero-sum in utility.</p><p>But I'm getting ahead of myself. Let's examine the technical meaning of ""zero sum"" more precisely.</p><h2>It's used to mean ""constant sum"".</h2><p>The term ""zero sum"" is often used as a technical term, referring to games where the payouts for different players always sums to the same thing.</p><p>For example, the game rock-paper-scissors is zero sum, because it always has one winner and one loser.</p><p>More generally, constant-sum means that if you add up the utility functions of the players, you get a perfectly flat function.</p><h2>""Constant sum"" doesn't really make sense as a category.</h2><p>It makes sense to conflate ""zero sum"" and ""constant sum"" because utility functions are <a href=""https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities"">equivalent under additive and positive multiplicative transforms</a>, so we can always transform a constant-sum game down to a zero-sum game. However, by that same token, <i><strong>the concept of ""constant sum"" is meaningless:</strong> </i>we can multiply the utility of one side or the other, and still have the same game. If you have good reflexes, you should hear ""zero sum""/""constant sum"" and shout ""Type error! <a href=""https://www.lesswrong.com/posts/CQkGJ2t5Rw8GcZKJm/pinpointing-utility"">Radiation leak!</a> You can't sum utilities without providing <a href=""https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities"">extra assumptions</a>!""</p><p>Let's look at the ""zero sum"" game <i>matching pennies</i> as an example. In this game, two players have to say ""heads"" or ""tails"" simultaneously. One player is trying to match the other, while the other player is trying to be different from the one. Here's one way of writing the payoff matrix (with Alice trying to match):</p><figure><table><tbody><tr><td>&nbsp;</td><td colspan=""3"">Bob</td></tr><tr><td rowspan=""3"">Alice</td><td>&nbsp;</td><td>Heads</td><td>Tails</td></tr><tr><td>Heads</td><td>Alice: 1<br>Bob: 0</td><td>Alice: 0<br>Bob: 1</td></tr><tr><td>Tails</td><td>Alice: 0<br>Bob: 1</td><td>Alice: 1<br>Bob: 0</td></tr></tbody></table></figure><p>In that case, the game has a constant sum of 1. We can re-scale it to have a constant sum of zero by subtracting 1/2 from all the scores:</p><figure><table><tbody><tr><td>&nbsp;</td><td colspan=""3"">Bob</td></tr><tr><td rowspan=""3"">Alice</td><td>&nbsp;</td><td>Heads</td><td>Tails</td></tr><tr><td>Heads</td><td>Alice: +1/2<br>Bob: -1/2</td><td>Alice: -1/2<br>Bob: +1/2</td></tr><tr><td>Tails</td><td>Alice: -1/2<br>Bob: +1/2</td><td>Alice: +1/2<br>Bob: -1/2</td></tr></tbody></table></figure><p>But notice that we could just as well have re-scaled it to be zero sum by subtracting 1 from Alice's score:</p><figure><table><tbody><tr><td>&nbsp;</td><td colspan=""3"">Bob</td></tr><tr><td rowspan=""3"">Alice</td><td>&nbsp;</td><td>Heads</td><td>Tails</td></tr><tr><td>Heads</td><td>Alice: 0<br>Bob: 0</td><td>Alice: -1<br>Bob: 1</td></tr><tr><td>Tails</td><td>Alice: -1<br>Bob: 1</td><td>Alice: 0<br>Bob: 0</td></tr></tbody></table></figure><p>Notice that this is <i>exactly the same game</i>, but psychologically, we think of it much differently. In particular, the game now seems unfair to Alice: Bob only stands to gain, but Alice can lose! Just like I mentioned earlier, we're tempted to think of the game as if it's an interaction in which resources are exchanged.</p><p>I'm not saying this is a bad thing to think about. In real life, there are situations we can understand as games of resource exchange <i>much more often </i>than there are single-shot games where the payoffs are clearly identifiable in utility terms. I just want to emphasize that <i>resource exchange is not what basic game theory is about, </i>so you should be very careful not to confuse the two!&nbsp;</p><p>Now, as I mentioned earlier, we can also re-scale utilities without changing what they mean, and therefore, without changing the game:</p><figure><table><tbody><tr><td>&nbsp;</td><td colspan=""3"">Bob</td></tr><tr><td rowspan=""3"">Alice</td><td>&nbsp;</td><td>Heads</td><td>Tails</td></tr><tr><td>Heads</td><td>Alice: 100<br>Bob: 0</td><td>Alice: 0<br>Bob: 1</td></tr><tr><td>Tails</td><td>Alice: 0<br>Bob: 1</td><td>Alice: 100<br>Bob: 0</td></tr></tbody></table></figure><p>This game is equivalent to the others, and so, must still be ""zero sum"" in the technical sense of game theory! Despite this, it isn't even constant-sum.</p><p>So, how can we fix our concept of ""zero sum"" to better fit the underlying mathematical phenomenon?</p><h1>Fixing ""Zero Sum""</h1><p>Since the underlying problem is that utilities are not comparable without further assumptions, an obvious solution would be to make the concept ""zero sum"" dependent on those <a href=""https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities"">further assumptions about how to compare utilities</a>. The term ""zero sum"" and ""constant sum"" would then be meaningful (and meaningfully distinct), <i>provided</i> one specifies how to sum utilities.</p><p>I don't think that's the right route, however. I think it's better to look at what ""zero sum"" is <i>trying </i>to do, and come up with a concept which does that more effectively.</p><h2>Linear Games</h2><p>Here are diagrams of the four game matrices I used earlier:</p><figure style=""width:44.91%;""><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9e6162d4b9bc7a351aed3d421be60d16c3147ce68d9ec6f.png/w_920 920w""></figure><p>Clearly, any game matrix which is zero-sum will occupy the&nbsp;<span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=-y""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span></span>&nbsp;line. Similarly, any constant-sum game matrix will occupy a line&nbsp;<span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x=c-y""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">c</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span></span></span></span></span>&nbsp;where&nbsp;<span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""c""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">c</span></span></span></span></span></span></span>&nbsp;is the constant sum. But when we apply a positive multiplicative transformation, we get a line&nbsp;<span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""ax=-by+c""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">c</span></span></span></span></span></span></span>&nbsp;for positive constants&nbsp;<span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""a,b""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.144em; padding-bottom: 0.519em;"">,</span></span><span class=""mjx-mi MJXc-space1""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span></span></span></span></span></span>. So what we can say about a zero-sum game which remains true regardless of transformations is <i><strong>outcomes fall on a line of negative slope.</strong></i></p><p>Note that if outcomes fall on a line of <i>positive</i> slope, then <i>the utility functions of the players must match up to inconsequential transforms</i>. In other words, the two players must have the same preferences.</p><p>So, for two-player zero-sum linear games, there are just three choices: players can have equivalent preferences, or players can have completely opposed preferences (their utility functions being equivalent to the negative of each other), or some players can have no preferences (the slope of the line is zero or infinite).</p><p>For multi-player games, things get a little more complicated. Two players need not be perfectly aligned with or opposed to each other. In fact, <i>any</i> two-player game can be embedded in a zero-sum three-player game (and furthermore, <a href=""https://en.wikipedia.org/wiki/Zero-sum_game#Extensions"">any N-player game can be embedded in a N+1-player zero-sum game</a>).</p><p>However, the concept of linear game still applies well to multi-player games, and we can still easily generalize the concept of zero-sum to <i>linear games where the slope between any two dimensions is negative.</i></p><p>Linear games with negative slope are <i>probably</i> what you should interpret ""zero sum"" to mean in most formal game-theoretic contexts. This is, after, all, the most general thing that can be re-scaled to create an equivalent game which is literally zero-sum. However, we can still generalize further.</p><h2>Completely Adversarial Games</h2><p>In the original paper about the Nash bargaining problem (John Nash, <a href=""http://www.rasmusen.org/GI/reader/12a.nash.bargaining.1950.pdf""><i>The Bargaining Problem</i></a>), Nash divides bargaining into two phases. In the first phase, players make <i>threats</i>: binding commitments about what they'll do if negotiations break down. In the second phase, players make <i>demands: </i>they ask for a specific level of resources. This demand is backed up by their earlier threats.</p><p>(Such an adversarial view of negotiation!)</p><p>The interesting thing for us here is that he observes that the threat portion of the game, if we consider it in isolation, is <i>not</i> zero sum, but <i>might as well be</i>: the payoff structure is just as adversarial. Because the outcome of the second part of the game is bound to be Pareto-optimal (under some assumptions about rational play), the choice of threats simply changes what trade-off will be arrived at along the Pareto-optimal surface:&nbsp;</p><figure style=""width:54.76%;""><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_110 110w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_220 220w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_330 330w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_440 440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_550 550w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_660 660w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_770 770w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_880 880w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_990 990w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/707eeef88e68627e56c7e757bbe792902b881afa5a3acd82.png/w_1012 1012w""></figure><p>The feasible outcomes for the <i>full</i> game include Pareto improvements (otherwise there could not possibly be any gains due to bargaining), but when we assume optimal play for the ""demand"" step, the threat step considered in isolation has only Pareto-optimal outcomes.</p><p>So, we could consider a game <i><strong>completely adversarial</strong></i> if it has a structure like this: <strong>no strategy profiles are a Pareto improvement over any others.</strong> In other words, the feasible outcomes of the game equal the game's Pareto frontier. All possible outcomes involve trade-offs between players.</p><p>Note, however, that if we allow mixed strategies, the only completely adversarial games are the linear ones. This is because mixed strategies imply that the space of possible outcomes is convex. The convex hull of a Pareto frontier that's not already linear <i>must</i> have a nonempty interior, implying the possibility of Pareto improvements.</p><p>So, since it's rather common to assume there are mixed strategies, this generalization will usually not be giving you anything over ""linear games of negative slope"".</p><h1>Notes on standard terminology and definitions.</h1><p>I would have referred to my generalizations of zero sum as ""adversarial games"", and advocate ""adversarial games"" as an alternative to the problematic term ""zero sum"", except that ""adversarial game"" is a common term for <a href=""https://en.wikipedia.org/wiki/Non-cooperative_game_theory"">non-cooperative game theory</a>. Cooperative game-theory is game theory with enforceable contracts. Non-cooperative game theory is the version of game theory readers are most likely familiar with, IE, studying games with nash equilibria etc. Therefore, games like Prisoner's Dilemma, Stag Hunt, etc are ""adversarial"" or ""non-cooperative"" in the standard parlance, even though they are far from zero-sum.</p><p>So, I propose ""completely adversarial"" as the general term one should use as a replacement of ""zero sum""; if someone asks you what this term means, you should say ""there are no Pareto improvements in the set of possible outcomes"" or something along those lines, and clarify that if we assume mixed strategies, this implies that the possible outcomes form a hyperplane.</p><p>In a comment, <a href=""https://www.lesswrong.com/posts/D8ds9idKWbwzCseCh/zero-sum-is-a-misnomer?commentId=DbhEbrwHbwiXLRiiR"">Vojta proposes</a> ""completely cooperative"" to point to the opposite: a game where Pareto-domination forms a total order on the strategy profiles. He suggests ""mixed-motive games"" for games which are neither completely cooperative nor completely adversarial.</p><p>Note that <i>if we assume mixed strategies</i>, completely cooperative games are just linear games of strictly positive slope, the same way completely adversarial games must be linear (of negative slope) when we assume the existence of mixed strategies.</p><p>(Note also that my use of the term ""linear"" to describe games is, afaik, very nonstandard. It would be more precise for me to say ""collinear"", as in, all feasible points are collinear.)</p><p>The <a href=""https://en.wikipedia.org/wiki/Zero-sum_game#Definition"">wikipedia article on zero-sum game</a> uses the term ""conflict game"" for what I term ""completely adversarial"". Feel free to use that term if you like, but I find ""completely adversarial"" to be more satisfying.</p><p>Frustratingly, Wikipedia currently <a href=""https://en.wikipedia.org/wiki/Game_theory#Zero-sum_/_non-zero-sum"">defines zero sum as a special case of constant sum</a>, erroneously implying that we can sensibly differentiate between the two. In the same section, it goes on to name resource-redistribution transactions like theft and gambling as examples of zero-sum games, which (as I mentioned in the introduction) is often far from the case.</p><p>The article on zero sum games includes a <a href=""https://en.wikipedia.org/wiki/Zero-sum_game#Universal_solution"">discussion of avoiding a game</a>, which asserts that if avoiding playing the game is an option, then it will always be an equilibrium strategy for one or both players. This further reinforces the idea that zero-sum games are considered as resource transfers, since presumably the idea is that at least one player has nothing to gain from a zero-sum interaction.</p><p>I'm not sure what terminology to suggest for the resource-transfer transactions which are usually <i>referred to as</i> zero sum, such as theft, political elections, etc. Maybe it's even fine to keep calling these ""zero sum"", using the term for what it intuitively invokes, since I'm already proposing that when discussing game theory we should use more technically accurate terms. But the risk is that you'll be mistakenly interpreted as invoking game theory.</p><p>There's a whole additional section I could write about how we can try to formally understand what these resource-transfer situations even are, and what it means for them to be ""zero sum"" in the intuitive sense, but I think I will leave things here for now.</p>",abramdemski,abramdemski,abramdemski,
cCK2F4MqnBPfqczQu,Evaluating Life Extension Advocacy Foundation,evaluating-life-extension-advocacy-foundation,https://www.lesswrong.com/posts/cCK2F4MqnBPfqczQu/evaluating-life-extension-advocacy-foundation,2020-09-30T18:04:51.346Z,7,3,7,False,False,,"<p>(Crossposted from the <a href=""https://forum.effectivealtruism.org/posts/QtcgqBm8y83icLF2F/the-case-for-life-extension-advocacy-foundation"">EA Forum</a>)</p><h1>Summary</h1><p>In this post, I try to evaluate Life Extension Advocacy Foundation, and I flesh out interview questions to ask them. LEAF has many foci, including crowdfunding, conference organizing, investor advisement, social media advocacy, news, YouTube shows and a podcast, partnering with YouTubers and content creators such as Kurzgesagt and Life Noggin, creating useful resources on aging research, and advocating for policy change, including communication with international organizations, such as the World Health Organization.</p><p>I think that LEAF's ratio of donations to money-brought-to-the-field is high. This organization's most easily measurable impact comes from its crowdfunding campaigns, which have brought $384k to other organizations (with LEAF's fee accounted for). I estimated its collaboration with Kurzgesagt to have brought to LEAF at least $3k per month for a couple of years from donors who were previously oblivious to the aging-research cause.</p><p>Other important sources of impact are LEAF’s investor network, which has a great potential influence on their ratio, the capital and human resources that it brings to the field in ways that are difficult to quantify, their role in public education on the topic, and its influence on policy and international organizations such as WHO, which may drive faster adoption of new biotechnologies, increased funding, and better international decision-making.</p><p>I invite the reader to come up with questions or criticize the questions that I have proposed. Please use the comment section in the forum or private messages.</p><h2>Introduction and Focuses</h2><p>Life Extension Advocacy Foundation (LEAF) is an advocacy and crowdfunding organization founded in 2014. It is now the leading advocacy organization in the space of aging research. Its foci include:</p><ul><li><a href=""https://www.lifespan.io/"">Crowdfunding</a> basic research projects.</li><li><a href=""https://www.leafscience.org/ending-age-related-diseases-advances-in-aging-research-and-investment-prospects-2019/"">Organizing conferences</a>.</li><li><a href=""https://www.leafscience.org/longevity-investor-network/"">Advising investors</a> and attracting capital.</li><li><a href=""https://www.facebook.com/lifespanio/"">Social media advocacy</a>.</li><li><a href=""https://www.leafscience.org/"">News and blogging</a> about aging research.</li><li><a href=""https://www.youtube.com/user/LifespanIO"">YouTube channel </a>with news about aging research.</li><li><a href=""https://www.youtube.com/watch?v=rJpGZmj3Mm8&amp;list=PLgiu-UInAXMOPlRGDOuBwZwrsq83Nx2iq"">YouTube show</a> (soon with its own channel) and <a href=""https://www.leafscience.org/the-rejuvenation-roundup-podcast/"">podcast</a> about aging.</li><li><a href=""https://www.youtube.com/channel/UCjS6vn6j_cmSXg-cKRJHZLw"">YouTube channel</a> about the positive impact of scientific research and how to support it.</li><li>Partnering with YouTubers and content creators in order to bring more donors and talent to the field and drive public awareness. The major milestones that they reached in this area are their collaborations with Kurzgesagt (<a href=""https://youtu.be/GoJsr4IwCm4"">link 1</a>, <a href=""https://youtu.be/MjdpR-TY6QU"">link 2</a>) and <a href=""https://youtu.be/EEO4bHbxRgo"">Life Noggin</a>, obtaining, in total, more than 11 million views.</li><li>Creating useful resources. Examples: the <a href=""https://www.lifespan.io/road-maps/the-rejuvenation-roadmap/"">longevity roadmap</a> and everything under the tab ""aging"" on <a href=""https://www.lifespan.io/""><u>lifespan.io</u></a>.</li><li>Advocating for policy change and defending the interests of the elderly by interacting with international organizations such as the World Health Organization. For example, in 2018, it had a part in <a href=""https://www.leafscience.org/world-health-organization-puts-the-elderly-back-in-the-picture/"">influencing WHO's policy</a>. How crucial this role was will be explored in the interview.</li></ul><h2>Considerations on cost-effectiveness</h2><p>One effect of advocacy is to multiply the value of donations by attracting more donations to the cause area of interest. It's not easy, given the evidence I have, to ascertain the full extent of this effect for LEAF. The money we would want to see multiplied are the monthly donations of lifespan.io's <a href=""https://www.lifespan.io/campaigns/join-us-become-a-lifespan-hero/"">recurring campaign</a>, the <a href=""https://www.lifespan.io/pricing-and-fees/"">share</a> it takes from crowdfunding campaigns (5% from successful campaigns for non-profits, 10% from successful campaigns for for-profits) and volunteer time converted into salary.</p><p>I've followed the growth of LEAF's recurring Lifespan Heroes campaign since it was started, and I’ve seen it spike after LEAF's collaboration with Kurzgesagt. If I remember correctly, the campaign was at around $3k per month, and in a period of a few weeks, it gained at least other $3k that probably wouldn't have been made otherwise, since the new donations were very probably coming from people completely unaware of LEAF's cause before watching Kurzgesagt's videos. Since then, the recurring campaign grew more, reaching around $8k. It would be useful to ask LEAF about its retention of donors and other ways in which the organization brings in donations (to itself or other organizations). If the $3k figure is correct, it should have brought in more than $72k from new sources (if the donor retention has been high, as I suspect). If I am correct, this would have allowed LEAF to grow, other than having had an impact through the videos. I will ask for more details about this in the interview.</p><p>The crowdfunding campaigns, all together, have brought more or less $384k (with fee accounted for) to nine organizations, seven of which are non-profits. I expect the total figure brought to the field to be significantly larger due to LEAF’s other activities. For example, its investor network may be having a large non-visible impact and could easily repay its recurring campaign multiple times.</p><p>Other unknowns are: In total, how much has been donated to LEAF through the recurring campaign so far? How much money donated to the crowdfunding campaigns would have been donated anyway to other organizations?</p><p>Less quantifiable sources of impact deriving from advocacy are:</p><ul><li>The money it brings to the field that gets donated to other organizations, and for which it's impossible to establish that the cause has been LEAF's advocacy.</li><li>The attraction of human resources to aging research.</li><li>A more positive attitude towards biotechnology, which may drive policy and faster adoption of biotechnologies.</li><li>Influencing the decision making of organizations such as WHO, which may drive faster adoption of new biotechnologies, increased funds to aging research, and better international decision-making.</li></ul><p>Impact of crowdfunding:</p><ul><li>Each crowdfunding project should be evaluated on its own merits, although we can gauge if LEAF's decision making on what to crowdfund is good. So far, I can comment on the SENS projects that it financed, which I think were among the most effective pursued at SENS, and in the cause area in general (see my evaluation of SENS and my interview with Aubrey de Grey.)</li></ul><h2>Interview questions</h2><p><strong>Questions on decision making</strong></p><p>How do you choose what projects to crowdfund? Is your decision-making process stable or potentially subject to change in the future? What kind of changes?</p><p>How do you choose what advocacy projects to undertake? Examples: YouTube videos, investor network, lobbying, etc.</p><p><strong>Questions on LEAF's advocacy in general</strong></p><p>What knowledge and behavior changes do you try to promote?</p><p>What evidence is available (especially academic evidence) regarding the likely impact of such changes in knowledge and behavior?</p><p>Have you tracked changes over time in knowledge and behavior? Have you tried to see whether these patterns point to your impact (i.e. if improvements are stronger in areas where your presence has been stronger)? Can you share this information?</p><p><strong>Questions to gauge the ratio of donations to money brought to the field</strong></p><p>How much money donated to crowdfunding campaigns do you think would have been donated anyway to other organizations?</p><p>How much has the recurring Lifespan Heroes campaign yielded in total?</p><p>How is your donor retention for the recurring Lifespan Heroes campaign?</p><p>How much of the recurring campaign earnings do you estimate to have come from people new to the field who have been recruited through advocacy (Example: after they watched Kurzgesagt videos).</p><p>From the Kurzgesagt videos alone, I estimate that you gained at least $3k per month, a figure that I suspect has been mostly retained for the last two years. In total, this should amount to more than $72k. This probably means you gained much more than what you spent for making the videos (which adds to the impact you had besides your gains). Am I correct? Can you share more precise data on how much you are in the black as a result of the Kurzgesagt collaborations and how they have contributed to your growth? How much of the money that you gained is due to relatively large contributions as opposed to small?</p><p>How many volunteer-hours do you use?</p><p>Are there other ways in which LEAF acquires donations, besides the recurring campaign and the share from the crowdfunding campaigns?</p><p>In what ways do you bring money in the space of aging research other than with the crowdfunding campaigns? And how much? What project has been the most effective so far at bringing money to the field?</p><p>How many new investors and how much new capital have you brought to the field through your investor network?</p><p>Many effects of advocacy might be large but not easily quantifiable. I expect you to have brought more talent and money to the field, contributed towards improving the attitude of the general public towards rejuvenation biotechnology (maybe even speeding up the future adoption of these technologies) and potentially influenced international decision making. How large do you think these less quantifiable effects are compared to the more quantifiable ones? Of all the effects that you have had that are not easily quantifiable, which do you think is the largest?</p><p>What policy changes do you claim partial (or full) credit for in the past? What was your role in advocating for these changes?</p><p><strong>Funding gap</strong></p><p>How much more money could you use? What would you do with that amount?</p><p>Have you ever had to shut down projects due to a lack of funding? What projects?</p><p>How things would look if you had all the funding you needed from the start?</p><p><strong>General questions regarding the landscape of aging research</strong></p><p>What do you think are the most impactful, tractable, and neglected areas in aging research?</p><h2>Suggest or criticize questions</h2><p>I invite the reader to come up with questions or criticize the questions I have proposed. Please use the comment section in the forum or private messages.</p>",emanuele-ascani,emanuele-ascani,emanuele ascani,
Kx7nv8dHtFig9ud7C,"[AN #119]: AI safety when agents are shaped by environments, not rewards",an-119-ai-safety-when-agents-are-shaped-by-environments-not,https://www.lesswrong.com/posts/Kx7nv8dHtFig9ud7C/an-119-ai-safety-when-agents-are-shaped-by-environments-not,2020-09-30T17:10:03.662Z,11,3,0,False,False,,"<p>Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter <strong><a href=""http://rohinshah.com/alignment-newsletter/"">resources here</a></strong>. In particular, you can look through <strong><a href=""https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing"">this spreadsheet</a></strong> of all summaries that have ever been in the newsletter.</p><p>Audio version <strong><a href=""http://alignment-newsletter.libsyn.com/alignment-newsletter-119"">here</a></strong> (may not be up yet). 			  </p><h1>    HIGHLIGHTS </h1><p><strong><a href=""https://www.alignmentforum.org/s/boLPsyNwd6teK5key"">Shaping Safer Goals</a></strong> <em>(Richard Ngo)</em> (summarized by Nicholas): Much of safety research focuses on a single agent that is directly incentivized by a loss/reward function to take particular actions. This sequence instead considers safety in the case of multi-agent systems interacting in complex environments. In this situation, even simple reward functions can yield complex and highly intelligent behaviors that are only indirectly related. For example, evolution led to humans who can learn to play chess, despite the fact that the ancestral environment did not contain chess games. In these situations, the problem is not how to construct an aligned reward function, the problem is how to shape the experience that the agent gets at training time such that the final agent policy optimizes for the goals that we want. This sequence lays out some considerations and research directions for safety in such situations.</p><p>One approach is to teach agents the generalizable skill of obedience. To accomplish this, one could design the environment to incentivize specialization. For instance, if an agent A is more powerful than agent B, but can see less of the environment than B, A might be incentivized to obey B&#x2019;s instructions if they share a goal. Similarly we can increase the ease and value of coordination through enabling access to a shared permanent record or designing tasks that require large-scale coordination.</p><p>A second approach is to move agents to simpler and safer training regimes as they develop more intelligence. The key assumption here is that we may require complex regimes such as competitive multi-agent environments to jumpstart intelligent behavior, but may be able to continue training in a simpler regime such as single-task RL later. This is similar to current approaches for training a language model via supervised learning and then finetuning with RL, but going in the opposite direction to increase safety rather than capabilities.</p><p>A third approach is specific to a collective AGI: an AGI that is composed of a number of separate general agents trained on different objectives that learn to cooperatively solve harder tasks. This is similar to how human civilization is able to accomplish much more than any individual human. In this regime, the AGI can be effectively sandboxed by either reducing the population size or by limiting communication channels between the agents. One advantage of this approach to sandboxing is that it allows us to change the effective intelligence of the system at test-time, without going through a potentially expensive retraining phase.</p><p><strong>Nicholas&apos; opinion:</strong> I agree that we should put more emphasis on the safety of multi-agent systems. We already have <strong><a href=""https://openai.com/blog/emergent-tool-use/"">evidence</a></strong> (<strong><a href=""https://mailchi.mp/3d4e6c2c206f/an-65learning-useful-skills-by-watching-humans-play"">AN #65</a></strong>) that complex behavior can arise from simple objectives in current systems, and this seems only more likely as systems become more powerful. Two-agent paradigms such as GANs, self-play, and debate, are already quite common in ML. Lastly, humans evolved complex behavior from the simple process of evolution so we have at least one example of this working. I also think this is an interesting area where there is lots to learn from other fields, such as game theory and evolutionary biology, </p><p>For any empirically-minded readers of this newsletter, I think this sequence opens up a lot of potential for research. The development of safety benchmarks for multi-agent systems and then the evaluation of these approaches seems like it would make many of the considerations discussed here more concrete. I personally would find them much more convincing with empirical evidence to back up that they work with current ML. </p><p><strong>Rohin&apos;s opinion:</strong> The AGI model here in which powerful AI systems arise through multiagent interaction is an important and plausible one, and I&apos;m excited to see some initial thoughts about it. I don&apos;t particularly expect any of these ideas to be substantially useful, but I&apos;m also not confident that they won&apos;t be useful, and given the huge amount of uncertainty about how multiagent interaction shapes agents, that may be the best we can hope for currently. I&apos;d be excited to see empirical results testing some of these ideas out, as well as more conceptual posts suggesting more ideas to try.</p><h1>    TECHNICAL AI ALIGNMENT </h1><br><h2>    LEARNING HUMAN INTENT </h2><p><strong><a href=""http://arxiv.org/abs/2008.03525"">Non-Adversarial Imitation Learning and its Connections to Adversarial Methods</a></strong> <em>(Oleg Arenz et al)</em> (summarized by Zach): Viewing imitation learning as a distribution matching problem has become more popular in recent years (see <strong><a href=""https://openreview.net/forum?id=Hyg-JC4FDr"">Value-Dice</a></strong> (<strong><a href=""https://mailchi.mp/2fbece2a4915/an-98understanding-neural-net-training-by-seeing-which-gradients-were-helpful"">AN #98</a></strong>) / <strong><a href=""http://arxiv.org/abs/2002.11879"">I2L</a></strong> (<strong><a href=""https://mailchi.mp/0f80d83f9c44/an-94ai-alignment-as-translation-between-humans-and-machines"">AN #94</a></strong>)). However, the authors in this paper argue that such methods are unstable due to their formulation as saddle-point problems which means they have weak convergence guarantees due to the assumption that the policy is slowly updated. In this paper, the authors reformulate <strong><a href=""https://arxiv.org/abs/1710.11248"">Adversarial IRL</a></strong> (<strong><a href=""https://mailchi.mp/ad852629e45a/alignment-newsletter-17"">AN #17</a></strong>) as a non-adversarial problem allowing for much stronger convergence guarantees to be proved. In particular, the authors derive a lower-bound on the discrimination reward which allows for larger policy updates and then introduce a method to iteratively tighten this bound. They also build on prior work for value-dice and derive a soft actor-critic algorithm (ONAIL) that they evaluate on a variety of control tasks. </p><p><strong>Zach&apos;s opinion:</strong> The experiments in this paper are a bit underwhelming. While they run a large number of experiments, ONAIL only occasionally outperforms value-dice consistently in the HalfCheetah environment. The authors justify this by noting that ONAIL wasn&apos;t regularized. Additionally, the policies are initialized with behavior cloning, something that value-dice doesn&apos;t require. However, the theoretical insight on iterative tightening is interesting, and together with the recent work on value-dice indicates that the design space of imitation learning algorithms is far from being exhausted.</p><h2>    FORECASTING </h2><p><strong><a href=""http://dmip.webs.upv.es/EPAI2020/papers/EPAI_2020_paper_4.pdf"">Canaries in Technology Mines: Warning Signs of Transformative Progress in AI</a></strong> <em>(Carla Zoe Cremer et al)</em> (summarized by Asya): In this paper, Cremer et al. propose a methodology for identifying early warning signs (&apos;canaries&apos;) for transformative AI progress. The methodology consists of identifying key milestones using expert elicitation, arranging those milestones into causal graphs where any given milestone may make another milestone more likely, and then using the causal graph representation to identify canaries-- nodes which have a significant number of outgoing nodes.</p><p>As an example, they give a partial implementation of using this methodology to identify canaries for high-level machine intelligence. Cremer et al. interview 25 experts in a variety of fields about the limitations of deep learning, then collate the named limitations and translate them into &apos;milestones&apos;. Interviewees name 34 (potentially overlapping) milestones in total, including causal reasoning, meta-learning, hierarchical decomposition, (abstract) representation, flexible memory, common sense, architecture search, and navigating brittle environments. </p><p>Cremer et al. then construct one possible causal graph for these milestones, and identify two that may act as canaries: <em>Symbol-like representations</em>, i.e. the ability to construct abstract, discrete, and disentangled representations of inputs, could underly grammar, mathematical reasoning, concept formation, and flexible memory. <em>Flexible memory</em>, the ability to store, recognize, and re-use knowledge, could unlock the ability to learn from dynamic data, the ability to do continuous learning, and the ability to learn how to learn.</p><p><strong>Asya&apos;s opinion:</strong> I like the methodology proposed in this paper, and I found the list of named limitations of deep learning interesting and informative. I&#x2019;m not sure that I personally agree with the particular canaries identified in the example (which the authors emphasize is just one possible causal graph). It seems plausible to me that both flexible memory and symbol-like representations would be an emergent property of any deep learning system with a sufficiently rich training dataset, curriculum, compute available, etc. and the real milestones to watch would be advances in those inputs.</p><h2>    MISCELLANEOUS (ALIGNMENT) </h2><p><strong><a href=""https://arxiv.org/abs/1911.09005"">Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments</a></strong> <em>(Roel Dobbe et al)</em> (summarized by Flo): This paper looks at AI Safety from the lens of Science &amp; Technology Studies. AI systems are framed as sociotechnical, meaning that both social and technical aspects influence their development and deployment. As AI systems scale, we may face difficult value choices: for example, how do we compare between values like equality and liberty when we cannot have both? This can be resolved using intuitive comparability (IC): even if it seems incomparable in the abstract, humans are still able to make deliberate tradeoffs that involve these values. This is particularly relevant for so-called hard choices where different alternatives seem to be on par, which require normative reasoning and the incorporation of values that were previously neglected. As AI systems can reshape the contexts in which stakeholders exist, we are likely to encounter many hard choices as new values emerge or become more salient. The IC perspective then suggests that AI systems and criteria for evaluation should be iteratively redesigned based on qualitative feedback from different stakeholders.</p><p>The authors then argue that as AI systems encode hard choices made by or for different stakeholders, they are fundamentally political. Developers are in a position of power and have the responsibility to take a political stance. A set of challenges to preserve stakeholders&apos; access to hard choices in an AI system&apos;s development are proposed: </p><p>1. The design of the system should involve the explicit negotiation of modelling assumptions or the lack thereof and learning goals as well as deliberation about future value conflicts or externalities that might make a reiteration of the design process necessary and give enough flexibility for stakeholders to imprint their own values during training and deployment. </p><p> 2. The training of the system should involve an impartial assessment of the tradeoff between visible performance and potential hidden disadvantages like bias, brittleness or unwanted strategic behaviour and involve stakeholders in the resolution. Furthermore, a team consensus about what can and cannot be done to improve performance should be established. </p><p> 3. During deployment, there should be an easily useable and trustworthy feedback channel for stakeholders, who should either have an explicit say in shaping the system (political setting) or the option to opt out of the system without major costs (market setting).</p><br><br><p>These challenges should be part of the training of AI designers and engineers, while the public needs to be sufficiently educated about the assumptions behind and the abilities and limitations of AI systems to allow for informed dissent.</p><p><strong>Flo&apos;s opinion:</strong> I agree that technology, especially widely used one, always has a political aspect: providing access to new capabilities can have large societal effects that affect actors differently, based on accessibility and how well the capabilities match with their existing ones. I also like the distinction between deployment in market settings, where opting out is possible, and political settings, even though this can obviously become quite fuzzy when network effects make competition difficult. Lastly, I strongly agree that we will need an iterative process involving qualitative feedback to ensure good outcomes from AI, but worry that competitive pressures or the underestimation of runaway feedback dynamics could lead to situations where AI systems directly or indirectly prevent us from adjusting them. </p><h1>    OTHER PROGRESS IN AI </h1><br><h2>    REINFORCEMENT LEARNING </h2><p><strong><a href=""https://arxiv.org/abs/2006.05990"">What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study</a></strong> <em>(Marcin Andrychowicz et al)</em> (summarized by Sudhanshu): In what is likely the largest study of on-policy reinforcement learning agents, this work unifies various algorithms into a collection of over 50 design choices, which the authors implement as tunable hyperparameters and systematically investigate how those parameters impact learning across five standard continuous control environments. Specifically, they choose subsets of these hyperparameters in eight experiment themes -- policy losses, network architectures, normalization and clipping, advantage estimation, training setup, timestep handling, optimizers, and regularization. They train thousands of agents for various choices within each theme, for a total of over 250,000 agents.</p><p>They present nearly a hundred graphs summarizing their experiments for the reader to make their own conclusions. Their own recommendations include: using the PPO loss, using separate value and policy networks, initializing the last policy layer with x100 smaller weights, using tanh as activation functions, using observation normalization, using generalized advantage estimation with &#x3BB; = 0.9, tuning the number of transitions gathered in each training loop if possible, tuning the discount factor, using the Adam optimizer with a linearly decaying learning rate, among several others.</p><p><strong>Sudhanshu&apos;s opinion:</strong> This is a paper worth a skim to glimpse at the complexity of today&apos;s RL research while noting how little we understand and can predict about the behaviour of our algorithms. A fun game to play here is to go through the graphs in Appendices D through K and arrive at one&apos;s own interpretations <em>before</em> comparing them to the authors&apos; in the main text. What was unsatisfying was that often there were contradictory results between environments, meaning there was no insight to be gleaned about what was happening: for instance, value function normalization always helps except in Walker2d where it significantly hurts performance. Such work raises more questions than it answers; perhaps it will motivate future research that fundamentally rethinks our environments and algorithms.</p><p>A more mundane, but alignment-relevant observation is that seeing how difficult it is to tune an agent for a task in simulation, and how much hyperparameters may vary across tasks, is weak evidence against powerful sim-to-real transfer performance arising out of the current paradigm of simulators/tasks and algorithms: agents will need to be trained in the real world, spawning associated risks which we may want to avoid.</p><h2>    DEEP LEARNING </h2><p><strong><a href=""https://arxiv.org/abs/2009.03300"">Measuring Massive Multitask Language Understanding</a></strong> <em>(Dan Hendrycks et al)</em> (summarized by Rohin): With the advent of large language models, there has been a shift to evaluating these models based on the knowledge they have acquired, i.e. evaluating their &#x201C;common sense&#x201D;. However, with <strong><a href=""https://arxiv.org/abs/2005.14165"">GPT-3</a></strong> (<strong><a href=""https://mailchi.mp/2485e6b42012/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals-for-ai-alignment"">AN #102</a></strong>) models have reached approximately human performance even on these benchmarks. What should be next?</p><p>We&#x2019;ve <strong><a href=""https://arxiv.org/abs/2008.02275"">previously seen</a></strong> (<strong><a href=""https://mailchi.mp/f064cf0b877f/an-113-checking-the-ethical-intuitions-of-large-language-models"">AN #113</a></strong>) a benchmark that evaluates models based on their knowledge of ethics. This benchmark (with many of the same authors) goes further by testing models with multiple choice questions on a variety of subjects that humans need to learn. These are not easy: their 57 subjects include advanced topics like Professional Medicine, College Mathematics, and International Law.</p><p>All but the largest of the GPT-3 models do about as well as random chance (25%). However, the largest 175 billion parameter model does significantly better, reaching an average score of 43.9%. This performance is very lopsided: on US Foreign Policy it gets almost 70%, while on College Chemistry and Moral Scenarios it gets about 25% (i.e. still random chance). The authors note that GPT-3 tends to do worse on subjects that require calculations and thus speculate that it is harder for GPT-3 to acquire procedural knowledge compared to declarative knowledge. The authors also find that GPT-3 is very uncalibrated about its answers in the zero-shot setting, and becomes more calibrated (though still not very good) in the few-shot setting.</p><p>It isn&#x2019;t <em>necessary</em> to have huge models in order to do better than chance: in fact, you can do better with a smaller model that is finetuned for question answering. In particular, the UnifiedQA system has an order of magnitude fewer parameters than GPT-3, but outperforms it with a score of 48.9% accuracy. This system was trained on other question answering datasets (but notably was not trained on the questions in this dataset, as this dataset is meant for evaluation rather than training). A small UnifiedQA model with only 60 million parameters (over 3 orders of magnitude smaller than GPT-3) can still do better than chance, achieving 29.3% on the dataset.</p><p><strong>Read more:</strong> <strong><a href=""https://jack-clark.net/2020/09/14/import-ai-214-nvidias-40bn-arm-deal-a-new-57-subject-nlp-test-ai-for-plant-disease-detection/"">Import AI summary</a></strong></p><p><strong>Rohin&apos;s opinion:</strong> The examples of the questions are pretty interesting, and show that this really is a hard challenge: while experts in each subject would probably get very high scores, if we tested me on all of these subjects I don&apos;t think I would do very well. I like this method of evaluation because it gets a bit closer to what we care about: whether our models can capture enough domain knowledge that they can then be used widely for automation. Depending on your beliefs about how AI will progress, there might be too much of a focus on this generality -- maybe our models only need to understand &#x201C;general reasoning&#x201D; and then we can finetune them for specific domains.</p><h2>    MISCELLANEOUS (AI) </h2><p><strong><a href=""https://arxiv.org/abs/2004.09044"">Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense</a></strong> <em>(Yixin Zhu et al)</em> (summarized by Rohin): This paper argues that current computer vision research focuses too much on a &#x201C;big data for small tasks&#x201D; paradigm that focuses only on the &#x201C;what&#x201D; and &#x201C;where&#x201D; of images. More work should be done on a &#x201C;small data for big tasks&#x201D; paradigm that focuses more on the &#x201C;how&#x201D; and &#x201C;why&#x201D; of images. These &#x201C;how&#x201D; and &#x201C;why&#x201D; questions focus attention on details of an image that may not be directly present in the pixels of the image, which the authors term &#x201C;dark&#x201D; data (analogously to dark matter in physics, whose existence is inferred, not observed). For example, by asking why a human is holding a kettle with the spout pointing down, we can infer that the kettle contains liquid that will soon come out of the kettle, even though there are no pixels that directly correspond to the liquid.</p><p>The authors propose five important areas for further research, abbreviated FPICU, and do a literature review within each one:</p><p>1. <strong>Functionality:</strong> Many objects, especially those designed by humans, can be better understood by focusing on what functionalities they have.</p><p>2. <strong>Physics:</strong> Cognitive science has shown that humans make extensive use of <em>intuitive physics</em> to understand the world. For example, simply reasoning about whether objects would fall can provide a lot of constraints on a visual scene; it would be weird to see an upright cup floating in the air.</p><p>3. <strong>Intent:</strong> The world is filled with goal-directed agents, and so understanding the world requires us to infer the goals that various agents have. This is a capability humans get very quickly -- at eighteen months of age, children can infer and imitate the intended goal of an action, even if the action fails to achieve the goal.</p><p>4. <strong>Causality:</strong> Much has already been written about causality; I will not bore you with it again. The authors see this as the most important factor that underlies the other four areas.</p><p>5. <strong>Utility:</strong> I didn&#x2019;t really get how this differed from intent. The section in the paper discusses utility theory, and then talks about work that infers utility functions from behavior.</p><p><strong>Rohin&apos;s opinion:</strong> I really liked the super detailed description of a large number of things that humans can do but current vision systems cannot do; it feels like I have a much more detailed sense now of what is missing from current approaches to vision. While the paper has a huge 491 references backing up its claims, I&#x2019;m not sure how relevant all of them are. For example, the reference to the revelation principle didn&#x2019;t really seem to justify the associated point. As a counterpoint, the discussion on utility functions in various fields was excellent. Unfortunately I&#x2019;m not familiar enough with most of the other areas to spot check them.</p><p>I read this paper because I heard that the last author, Song-Chun Zhu, was leaving his job as a professor at UCLA to set up a research institute on &#x201C;general AI&#x201D; in Beijing, and I wanted to get a sense of what the institute was likely to work on. It seems like the institute will probably pursue an agenda that focuses on building the five particular facets of intelligence into AI systems, as a form of inductive bias: this is how you&#x2019;d get to a &#x201C;small data for big tasks&#x201D; paradigm. If that&#x2019;s right, it would be in stark contrast to the neural network approaches taken by most of industry, and the biology-inspired approaches taken by (say) the Human Brain Project, but it would feel quite aligned with the views of many academics (like Josh Tenenbaum, who is a coauthor on this paper).</p><h1>    NEWS </h1><p><strong><a href=""https://openai.com/blog/openai-licenses-gpt-3-technology-to-microsoft/"">OpenAI Licenses GPT-3 Technology to Microsoft</a></strong> (summarized by Rohin): In the <strong><a href=""https://openai.com/blog/microsoft/"">initial announcement of Microsoft&#x2019;s investment in OpenAI</a></strong> (<strong><a href=""https://mailchi.mp/2abdf19aa813/an-61ai-policy-and-governance-from-two-people-in-the-field"">AN #61</a></strong>), OpenAI suggested that they would likely license pre-AGI technologies to Microsoft in order to get enough capital to run high-compute experiments. This has now happened with the <strong><a href=""https://openai.com/blog/openai-api/"">GPT-3 API</a></strong> (<strong><a href=""https://mailchi.mp/ba4d1765368f/an-104-the-perils-of-inaccessible-information-and-what-we-can-learn-about-ai-alignment-from-covid"">AN #104</a></strong>).</p><h4><strong>FEEDBACK</strong></h4><p> I&apos;m always happy to hear feedback; you can send it to me, <strong><a href=""https://rohinshah.com/"">Rohin Shah</a></strong>, by <strong>replying to this email</strong>.                         </p><h4><strong>PODCAST</strong></h4><p>An audio podcast version of the <strong>Alignment Newsletter</strong> is available. This podcast is an audio version of the newsletter, recorded by <strong><a href=""http://robertskmiles.com/"">Robert Miles</a></strong>.</p>",rohinmshah,rohinmshah,Rohin Shah,
qLqyPMfc8epav72JF,Learning how to learn ,learning-how-to-learn,https://www.lesswrong.com/posts/qLqyPMfc8epav72JF/learning-how-to-learn,2020-09-30T16:50:19.356Z,45,24,0,False,False,https://www.neelnanda.io/blog/34-learning,"<h2>Introduction</h2><p>The skill of learning things fast and well is <em>phenomenally </em>valuable. It serves as a force multiplier on almost everything you&#x2019;ll do later in life, and I think it is worth invest a <em>ton </em>of effort into cultivating and developing. Getting better at learning is one of the most valuable things I&#x2019;ve learned in my degree, and I think it&#x2019;s a strong contender for my most useful and employable skill. And I think it&#x2019;s significantly increased the chance that I can do something awesome with my life. I think that anyone who isn&#x2019;t putting significant effort into improving at this, especially if you&#x2019;re a student, is systematically screwing up.</p><p>This post is an attempt to outline my philosophy of learning, and the tactics/mindset which have worked best for me. </p><p>Learning is a useful skill in <em>many </em>different areas of life, with some general skills and some specific skills. I&#x2019;ll outline many different techniques and approaches, and I recommend reading this post while keeping in mind <em>your </em>goals and contexts, and focusing on the ideas most relevant to you. A lot of my ideas are likely best suited to learning pure maths well, but they&#x2019;ve seemed to generalise well for me, so I hope this post is of value! And many of my examples will revolve around students, since that&#x2019;s <em>my </em>context, but the core ideas of effective learning generalise far beyond that. The world is complex and messy, and the process of becoming awesome is a constant process of getting better.</p><p>I&#x2019;d also highly recommend the course <a href=""https://www.coursera.org/learn/learning-how-to-learn/"">Learning How to Learn</a> as a usefully different perspective on good learning! (and from which I shamelessly stole the title of this post)</p><h2>Iterate!</h2><p>The most important insight, is that learning is a <strong>personal </strong>process. There are <em>many </em>possible strategies for how to learn, with different pros and cons. There isn&#x2019;t a perfect killer app, to find <em>your </em>ideal learning strategy, you just need to try a lot of different stuff! Try something new, review it and see what worked well and didn&#x2019;t, adapt it and repeat this process until it feels like it works well. This post is essentially a collection of my most successful experiments.</p><p>Expect to try many things, and expect most of them to fail. But this is totally fine! This is a textbook example of <a href=""https://www.neelnanda.io/blog/33-upside-risk"">upside risk</a>! <strong>You are going to spend the rest of your life learning new things</strong>. If you can marginally improve, that will stick with you for the rest of your life, while a failed experiment has a one off cost. Take a sacrifice in the moment to help your long-term self.</p><p>And notice when your <em>current </em>system is broken! If you&#x2019;re perpetually confused, lose track of what&#x2019;s going on, and never retain anything, this is a strong sign that it&#x2019;s worth trying something different. Some things <em>are </em>just intrinsically hard, but you should be able to tell if you&#x2019;re making progress</p><p>Further, a common failure mode is not realising how <em>creative </em>you can be with your learning style! A lot of students I see just absorb a default learning method of going to fast paced lectures and writing down everything they see on the board. And they stick to this style, no matter how confusing or inefficient it feels. But there are so many other ways out there! Ask your friends how they learn things. Ask the smartest people you know how they learn. Try something wild, that you&#x2019;d never normally do. Don&#x2019;t be afraid to deviate from the default.</p><p><strong>Exercise: </strong>Pick something you learned recently, and think about how you learned. Now, set a <a href=""https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers"">5 minute timer</a>, and list other ways you <em>could </em>have learned it. Aim for as many as possible! And don&#x2019;t confuse the feeling of it being <em>hard </em>to generate more ideas with it being <em>impossible</em>.</p><h2>Introspection</h2><p>A significant mistake I see is not respecting the role of emotions, intuitions and introspection in effective learning. Our minds are phenomenally powerful computers, yet our system 2, the conscious verbal part we have access to, is just the tip of the iceberg. Our system 2 alone is utterly incapable of tasks like maintaining a conversation, which computers struggle with, yet our intuitions can manage just fine. A <em>ton </em>of valuable information can be found by paying close attentions to your emotions and intuitions, yet people often dismiss those as &#x201C;irrational&#x201D; and not worth listening to.</p><p>A few different ways this is helpful:</p><ul><li>Notice the feeling of confusion or surprise. This should feel <em>important</em>, and direct you to spend more effort understanding those!</li><ul><li>The default state of the world is that you will fail to deeply understand and retain things. A glimmer of confusion or surprise often indicates those weak points, which you <em>know </em>are out there. And you can&#x2019;t learn effectively until you become able to address those weak points!</li><li>If confused, ask somebody for help! Or google &#x201C;intuitive explanation of &#x2026;&#x201D;</li></ul><li>Notice the feeling of &#x201C;this proves too much&#x201D; - often this shows you&#x2019;re missing a key limitation of the idea</li><li>Notice the feeling of &#x201C;this feels useless and ungrounded&#x201D; - often this shows you&#x2019;ve lost track of the context and bigger picture</li><ul><li>Similarly, notice the feeling of following locally but not globally - where you followed each logical step, but feel like there was a trick somewhere you can&apos;t put your finger on</li></ul><li>Notice what you&#x2019;re <em>enjoying</em>! Learning is significant cognitive labour, and takes effort. It&#x2019;s <em>way </em>easier when you <a href=""https://www.neelnanda.io/blog/mini-blog-post-11-live-a-life-you-feel-excited-about"">feel excited about</a> what you&#x2019;re working on. Give yourself permission to follow your curiosity, and dive down rabbit holes. Notice the spark of curiosity and nurture it into drive for what you&#x2019;re trying to do</li></ul><p>I find the <a href=""https://www.lesswrong.com/posts/GLPaZamxqkx7XJbXv/the-skill-of-noticing-emotions"">technique of Noticing</a> to be super useful for getting better at introspection like this, and converting those vague feelings into something <em>useful</em>. I find this emotional awareness significantly drives my actions when learning something new, and triggers several times a day.</p><h2>The 80/20 Rule</h2><p>Another key point: there are a <em>lot </em>of things to learn, and a lot of details, and you only have finite time. <strong>You need to prioritise</strong>. <a href=""https://www.neelnanda.io/blog/mini-blog-post-22-the-8020-rule"">The world is full of wasted motion</a>, and will rarely prioritise for you. A <em>ton </em>of mileage comes from identifying how to get 80% of the value from 20% of the effort, doing that first, and then moving on!</p><p>Often the most important parts are getting a handle on the <strong>big picture</strong> and <strong>high-level understanding</strong>. Good prompts:</p><ul><li>What is going on here? What problems are we trying to solve?</li><li>Why do we care about this?</li><li>How could I rederive/generate this idea from scratch if I needed to?</li><li>Notice when a decision feels arbitrary and unmotivated, and try to figure out where it comes from</li><li>Sit down, and try to write out the key points of a topic from memory. What are the biggest holes in your understanding? What&#x2019;s missing?</li><li>What is currently confusing me? What felt surprising?</li></ul><p>Getting the high-level picture is <em>phenomenally </em>valuable - it means that when learning any object level details, I can instantly see where they slot in, whether I care about them or not, what other ideas they connect to, how this could be useful in future, etc. </p><p>Another valuable skill is learning when to give up on something and when to move on. What things are <em>not </em>worth caring about? Notice when something is a BS magic trick, or a fiddly detail. I find that maths proofs are often 80% trivial algebra, and 10% magic tricks that are only relevant to that specific problem, and 10% key ideas that are used again and again. And I want to <em>only </em>remember those key ideas, all else is a waste of my time. Some good prompts:</p><ul><li>Optimise for what will help you understand <em>other </em>things. Can you imagine this coming up again?</li><li>What is the minimal set of ideas I&#x2019;d need to remember to rederive everything else here?</li><li>Ask someone who&#x2019;s more experienced than you whether this is worth your time</li><li>Notice when things feel arbitrary or unmotivated</li><ul><li>This is hard because sometimes this shows you haven&#x2019;t grasped the high-level picture - mentors are useful for resolving this!</li></ul></ul><p>This principles also applies to <strong>simplifying</strong> and <strong>compressing</strong>. After solving a tricky problem, or understanding a hard concept, compress your understanding to as few words and ideas as possible. Eg, challenge yourself to write down the key ideas of a course in &lt;10 minutes. This pressure often forces me to identify the truly important ideas.</p><p>This also applies to the resources you learn from! Some resources are <em>way </em>better than others, and there is significant cognitive labour to learning from a bad resource. And sometimes it&#x2019;s not even worth bothering! I recommend regularly asking &#x201C;am I getting value from this?&#x201D; and every hour writing down what you just learned. If you consistently write down nothing, there&#x2019;s a decent chance you should change up how you&#x2019;re doing things!</p><p>Two specific resources I&#x2019;d highly recommend:</p><ul><li><a href=""https://web.evanchen.cc/napkin.html"">The Napkin</a> by Evan Chen, is far and away the best pure maths book I&#x2019;ve ever read for getting a feel for the bigger picture</li><li>The <a href=""https://fast.ai/"">fast.ai</a> course is an <em>excellent </em>introduction to the practical details and concepts of deep learning, from a very &#x201C;make things that work&#x201D; point of view</li></ul><h2>Teaching</h2><p>A phenomenally powerful way of learning something deeply is to teach it to somebody else! </p><p>One model I have of learning, is that I take <em>in </em>information in the format of language. But in my head, they&#x2019;re stored in a more abstract, conceptual format. And the key challenge of learning is having a concept translated into language in the first place, and then translate from language into concepts, with as few errors as possible. And this will inevitably leave corruptions and holes, but these aren&#x2019;t always clear (especially if I struggle to notice confusion or surprise!). Teaching forces <em>me </em>to convert this from concepts back into language, but using different words and framings, so it must go <em>via </em>the conceptual framework in my head. And this both reinforces those concepts, and make the corruptions and holes <em>way </em>more visceral. It&#x2019;s easy to brush over something confusing when rushing through a textbook, but you can&#x2019;t brush over it when explaining to somebody else.</p><p>Teaching also favours a high-level understanding - it&#x2019;s really <em>annoying </em>to verbally convey fiddly details to somebody else. My aesthetic sense <em>forces </em>me to favour a coherent, high-level picture, prioritising the key bits and skipping over everything I can justify skipping out. And it&#x2019;s <em>much </em>more obvious when an explanation is confused, and forces me to go back and understand it more deeply</p><p>There are a few different approaches to teaching, and they all have elements of these benefits:</p><ul><li>Giving talks</li><ul><li>I find giving talks <em>great </em>fun (because I&#x2019;m a massive extrovert and attention seeker), and it serves as an excellent commitment device. I feel obliged to hold myself to high standards</li><li>There are <a href=""https://www.neelnanda.io/blog/mini-blog-post-10-seek-positive-externalities"">significant positive externalities</a> to doing this! Way more people should give talks</li></ul><li>Explaining 1 on 1</li><ul><li>This is less formal, and lower stakes/lower pressure</li><li>It&#x2019;s also much lower friction! Find a friend, and offer to explain something cool to them. Or find a precocious student in the year below who wants to learn something advanced</li></ul><li>Writing things!</li><ul><li><a href=""https://www.neelnanda.io/blog/27-retrospective"">Blogging is great</a></li><li>I found it super valuable to write notes trying to teach the intuitions behind my courses</li><ul><li>Knowing that somebody else will read the notes held me to a much higher standard</li><li>And this was <em>really </em>low friction - it was like normal note writing, but just with a different perspective</li></ul><li>Even if you don&#x2019;t intend to publish things, writing them down is still awesome! It forces you to crystallise things</li><ul><li>Ensure it&#x2019;s in your own words, rather than copying! You need to go <em>via </em>the conceptual language in your head. This forces you to process things</li><li>Back when I still went to lectures (rookie mistake), I got much more mileage out of only making notes with summaries of key concepts</li></ul></ul></ul><p>When teaching, I find it most useful to focus on the structure, motivation and high-level picture. Thinking about how to <em>efficiently </em>convey ideas into a students mind when they lack context is an <em>excellent </em>way to structure the ideas better in <em>your </em>head. </p><p>I think teaching is also excellent, because teaching teaches you better meta-skills for learning well! Teaching and learning are two sides of the same coin - often there is cognitive labour that either the student <em>or </em>the teacher could do. And getting familiar with both points of view makes it clearer what these are, and ways you can fail to learn or fail to teach. I outline this perspective in more detail in my post on <a href=""https://www.neelnanda.io/blog/mini-blog-post-18-how-to-teach-things-well"">good teaching</a>.</p><h2>Asking Questions</h2><p>I personally learn <em>really </em>well from other people. And my main tool for this is to ask questions (in both 1-1 or group settings). This is particularly salient to me, because questions are a key part of how I learn, but it feels like most people don&#x2019;t even conceive of it as a skill worth cultivating or trying at. One of my general life goals is to surround myself with as many smart people as possible, and questions are the process by which I can move knowledge from their head to mine! </p><p>It&#x2019;s easy to conceive of learning as a passive process, where knowledge is dispensed by a mentor and I just absorb it, but I think this is a <em>terrible </em>approach. Learning should always be active, as you compress the ideas into the conceptual language in your head, and notice errors, holes and confusions. And when learning from talk or a mentor, asking questions is the main way to actively engage! And if you are not constantly on top of things and processing information, I think you&#x2019;re significantly and systematically missing out.</p><p>There are a few different mindsets for asking questions:</p><ul><li>When learning, you want to fit concepts into your <strong>knowledge graph</strong>, the web of ideas in your mind and how they fit together. If you&#x2019;re not sure</li><li>When learning, you&#x2019;re always trying to fit the new ideas into your head. But sometimes this doesn&#x2019;t quite fit, and you feel confused. In my head, I try to ensure this feels like an <strong>error</strong>, and feels <strong>important</strong>. If this ever happens, ask about it!</li><ul><li>It&#x2019;s easy to be insecure about this and fear looking stupid. I think this is generally an unhelpful mindset. If you&#x2019;re confused, likely other people are too! I try to conceive of question asking as a public service, and people generally appreciate my questions!</li><li>It&#x2019;s easy to hold yourself to high standards with questions, but I think this is also silly. An excellent question is &#x201C;when you said &#x2026; I felt confused. Can you elaborate?&#x201D;</li><ul><li><em>Teaching </em>is hard! It&#x2019;s hard to tell what the audience most wants to hear! Asking questions is an excellent way to create a dialogue and make yourself easier to teach</li></ul></ul><li>A useful concept for learning is <strong>surface area</strong> - getting context, connections and associated ideas to the object-level thing you care about. And questions can be optimised for getting surface area!</li><ul><li>Open ended questions, that rely on <em>their </em>intuitions: </li><ul><li>What are common misconceptions in your field</li><li>What did you find most surprising when learning about this?</li><li>What are the most important problems/ideas?</li><li>What are you currently working on, and why?</li><li>The key here is that context &amp; surface area lets <em>you </em>ask questions. But the <em>teacher </em>has lots already. And good questions can use <em>their </em>context to find the right questions, without taking much work from you</li></ul><li>Ask for examples! Especially for typical examples, and for counter-examples. </li><ul><li>Volunteer your <em>own </em>examples and ask whether they&#x2019;re suitable</li></ul></ul><li>Try to <strong>paraphrase</strong> back the key ides in your own words, and ask whether it&#x2019;s a good summary</li><ul><li>Paraphrasing is a <em>phenomenally </em>valuable learning technique. If the only thing you retain from this post is paraphrasing, I am happy!</li><li>Paraphrasing forces you to convert the ideas into conceptual language and back <em>immediately</em>. It creates a tight feedback loop that highlights errors and confusions immediately</li><ul><li>And remember, <strong>the default state of the world is that you fail to learn things well</strong>! Paraphrasing can reveal this and fix it <em>immediately</em>. You&#x2019;re definitely wrong about something, and want to learn what</li></ul><li>Paraphrasing forces you to be constantly active and on your toes! It&#x2019;s hard to zone out if you know you&#x2019;re going to be paraphrasing back. </li><ul><li>It&#x2019;s useful in the same way that teaching is great!</li></ul><li>In a 1-1 conversation, I paraphrase something at least every 5 minutes, and this is a <em>key </em>part of how I learn</li><li>Paraphrasing is great for the teacher - it shows you&#x2019;re paying attention, and that they&#x2019;ve communicated well</li><ul><li>In a group setting, it&#x2019;s often helpful for other people to hear a different framing and perspective, or to highlight something <em>they </em>were confused about but hadn&#x2019;t realised yet</li></ul><li>It&#x2019;s much easier to <em>correct </em>than to communicate well in the first place, so this can identify misconceptions!</li><li>Another failure mode is focusing on someone&#x2019;s <strong>conclusions</strong>, not their <strong>model</strong>. But the model, the way they <em>generate </em>conclusions, is <em>way </em>more interesting, and far more educational. If passive, it&#x2019;s easy to see the conclusions and take them at face values. But paraphrasing makes a missing model far more visceral. </li></ul></ul><p>On a more meta-level, another way to use questions is by seeking <strong>feedback</strong>. As I <a href=""https://www.neelnanda.io/blog/26-accurate-self-image"">outline in more detail here</a>, seeking feedback is <em>really </em>important. It can identify your weak points, your misunderstanding, and your strengths. All of life is a trade-off, and you need to allocate your ability to understand improve accordingly. Feedback gives valuable information for doing this. </p><p>Feedback is also a great way to mine a mentor or teacher for information! You use <em>their </em>context to identify what <em>you </em>should most care about. And again, it&#x2019;s much easier to correct than to teach!</p><p>Some tips:</p><ul><li>Be open to criticism! Feedback is useful and valuable, and you likely have a reflexive to be defensive. This is a bias, and systematically deviates you from the truth, and <a href=""https://www.neelnanda.io/blog/31-overcoming-bias"">should be solved with a counter-bias</a>. </li><ul><li>Giving good feedback can be hard, and takes cognitive and emotional labour. Clearly signalling you&#x2019;ll take it well is both polite, and makes you more likely to <em>get </em>it</li></ul><li>Paraphrase back the feedback, and ensure you&#x2019;ve understand it well. Try to reveal the <em>model </em>beneath it, and see what you can learn from that</li><li>Remember, feedback is <em>awesome</em>! <a href=""https://www.neelnanda.io/blog/mini-blog-post-7-problems-are-for-fixing"">Problems are for fixing</a>, but you can&#x2019;t fix a problem until you identify it. Feedback identifies your weak points, and you can iterate and improve them, get more feedback, and steadily become more awesome. High-quality feedback is a sign that somebody is an ally, not an enemy - <strong>they&#x2019;re helping you become stronger</strong>.</li><li>Ask for feedback for the most <em>important </em>things, you can prioritise even here! </li><ul><li>Eg, ask for feedback on how you learn, how you prioritise, etc. All of the most important meta skills!</li><li>Ask for feedback for what you&#x2019;re <em>uncertain </em>about! Information is most valuable on the things you understand the least</li><li>Ask for feedback on the weak points you&#x2019;re <a href=""https://www.neelnanda.io/blog/mini-blog-post-24-on-procrastination-the-art-of-shaping-your-future-actions"">procrastinating on doing anything about</a>, if it might serve as useful motivation!</li></ul></ul><h2>Spaced Repetition</h2><p>An underrated specific technique for learning well is <strong>spaced repetition</strong>. <a href=""https://www.gwern.net/Spaced-repetition"">Education research has robustly shown</a> that the best way to retain information is to be repeatedly and actively tested on it. And, the more times you&#x2019;ve been tested on it, the longer it takes to forget it. So, if you review at the optimal time, the intervals increase exponentially, <em>and </em>your retention is good that entire time. So the total effort to remember something for the rest of my life just isn&#x2019;t that high. And there is now software like <a href=""https://apps.ankiweb.net/"">Anki</a> that can do this for you! </p><p>Rote memorisation tends to get a bad rap. I think it&#x2019;s a valuable part of learning, because it ensures I have important knowledge <strong>at my fingertips</strong>. The difference between <em>immediately </em>knowing a fact and having to look it up is significant, because this removes <strong>friction</strong>. And friction is a significant hindrance on my ability to do things, be creative and make progress. Though, importantly, spaced repetition shouldn&#x2019;t be used to achieve <em>understanding</em>. The ideal use case is taking something you already have a <em>high</em>-level picture for, and to ensure it sticks</p><p>A few tips for doing this well:</p><ul><li>Anki works <em>excellently </em>for simple facts. Remembering statistics, facts about the world, programming syntax, learning a foreign alphabet, etc</li><ul><li>In large part, this is because it&#x2019;s designed for flashcard style, question and answer format. But this is surprisingly versatile!</li><li>I&#x2019;ve also found it useful for intuitions, motivations and getting a high-level picture of things! Eg, to make maths cards, schema like &#x201C;we care about &#x2026; because &#x2026;&#x201D;, &#x201C;the key move to prove &#x2026; is &#x2026;&#x201D;, &#x201C;the definition of &#x2026; is sensible because &#x2026;&#x201D;</li><ul><li>This takes an understanding I <em>currently </em>have, and ensures it sticks</li></ul></ul><li>Spaced repetition really shines for <strong>long-term gain</strong>. It&#x2019;s <em>really </em>depressing to learn something awesome and to forget it a few months later</li><ul><li>The structure of school and university sucks for this, you&#x2019;re incentivised to cram for the exam, and to forget it all immediately after</li><li>I&#x2019;m currently in the process of converting the useful 20% of my degree into Anki cards, while I still have it in my head</li><ul><li>If you&#x2019;re a student and think what you&#x2019;re learning in your degree is intrinsically worthwhile and <em>don&#x2019;t </em>use spaced repetition, I think you&#x2019;re systematically missing out!</li></ul><li>A good litmus test: Would I like to retain this 5 years from now?</li><ul><li>This differentiates the trivial bullshit details, from the key points and understanding that I <em>actually </em>care about</li></ul></ul><li>As with all things, a good approach to Anki is <a href=""https://www.neelnanda.io/blog/mini-blog-post-19-on-systems-living-a-life-of-zero-willpower"">systematised</a>! I try to always spend 10-15 minutes in the morning on it, while brushing my teeth and getting ready, and have locked everything else on my phone during this time</li><ul><li>Generally, I think more people don&#x2019;t use spaced repetition <em>because </em>it&#x2019;s slow, unrewarding and only pays off in the long-term. But building systems can reduce these costs while preserving the long-term rewards</li></ul><li>Cloze deletions are <em>awesome </em>- they let me write the idea down in prose and to delete the key sections, making cards much lower overhead to generate!</li><ul><li>A cloze deletion takes a sentence like &#x201C;Golden retrievers are a species of [dog]&#x201D; and makes the front &#x201C;Golden retrievers are a species of [&#x2026;]&#x201D;, and the back &#x201C;Golden retrievers are a species of [dog]&#x201D;</li></ul><li>The main failure mode of early Anki users is making cards that are <strong>long </strong>and <strong>effortful</strong>. I try to make all cards answerable in &lt;10s if I remember things. If you&#x2019;re just starting out, aim to make <em>many </em>cards, and to power through quickly</li><li>It isn&#x2019;t always obvious what to use Anki for. I&#x2019;m trying to cultivate the habit of noticing all cool ideas I read or learn that feel worth retaining, and making cards for those</li><li><em><a href=""https://www.supermemo.com/en/archives1990-2015/articles/20rules"">20 more great tips</a></em>!</li></ul><p>Anki isn&#x2019;t for everyone, but if you haven&#x2019;t tried it, I think there is <a href=""https://www.neelnanda.io/blog/33-upside-risk"">significant value of information</a> to trying it out! As Michael Nielson puts it, &#x201C;<strong><a href=""http://augmentingcognition.com/ltm.html"">Anki makes long-term memory a choice</a></strong>&#x201D;. Don&#x2019;t let long-term memory be something that just <em>happens </em>to you.</p><h2>Conclusion</h2><p>Overall, I think learning well is a <em>phenomenally </em>useful skill, and cultivating it should be a major life priority! The ability to learn well is a key force multiplier on anything else I might want to do. And it makes <em>sense </em>that most people underinvest in it - the short-term costs of doing anything different can be high, and the biggest payoffs are over the rest of your life. We have a systematic bias against this kind of longterm thinking!</p><p>Hopefully the ideas I&#x2019;ve outlined in this post can serve as some useful starting points and prompts. I&#x2019;d love to hear anybody else&#x2019;s perspective on effective learning, if your methods and philosophy differ from mine!</p><p>To summarise some of the key high-level framework I&#x2019;ve outlined in this post:</p><ul><li>Learning is an <strong>iterative </strong>process. I receive ideas as <strong>language</strong>, convert them into <strong>concepts </strong>in my head. </li><ul><li><strong>The default state of the world is that this conversion process will go wrong</strong>. This is fine, and to be expected! I <em>will </em>have errors, and want to identify and iterate on those errors ASAP</li><ul><li>The feelings of <strong>confusion </strong>and <strong>surprise </strong>are the first warning signs of this, and I want to be as sensitive as possible to these</li><li>Paraphrasing and teaching is a great way to further</li></ul><li>To fit them in well, I need to know <em>where </em>to add them in. This requires having a clear picture of the high-level context. This is of <em>fundamental </em>importance, and should be prioritised</li><li>This is an <em>active </em>process. I need to be <em>doing </em>things, actively participating. If I&#x2019;m being passive, I&#x2019;m systematically screwing something up. </li><li>This is fundamentally a process of <strong>information compression</strong>. Most details are easy, a few are <em>crucial</em>. I want to become sensitive to what is and is not worth prioritising, and learning when to cut my losses.</li><ul><li>Often I need to look past the surface level details to identify the key important ideas beneath the surface</li></ul></ul><li>Learning takes significant cognitive labour. This is something I want to allocate well, and so I should figure out what is and is not worth my time - most things aren&#x2019;t! Prioritisation for <em>topics </em>is key! </li></ul><p>And remember, learning is a personal, iterative process! Try things, experiment, review how well it worked, and iterate! Your life should be a steady process of getting <em>better </em>at learning. And if you&#x2019;ve been stuck in a rut learning wise, are you happy with that? Or could things be <strong>better</strong>?</p>",neel-nanda-1,neel-nanda-1,Neel Nanda,
HKfBeWN8ufNdFgzG6,Industrial literacy,industrial-literacy,https://www.lesswrong.com/posts/HKfBeWN8ufNdFgzG6/industrial-literacy,2020-09-30T16:39:06.520Z,312,193,130,False,False,https://rootsofprogress.org/industrial-literacy,"<p>I’ve said before that understanding where our modern standard of living comes from, at a basic level, is a <a href=""https://rootsofprogress.org/progress-studies-a-civic-duty"">responsibility of every citizen</a> in an industrial civilization. Let’s call it “industrial literacy.”</p><p>Industrial literacy is understanding…</p><ul><li>That the food you eat is grown using <a href=""https://rootsofprogress.org/turning-air-into-bread"">synthetic fertilizers</a>, and that this is needed for agricultural productivity, because all soil loses its fertility naturally over time if it is not deliberately replenished. That before we had modern agriculture, more than half the workforce had to labor on farms, just to feed the other half. That if synthetic fertilizer was suddenly lost, a mass famine would ensue and billions would starve.</li><li>That those same crops would not be able to feed us if they were not also protected from pests, who will ravage entire fields if given a chance. That whole regions used to see seasons where they would lose large swaths of their produce to swarms of insects, such as <a href=""https://news.ncsu.edu/2017/05/boll-weevil-war-2017/"">boll weevils attacking cotton plants</a> in the American South, or the <a href=""https://en.wikipedia.org/wiki/Great_French_Wine_Blight"">phylloxera devouring grapes</a> in the vineyards of France. That before synthetic pesticides, farmers were forced to rely on <i>much</i> more toxic substances, such as <a href=""https://rootsofprogress.org/pesticides-and-old-lace"">compounds of arsenic</a>.</li><li>That before we had electricity and clean natural gas, people burned unrefined solid fuels in their homes—wood, coal, even dung (!)—to cook their food and to keep from freezing in winter. That these primitive fuels, dirty with contaminants, created toxic smoke: <i>indoor</i> air pollution. That indoor air pollution remains a problem today for <a href=""https://ourworldindata.org/indoor-air-pollution#access-to-clean-fuels-for-cooking"">40% of the world population</a>, who still rely on pre-industrial fuels.</li><li>That before twentieth-century appliances, housework was a full-time job, which invariably fell on women. That each household would spend <a href=""https://ourworldindata.org/working-hours#as-productivity-in-the-household-increased-working-hours-in-the-household-declined"">almost 60 hours a week</a> on manual labor: hauling water from the well for drinking and cooking, and then carrying the dirty water outside again; sewing clothes by hand, since store-bought ones were too expensive for most families; laundering clothes in a basin, scrubbing laboriously by hand, then hanging them up to dry; cooking every meal from scratch. That the washing machine, clothes dryer, dishwasher, vacuum cleaner, and microwave are the equivalent of a full-time mechanical servant for every household.</li><li>That plastics are produced in enormous quantities because, for so many purposes—from food containers to electrical wire coatings to children’s toys—we need a material that is cheap, light, flexible, waterproof, and insulating, and that can easily be made in any shape and color (including transparent!) That before plastic, <a href=""https://rootsofprogress.org/unsustainable"">many of these applications used animal parts</a>, such as ivory tusks, tortoise shells, or whale bone. That in such a world, those products were a luxury for a wealthy elite, instead of a commodity for the masses, and the animals that provided them were hunted to near extinction.</li><li>That automobiles are a lifeline to people who live in rural areas (<a href=""https://www.census.gov/newsroom/press-releases/2016/cb16-210.html"">almost 20% in the US alone</a>), and who were deeply isolated in the era before the car and the telephone. That in a world without automobiles, we relied on millions of horses, which in New York City around 1900 dumped a hundred thousand gallons of urine and millions of pounds of manure on the streets <i>daily</i>.</li><li>That <a href=""https://ourworldindata.org/child-mortality#child-mortality-in-the-past"">half of everyone you know over the age of five</a> is alive today only because of antibiotics, <a href=""https://rootsofprogress.org/smallpox-and-vaccines"">vaccines</a>, and <a href=""https://rootsofprogress.org/draining-the-swamp"">sanitizing chemicals in our water supply</a>. That before these innovations, infant mortality (in the first year of life) was as high as 20%.</li></ul><p>When you know these facts of history—which many schools do not teach—you understand what “industrial civilization” is and why it is the benefactor of everyone who is lucky enough to live in it. You understand that the electric generator, the automobile, the chemical plant, the cargo container ship, and the microprocessor are essential to our health and happiness.</p><p>This doesn’t require a deep or specialized knowledge. It only requires knowing the basics, the same way every citizen should know the outlines of history and the essentials of how government works.</p><p>Industrial literacy means understanding that the components of the global economy are not arbitrary. Each one is there for a reason—often a matter of life and death. The reasons are the immutable facts of what it takes to survive and prosper: the laws of physics, chemistry, biology, and economics that govern our daily existence.</p><p>With industrial literacy, you can see the economy as a set of <i>solutions to problems</i>. Then, and only then, are you informed enough to have an opinion on how those solutions might be improved.</p><p>A lack of industrial literacy (among other factors) is turning what ought to be economic discussions about how best to improve human health and prosperity into political debates fueled by misinformation and scare tactics. We see this on climate change, plastic recycling, automation and job loss, even vaccines. Without knowing the basics, industrial civilization is one big <a href=""https://wiki.lesswrong.com/wiki/Chesterton%27s_Fence"">Chesterton’s Fence</a> to some people: they propose tearing it down, because they don’t see the use of it.</p><p>Let’s recognize the value of industrial literacy and commit to improving it—starting with ourselves.</p>",jasoncrawford,jasoncrawford,jasoncrawford,
4A7SW4SGCZsKedJqo,Jason Crawford on the non-linear model of innovation: SSC Online Meetup,jason-crawford-on-the-non-linear-model-of-innovation-ssc-1,https://www.lesswrong.com/posts/4A7SW4SGCZsKedJqo/jason-crawford-on-the-non-linear-model-of-innovation-ssc-1,2020-09-30T10:13:13.853Z,7,1,1,False,False,,"<p>Jason Crawford on the <i>non</i>-linear model of innovation.</p><p>Please signup here by an hour before the event, and we will send you an invitation</p><p><a href=""https://forms.gle/XXk3hPNkcEJJbHFC9?fbclid=IwAR26zp7cKuT2PChZCDM0A6DOYM9j1xo4KD2oURYGNoohvZeOz51i-b_-hgs"">https://forms.gle/XXk3hPNkcEJJbHFC9</a></p><p>IMPORTANT: As Daylight Savings Time approaches an end, note the times: 18:30 UTC, 20:30 IST, 11:30 PDT.&nbsp;</p><p>Innovation is often described with a “linear” model from discovery to invention to distribution. There is an element of truth in this, but a naive interpretation of the model does not match the reality of science and invention. In this talk, I'll show the feedback mechanisms between discovery and invention and how they are intertwined, using examples including the transistor at Bell Labs and the career of Louis Pasteur.</p><p>Jason Crawford is the author of The Roots of Progress (<a href=""https://l.facebook.com/l.php?u=https%3A%2F%2Frootsofprogress.org%2F%3Ffbclid%3DIwAR3d_9hkZqTRrOJ7MZoOV4tqcU6JvvRq8CA2UWSvpdQY9d5ZlRm7Qu8xekY&amp;h=AT1ieq2ANbRdh7rMCwopdw-prNGjX2vXn7BhhruamxiVVsc_F4dEIgab0EE6izys3bbqD3H5I-q4yRMMFW8LH58gyhtVyD0NUc4RYhmRqhcrLYkWIycug1vqAi0wSeEl5W3eMHY"">https://rootsofprogress.org/</a>), where he writes about the history of technology and the philosophy of progress. Previously, he spent 18 years as a software engineer, engineering manager, and startup founder.</p>",JoshuaFox,joshuafox,JoshuaFox,
FMbKoa8QtAzPChgPy,Holy Grails of Chemistry,holy-grails-of-chemistry,https://www.lesswrong.com/posts/FMbKoa8QtAzPChgPy/holy-grails-of-chemistry,2020-09-30T02:03:07.289Z,34,16,2,False,False,,"<p>This is a linkpost-slash-suggestion to check out <a href=""https://www.chemistryworld.com/holy-grails/"">https://www.chemistryworld.com/holy-grails/</a>.</p><p>In 1995, the journal <a href=""https://pubs.acs.org/journal/achre4""><i>Accounts of Chemical Research</i> </a>published a series of articles called <a href=""https://pubs.acs.org/doi/10.1021/ar00051a001"">Holy Grails of Chemistry</a>. &nbsp;This series of articles represents an attempt by a group of chemists to make a partial list of <a href=""https://www.lesswrong.com/posts/P5k3PGzebd5yYrYqd/the-hamming-question"">Hamming questions</a> for chemistry. &nbsp;They chose eight research goals that they thought were worthy of the moniker of ""Holy Grail"", and asked key researchers in those fields to write essays about why they were important and the potential gains if the goal were realized. &nbsp;Although not <a href=""https://pubs.acs.org/doi/10.1021/ar00051a600"">everyone was impressed</a> with their choices, I think their selections have stood the test of time fairly well. &nbsp;The selected topics were <a href=""https://pubs.acs.org/doi/10.1021/ar00051a002"">Manipulation of Matter at the Atomic and Molecular Levels</a>, <a href=""https://pubs.acs.org/doi/10.1021/ar00051a003"">Room Temperature Superconductors</a>, <a href=""https://pubs.acs.org/doi/10.1021/ar00051a004"">Unnatural Selection in Chemical Systems</a>, <a href=""https://pubs.acs.org/doi/10.1021/ar00051a005"">Direct Observation of the Transition State</a>, <a href=""https://pubs.acs.org/doi/10.1021/ar00051a006"">Controlling the Future of Matter</a>, <a href=""https://pubs.acs.org/doi/10.1021/ar00051a007"">Artificial Photosynthesis, </a><a href=""https://pubs.acs.org/doi/10.1021/ar00051a008"">Biomimetic Chemistry and Artificial Enzymes</a>, and <a href=""https://pubs.acs.org/doi/10.1021/ar00051a009"">Selective Intermolecular Carbon-Hydrogen Bond Activation</a>.&nbsp;</p><p>In September 2020, <a href=""https://www.chemistryworld.com/"">Chemistry World</a>, the member magazine of the Royal Society for Chemistry,<a href=""https://www.chemistryworld.com/holy-grails/""> took a look at these ""Holy Grails""</a> 25 years later. &nbsp;Most have seen significant progress and we seem to be on the cusp of realizing the original aspirations of a couple.</p><p>I think the LessWrong audience might enjoy both the original articles as well as the follow-ups because of the community's focus on intellectual progress and choosing important problems. &nbsp;Plus, there's just some incredibly cool science summarized in the follow-up pieces. &nbsp;For instance, did you know that <a href=""https://www.chemistryworld.com/holy-grails/the-grails/room-temperature-superconductors"">some hydride materials are superconductive up to 260 K</a> if you subject them to crazy high pressures? &nbsp;Or that you can use <a href=""https://en.wikipedia.org/wiki/Directed_evolution"">directed evolution</a> to <a href=""https://www.chemistryworld.com/holy-grails/the-grails/artificial-enzymes"">expand the kinds of chemistry enzymes can catalyze</a>? &nbsp;Or that we're <a href=""https://www.chemistryworld.com/holy-grails/the-grails/c-h-bond-activation"">getting pretty good</a> at functionalizing particular C-H bonds in complex organic molecules?</p><p>You can find the Chemistry World series starting from the splash page <a href=""https://www.chemistryworld.com/holy-grails/"">here</a>. &nbsp;I would have come up with a somewhat different list of ""most important topics in chemistry"" than the original authors did, but that's no surprise in a field as big and varied as chemistry is. &nbsp;I think the Chemistry World team did a good job of creating accessible summaries of the chemistry community's ongoing quests for these ""Holy Grails"".</p><p>Note: the <i>Acc. Chem. Res.</i> articles are paywalled but the <i>Chemistry World</i> articles are not.</p>",chemslug,chemslug,chemslug,
saRRRdMnMPXXtQBNi,“Unsupervised” translation as an (intent) alignment problem,unsupervised-translation-as-an-intent-alignment-problem,https://www.lesswrong.com/posts/saRRRdMnMPXXtQBNi/unsupervised-translation-as-an-intent-alignment-problem,2020-09-30T00:50:06.077Z,62,21,15,False,False,,"<p>Suppose that we want to translate between English and an alien language (Klingon). We have plenty of Klingon text, and separately we have plenty of English text, but it’s not matched up and there are no bilingual speakers.</p><p>We train GPT on a mix of English and Klingon text and find that it becomes fluent in both. In some sense this model “knows” quite a lot about both Klingon and English, and so it should be able to read a sentence in one language, understand it, and then express the same idea in the other language. But it’s not clear how we could train a translation model.</p><p>Of course some concepts won’t have translations, and the model will often be uncertain about the translation of a term. But we can still ask for a model to explain the meaning of a Klingon expression as best as it can to an English-speaking user. For example, it could say “This is an idiomatic expression that’s often used to express great uncertainty” or “This is a small animal that is familiar to most Klingon speakers, I think it’s kind of like a frog but am not really sure” rather than translating a sentence directly.</p><p>How can we construct an objective that incentivizes the model to “try its best” at this translation task?</p><h3>Translation-specific approaches</h3><p>There are many published heuristics for unsupervised translation (e.g. <a href=""https://arxiv.org/pdf/1711.00043.pdf"">Lample et al</a>). I don’t think those techniques should completely satisfy us:</p><ul><li>Existing methods can’t lead to a model that appropriately describes its uncertainty or talks the user through a hard-to-translate expression. (At least as far as I’m aware.)</li><li>We have no real reason to think existing methods fully utilize the model’s understanding, or to expect those methods to scale well. (In practice, I think they are impressive but still lag behind the quality of our models’ understanding.)</li><li>These heuristics are specific to translation, whereas we’d like to find general methods that can scale up to harder problems.</li></ul><h3>Existing alignment techniques</h3><p>If we try to apply RL from human feedback to translation, we immediately run into a problem: how am I supposed to judge which of two English explanations of a Klingon sentence is better, given that I don’t know Klingon?</p><p>Debate doesn’t easily address this difficulty either — if one model claims that “qapla” means “great success” and the other claims it means “minor success,” I can’t easily decompose that disagreement into simpler sub-questions that debaters disagree about. Debaters could cite phrases in the database where “qapla” is used, but they’d need to average weak evidence over many phrases. Making things worse, to interpret each usage they’d need to agree about the meaning of the rest of the phrase — -which isn’t necessarily any simpler than the original disagreement about “qapla.” Even if this process was possible, it’s not at all clear that GPT would be able to do it — -being able to translate between Spanish and English doesn’t mean I have an encyclopedic knowledge of all the documents from which I built up my intuitive sense of a particular word’s meaning (which I’d need in order to win such a debate).</p><p>Right now I don’t think we have any scalable strategies to this kind of problem; I think it’s a core open question for alignment.</p><h3>Unsupervised translation seems like a good problem to think about for alignment</h3><p>I think the key feature of this situation is that our model has acquired a bunch of intuitions about the domain which are only justified empirically — the model “knows” about the meaning of phrases only insofar as it has a very complex hypothesis that was supported by the data.</p><p>This situation is going to become increasingly common as we train more powerful models, and will immediately be a real problem if we are applying human feedback to fine-tune GPT; while GPT is subhuman in many ways, it’s already acquired plenty of knowledge that any given human contractor would lack.</p><p>Most of GPT’s knowledge is something that came from <em>some</em> human, but ultimately we will be training models that generate new knowledge (e.g.by searching over plans in realistic environments, or by writing code on their own and learning about what works), and <em>no</em> human will have that knowledge. So we can’t hope to get around this problem by simply hiring more knowledgeable contractors.</p><p>This can leave us in a situation where it’s extremely difficult for humans to oversee AI decisions. If a model says “My intuition is that this business plan will make a lot of money” the user will need to decide whether or not to trust it. If they don’t, then they may find themselves at an increasing economic disadvantage. If they do, then they may have lost the ability to effectively oversee AI systems except by evaluating the consequences of their actions. That leads directly into the classical challenges of AI safety, namely that AI systems evaluated exclusively on the basis of measured outcomes have a tendency to push the world in undesirable directions (since we can’t measure what we care about) and to corrupt our measurements.</p><h3>My vague hope</h3><p>I’m hoping we can address this using the kind of approach discussed in <a href=""https://ai-alignment.com/learning-the-prior-48f61b445c04"">learning the prior</a>. That might look like:</p><ul><li>In parallel with training GPT, train a helper model that explains the meaning of phrases (it can also provide other intuitions or background facts that are useful for predicting the next word).</li><li>As we train on Klingon text, we sample phrases and then ask a human “which word will come next?” The human uses the helper model to understand what is being discussed and make a prediction.</li><li>We optimize the helper model to make the human’s next-word predictions good (in parallel with generative pre-training).</li><li>Finally, a human uses the same helper model to evaluate a proposed Klingon → English translation, and we use this to train a translator by RL.</li></ul><p>That short description sweeps a lot of complexity under the rug. Most importantly, the success of the scheme relies on the correctness of the prior over helper models (or else the helper could just be another copy of GPT-Klingon), and we don’t have a credible strategy for representing and manipulating our prior over complex programs.</p><p>Overall, I’d say that this is more at the level of “vague hope” rather than “concrete proposal.” I think it’s an open question whether anything in this space will work.</p><p>I think that this is the kind of problem which makes e.g. MIRI researchers justifiably skeptical that scalable ML alignment is possible at all, and it’s the main focus of my current conceptual work on AI alignment. I’m glad that this kind of theoretical crux also looks like it will soon be relevant to ML practice, since I think it will make it much easier to close the gap between people who work on ML and people who work on alignment.</p><img src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=99ae1f9b6b68"" /><hr /><p><a href=""https://ai-alignment.com/unsupervised-translation-as-a-safety-problem-99ae1f9b6b68"">“Unsupervised” translation as a safety problem</a> was originally published in <a href=""https://ai-alignment.com"">AI Alignment</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",paulfchristiano,paulfchristiano,paulfchristiano,
B2DFgzG6bptZvin9L,Examples of self-governance to reduce technology risk?,examples-of-self-governance-to-reduce-technology-risk,https://www.lesswrong.com/posts/B2DFgzG6bptZvin9L/examples-of-self-governance-to-reduce-technology-risk,2020-09-29T19:31:41.405Z,10,3,4,False,True,,"<p><em>Cross-posted from the <a href=""https://forum.effectivealtruism.org/posts/KJw6RDm4M6gAfqW6X/examples-of-self-governance-to-reduce-technology-risk"">EA Forum</a></em></p>
<p>I'm searching for examples of self-governance efforts to reduce technology risk. Do people have cases to suggest?</p>
<p>The more similar to AI development the better. That is, efforts by companies or academic communities to address risks that affect third parties, with minimal involvement from governments beyond basic law and order.</p>
<p>Examples from academia:</p>
<ul>
<li>Leo Szilard and other physicists coordinating to prevent Germany from obtaining atomic bomb data, 1939-1940</li>
<li>Various efforts in biotechnology:
<ul>
<li>Asilomar conference on recombinant DNA, 1975</li>
<li>Mutations Database Initiative, 1999-2001</li>
<li>Synthetic biology conferences SB1.0 and SB2.0, 2006</li>
<li>Biology journals discussing publication restrictions, 2001-2011</li>
</ul>
</li>
</ul>
<p>Examples from the commercial sector:</p>
<ul>
<li>DNA synthesis companies screening orders and buyers, 2004-2012</li>
<li>Efforts by the nanotechnology companies in the US, UK and Europe, 2004-2007</li>
</ul>
",Jia,jia,Jia,
bz5GdmCWj8o48726N,AGI safety from first principles: Goals and Agency,agi-safety-from-first-principles-goals-and-agency,https://www.lesswrong.com/posts/bz5GdmCWj8o48726N/agi-safety-from-first-principles-goals-and-agency,2020-09-29T19:06:30.352Z,77,32,15,False,False,,"<p>The fundamental concern motivating the second species argument is that AIs will gain too much power over humans, and then use that power in ways we don’t endorse. Why might they end up with that power? I’ll distinguish three possibilities:</p>
<ol>
<li>AIs pursue power for the sake of achieving other goals; i.e. power is an instrumental goal for them.</li>
<li>AIs pursue power for its own sake; i.e. power is a final goal for them.</li>
<li>AIs gain power without aiming towards it; e.g. because humans gave it to them.</li>
</ol>
<p>The first possibility has been the focus of most debate so far, and I’ll spend most of this section discussing it. The second hasn’t been explored in much depth, but in my opinion is still important; I’ll cover it briefly in this section and the next. <a href=""https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like"">Following Christiano</a>, I’ll call agents which fall into either of these first two categories <em>influence-seeking</em>. The third possibility is largely outside the scope of this document, which focuses on dangers from the intentional behaviour of advanced AIs, although I’ll briefly touch on it here and in the last section.</p>
<p>The key idea behind the first possibility is Bostrom’s <a href=""https://www.nickbostrom.com/superintelligentwill.pdf"">instrumental convergence thesis</a>, which states that there are some instrumental goals whose attainment would increase the chances of an agent’s final goals being realised for a wide range of final goals and a wide range of situations. Examples of such instrumentally convergent goals include self-preservation, resource acquisition, technological development, and self-improvement, which are all useful for executing further large-scale plans. I think these examples provide a good characterisation of the type of power I’m talking about, which will serve in place of a more explicit definition.</p>
<p>However, the link from instrumentally convergent goals to dangerous influence-seeking is only applicable to agents which have final goals large-scale enough to benefit from these instrumental goals, and which identify and pursue those instrumental goals even when it leads to extreme outcomes (a set of traits which I’ll call <em>goal-directed agency</em>). It’s not yet clear that AGIs will be this type of agent, or have this type of goals. It seems very intuitive that they will because we all have experience of pursuing instrumentally convergent goals, for example by earning and saving money, and can imagine how much better we’d be at them if we were more intelligent. Yet since evolution has ingrained in us many useful short-term drives (in particular the drive towards power itself), it’s difficult to determine the extent to which human influence-seeking behaviour is caused by us reasoning about its instrumental usefulness towards larger-scale goals. Our conquest of the world didn’t require any humans to strategise over the timeframe of centuries, but merely for many individuals to expand their personal influence in a relatively limited way - by inventing a slightly better tool, or exploring slightly further afield.</p>
<p>Furthermore, we should take seriously the possibility that superintelligent AGIs might be even less focused than humans are on achieving large-scale goals. We can imagine them possessing final goals which don’t incentivise the pursuit of power, such as deontological goals, or small-scale goals. Or perhaps we’ll build “tool AIs” which obey our instructions very well without possessing goals of their own - in a similar way to how a calculator doesn’t “want” to answer arithmetic questions, but just does the calculations it’s given. In order to figure out which of these options is possible or likely, we need to better understand the nature of goals and goal-directed agency. That’s the focus of this section.</p>
<h2>Frameworks for thinking about agency</h2>
<p>To begin, it’s crucial to distinguish between the goals which an agent has been <em>selected</em> or <em>designed</em> to do well at (which I’ll call its <a href=""https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1""><em>design objectives</em></a><em>)</em>, and the goals which an agent itself wants to achieve (which I’ll just call “the agent’s goals”).<sup class=""footnote-ref""><a href=""#fn-dyzhHG8uaZ2SpNDQQ-1"" id=""fnref-dyzhHG8uaZ2SpNDQQ-1"">[1]</a></sup> For example, insects can contribute to complex hierarchical societies only because evolution gave them the instincts required to do so: to have “competence without comprehension”, in Dennett’s terminology. This term also describes current image classifiers and (probably) RL agents like AlphaStar and OpenAI Five: they can be competent at achieving their design objectives without understanding what those objectives are, or how their actions will help achieve them. If we create agents whose design objective is to accumulate power, but without the agent itself having the goal of doing so (e.g. an agent which plays the stock market very well without understanding how that impacts society) that would qualify as the third possibility outlined above.</p>
<p>By contrast, in this section I’m interested in what it means for an agent to have a goal of its own. Three existing frameworks which attempt to answer this question are Von Neumann and Morgenstern’s <a href=""https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem""><em>expected utility maximisation</em></a>, Daniel Dennett’s <a href=""https://en.wikipedia.org/wiki/Intentional_stance""><em>intentional stance</em></a>, and Hubinger et al’s <a href=""https://arxiv.org/abs/1906.01820""><em>mesa-optimisation</em></a>. I don’t think any of them adequately characterises the type of goal-directed behaviour we want to understand, though. While we can prove elegant theoretical results about utility functions, they are such a broad formalism that <a href=""https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent"">practically any behaviour</a> can be described as maximising some utility function. So this framework doesn’t constrain our expectations about powerful AGIs.<sup class=""footnote-ref""><a href=""#fn-dyzhHG8uaZ2SpNDQQ-2"" id=""fnref-dyzhHG8uaZ2SpNDQQ-2"">[2]</a></sup> Meanwhile, Dennett argues that taking the intentional stance towards systems can be useful for making predictions about them - but this only works given prior knowledge about what goals they’re most likely to have. Predicting the behaviour of a trillion-parameter neural network is very different from applying the intentional stance to existing artifacts. And while we do have an intuitive understanding of complex human goals and how they translate to behaviour, the extent to which it’s reasonable to extend those beliefs about goal-directed cognition to artificial intelligences is the very question we need a theory of agency to answer. So while Dennett’s framework provides some valuable insights - in particular, that assigning agency to a system is a modelling choice which only applies at certain levels of abstraction - I think it fails to reduce agency to simpler and more tractable concepts.</p>
<p>Additionally, neither framework accounts for bounded rationality: the idea that systems can be “trying to” achieve a goal without taking the best actions to do so. In order to figure out the goals of boundedly rational systems, we’ll need to scrutinise the structure of their cognition, rather than treating them as black-box functions from inputs to outputs - in other words, using a “cognitive” definition of agency rather than “behavioural” definitions like the two I’ve discussed so far. Hubinger et al. use a cognitive definition in their paper on <a href=""https://arxiv.org/abs/1906.01820""><em>Risks from Learned Optimisation in Advanced ML systems</em></a>: “a system is an optimizer if it is internally searching through a search space (consisting of possible outputs, policies, plans, strategies, or similar) looking for those elements that score high according to some objective function that is explicitly represented within the system”. I think this is a promising start, but it has some significant problems. In particular, the concept of “explicit representation” seems like a tricky one - what is explicitly represented within a human brain, if anything? And their definition doesn’t draw the important distinction between “local” optimisers such as gradient descent and goal-directed planners such as humans.</p>
<p>My own approach to thinking about agency tries to improve on the approaches above by being more specific about the cognition we expect goal-directed systems to do. Just as “being intelligent” involves applying a range of abilities (as discussed in the previous section), “being goal-directed” involves a system applying some specific additional abilities:</p>
<ol>
<li><em>Self-awareness</em>: it understands that it’s a part of the world, and that its behaviour impacts the world;</li>
<li><em>Planning</em>: it considers a wide range of possible sequences of behaviours (let’s call them “plans”), including long plans;</li>
<li><em>Consequentialism</em>: it decides which of those plans is best by considering the value of the outcomes that they produce;</li>
<li><em>Scale</em>: its choice is sensitive to the effects of plans over large distances and long time horizons;</li>
<li><em>Coherence</em>: it is internally unified towards implementing the single plan it judges to be best;</li>
<li><em>Flexibility</em>: it is able to adapt its plans flexibly as circumstances change, rather than just continuing the same patterns of behaviour.</li>
</ol>
<p>Note that none of these traits should be interpreted as binary; rather, each one defines a different spectrum of possibilities. I’m also not claiming that the combination of these six dimensions is a precise or complete characterisation of agency; merely that it’s a good starting point, and the right <em>type</em> of way to analyse agency. For instance, it highlights that agency requires a combination of different abilities - and as a corollary, that there are many different ways to be less than maximally agentic. AIs which score very highly on some of these dimensions might score very low on others. Considering each trait in turn, and what lacking it might look like:</p>
<ol>
<li><em>Self-awareness</em>: for humans, intelligence seems intrinsically linked to a first-person perspective. But an AGI trained on abstract third-person data might develop a highly sophisticated world-model that just doesn’t include itself or its outputs. A sufficiently advanced language or physics model might fit into this category.</li>
<li><em>Planning</em>: highly intelligent agents will by default be able to make extensive and sophisticated plans. But in practice, like humans, they may not always apply this ability. Perhaps, for instance, an agent is only trained to consider restricted types of plans. Myopic training attempts to implement such agents; more generally, an agent could have limits on the actions it considers. For example, a question-answering system might only consider plans of the form “first figure out subproblem 1, then figure out subproblem 2, then...”.</li>
<li><em>Consequentialism</em>: the usual use of this term in philosophy describes agents which believe that the moral value of their actions depends only on those actions’ consequences; here I’m using it in a more general way, to describe agents whose subjective preferences about actions depend mainly on those actions’ consequences. It seems natural to expect that agents trained on a reward function determined by the state of the world would be consequentialists. But note that humans are far from fully consequentialist, since we often obey deontological constraints or constraints on the types of reasoning we endorse.</li>
<li><em>Scale</em>: agents which only care about small-scale events may ignore the long-term effects of their actions. Since agents are always trained in small-scale environments, developing large-scale goals requires generalisation (in ways that I discuss below).</li>
<li><em>Coherence</em>: humans lack this trait when we’re internally conflicted - for example, when our system 1 and system 2 goals differ - or when our goals change a lot over time. While our internal conflicts might just be an artefact of our evolutionary history, we can’t rule out individual AGIs developing modularity which might lead to comparable problems. However, it’s most natural to think of this trait in the context of a collective, where the individual members could have more or less similar goals, and could be coordinated to a greater or lesser extent.</li>
<li><em>Flexibility</em>: an inflexible agent might arise in an environment in which coming up with one initial plan is usually sufficient, or else where there are tradeoffs between making plans and executing them. Such an agent might display <a href=""http://www.personalityresearch.org/evolutionary/sphexishness.html"">sphexish</a> behaviour. Another interesting example might be a multi-agent system in which many AIs contribute to developing plans - such that a single agent is able to execute a given plan, but not able to rethink it very well.</li>
</ol>
<p>A question-answering system (aka an oracle) could be implemented by an agent lacking either planning or consequentialism. For AIs which act in the real world I think the scale of their goals is a crucial trait to explore, and I’ll do so later in this section. We can also evaluate other systems on these criteria. A calculator probably doesn’t have any of them. Software that’s a little more complicated, like a GPS navigator, should probably be considered consequentialist to a limited extent (because it reroutes people based on how congested traffic is), and perhaps has some of the other traits too, but only slightly. Most animals are self-aware, consequentialist and coherent to various degrees. The traditional conception of AGI has all of the traits above, which would make it capable of pursuing influence-seeking strategies for instrumental reasons. However, note that goal-directedness is not the only factor which determines whether an AI is influence-seeking: the content of its goals also matter. A highly agentic AI which has the goal of remaining subordinate to humans might never take influence-seeking actions. And as previously mentioned, an AI might be influence-seeking because it has the final goal of gaining power, even without possessing many of the traits above. I’ll discuss ways to influence the content of an agent’s goals in the next section, on Alignment.</p>
<h2>The likelihood of developing highly agentic AGI</h2>
<p>How likely is it that, in developing an AGI, we produce a system with all of the six traits I identified above? One approach to answering this question involves predicting which types of model architecture and learning algorithms will be used - for example, will they be model-free or model-based? To my mind, this line of thinking is not abstract enough, because we simply don’t know enough about how cognition and learning work to map them onto high-level design choices. If we train AGI in a model-free way, I predict it will end up <a href=""https://arxiv.org/abs/1901.03559"">planning using an implicit model</a> anyway. If we train a model-based AGI, I predict its model will be so abstract and hierarchical that looking at its architecture will tell us very little about the actual cognition going on.</p>
<p>At a higher level of abstraction, I think that it’ll be easier for AIs to acquire the components of agency listed above if they’re also very intelligent. However, the extent to which our most advanced AIs are agentic will depend on what type of training regime is used to produce them. For example, our best language models already generalise well enough from their training data that they can answer a wide range of questions. I can imagine them becoming more and more competent via unsupervised and supervised training, until they are able to answer questions which no human knows the answer to, but still without possessing any of the properties listed above. A relevant analogy might be to the human visual system, which does very useful cognition, but which is not very “goal-directed” in its own right.</p>
<p>My underlying argument is that agency is not just an emergent property of highly intelligent systems, but rather a set of capabilities which need to be developed during training, and which won’t arise without selection for it. One piece of supporting evidence is <a href=""https://en.wikipedia.org/wiki/Moravec%27s_paradox"">Moravec’s paradox</a>: the observation that the cognitive skills which seem most complex to humans are often the easiest for AIs, and vice versa. In particular, Moravec’s paradox predicts that building AIs which do complex intellectual work like scientific research might actually be easier than building AIs which share more deeply ingrained features of human cognition like goals and desires. To us, understanding the world and changing the world seem very closely linked, because our ancestors were selected for their ability to act in the world to improve their situations. But if this intuition is flawed, then even reinforcement learners may not develop all the aspects of goal-directedness described above if they’re primarily trained to answer questions.</p>
<p>However, there are also arguments that it will be difficult to train AIs to do intellectual work without them also developing goal-directed agency. In the case of humans, it was the need to interact with an <a href=""https://arxiv.org/abs/2006.07495"">open-ended environment</a> to achieve our goals that pushed us to develop our sophisticated general intelligence. The central example of an analogous approach to AGI is training a reinforcement learning agent in a complex simulated 3D environment (or perhaps via extended conversations in a language-only setting). In such environments, agents which strategise about the effects of their actions over long time horizons will generally be able to do better. This implies that our AIs will be subject to optimisation pressure towards becoming more agentic (by my criteria above) will do better. We might expect an AGI to be even more agentic if it’s trained, not just in a complex environment, but in a complex competitive multi-agent environment. Agents trained in this way will need to be very good at flexibly adapting plans in the face of adversarial behaviour; and they’ll benefit from considering a wider range of plans over a longer timescale than any competitor. On the other hand, it seems very difficult to predict the overall effect of interactions between many agents - in humans, for example, it led to the development of (sometimes non-consequentialist) altruism.</p>
<p>It’s currently very uncertain which training regimes will work best to produce AGIs. But if there are several viable ones, we should expect economic pressures to push researchers towards prioritising those which produce the most agentic AIs, because they will be the most useful (assuming that alignment problems don’t become serious until we’re close to AGI). In general, the broader the task an AI is used for, the more valuable it is for that AI to reason about how to achieve its assigned goal in ways that we may not have specifically trained it to do. For example, a question-answering system with the goal of helping its users understand the world might be much more useful than one that’s competent at its design objective of answering questions accurately, but isn’t goal-directed in its own right. Overall, however, I think most safety researchers would argue that we should prioritise research directions which produce less agentic AGIs, and then use the resulting AGIs to help us align later more agentic AGIs. There’s also been some work on directly making AGIs less agentic (such as <a href=""http://intelligence.org/files/QuantilizersSaferAlternative.pdf"">quantilisation</a>), although this has in general been held back by a lack of clarity around these concepts.</p>
<p>I’ve already discussed recursive improvement in the previous section, but one further point which is useful to highlight here: since being more agentic makes an agent more capable of achieving its goals, agents which are capable of modifying themselves will have incentives to make themselves more agentic too (as humans already try to do, albeit in limited ways).<sup class=""footnote-ref""><a href=""#fn-dyzhHG8uaZ2SpNDQQ-3"" id=""fnref-dyzhHG8uaZ2SpNDQQ-3"">[3]</a></sup> So we should consider this to be one type of recursive improvement, to which many of the considerations discussed in the previous section also apply.</p>
<h2>Goals as generalised concepts</h2>
<p>I should note that I don’t expect our training tasks to replicate the scale or duration of all the tasks we care about in the real world. So AGIs won’t be directly selected to have very large-scale or long-term goals. Yet it’s likely that the goals they learn in their training environments will generalise to larger scales, just as humans developed large-scale goals from evolving in a relatively limited ancestral environment. In modern society, people often spend their whole lives trying to significantly influence the entire world - via science, business, politics, and many other channels. And some people aspire to have a worldwide impact over the timeframe of centuries, millennia or longer, even though there was never significant evolutionary selection in favour of humans who cared about what happened in several centuries’ time, or paid attention to events on the other side of the world. This gives us reason to be concerned that even AGIs which aren’t explicitly trained to pursue ambitious large-scale goals might do so anyway. I also expect researchers to actively aim towards achieving this type of generalisation to longer time horizons in AIs, because some important applications rely on it. For long-term tasks like being a CEO, AGIs will need the capability and motivation to choose between possible actions based on their worldwide consequences on the timeframe of years or decades.</p>
<p>Can we be more specific about what it looks like for goals to generalise to much larger scales? Given the problems with the expected utility maximisation framework I identified earlier, it doesn’t seem useful to think of goals as utility functions over states of the world. Rather, an agent’s goals can be formulated in terms of whatever concepts it possesses - regardless of whether those concepts refer to its own thought processes, deontological rules, or outcomes in the external world.<sup class=""footnote-ref""><a href=""#fn-dyzhHG8uaZ2SpNDQQ-4"" id=""fnref-dyzhHG8uaZ2SpNDQQ-4"">[4]</a></sup> And insofar as an agent’s concepts flexibly adjust and generalise to new circumstances, the goals which refer to them will do the same. It’s difficult and speculative to try to describe how such generalisation may occur, but broadly speaking, we should expect that intelligent agents are able to abstract away the differences between objects or situations that have high-level similarities. For example, after being trained in a simulation, an agent might transfer its attitudes towards objects and situations in the simulation to their counterparts in the (much larger) real world.<sup class=""footnote-ref""><a href=""#fn-dyzhHG8uaZ2SpNDQQ-5"" id=""fnref-dyzhHG8uaZ2SpNDQQ-5"">[5]</a></sup> Alternatively, the generalisation could be in the framing of the goal: an agent which has always been rewarded for accumulating resources in its training environment might interalise the goal of “amassing as many resources as possible”. Similarly, agents which are trained adversarially in a small-scale domain might develop a goal of outcompeting each other which persists even when they’re both operating at a very large scale.</p>
<p>From this perspective, to predict an agent’s behaviour, we will need to consider what concepts it will possess, how those will generalise, and how the agent will reason about them. I’m aware that this appears to be an intractably difficult task - even human-level reasoning can lead to extreme and unpredictable conclusions (as the history of philosophy shows). However, I hope that we can instill lower-level mindsets or values into AGIs which guide their high-level reasoning in safe directions. I’ll discuss some approaches to doing so in the next section, on Alignment.</p>
<h2>Groups and agency</h2>
<p>After discussing collective AGIs in the previous section, it seems important to examine whether the framework I’ve proposed for understanding agency can apply to a group of agents as well. I think it can: there’s no reason that the traits I described above need to be instantiated within a single neural network. However, the relationship between the goal-directedness of a collective AGI and the goal-directedness of its individual members may not be straightforward, since it depends on the internal interactions between its members.</p>
<p>One of the key variables is how much (and what types of) experience those members have of interacting with each other during training. If they have been trained primarily to cooperate, that makes it more likely that the resulting collective AGI is a goal-directed agent, even if none of the individual members is highly agentic. But there are <a href=""https://arxiv.org/abs/1903.00742"">good reasons</a> to expect that the training process will involve some competition between members, which would undermine their coherence as a group. Internal competition might also increase short-term influence-seeking behaviour, since each member will have learned to pursue influence in order to outcompete the others. As a particularly salient example, humanity managed to take over the world over a period of millennia not via a unified plan to do so, but rather as a result of many individuals trying to expand their short-term influence.</p>
<p>It’s also possible that the members of a collective AGI have not been trained to interact with each other at all, in which case cooperation between them would depend entirely on their ability to generalise from their existing skills. It’s difficult to imagine this case, because human brains are so well-adapted for group interactions. But insofar as humans and aligned AGIs hold a disproportionate share of power over the world, there is a natural incentive for AGIs pursuing misaligned goals to coordinate with each other to increase their influence at our expense.<sup class=""footnote-ref""><a href=""#fn-dyzhHG8uaZ2SpNDQQ-6"" id=""fnref-dyzhHG8uaZ2SpNDQQ-6"">[6]</a></sup> Whether they succeed in doing so will depend on what sort of coordination mechanisms they are able to design.</p>
<p>A second factor is how much specialisation there is within the collective AGI. In the case where it consists only of copies of the same agent, we should expect that the copies understand each other very well, and share goals to a large extent. If so, we might be able to make predictions about the goal-directedness of the entire group merely by examining the original agent. But another case worth considering is a collective consisting of a range of agents with different skills. With <a href=""https://www.alignmentforum.org/posts/7jNveWML34EsjCD4c/safety-via-selection-for-obedience"">this type of specialisation</a>, the collective as a whole could be much more agentic than any individual agent within it, which might make it easier <a href=""https://www.alignmentforum.org/posts/Fji2nHBaB6SjdSscr/safer-sandboxing-via-population-separation"">to deploy subsets of the collective</a> safely.</p>
<hr>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-dyzhHG8uaZ2SpNDQQ-1"" class=""footnote-item""><p>AI systems which learn to pursue goals are also known as <em>mesa-optimisers</em>, as coined in <a href=""https://arxiv.org/abs/1906.01820"">Hubinger et al’s paper</a> <em>Risks from Learned Optimisation in Advanced Machine Learning Systems</em>. <a href=""#fnref-dyzhHG8uaZ2SpNDQQ-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-dyzhHG8uaZ2SpNDQQ-2"" class=""footnote-item""><p>Related arguments exist which attempt to do so. For example, Eliezer Yudkowsky <a href=""https://www.alignmentforum.org/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq?commentId=79jM2ecef73zupPR4"">argues here</a> that, “while corrigibility probably has a core which is of lower algorithmic complexity than all of human value, this core is liable to be very hard to find or reproduce by supervised learning of human-labeled data, because deference is an unusually anti-natural shape for cognition, in a way that a simple utility function would not be an anti-natural shape for cognition.” Note, however, that this argument relies on the intuitive distinction between natural and anti-natural shapes for cognition. This is precisely what I think we need to understand to build safe AGI - but there has been little explicit investigation of it so far. <a href=""#fnref-dyzhHG8uaZ2SpNDQQ-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-dyzhHG8uaZ2SpNDQQ-3"" class=""footnote-item""><p>I believe this idea comes from Anna Salamon; unfortunately I’ve been unable to track down the exact source. <a href=""#fnref-dyzhHG8uaZ2SpNDQQ-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-dyzhHG8uaZ2SpNDQQ-4"" class=""footnote-item""><p>For example, when people want to be “cooperative” or “moral”, they’re often not just thinking about results, but rather the types of actions they should take, or the types of decision procedures they should use to generate those actions. An additional complication is that humans don’t have full introspective access to all our concepts - so we need to also consider unconscious concepts. <a href=""#fnref-dyzhHG8uaZ2SpNDQQ-4"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-dyzhHG8uaZ2SpNDQQ-5"" class=""footnote-item""><p>Consider if this happened to you, and you were pulled “out of the simulation” into a real world which is quite similar to what you’d already experienced. By default you would likely still want to eat good food, have fulfilling relationships, and so on, despite the radical ontological shift you just underwent. <a href=""#fnref-dyzhHG8uaZ2SpNDQQ-5"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-dyzhHG8uaZ2SpNDQQ-6"" class=""footnote-item""><p>In addition to the prima facie argument that intelligence increases coordination ability, it is likely that AGIs will have access to commitment devices not available to humans by virtue of being digital. For example, they could send potential allies a copy of themselves for inspection, to increase confidence in their trustworthiness. However, there are also human commitment devices that AGIs will have less access to - for example, putting ourselves in physical danger as an honest signal. And it’s possible that the relative difficulty of lying versus detecting lying shifts in favour of the former for more intelligent agents. <a href=""#fnref-dyzhHG8uaZ2SpNDQQ-6"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",ricraz,ricraz,Richard_Ngo,
gDN26whdhcPjcH7XM,Seek Upside Risk,seek-upside-risk,https://www.lesswrong.com/posts/gDN26whdhcPjcH7XM/seek-upside-risk,2020-09-29T16:47:14.033Z,20,12,6,False,False,https://www.neelnanda.io/blog/33-upside-risk,"<p>When I meet somebody new, there are two interesting things that could happen. We could get on <em>super </em>well, become great friends, and be part of each other&#x2019;s lives for year to come. Or we don&#x2019;t. Maybe we like each other, maybe we don&#x2019;t, but we likely just go our separate ways. But the upside in the first case <em>massively </em>outweighs the downside in the second case. Yet, we all spend far less time meeting new people than we could. Why?</p><h2>Upside Risk</h2><p>I call this general phenomena of a small chance of a massive win, vs a reasonable chance of a tiny loss, <strong>upside risk</strong>. And when I think about some of the <em>really awesome </em>things in my life, a lot of it came from sources of upside risk:</p><ul><li>I have a lot of <em>amazing </em>friends, most of whom I went <a href=""https://www.neelnanda.io/blog/mini-blog-post-23-taking-social-initiative"">out of my way to meet and keep in touch with</a></li><li>Hobbies! I tried blogging, it might have been a bit of a waste of time, but <a href=""https://www.neelnanda.io/blog/27-retrospective"">turned out to be fucking awesome</a>. I could have lost a bit of time, but I now have a great new creative outlet!</li><li>Asking for advice and favours - I&#x2019;m going to be spending the next 3 months doing a research project, and this would never have happened if I hadn&#x2019;t been asking loads of people for advice and suggestions!</li></ul><p>In general, I think there are a <em>lot </em>of sources of upside risk in life, and that they&#x2019;re extremely positive in expectation. And if you do enough of them, this pays out well. So I think it&#x2019;s <em>extremely </em>valuable to actively seek out upside risk, and to shape your life to welcome it. </p><p>There&#x2019;s even a fairly solid mathematical justification for this! This is known as the <strong>explore/exploit problem</strong> in reinforcement learning - do I <strong>exploit</strong> and do what I currently think is the best idea, or do I <strong>explore</strong> and try something new, something that might give me <strong>information</strong> that I can use to inform future actions. The problem is essentially pricing the value of information. </p><p>And a pretty decent (and easy-to-compute) strategy is to take the <strong><a href=""https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html#upper-confidence-bounds"">Upper Confidence Bound</a></strong>. We have uncertainty over each strategy, and pick the strategy with the highest 95% confidence bound. Essentially, it pays to be an optimist, to always take the action with the best chance of being <em>awesome</em>. </p><p>This post is going to be about why we <em>don&#x2019;t </em>seek upside risk as much as we could, and how we can change this.</p><h2>Becoming emotionally OK with it</h2><p>So, this is a pretty simple argument. Yet, I do this far less than I could. Why?</p><p>The root cause, is that my mind <em>sucks </em>at thinking about expected values. When I try something new, the <strong>cost</strong> is clear, concrete and salient. The <strong>benefit </strong>is fuzzy, distant and unclear. And so if I try it and only get the cost, it feels like I <strong>failed</strong>. Hindsight bias kicks in, and it feels <em>obvious </em>that it wasn&#x2019;t worth trying. The subtle, insidious voice of insecurity in my head pushes me to play it safe, not stick my neck out, to not risk it. That it&#x2019;s not worth the effort, and things will never work out well. I&#x2019;m a loss averse person, and I can&#x2019;t see a <em>guaranteed </em>story for how this upside will pay out, so it feels not worth reaching for.</p><p>I think this is one of the most poisonous and damaging bugs currently in my life. I think this <em>significantly </em>cuts off a lot of awesome potential in my life. So <strong>fuck that. </strong>I don&#x2019;t <em>need </em>a precise story for how to get this upside, I just need to expose myself to as much as possible. So how can I reframe this and <em>do </em>something about it?</p><p>Some of my favourite strategies:</p><ul><li><strong><em>Make </em>the upside concrete</strong>: Try to flesh out a concrete scenario where this goes awesomely. Imagine it&#x2019;s in the 90th percentile of awesome, what happened? Add as many specific details and ideas as possible. And if the creeping voice of insecurity in your head triggers, remember that it&#x2019;s <em>explicitly </em>a hypothetical where everything goes well, so you can ignore it!</li><ul><li>And now, at the end, do you feel any more excited about this upside?</li></ul><li><strong>Think in expected value</strong>: When I&#x2019;m trying something new, I&#x2019;m operating in a position of uncertainty. I need to choose based on the expected value (EV). But <em>afterwards </em>I know the actualised value. And if I don&#x2019;t stop myself, hindsight bias creeps in, and it feels <em>obvious </em>that I should have known that all along.<br>But, if I&#x2019;m trying to update what I do in future, this is dumb! All future choices are made based on EV, so that&#x2019;s what I should actually care about. So keep score in EV! If you took a bunch of positive EV actions and are confident it was the right call, focus on that, not on the statistical noise of the results</li><ul><li>This is awesome, because it <em>redefines failure</em>. If I take a positive EV action that goes wrong, who cares! I still won</li><ul><li>I&#x2019;ve thought about this mindset enough that I&#x2019;m now happy if I ask somebody out and am rejected - the action was still positive EV!</li></ul><li>Alternate framing: Focus on the <strong>value of information</strong>. If I try swing dancing and it sucks, this is <em>great</em>! I learned something new about myself, and need never try swing dancing again</li></ul><li>Strive to become <a href=""https://www.neelnanda.io/blog/become-a-person-who-actually-does-things"">someone who Actually Does Things</a>. Score things by how they shape my future actions and bring me closer to the kind of person I want to be</li><li><strong>Do it for the joy of novelty</strong>: Trying new things is <strong>fun!</strong> I can learn new things! Have new experiences!</li><ul><li>I find it pretty easy to get joy just from doing something new - satisfying curiosity, learning something new</li><li>If I notice my life in a bit of a rut, I find it super motivating if I can start pushing myself out of it by adding in some novelty</li></ul><li><strong>Just do it</strong>: Have a rule of just <em>doing </em>any low cost, high upside idea. If you have a request you want to make of someone, just do it! If you want to invite a friend to start an ambitious project, just do it!</li><ul><li>For me, insecurities are most insidious when there is <em>uncertainty</em>. When there is a <em>choice</em>. Choices take energy and willpower, and it&#x2019;s easy for insecurity to massively raise this cost. The beauty of a <em>policy</em> is that <strong><a href=""https://www.neelnanda.io/blog/mini-blog-post-19-on-systems-living-a-life-of-zero-willpower"">changes the default</a></strong>. Figuring out whether it&#x2019;s worth it isn&#x2019;t worth the computational resources, just do it!</li><ul><li>Policies can build momentum - if you can push yourself through the first few times, it feels aversive to <em>not </em>keep going, because you&#x2019;re breaking a streak, and screwing over your future self!</li></ul><li>This also redefines failure - the goal is to follow the policy, because on average the policy works! Who cares if it doesn&#x2019;t work sometimes?</li></ul><li><strong>Opportunity framings</strong>: If I&#x2019;m asking a favour of somebody <em>else</em>, I find it super helpful to frame it as presenting them with an opportunity. If I ask somebody for a job and they say yes, they aren&#x2019;t doing me a <em>favour</em>, they&#x2019;re doing it because it&#x2019;s helpful to them! So, asking in the first place is giving them <strong>free option value</strong>, they can always say no! </li><ul><li>So, when asking for something, think about how it can benefit the <em>other </em>person!</li><ul><li>This can work even when there&#x2019;s no direct benefit - I generally find it fun to give advice to people, even though I get no benefit from it. Which means at least some of the people who give advice to me enjoy it!</li></ul><li>This relies on the other person being comfortable saying no in a low cost way. Conveniently, this is normally true in the more intimidating scenarios, like asking for a job! And if I&#x2019;m asking for somebody&#x2019;s time, the busier and higher status they are</li><li>Conversely, I know that for some of my friends, saying no is hard. So if they say no to an offer I give them, I make a point of congratulating them on practicing the skill of declining things! And this utterly takes the sting out of rejection, and makes it feel lower cost to ask for favours next time :)</li></ul></ul><h2>Implementing</h2><p>So, say you now agree that upside risk is extremely valuable, and you&#x2019;ve somewhat overcome the aversion. How can we make this <em>actionable</em>?</p><p>There are a few different ways of framing the core lesson:</p><ul><li>Do things that are high variance!</li><li>Optimise for the best-case scenario, be an optimist!</li><li>Optimise for information value, what can <em>teach </em>you the most? Do things where if it goes well, you can do it again and again</li><li>Seek novelty</li></ul><p>I find seeking novelty to be particularly fruitful. A few ways to do this:</p><ul><li>Be willing to try anything once, default to yes. </li><ul><li>I try to do this for life hacks/productivity advice - I&#x2019;ve tried a lot of crap ones that I&#x2019;ve discarded, but the ones that work well are now a <em>core </em>part of my life. Eg, discovering the value of a consistent routine, and how to use financial accountability right have been phenomenally valuable to me</li></ul><li>Make seeking novelty a <strong>quest</strong>. Set a bounded period of your life, say the next 1-2 months, where you are going to go out of your way to do novel things, and see what it&#x2019;s like. You aren&#x2019;t trying to evaluate what&#x2019;s worthwhile or do a cost-benefit analysis, just doing whatever is most fun and feels shiniest</li><ul><li>When I decided to try this, I went to taster sessions for about 10 societies. And most were crap! But it turns out that gliding, pole-dancing, ceilidh dancing and trampolining are all <em>super </em>awesome</li><ul><li>I also ended up donating blood, learning how to cycle, learning how to bake, among a bunch of other stuff. My life was <em>way </em>more fun that term, and a few things have stuck!</li></ul></ul><li>If something is boring, think about how you could reframe it to <em>make </em>it novel. </li><ul><li>For exams in my second year, I felt obliged to spend 2 months revising, but expected this to be a poor use of my time. So I reframed it to be about trying different ways of learning, and getting information about how I learned best. </li><li>A few findings:</li><ul><li>I find <a href=""https://www.youtube.com/channel/UCzoIa8e4vhfRIxL_Kj6yy0Q?view_as=subscriber"">giving talks</a> <em>super </em>motivating and fun, and it holds me to a high standard</li><li>I really enjoy teaching and tutoring, trying to explain content intuitively to other people</li><li><a href=""http://augmentingcognition.com/ltm.html"">Spaced repetition</a> is fucking awesome</li><li><a href=""https://dynalist.io/d/ToOhEKlz9qC2PmpjgyTpXQJz"">Making notes</a> with the intent of publishing holds me to a <em>way </em>higher standard (and <a href=""https://www.neelnanda.io/blog/mini-blog-post-10-seek-positive-externalities"">has positive externalities</a>!)</li><li>There were a handful of failed experiments too, but I can&#x2019;t really remember them. And really, I think that summarises this entire post</li></ul></ul><li>The idea of <strong>Comfort Zone Expansion (CoZE)</strong>. Finding things outside your comfort zone, that you&#x2019;d normally find super aversive, and finding ways to try them in a safe, supportive environment.</li><ul><li>CFAR have a CoZE section at their workshops, and I found it super valuable every time! I&#x2019;ve spent CoZE workshops:</li><ul><li>Going around a room asking people for sincere criticism, and not becoming defensive</li><li>Asking people for favours (man, I suck at this)</li><li>Having an open, honest and vulnerable conversation with a relative stranger, where we agreed to not deflect or use defence mechanisms (which was the start of becoming <em>way </em>better at this)</li></ul></ul></ul><p>Another mindset is to optimise for the best-case scenario, which I like to think of as <strong>fishing for white swans</strong>. Some strategies:</p><ul><li>When doing anything, use pre-hindsight. Assume it went <em>phenomenally </em>well, and try to explain what happened? And when you have an explanation, try to <em>make </em>it happen</li><li>High-variance small talk! The social norm is normally for first meetings to be dull, and keep to safe topics. I generally try to skip to whatever I find interesting - digging into people&#x2019;s motivations, dreams, stories and fascinations. Getting to something genuinely interesting rapidly <a href=""https://www.neelnanda.io/blog/mini-blog-post-5-how-to-learn-from-conversations"">seems to be a trainable skill</a></li><ul><li>I gather this makes me come across as somewhat direct and intense, but the world is full of people! This probably filters out some people I&#x2019;d have gotten on well with, but has found me a lot of awesome close friends. </li></ul><li>When asking for advice, it&#x2019;s easy to focus on minor uncertainties, and to be scared of asking big questions. But advice is most useful when it <em>radically </em>changes something</li><ul><li>&#x201C;What&#x2019;s a bit of life-changing advice you&#x2019;ve received?&#x201D;</li><li>&#x201C;What is the biggest problem with my current life plan?&#x201D;</li><li>&#x201C;Suppose in 5 years you realise everything you just told me was wrong. What happened?&#x201D;</li><li>&#x201C;What&#x2019;s something it could make sense for me to try with <em>super </em>high potential upside?&#x201D;</li></ul><li>Interestingly, this strategy is often <em>most </em>useful when things are already going well for me. </li><ul><li>If I already have a bunch of great friends, I want to filter harder for <em>exceptionally </em>awesome new friends</li><li>If I have a great life plan, I want to try something wild and high-variance, since I&#x2019;m optimising for the probability of finding something <em>better</em></li></ul></ul><p>Some generic examples of things worth doing:</p><ul><li>Going on dates</li><li>Meeting new people</li><li>Applying for jobs and problems that seem out of your league</li><li>Trying a hobby! An instrument, improv comedy, public speaking, blogging. Something outside your comfort zone</li><li>Notice the feeling of reluctance and procrastination, and check whether it feels justified</li><li>Follow the crazy ideas that pop into your head! If you have a wild new idea for a party or event, go ahead and organise it</li><ul><li>It helps to be in an environment that facilitates this kind of thing, I&#x2019;ve found it pretty useful to be loosely involved in student societies for this</li></ul></ul><p>And these are just my examples! Upside risk is a function of <em>your </em>life, <em>your </em>values, and <em>your </em>circumstances. What are some examples of this in <em>your </em>life?</p><p><strong>Exercise: </strong>Set a <a href=""https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers"">5 minute timer</a>, and think about the ideas in this post. What are sources of upside risk <em>you </em>could seek out?</p><h2>Conclusion</h2><p>Obviously, I&#x2019;m making an overly strong case in this post. We have finite time, and there are trade-offs. If you <em>always </em>chase upside risk, you&#x2019;re Pascal&#x2019;s Mugging yourself out of any free time. Sometimes there are hidden big <em>downside </em>risks. But I see a <em>lot </em>of instances of safe, moderate actions with upside risk that aren&#x2019;t taken. And, at least in my social circles, I think on the margin people should seek upside risk way more! </p><p>So if the ideas in this post resonated with you, <strong>live it</strong>! Keep score in terms of EV. Seek novelty. Try new things. Go outside your comfort zone. Ask for things. Notice the <em>massive </em>pile of potential wins in your life, and ask whether you&#x2019;re OK with leaving all of that on the table. </p><p>What&#x2019;s a source of upside risk you could seek <em>right now</em>?</p>",neel-nanda-1,neel-nanda-1,Neel Nanda,
LbWzgZmYqTM8GmLxm,Doing discourse better: Stuff I wish I knew,doing-discourse-better-stuff-i-wish-i-knew-1,https://www.lesswrong.com/posts/LbWzgZmYqTM8GmLxm/doing-discourse-better-stuff-i-wish-i-knew-1,2020-09-29T14:34:55.913Z,27,12,11,False,False,https://dyno-might.github.io/2020/09/29/doing-discourse-better-stuff-i-wish-i-knew/,"<p>Nick and Maria want to talk about sugar. Does it reduce lifespan? They disagree, but are honest and principled. They want to find the truth and agree on it.</p>",dynomight,dynomight,dynomight,
groAB7XtQQ3SdjJNu,David Friedman on Legal Systems Very Different from Ours: SlateStarCodex Online Meetup,david-friedman-on-legal-systems-very-different-from-ours-1,https://www.lesswrong.com/posts/groAB7XtQQ3SdjJNu/david-friedman-on-legal-systems-very-different-from-ours-1,2020-09-29T11:18:07.901Z,10,3,1,False,False,,"<p>David Friedman on Legal Systems Very Different from Ours: A brief survey of a range of legal system, past and present, from Imperial China and Periclean Athens to modern Amish and Romany.</p><p>The event will be Oct 11, 2020, at &nbsp; &nbsp;17:30 UTC, 20:30 Israel Daylight Time, 10:30 Pacific Daylight Time.</p><p><a href=""https://forms.gle/K8Y9ni7LHWRY1ziS9"">Sign up here</a>, up to an hour before the event, and we'll send you an invitation to the online meetup &nbsp;</p><p>David Friedman is an academic economist with a doctorate in physics recently retired from spending the previous twenty-three years teaching in a law school. His first book, The Machinery of Freedom: Guide to a Radical Capitalism, was published in 1973 and includes a description of how a society with property rights and without government might function. There as elsewhere, he offers a consequentialist defense of libertarianism.</p><p>His most recent non-fiction book is Legal Systems Very Different from Ours, covering systems from Periclean Athens through modern Amish and Romany. He is also the author of three novels, one commercially published and two self-published, and, with his wife, a self-published medieval and renaissance cookbook and a larger self-published book related to their hobby of historical recreation.</p><p>Much of his published work, including journal articles, essays, drafts of forthcoming work and the full text of several books, can be read on his web page:&nbsp;<a href=""https://www.daviddfriedman.com/""> daviddfriedman.com</a></p>",JoshuaFox,joshuafox,JoshuaFox,
djB4isB33Lywyjs65,Reading Discussion Group,reading-discussion-group-1,https://www.lesswrong.com/events/djB4isB33Lywyjs65/reading-discussion-group-1,2020-09-29T03:59:14.510Z,6,1,0,False,False,,"<p>The Cambridge, MA rationalist community is hosting a reading discussion group on Monday, Oct 12.</p>
<p><a href=""https://forms.gle/UkSrxwXdbCmKdM7n8"">Vote</a> on what article to discuss by Friday, Oct 9.</p>
",AspiringRationalist,nosignalnonoise,NoSignalNoNoise,
u7FgG558F75LEXLYC,C̶a̶m̶b̶r̶i̶d̶g̶e̶ Virtual LW/SSC Meetup,c-a-m-b-r-i-d-g-e-virtual-lw-ssc-meetup-5,https://www.lesswrong.com/events/u7FgG558F75LEXLYC/c-a-m-b-r-i-d-g-e-virtual-lw-ssc-meetup-5,2020-09-29T03:42:07.369Z,6,1,0,False,False,,"<p>The September Cambridge Less Wrong / Slate Star Codex (it has risen!) meetup will be held online due to the plague.</p>
<p><a href=""https://hangouts.google.com/group/Lwh1Yg3EABfhQS9L8"">Hangouts link</a></p>
",AspiringRationalist,nosignalnonoise,NoSignalNoNoise,
eG3WhHS8CLNxuH6rT,AGI safety from first principles: Superintelligence,agi-safety-from-first-principles-superintelligence,https://www.lesswrong.com/posts/eG3WhHS8CLNxuH6rT/agi-safety-from-first-principles-superintelligence,2020-09-28T19:53:40.888Z,89,41,7,False,False,,"<p>In order to understand superintelligence, we should first characterise what we mean by intelligence. We can start with Legg’s well-known definition, which identifies intelligence as <a href=""https://arxiv.org/abs/0712.3329"">the ability to do well on a broad range of cognitive tasks</a>.<sup class=""footnote-ref""><a href=""#fn-fY9S4v85NDnW9simo-1"" id=""fnref-fY9S4v85NDnW9simo-1"">[1]</a></sup> The key distinction I’ll draw in this section is between agents that understand how to do well at many tasks because they have been specifically optimised for each task (which I’ll call the task-based approach to AI), versus agents which can understand new tasks with little or no task-specific training, by generalising from previous experience (the generalisation-based approach).</p>
<h2>Narrow and general intelligence</h2>
<p>The task-based approach is analogous to how humans harnessed electricity: while electricity is a powerful and general technology, we still need to design specific ways to apply it to each task. Similarly, computers are powerful and flexible tools - but even though they can process arbitrarily many different inputs, detailed instructions for how to do that processing needs to be individually written to build each piece of software. Meanwhile our current reinforcement learning algorithms, although powerful, produce agents that are only able to perform well on specific tasks at which they have a lot of experience - Starcraft, DOTA, Go, and so on. In <a href=""https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf""><em>Reframing Superintelligence</em></a>, Drexler argues that our current task-based approach will scale up to allow superhuman performance on a range of complex tasks (although <a href=""https://www.alignmentforum.org/posts/HvNAmkXPTSoA4dvzv/comments-on-cais"">I’m skeptical of this claim</a>).</p>
<p>An example of the generalisation-based approach can be found in large language models like GPT-2 and GPT-3. GPT-2 was first trained on the task of predicting the next word in a corpus, and then achieved <a href=""https://openai.com/blog/better-language-models/"">state of the art results</a> on many other language tasks, without any task-specific fine-tuning! This was a clear change from previous approaches to natural language processing, which only scored well when trained to do specific tasks on specific datasets. Its successor, GPT-3, has displayed <a href=""https://twitter.com/xuenay/status/1283312640199196673?s=20"">a range of even more impressive behaviour</a>. I think this provides a good example of how an AI could develop cognitive skills (in this case, an understanding of the syntax and semantics of language) which generalise to a range of novel tasks. The field of meta-learning aims towards a similar goal.</p>
<p>We can also see the potential of the generalisation-based approach by looking at how humans developed. As a species, we were “trained” by evolution to have cognitive skills including rapid learning capabilities; sensory and motor processing; and social skills. As individuals, we were also “trained” during our childhoods to fine-tune those skills; to understand spoken and written language; and to possess detailed knowledge about modern society. However, the key point is that almost all of this evolutionary and childhood learning occurred on different tasks from the economically useful ones we perform as adults. We can perform well on the latter category only by reusing the cognitive skills and knowledge that we gained previously. In our case, we were fortunate that those cognitive skills were not too specific to tasks in the ancestral environment, but were rather very <em>general</em> skills. In particular, the skill of abstraction allows us to extract common structure from different situations, which allows us to understand them much more efficiently than by learning about them one by one. Then our communication skills and theories of mind allow us to share our ideas. This is why humans can make great progress on the scale of years or decades, not just via evolutionary adaptation over many lifetimes.</p>
<p>I should note that I think of task-based and generalisation-based as parts of a spectrum rather than a binary classification, particularly because the way we choose how to divide up tasks can be quite arbitrary. For example, AlphaZero trained by playing against itself, but was tested by playing against humans, who use different strategies and playing styles. We could think of playing against these two types of opponents as two instances of a single task, or as two separate tasks where AlphaZero was able to generalise from the former task to the latter. But either way, the two cases are clearly very similar. By contrast, there are many economically important tasks which I expect AI systems to do well at primarily by generalising from their experience with very different tasks - meaning that those AIs will need to generalise much, much better than our current reinforcement learning systems can.</p>
<p>Let me be more precise about the tasks which I expect will require this new regime of generalisation. To the extent that we can separate the two approaches, it seems plausible to me that the task-based approach will get a long way in areas where we can gather a lot of data. For example, I'm confident that it will produce superhuman self-driving cars well before the generalisation-based approach does so. It may also allow us to automate most of the tasks involved even in very cognitively demanding professions like medicine, law, and mathematics, if we can gather the right training data. However, some jobs crucially depend on the ability to analyse and act on such a wide range of information that it’ll be very difficult to train directly for high performance on them. Consider the tasks involved in a role like CEO: setting your company’s strategic direction, choosing who to hire, writing speeches, and so on. Each of these tasks sensitively depends on the broader context of the company and the rest of the world. What industry is their company in? How big is it; where is it; what’s its culture like? What’s its relationship with competitors and governments? How will all of these factors change over the next few decades? These variables are so broad in scope, and rely on so many aspects of the world, that it seems virtually impossible to generate large amounts of training data via simulating them (like we do to train game-playing AIs). And the number of CEOs from whom we could gather empirical data is very small by the standards of reinforcement learning (which often requires billions of training steps even for much simpler tasks). I’m not saying that we’ll never be able to exceed human performance on these tasks by training on them directly - maybe a herculean research and engineering effort, assisted by other task-based AIs, could do so. But I expect that well before such an effort becomes possible, we’ll have built AIs using the generalisation-based approach which know how to perform well even on these broad tasks.</p>
<p>In the generalisation-based approach, the way to create superhuman CEOs is to use other data-rich tasks (which may be very different from the tasks we actually want an AI CEO to do) to train AIs to develop a range of useful cognitive skills. For example, we could train a reinforcement learning agent to follow instructions in a simulated world. Even if that simulation is very different from the real world, that agent may acquire the planning and learning capabilities required to quickly adapt to real-world tasks. Analogously, the human ancestral environment was also very different to the modern world, but we are still able to become good CEOs with little further training. And roughly the same argument applies to people doing other highly impactful jobs, like paradigm-shaping scientists, entrepreneurs, or policymakers.</p>
<p>One potential obstacle to the generalisation-based approach succeeding is the possibility that <a href=""https://www.alignmentforum.org/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development"">specific features of the ancestral environment</a>, or of human brains, were necessary for general intelligence to arise. For example, <a href=""http://archives.evergreen.edu/webpages/curricular/2006-2007/languageofpolitics/files/languageofpolitics/Evol_Anthrop_6.pdf"">some have hypothesised</a> that a social “arms race” was required to give us enough social intelligence to develop large-scale cultural transmission. However, most possibilities for such crucial features, including this one, could be recreated in artificial training environments and in artificial neural networks. Some features (such as quantum properties of neurons) would be very hard to simulate precisely, but the human brain operates under conditions that are too messy to make it plausible that our intelligence depends on effects at this scale. So it seems very likely to me that eventually we will be able to create AIs that can generalise well enough to produce human-level performance on a wide range of tasks, including abstract low-data tasks like running a company. Let’s call these systems artificial general intelligences, or AGIs. <a href=""https://arxiv.org/abs/1705.08807"">Many AI researchers expect</a> that we’ll build AGI within this century; however, I won’t explore arguments around the timing of AGI development, and the rest of this document doesn’t depend on this question.</p>
<h2>Paths to superintelligence</h2>
<p><a href=""https://en.wikipedia.org/wiki/Superintelligence"">Bostrom defines a superintelligence</a> as “any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest”. For the purposes of this report, I’ll operationalise “greatly exceeding human performance” as doing better than all of humanity could if we coordinated globally (unaided by other advanced AI). I think it’s difficult to deny that in principle it’s possible to build individual generalisation-based AGIs which are superintelligent, since human brains are constrained by <a href=""https://intelligenceexplosion.com/2011/plenty-of-room-above-us/"">many factors</a> which will be much less limiting for AIs. Perhaps the most striking is the vast difference between the speeds of neurons and transistors: the latter pass signals about four million times more quickly. Even if AGIs never exceed humans in any other way, a speedup this large would allow one to do as much thinking in minutes or hours as a human can in years or decades. Meanwhile our brain size is important in making humans more capable than most animals -&nbsp; but I don’t see any reason why a neural network couldn’t be several orders of magnitude larger than a human brain. And while evolution is a very capable designer in many ways, it hasn’t had much time to select specifically for the skills that are most useful in our modern environment, such as linguistic competence and mathematical reasoning. So we should expect that there are low-hanging fruit for improving on human performance on the many tasks which rely on such skills.<sup class=""footnote-ref""><a href=""#fn-fY9S4v85NDnW9simo-2"" id=""fnref-fY9S4v85NDnW9simo-2"">[2]</a></sup></p>
<p>There are significant disagreements about how long it will take to transition from human-level AGI to superintelligence, which won’t be a focus of this report, but which I’ll explore briefly in the section on Control. In the remainder of this section I’ll describe in qualitative terms how this transition might occur. By default, we should expect that it will be driven by the standard factors which influence progress in AI: more compute, better algorithms, and better training data. But I’ll also discuss three factors whose contributions to increasing AI intelligence will become much greater as AIs become more intelligent: replication, cultural learning, and recursive improvement.</p>
<p>In terms of replication, AIs are much less constrained than humans: it’s very easy to create a duplicate of an AI which has all the same skills and knowledge as the original. The cost of compute for doing so is likely to be many times smaller than the original cost of training an AGI (since training usually involves running many copies of an AI much faster than they’d need to be run for real-world tasks). Duplication currently allows us to apply a single AI to many tasks, but not to expand the range of tasks which that AI can achieve. However, we should expect AGIs to be able to decompose difficult tasks into subtasks which can be tackled more easily, just as humans can. So duplicating such an AGI could give rise to a superintelligence composed not of a single AGI, but rather a large group of them (which, following Bostrom, I’ll call a <a href=""https://www.alignmentforum.org/posts/HekjhtWesBWTQW5eF/agis-as-populations""><em>collective AGI</em></a>), which can carry out significantly more complex tasks than the original can.<sup class=""footnote-ref""><a href=""#fn-fY9S4v85NDnW9simo-3"" id=""fnref-fY9S4v85NDnW9simo-3"">[3]</a></sup> Because of the ease and usefulness of duplicating an AGI, I think that collective AGIs should be our default expectation for how superintelligence will be deployed.</p>
<p>The efficacy of a collective AGI might be limited by coordination problems between its members. However, most of the arguments given in the previous paragraphs are also reasons why individual AGIs will be able to surpass us at the skills required for coordination (such as language processing and theories of mind). One particularly useful skill is cultural learning: we should expect AGIs to be able to acquire knowledge from each other and then share their own discoveries in turn, allowing a collective AGI to solve harder problems than any individual AGI within it could. The development of this ability in humans is what allowed the dramatic rise of civilisation over the last ten thousand years. Yet there is little reason to believe that we have reached the peak of this ability, or that AGIs couldn’t have a much larger advantage over a human than that human has over a chimp, in acquiring knowledge from other agents.</p>
<p>Thirdly, AGIs will be able to improve the training processes used to develop their successors, which then improve the training processes used to develop their successors, and so on, in a process of <em>recursive improvement</em>.<sup class=""footnote-ref""><a href=""#fn-fY9S4v85NDnW9simo-4"" id=""fnref-fY9S4v85NDnW9simo-4"">[4]</a></sup> Previous discussion has mostly focused on recursive <em>self</em>-improvement, involving a single AGI “<a href=""http://intelligence.org/files/LOGI.pdf"">rewriting its own source code</a>”. However, I think it’s more appropriate to focus on the broader phenomenon of AIs advancing AI research, for several reasons. Firstly, due to the ease of duplicating AIs, there’s no meaningful distinction between an AI improving “itself” versus creating a successor that shares many of its properties. Secondly, modern AIs are more accurately characterised as models which could be retrained, rather than software which could be rewritten: almost all of the work of making a neural network intelligent is done by an optimiser via extensive training. Even a superintelligent AGI would have a hard time significantly improving its cognition by modifying its neural weights directly; it seems analogous to making a human more intelligent via brain surgery (albeit with much more precise tools than we have today). So it’s probably more accurate to think about self-modification as the process of an AGI modifying its high-level architecture or training regime, then putting itself through significantly more training. This is very similar to how we create new AIs today, except with humans playing a much smaller role. Thirdly, if the intellectual contribution of humans does shrink significantly, then I don’t think it’s useful to require that humans are <em>entirely</em> out of the loop for AI behaviour to qualify as recursive improvement (although we can still distinguish between cases with more or less human involvement).</p>
<p>These considerations reframe <a href=""https://intelligence.org/files/IEM.pdf"">the classic view of recursive self-improvement</a> in a number of ways. For example, the retraining step may be bottlenecked by compute even if an AGI is able to design algorithmic improvements very fast. And for an AGI to trust that its goals will remain the same under retraining will likely require it to solve many of the same problems that the field of AGI safety is currently tackling - which should make us more optimistic that the rest of the world could solve those problems before a misaligned AGI undergoes recursive self-improvement. However, to be clear, this reframing doesn’t imply that recursive improvement will be unimportant. Indeed, since AIs will eventually be the primary contributors to AI research, recursive improvement as defined here will eventually become the key driver of progress. I’ll discuss the implications of this claim in the section on Control.</p>
<p>So far I’ve focused on how superintelligences might come about, and what they will be able to do. But how will they decide what to actually do? For example, will the individuals within a collective AGI even <em>want</em> to cooperate with each other to pursue larger goals? Will an AGI capable of recursive improvement have any reason to do so? I’m wary of phrasing these questions in terms of the goals and motivations of AGIs, without exploring more thoroughly what those terms actually mean. That’s the focus of the next section.</p>
<hr>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-fY9S4v85NDnW9simo-1"" class=""footnote-item""><p>Unlike the standard usage, in this technical sense an “environment” also includes a specification of the input-output channels the agent has access to (such as motor outputs), so that solving the task only requires an agent to process input information and communicate output information. <a href=""#fnref-fY9S4v85NDnW9simo-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-fY9S4v85NDnW9simo-2"" class=""footnote-item""><p>This observation is closely related to Moravec’s paradox, which I discuss in more detail in the section on Goals and Agency. Perhaps the most salient example is how easy it was for AIs to beat humans at chess. <a href=""#fnref-fY9S4v85NDnW9simo-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-fY9S4v85NDnW9simo-3"" class=""footnote-item""><p>It’s not quite clear whether the distinction between “single AGIs” and collective AGIs makes sense in all cases, considering that a single AGI can be composed of many modules which might be very intelligent in their own right. But since it seems unlikely that there will be hundreds or thousands of modules which are each generally intelligent, I think that the distinction will in practice be useful. See also the discussion of “collective superintelligence” in Bostrom’s Superintelligence. <a href=""#fnref-fY9S4v85NDnW9simo-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-fY9S4v85NDnW9simo-4"" class=""footnote-item""><p>Whether it’s more likely that the successor agent will be an augmented version of the researcher AGI itself or a different, newly-trained AGI is an important question, but one which doesn’t affect the argument as made here. <a href=""#fnref-fY9S4v85NDnW9simo-4"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",ricraz,ricraz,Richard_Ngo,
8xRSjC76HasLnMGSf,AGI safety from first principles: Introduction,agi-safety-from-first-principles-introduction,https://www.lesswrong.com/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction,2020-09-28T19:53:22.849Z,128,72,18,False,False,,"<p><em>This is the first part of a six-part report called</em> AGI safety from first principles, <em>in which I've attempted to put together the most complete and compelling case I can for why the development of AGI might pose an existential threat. The report stems from my dissatisfaction with existing arguments about the potential risks from AGI. Early work tends to be less relevant in the context of modern machine learning; more recent work is scattered and brief. I originally intended to just summarise other people's arguments, but as this report has grown, it's become more representative of my own views and less representative of anyone else's. So while it covers the standard ideas, I also think that it provides a new perspective on how to think about AGI - one which doesn't take any previous claims for granted, but attempts to work them out from first principles.</em></p>
<p><em>Having said that, the breadth of the topic I'm attempting to cover means that I've included many arguments which are only hastily sketched out, and undoubtedly a number of mistakes. I hope to continue polishing this report, and I welcome feedback and help in doing so. I'm also grateful to many people who have given feedback and encouragement so far. I plan to cross-post some of the most useful comments I've received to the Alignment Forum once I've had a chance to ask permission. I've posted the report itself in six sections; the first and last are shorter framing sections, while the middle four correspond to the four premises of the argument laid out below.</em></p>
<h2>AGI safety from first principles</h2>
<p>The key concern motivating technical AGI safety research is that we might build autonomous artificially intelligent agents which are much more intelligent than humans, and which pursue goals that conflict with our own. Human intelligence allows us to coordinate complex societies and deploy advanced technology, and thereby control the world to a greater extent than any other species. But AIs will eventually become more capable than us at the types of tasks by which we maintain and exert that control. If they don’t want to obey us, then humanity might become only Earth's second most powerful ""species"", and lose the ability to create a valuable and worthwhile future.</p>
<p>I’ll call this the “second species” argument; I think it’s a plausible argument which we should take very seriously.<sup class=""footnote-ref""><a href=""#fn-NdS6acmcvABYgxGD9-1"" id=""fnref-NdS6acmcvABYgxGD9-1"">[1]</a></sup> However, the version stated above relies on several vague concepts and intuitions. In this report I’ll give the most detailed presentation of the second species argument that I can, highlighting the aspects that I’m still confused about. In particular, I’ll defend a version of the second species argument which claims that, without a concerted effort to prevent it, there’s a significant chance that:</p>
<ol>
<li>We’ll build AIs which are much more intelligent than humans (i.e. superintelligent).</li>
<li>Those AIs will be autonomous agents which pursue large-scale goals.</li>
<li>Those goals will be misaligned with ours; that is, they will aim towards outcomes that aren’t desirable by our standards, and trade off against our goals.</li>
<li>The development of such AIs would lead to them gaining control of humanity’s future.</li>
</ol>
<p>While I use many examples from modern deep learning, this report is also intended to apply to AIs developed using very different models, training algorithms, optimisers, or training regimes than the ones we use today. However, many of my arguments would no longer be relevant if the field of AI moves away from focusing on machine learning. I also frequently compare AI development to the evolution of human intelligence; while the two aren’t fully analogous, humans are the best example we currently have to ground our thinking about generally intelligent AIs.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-NdS6acmcvABYgxGD9-1"" class=""footnote-item""><p>Stuart Russell also refers to this as the “gorilla problem” in his recent book, <em>Human Compatible</em>. <a href=""#fnref-NdS6acmcvABYgxGD9-1"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",ricraz,ricraz,Richard_Ngo,
E4KsKbxCCFxZQHXaB,is scope insensitivity really a brain error?,is-scope-insensitivity-really-a-brain-error,https://www.lesswrong.com/posts/E4KsKbxCCFxZQHXaB/is-scope-insensitivity-really-a-brain-error,2020-09-28T18:37:37.715Z,4,5,15,False,True,,"<p>I am reading a post called <a href=""https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM/p/teaxCFgtmCQ3E9fy8""><strong>The Martial Art of Rationality</strong></a> by Eliezer Yudkowsky, in which he makes the following claim:</p><p>&nbsp;</p><p>""If you’re a fast learner, you might learn faster—but the art of rationality isn’t about that; it’s about training brain machinery we all have in common. And where there are systematic errors human brains tend to make—like an insensitivity to scope—rationality is about fixing those mistakes, or finding work-arounds.""</p><p>&nbsp;</p><p>this post follows one in which he explained the concept of scope insensitivity by discussing a study that found that persons asked to contribute to dealing with the consequences of fuel spill on wildlife habitat did not contribute more money to save more birds. the contributors were deemed to be insensitive to the scope of the suffering.</p><p>&nbsp;</p><p>the point I want to discuss is whether it is entirely fair to describe scope insensitivity, as defined in this way, &nbsp;as a ""systematic human brain error""?</p><p>&nbsp;</p><p>it seems to me that this is bordering on saying that persons who made a different choice to yours are therefore not just wrong, but suffering from something, their brain is not working properly and they need to be taught how to make better choices. where ""better"" obviously means, more in line with the choice you would make.</p><p>&nbsp;</p><p>scope insensitivity would only be irrational if saving birds were the only criteria in play. to save more birds, give more money. but this is almost never the case, people are more complex than this and they need to consider more criteria than this and each person may consider different criteria and weight them differently. to label those differences as ""systematic human brain error"" seems to be a very one-dimensional response.</p><p>&nbsp;</p><p>I think we need to bear in mind that the original study did not allow for the possibility that folk did not pay more because they were unable to afford more, or because they would prefer to allocate their charitable spending to alleviate human suffering rather than animal suffering. in fact, the study explicitly said that they were unable to account for the lack of sensitivity to scope. it seems wrong, in fact plain anti-intellectual, for Yudkowsky to claim that their scope insensitivity is a ""systematic human brain error""?</p><p>&nbsp;</p><p>please discuss.</p>",Kaarlo Tuomi,kaarlo-tuomi,Kaarlo Tuomi,
8Ziz5BQjtuhr9orm4,What Decision Theory is Implied By Predictive Processing?,what-decision-theory-is-implied-by-predictive-processing,https://www.lesswrong.com/posts/8Ziz5BQjtuhr9orm4/what-decision-theory-is-implied-by-predictive-processing,2020-09-28T17:20:51.946Z,59,19,17,False,True,,"<p>At a fairly abstract/stylized level, <a href=""https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/"">predictive processing</a> models human cognition and behavior as always minimizing predictive error. Sometimes, the environment is ""fixed"" and our internal models are updated to match it - e.g. when I see my untied shoelace, my internal model updates to include an untied shoelace. Other times, our internal model is ""fixed"", and we act on the environment to make it better match the model - e.g. ""wanting food"" is internally implemented as a strong expectation that I'm going to eat soon, which in turn makes me seek out food in order to make that expectation true. Rather than having a utility function that values food or anything like that, the decision theory implied by predictive processing just has a model in which we obtain food, and we try to make the model match reality.</p><p>Abstracting out the key idea: we pack all of the complicated stuff into our world-model, hardcode some things into our world-model which we <i>want</i> to be true, then generally try to make the model match reality.</p><p>While making the model match reality, there will be knobs we can turn both ""in the model"" (i.e. updates) and ""in reality"" (i.e. actions); there's no hard separation between the two. There will be things in both map and reality which we can change, and there will be things in both map and reality which we can't change. It's all treated the same. At first glance, that looks potentially quite useful for <a href=""https://www.lesswrong.com/posts/p7x32SEt43ZMC9r7r/embedded-agents"">embedded agency</a>.</p><p>(My own interest in this was piqued partly because a predictive-processing-like decision theory seems likely to produce <a href=""https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/JasCkaPtZEJsYDX8H"">abstraction boundaries which look like Cartesian boundaries</a>. As in that post, it seems like some of the intuitive arguments we make around decision theories would naturally drop out of a predictive-processing-like decision theory.)</p><p>What problems does such a decision theory run into? What sort of things can we hardcode into our world-model without breaking it altogether? What things must be treated as ""fixed"" when making the model match reality? Does such an approach have any ""invariant"" implications, i.e. implications independent of <i>which</i> model we're trying to match? What further requirements are there on the target model in order for a predictive-processing-style agent to have ""good"" behavior, in the ways characterized by other decision theories?</p><p>This is intended to be an open-ended research question, but off-the-cuff thoughts and links to relevant work are welcome.</p>",johnswentworth,johnswentworth,johnswentworth,
Fjfe2BuihTAE58A27,What are examples of Rationalist fable-like stories?,what-are-examples-of-rationalist-fable-like-stories,https://www.lesswrong.com/posts/Fjfe2BuihTAE58A27/what-are-examples-of-rationalist-fable-like-stories,2020-09-28T16:52:13.500Z,19,11,42,False,True,,"<p>I will post some of the collection I've acquired. I have ideas for more. And would be interested in seeing more.</p>
<p>If there's enough interest, and with the permission of the authors, I would like to make a book of Rationalist fables / parables (but with a more generic name). Maybe it could be adapted for children. Thoughts?</p>
<p>Let me know if you're interested in helping me with this project, including but not limited to: finding stories, selecting stories, adapting them, reaching out to authors, writing more stories, writing their morales (?), making a book cover, (self?) publishing it, marketing it, choosing a title, donating money to contract external help, etc.</p>
",MathieuRoy,mathieuroy,Mati_Roy,
3mujmMMfHRHzhhd5e,Macro-Procrastination,macro-procrastination,https://www.lesswrong.com/posts/3mujmMMfHRHzhhd5e/macro-procrastination,2020-09-28T16:07:48.670Z,9,5,0,False,False,https://www.neelnanda.io/blog/32-macro-procrastination,"<h2>The Problem</h2><p>As readers of this blog <a href=""https://www.neelnanda.io/blog/mini-blog-post-24-on-procrastination-the-art-of-shaping-your-future-actions"">will know</a>, I massively procrastinate. And this is a pretty big problem in my life. But it&#x2019;s a problem I&#x2019;m aware of, that mostly resolves itself. I have deadlines, accountability mechanisms, and the latent, creeping guilt of something I know I&#x2019;m putting off. And <a href=""https://www.neelnanda.io/blog/mini-blog-post-24-on-procrastination-the-art-of-shaping-your-future-actions"">all of my solutions</a> centre around <em>noticing </em>this creeping guilt and converting into <em>action</em>. And generally, the thing will get done! There are costs - it&#x2019;s often done late, or to a lower standard. There&#x2019;s often <a href=""https://www.neelnanda.io/blog/mini-blog-post-22-the-8020-rule"">a lot of wasted motion</a>, time spent on mindless busywork with opportunity costs for what I actually care about. But the costs are bounded, and I can force myself to get things done in the end.</p><p>But, when I actually think about it, this system is <em>weird</em>. There&#x2019;s a major disconnect here. I get over procrastination because I feel guilty about it. But I <em>care </em>about procrastination as an issue because on some level I <em>want </em>to do these tasks. These tasks are important to my <a href=""https://www.neelnanda.io/blog/prioritisation"">long-term goals</a> and procrastination is bad because it holds me back from my goals. I am allocating my time badly, and this is sad. The guilt is a useful incidental property of the fact that there are deadlines, but the fact that it&#x2019;s a <em>problem</em> is nothing to do with the deadlines. <strong>Procrastination is fundamentally a problem of prioritisation, and bad allocation of time</strong>.</p><p>And this is an <em>important </em>disconnect, because there are worthwhile tasks I procrastinate on that <em>don&#x2019;t </em>have deadlines. And these are often <em>really </em>important! Things that will help me towards becoming a happier person, helpful to my career, and generally improving the world. It is a really big deal if I never do them. But nobody is holding me <strong>accountable</strong>, there is no <strong>urgency</strong>, and so <strong>nothing ever happens</strong>. I call this problem <strong>macro-procrastination</strong>.</p><p>Macro-procrastination was recently made salient to me, because I realised it was happening in a <em>lot </em>of areas of my life. Some of the most noticeable:</p><ul><li>Exercise is obviously important to my lifespan, healthspan, general happiness, mental energy and appearance. And is obviously an <em>insanely </em>good use of time. But I just wasn&#x2019;t in the habit of doing any </li><ul><li>I&#x2019;ve since started <a href=""https://play.google.com/store/apps/details?hl=en_GB&amp;id=com.phe.couchto5K"">Couch to 5K</a>, which I&#x2019;d highly recommend</li></ul><li>Careers: I&#x2019;m pretty seriously considering a career in AI Safety. But I&#x2019;d put approximately 0 effort into just reading about the field, getting an awareness of what people work on, and what my personal views on it are.</li><ul><li>I&#x2019;ve done low effort things like signing up for the <a href=""https://rohinshah.com/alignment-newsletter/"">Alignment Newsletter</a>, but currently still mostly suck at this.</li></ul><li>Having long-term goals: Often my life feels a bit aimless, because there are a lot of things I care about, but I don&#x2019;t know how to prioritise between them. And I don&#x2019;t have a clear sense of my long-term goals. This felt important enough to be a contender for my current Hamming Problem, yet I&#x2019;d never even sat down for an afternoon and tried to write down my high-level goals.</li><ul><li>I <a href=""https://www.neelnanda.io/blog/prioritisation"">then spent that afternoon</a> and it was time well spent!</li><li>I&#x2019;m currently trying to spend an hour a week thinking about this and how well what I&#x2019;m doing meets my goals</li></ul><li>Pursuing ambitious personal projects: Something where there&#x2019;s no <em>need </em>to do it, but I&#x2019;d feel sad if I <em>never </em>do it</li><ul><li>Blogging was this for a while, but I think I&#x2019;ve decisively solved that!</li><li>Getting good at human forecasting is something I&#x2019;m still procrastinating on</li></ul><li>Romance: I&#x2019;ve been single for a while, and expect it&#x2019;d significantly increase my life happiness, but have done approximately nothing about fixing this</li><ul><li>Turns out I find doing anything about this <em>deeply </em>aversive for reasons I&#x2019;m still trying to debug :( (Note: If you know me in real life, and know anyone you think I might be romantically compatible with, hit me up &lt;3 )</li><ul><li>Though <a href=""https://putanumonit.com/2016/02/03/015-dating_1/"">this blog post</a> was surprisingly inspiring (especially the aside on the Gale-Shapley algorithm!)</li></ul></ul></ul><p>These are just my personal examples, but I think this is a <em>really </em>common failure mode. And I think this is one of the major root causes behind people who complain about problems without ever really doing anything about them. Ultimately, this is a failure to prioritise long-term goals over short-term goals, and <strong>this is exactly what you&#x2019;d expect to happen</strong>. People do hyperbolic discounting, respond to deadlines and accountability mechanisms, and don&#x2019;t think about their futures. Short-term things feel <em>urgent</em>, you always feel <em>busy</em>, and it is hard and painful to realise you need to prioritise longer term things. </p><p><strong>Exercise: </strong>Set a <a href=""https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers"">5 minute timer</a>, and list the things <em>you</em> are macro-procrastinating about. What is wrong with your life at the moment? What are the long-term goals you&#x2019;re neglecting? What do you always put off, that you know you really <em>should </em>get round to some day? What&#x2019;s something <em>awesome </em>you&#x2019;ve always wanted to do, but never got round to?</p><h2>Noticing</h2><p>So this is obviously a problem, and it&#x2019;s obviously something worth trying to fix. But what can you actually do about it?</p><p>The core to <a href=""https://www.neelnanda.io/blog/mini-blog-post-24-on-procrastination-the-art-of-shaping-your-future-actions"">my solutions to normal procrastination</a> is the act of <strong>noticing </strong>that I&#x2019;m procrastinating. The creeping deadlines, accountability mechanisms, etc all make the task feel <em>obvious </em>and hard to forget about. And eventually something flips - procrastination goes from something I&#x2019;m <strong>aware </strong>of in the background, to something that feels <strong>urgent</strong>. And this is the hook that I can convert into action - remind myself of why I really care about the problem and flipping the guilt into intrinsic motivation, making concrete plans for what I&#x2019;ll do about it, and committing my future self to this! </p><p>So the obvious solution to macro-procrastination is trying to ensure that this moment of noticing happens! </p><p>The obvious first way to do this is to be <strong>as sensitive as possible </strong>to any time I <em>do </em>notice I&#x2019;m macro-procrastinating. I find it helpful to associate a strong feeling to the word &#x201C;should&#x201D; - if you <em>ever </em>say &#x201C;I should do that someday&#x201D; <strong>this is a really big deal and you should pay attention</strong>. When you hear a good idea, with no deadline attached, <strong>do something about it in the moment</strong>. The things without deadlines don&#x2019;t feel as <em>strong </em>or as <em>urgent</em> but these feelings are still there, and can be fed.</p><p>But this isn&#x2019;t a very robust solution - a large part of the problem with macro-procrastination is that you often <em>don&#x2019;t </em>notice by default. And, <a href=""https://www.neelnanda.io/blog/mini-blog-post-19-on-systems-living-a-life-of-zero-willpower"">as with all problems in life</a>, this can be systematised! So, how can you <strong>systematise </strong>the process of noticing? I have a few tips here:</p><ul><li>Make regular time to reflect, and <em>check </em>for this kind of thing</li><ul><li>Do a monthly review, where you go through your long-term goals, and notice the ones you&#x2019;re neglecting</li><ul><li>In this review, set a 5 minute timer, and list the things you&#x2019;re macro-procrastinating about</li></ul><li>Make time once a week to check your long-term goals, and see how well your actions this week achieved them</li><li>More generally, <a href=""https://www.neelnanda.io/blog/mini-blog-post-17-prioritisation-part-2-achieving-goals"">have systems for prioritisation</a>!</li></ul><li>Set a clear norm that your friends should call you on your BS if they notice you macro-procrastinating, and explicitly point this out to you</li><ul><li>Note: If you make this request and they actually do it, I think you have a moral obligation to take them seriously - it&#x2019;s easy to reflexively brush off criticism, but I think this is being a bad friend. </li><ul><li>A robust and nuanced outside view is <a href=""https://www.neelnanda.io/blog/25-friendship"">one of the most valuable things a friend can give you!</a></li></ul><li>I&#x2019;ve had a fair amount of success with friends doing this for me! (Eg, this triggered me realising I was being an idiot about romance)</li></ul><li>Make doing something <strong>the default</strong></li><ul><li>Make regular time where you <em>have </em>to work towards long-term goals. Carve out an afternoon a week for side-projects, or debugging, or long-term goals. Have a clear part of this where you <em>think </em>about what you&#x2019;re putting off and what it&#x2019;d be valuable to do</li><ul><li>It&#x2019;s important to make this feel <strong>sacrosanct</strong>, not something you&#x2019;re willing to give up for short-term important things. The <em>entire point </em>is that you can&#x2019;t trust yourself to trade-off short-term against long-term goals</li></ul><li>In a weekly review, record what you <em>have </em>achieved towards your long-term goals. Make it feel the default that you put something down each week. And if you consistently don&#x2019;t, <strong>this is a big deal</strong>. </li></ul><li>Another framing: Imagine you had a year of free time, and make a list of at least 20 things you&#x2019;d do with that time - projects you&#x2019;d work on, new things you&#x2019;d try, problems in a life you could try to resolve, something you&#x2019;d always wanted to learn. Then, at the end, go back through that list and ask yourself which of those you could do <strong>now</strong>. </li><ul><li>20 is a deliberately ambitious number, <a href=""https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers"">set yourself a 20 minute timer </a>while generating these. It&#x2019;s easy to confuse the feeling of &#x201C;this is hard to generate&#x201D; with &#x201C;this is <em>impossible</em> to generate&#x201D;</li><li>I find this framing valuable, because it&#x2019;s simulating a life where I have a <em>lot </em>of Slack. Some of the ideas will be too ambitious, but some are small enough that I <em>could </em>do them around my day-to-day life! It just takes a reframing to realise I care enough to make time for them.</li></ul></ul><h2>Resolving</h2><p>If the above steps worked, and you <em>catch </em>macro-procrastination, then this reduces to <a href=""https://www.neelnanda.io/blog/mini-blog-post-24-on-procrastination-the-art-of-shaping-your-future-actions"">the problem of normal procrastination</a>. And you can use whatever your standard toolkit is for that. But you need to take actually <em>doing </em>something about it seriously! Unlike for tasks with deadlines, if you let it slip, it&#x2019;s much less likely that you&#x2019;ll pick it up again.</p><p>I think a key step here is ensuring that you have <strong><a href=""https://slatestarcodex.com/2020/05/12/studies-on-slack/"">slack</a></strong> in your life. Have enough spare capacity that you <em>can </em>make time for important and non-urgent things. If it feels like you need to sacrifice something else important in your life to make time for your long-term goals, maybe you have too much stuff in your life? Working towards your long-term goals is good, and virtuous, and what matters. And if it feels like a sacrifice, I think something is going wrong. </p><p>Another thing I find valuable is having <strong>scaffold systems</strong>. Existing systems that make it very easy to commit my future self to tasks. Some of my favourite scaffold systems:</p><ul><li>Having a calendar, and sticking to what&#x2019;s in it. So to ensure something gets done, I just need to make time for it</li><li>Using friends for accountability - telling them I intend to do something, and asking them to remind me</li><ul><li>Or even public commitments, if you&#x2019;re feeling bold!</li></ul><li>Financial accountability - making commitment, and losing money if I break it</li><ul><li>My favourite method here is to ask a friend to hold me accountable, and to pay them if I fail</li></ul></ul><p>When I notice myself procrastinating on something, step one is to come up with <a href=""https://www.neelnanda.io/blog/mini-blog-post-21-taking-the-first-step"">a concrete idea of the first step</a>, and then to use a scaffold system to ensure I actually do it.</p><h2>Applying this</h2><p>I think it&#x2019;s very easy to think about this kind of problem in the abstract, conceive of it as mostly something that happens to other people, and move on. But I expect this is something that applies to <em>your </em>life (unless you are an unusually exceptional person, in which case kudos!). And there are no deadlines attached to this problem. This will continue to be a problem, until and unless you <em>do </em>something about it.</p><p>I gave an exercise in the introduction, and I&#x2019;ll repeat it here: &#x201C;Set a <a href=""https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers"">5 minute timer</a>, and list the things <em>you</em> are macro-procrastinating about&#x201D;. We often know what we&#x2019;re neglecting on <em>some </em>level, and framing the question right can make this clear. If you skipped over this step earlier, I would highly recommend actually taking five minutes to think about it now.</p><p>Some prompts:</p><ul><li>What would you like to get round to some day?</li><li>What&#x2019;s currently missing in your life?</li><li>What is your biggest bottleneck?</li><li>What are your long-term goals, and which haven&#x2019;t you made progress on recently?</li><li>What do you keep putting off?</li></ul><p>Some things I think should be on almost <em>everyone&#x2019;s </em>long-term goal list:</p><ul><li>Careers - thinking about options, skill-building, thinking long-term and seeking advice</li><li>Learning new things, exploring, and finding new things</li><li>Finding a stable, loving romantic relationship</li><li>Exercising regularly</li><li>Eating healthily</li><li>Working on your mental health</li><li>Sleeping well</li><li>Having a good social network, emotional support and friends you value</li><li>Building good meta-skills - being reflective, good at introspecting, good at <a href=""https://www.neelnanda.io/blog/mini-blog-post-7-problems-are-for-fixing"">fixing problems in your life</a></li></ul><p>Are there any of those that you value and are neglecting?</p><p>At this stage, it&#x2019;s easy to make excuses. I&#x2019;m busy! Time is scarce! I can&#x2019;t do everything! Sometimes this is just BS and a desire to put off effort, and sometimes these are legitimate. So how can you tell the difference?</p><p>I find the following litmus test useful:</p><ul><li>Would I be sad if, one year from now, I still hadn&#x2019;t got round to it?</li><ul><li>If you wouldn&#x2019;t be sad, this probably isn&#x2019;t that important!</li></ul><li>And if so, imagine it&#x2019;s one year from now and you haven&#x2019;t gotten round to it. Are you surprised by this?</li></ul><p>If it passes both tests, <strong>this is important</strong>. And I think often, the right next step is to do something <strong>right now</strong>! But if you still feel like you&#x2019;re too busy, ask when you will next <em>not </em>be busy. If there&#x2019;s no good time, then you&#x2019;re macro-procrastinating, and should do it now rather than never. And if there <em>is </em>a better time, what can you do <em>right now</em> to ensure that the default action is that you <strong>actually do it when you&#x2019;re less busy</strong>?</p><p>Often you <em>will </em>need to give up something in the short-term to resolve macro-procrastination! We all have limited time, energy, and capacity to get shit done. Everything is a trade-off. And trade-offs are painful to think about! But trade-offs are bad, not because of this pain, but because you have to lose something! And this loss is unavoidable. If you don&#x2019;t think about it, you don&#x2019;t <em>avoid </em>a loss, you just give up your ability to choose.</p><h2>Conclusion</h2><p>Macro-procrastination is an important and universal problem. <strong>And</strong> <strong>it matters</strong>. There is nobody forcing you to solve it. There are no deadlines. There are no obvious negative consequences. Just the creeping opportunity cost of slowly missing out on some goals, and not being as awesome and actualised as I <em>could </em>be. Not being as happy as I could be. </p><p>And it&#x2019;s easy to orient towards this with guilt, and I am deliberately trying to build pressure to <em>do </em>something about this. But I also think this is an opportunity to <strong>become the kind of person I want to be</strong>. Somebody who is <strong>active</strong>, not passive. Somebody who can prioritise well, be an agent. <a href=""https://www.neelnanda.io/blog/become-a-person-who-actually-does-things"">Somebody who actually does things</a>. And if macro-procrastination is a problem you suffer from, this is an opportunity for <em>you </em>to <strong>do something about it</strong>. </p><p>So if the ideas in this post resonated with you, <em>do something about it</em>! Notice what you are currently macro-procrastinating about, and take steps to resolve it. And <strong>systematise it</strong>, so this will be less of a problem when it inevitably bites your future self. Make the <strong>default state of the world </strong>that you won&#x2019;t macro-procrastinate. Build systems and backstops. Any particular goal isn&#x2019;t <em>urgent</em>, but it won&#x2019;t be resolved by itself. You&#x2019;ll need to do something about it.</p><p>It&#x2019;s easy to read a post like this, agree, but ultimately change nothing. I expect most people reading this post will do so. The default state of the world is that you&#x2019;ll do nothing about it, and your problems will continue to be problems. And so now you have a choice, about the kind of person you want to be. And you can&#x2019;t avoid this choice, just abdicate your capacity to choose and run with the default. </p><p>So, what are you putting off? And what are you going to do about it? <strong>Which choice brings you closer to the person you want to be?</strong></p>",neel-nanda-1,neel-nanda-1,Neel Nanda,
tLJ87uJztSPpnjawb,What are good ice breaker questions for meeting people in this community?,what-are-good-ice-breaker-questions-for-meeting-people-in,https://www.lesswrong.com/posts/tLJ87uJztSPpnjawb/what-are-good-ice-breaker-questions-for-meeting-people-in,2020-09-28T15:07:16.798Z,9,4,2,False,True,,"<p>I was <a href=""https://www.lesswrong.com/posts/bh76S7YhiHYufjpk9/petrov-day-2020-virtual-celebration?commentId=uwexx2krCfp3jSm42"">praising the Petrov Day event</a> and saying I liked the <a href=""https://icebreaker.video/"">Icebreaker</a> app.</p>
<p><a href=""https://www.lesswrong.com/posts/bh76S7YhiHYufjpk9/petrov-day-2020-virtual-celebration?commentId=wzvnbpoyECahnuXaR"">parsley</a> said:</p>
<blockquote>
<p>icebreaker.video allows users to use their own questions. I didn't have the time to come up with my own, so I just resorted to the boilerplate ones provided by the website. I wonder if there's a ""rationalist ice breaker"" list somewhere.</p>
</blockquote>
<p>That's a question I've had in mind for years, and I'm now asking it to y'all ^_^</p>
",MathieuRoy,mathieuroy,Mati_Roy,
K7jrkyKArvxJ224GD,On Destroying the World,on-destroying-the-world,https://www.lesswrong.com/posts/K7jrkyKArvxJ224GD/on-destroying-the-world,2020-09-28T07:38:38.319Z,82,57,87,False,False,,"<p>Yesterday I <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020?commentId=QjKagi2hEqLcPK4B9#QjKagi2hEqLcPK4B9"">blew up the front page</a>. This was unintentional. I was tricked by one of my friends:</p><blockquote><p>petrov_day_admin_account September 26, 2020 11:26 AM Hello Chris_Leong,</p><p>You are part of a smaller group of 30 users who has been selected for the second part of this experiment. In order for the website not to go down, at least 5 of these selected users must enter their codes within 30 minutes of receiving this message, and at least 20 of these users must enter their codes within 6 hours of receiving the message. To keep the site up, please enter your codes as soon as possible. You will be asked to complete a short survey afterwards.</p></blockquote><p>In retrospect, this was quite silly of me. I actually noticed that the account was different from the one that sent the first message, which should have given it away, but the message really did feel legit so I trusted it anyway.</p><p>But beyond this, there were further details that should have made the message somewhat suspicious. The first is that this experiment occurred after midnight for San Fransisco. Given that most of the users on this site are based in the US, they wouldn't have been awake. While they might have specifically chosen users from suitable timezones, it would have made much more sense for them to just wait until more users woke up. Secondly, 20/30 users within 6 hours seems a bit high given that users weren't told in advance that the game was going on, so it's not clear how many would be available even if they knew.</p><p>One thing that greatly surprised me was how much the <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020?commentId=ykEvvDkvTtYPbDYh5"">following comment</a> was misunderstood:</p><blockquote><p>Should I press the button or not? I haven't pressed the button at the current time as it would be disappointing to people if they received the email, but someone pressed it while they were still asleep.</p></blockquote><p>People <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020?commentId=jRgXwNpu438pzytvA"">read the comment</a> and assumed I was intending the press the button and the only different the trick meant was that it occurred earlier. One of the risks when writing comments quickly is that the meaning might not be very clear at all. I hadn't actually made up my mind about whether to push the button or not as I was waiting for comments to come in. All I had decided was that I didn't want the site to blow up while people were still asleep because I thought it'd be less fun for them. That said, I was entirely open to blowing up the site if I thought that the argument for was stronger than the argument against.</p><p><a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020?commentId=yDEszXP248iyjYmZz"">Ruby</a> pointed out that I didn't spend as much thinking about this:</p><blockquote><p>This seems plausible. I do want to note that your received message was timestamped 11:26 (local to you) and the button was pressed at 11:33:30 (The received message said the time limit was 30 minutes.), which doesn’t seems like an abundance of caution and hesitation to blow up the frontpage, as far as I can tell. :P</p></blockquote><p>I saw the email notification almost immediately after it was sent and I thought about it for a bit before deciding that it really just felt legit. I considered messaging the mods, but I assumed they were asleep as it was like 2 am over there. The timestamps indicate that I only spent about seven minutes thinking about it, but it definitely felt longer.</p><p>I responded to Ruby with the following <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020?commentId=oNowszdjZmzmb29RL"">comment</a>, which certainly wasn't the best comment that I've ever made.</p><blockquote><p>Well, it was just a game and I had other things to do. Plus I didn't feel a duty to take it 100% seriously since, as happy as I was to have the chance to participate, I didn't actually choose to play.</p></blockquote><p>I suppose the thing I should clarify about this comment is, ""I didn't actually choose to play"", as I did kind of choose to play by posting comments asking whether I should press the button on not. What I could have said if I had wanted to be more precise is that at most my commitment to engage was to read the comments that people posted and to take them into consideration. That is, to not waste the time of people who took the effort to reply.</p><p>I don't think I really had a duty to do anything further, including spending the full or most of the half an hour considering the decision. <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020?commentId=jq5dWxyZgwPTZ5GtL"">JacobJacob</a> wants to draw a distinction between acting and not acting and I think that's fair enough for the original version of the game, but as soon as I received the email, the difference between acting and not acting collapsed, and the decision not to act would have been an action in and of itself.</p><p>This brings me to <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020?commentId=QjKagi2hEqLcPK4B9"">Oliver Habryka's comment</a>:</p><blockquote><p>To be clear, while there is obviously some fun intended in this tradition, I don't think describing it as ""just a game"" feels appropriate to me. I do actually really care about people being able to coordinate to not take the site down. It's an actual hard thing to do that actually is trying to reinforce a bunch of the real and important values that I care about in Petrov day. Of course, I can't force you to feel a certain way, but like, I do sure feel a pretty high level of disappointment reading this response.</p></blockquote><p>We'll come to this in a moment, but first I want to address his final sentence: ""Like, the email literally said you were chosen to participate because we trusted you to not actually use the codes"". I've played lot of role-playing games back in the day and often people write all kinds of things as flavor text. And none of it is meant to be taken literally.</p><p>I want to point out a few things in particular. Firstly, the email was sent out to 270 users which from my perspective made it seem that the website was almost guaranteed to go down at some time, with the only question being when (I was aware the game was played last year, but I had no memory of the outcome or the number of users).</p><p>Beyond this, the fact that the message said, ""Hello Chris_Leong"" and that it was sent to 270 users meant that it didn't really feel like a personal request from Ben Pace. Additionally, note the somewhat jokey tone of the <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020"">final sentence</a>, ""I hope to see you in the dawn of tomorrow, with our honor still intact"". Obviously, someone pressing the button wouldn't damage the honor or reputation of Less Wrong and so it seemed to indicate that this was just a bit of fun.</p><p>But beyond this, I remember when I was a kid and I played games super seriously, while other kids just wanted to have some fun. And I was annoyed with them because I wanted to win, but I felt that others on my team were holding me back. It wasn't until I was an adult that I realized I was wrong to insist that they had to engage with the game in the same way as me, when it was really their choice.</p><p>Now Habryka is annoyed because he was trying to run a specific experiment and that experiment wasn't, ""Can people who kind of care about the game, but don't care too much get fooled into taking down the site"". I can understand that, I imagine that this experiment took a lot of time to set up and he was probably looking forward to it for a while.</p><p>At the same, I don't think that the purpose of this experiment was clear at all. &nbsp;I wasn't sure if it was having fun, increasing awareness or gaining insight into people's psychology. I read the email and the post, and the feeling of this ""I do actually really care about people being able to coordinate to not take the site down. It's an actual hard thing to do that actually is trying to reinforce a bunch of the real and important values that I care about in Petrov day"" wasn't really articulated anywhere. And if there was a particular goal, instead of us being supposed to decide for ourselves what the goal was, then maybe it would have made sense to have been clear about it?</p>",Chris_Leong,chris_leong,Chris_Leong,
scL68JtnSr3iakuc6,"""Win First"" vs ""Chill First""",win-first-vs-chill-first,https://www.lesswrong.com/posts/scL68JtnSr3iakuc6/win-first-vs-chill-first,2020-09-28T06:48:21.511Z,101,58,20,False,False,,"<p>Around a month ago, I found an <i>incredibly</i> insightful quote deep in the Reddit comments about a particular basketball player who had recently changed teams.</p><blockquote><p>People who try hard to win first and foremost make it uncomfortable when people are trying to just have a good time and do well. And this is aside from whether they're assholes or not, unscrupulous or not. <strong>It's about win first vs chill first.</strong></p><p>At some point, there's always a conflict between the two types. Someone has to chill, someone has to turn up the intensity, or someone has to leave.</p></blockquote><p>- <a href=""https://old.reddit.com/r/nba/comments/ikbph8/jimmy_butler_on_his_fourthquarter_takeover_say/g3jwkcp/""><strong>Source</strong></a>, minor formatting cleanup and emphasis added</p><p>Now, I'm not a particularly avid follower of sports. But this was a rather unusually fascinating case.</p><p>To simplify a very long story, there's a professional basketball team — the Philadelphia 76ers — that on paper have had <i>a lot</i> of really good players the last few years, yet have consistently underperformed expectations.</p><p>Last year, they traded for a player who is known as being super-crazy-hardcore-intense. That player was <a href=""https://en.wikipedia.org/wiki/Jimmy_Butler"">Jimmy Butler</a>, who was an unheralded quite low draft pick (the 30th player chosen his year, meaning almost every team passed on him at least once) who worked very, very hard to turn himself into a star.&nbsp;</p><p>He wasn't one of those players who was really good right when joined the League — he didn't start in pro basketball until he was 22 years old, and wasn't really good until he was 25.&nbsp;</p><p><a href=""https://www.basketball-reference.com/players/b/butleji01.html"">His career stats are here</a>; you don't need to know much about basketball to see the trend of going from scoring 2.6 points per game your first year in the league, to 8.6 your second, to 13.1 your third year, to 20.0 your fourth year in the league is (1) someone who was not-at-all ""anointed"" or had an easy path for himself, and (2) showed really incredible year-over-year improvements.&nbsp;</p><p>Eventually, Jimmy became a consistent All-Star.</p><p>Last year, he joined the Philadelphia 76ers.</p><p>And it didn't go very well.</p><p>Although much of the story is secondhand and hearsay, apparently Jimmy Butler didn't get along with everyone else on the 76ers. During a film session to prepare for an upcoming opponent, there were reports that other players were sleeping or goofing around and Jimmy shouted at them.</p><p>Jimmy would yell at people who weren't training hard at practice, get under his teammate's skins, etc. If anyone wanted to relax and refused to go full-out competitive in pursuit of being the best individual player they could be, the best teammate they could be, and giving their utmost towards every game — Jimmy wasn't having it.&nbsp;</p><p>And the 76ers, by all accounts, had something of a ""chill first"" culture, despite having — on paper — really really good players. Anyway, much of this is hearsay but some of it isn't — obvious examples being when a coach publicly instructed one player who refused to follow instructions, a star player being noticeably out of shape and heavyset and suffering at the end of games, things like that.</p><p>Of course, these are still some of the finest athletes in the world — but the 76ers didn't have whatever that fanatic intensity that Michael Jordan was famous for, with all its advantages towards winning along with all its undeniable nasty side effects on stress and toxicity and lack of amicability.</p><p>Well, at the end of last season, Jimmy Butler's contract expired and he left the Philadelphia 76ers.&nbsp;</p><p>He went from Philadelphia to a team that missed the playoffs entirely that year, the Miami Heat, saying he went just because the culture there was intense and he felt his intensity would be appreciated there.&nbsp;</p><p>Jimmy Butler was roundly mocked for his decision. On paper, the 76ers looked like one of the best teams in basketball and the Miami Heat looked like a very subpar team.</p><p>Well, one year later, the 76ers just underperformed and were eliminated early again this year — and the team Jimmy Butler joined, Miami (which missed the playoffs last year)... is now heading to the NBA Finals as of tonight.&nbsp;</p><p>There's no doubt in my mind that there's a lot of people more <i>content</i> than Jimmy Butler, more <i>amicable</i> than Jimmy Butler, way more fun to <i>chill out with</i> than Jimmy Butler... there's not a single doubt in my mind that there are <i>very many downsides</i> to that fanatic junkyard dog mentality, that it's <i>incredibly stressful</i>, often <i>painful</i>, risks destroying relationships and discordancy rather than the more guaranteed affability and amicability of ""chill first, don't worry about it""...</p><p>... and yet, y'know, I saved this comment over a month ago, before any of this could be truly foreseen, since it seemed to sum up a point rather elegantly:</p><blockquote><p>People who try hard to win first and foremost make it uncomfortable when people are trying to just have a good time and do well. And this is aside from whether they're assholes or not, unscrupulous or not. <strong>It's about win first vs chill first.</strong></p><p>At some point, there's always a conflict between the two types. Someone has to chill, someone has to turn up the intensity, or someone has to leave.</p></blockquote>",lionhearted,lionhearted-sebastian-marshall,lionhearted (Sebastian Marshall),
tdLzqMueEp8zkqxfF,"On ""Not Screwing Up Ritual Candles""",on-not-screwing-up-ritual-candles,https://www.lesswrong.com/posts/tdLzqMueEp8zkqxfF/on-not-screwing-up-ritual-candles,2020-09-27T21:55:10.793Z,48,19,7,False,False,,"<p>I do a lot of experimentation with <a href=""https://www.lesswrong.com/tag/ritual"">rational ritual</a>. Rituals often use lots of candles. I used to just buy Random Cheap Candles. Then I kept running into problems. Now I have very strong opinions about what kinds of candles to buy.</p><p>The first problem was simply that I wanted impressive photos for Secular Solstice – I wanted to show off how it felt to people who'd never seen it before. I found myself constantly re-using the photos from Bay Solstice 2013, where the organizer (Ben Landau Taylor) had splurged on large, satisfying-looking-from-a-distance candles:</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_120 120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_360 360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_720 720w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_840 840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_960 960w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_1080 1080w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1995e827f388438df6e680ac430170779319d35266683d83.png/w_1134 1134w""></figure><p>The second problem was... candles are actually pretty logistically complicated. They drip wax. Rituals often require you to light one candle off another, but only certain shapes of candle can do that. Some candles burn really quickly and flicker out halfway through your ritual.</p><p>I spent several years buying lots of candles and trying them out. I kinda became this guy:</p><figure style=""width:475px;""><img src=""https://www.mikeeschmee.com/blog/wp-content/uploads/2019/08/dril-candle-tweet.png""></figure><p>Okay, I don't actually spend $3,600 a month, I've spent maybe $250 total. And I'm going to pitch you on spending... maybe $50, which I claim is Worth It.</p><p>I've tried to tell people ""man, you should really get some nice candles for your rituals"", but other people hadn't experimented with different types of candles and they were like ""really? does it matter? idk man, they're candles. Are you sure you're not just getting absurdly into candles and having the <a href=""https://xkcd.com/915/"">Joe Biden Sandwhich Photo effect</a>?""</p><p>I can't be 100% confident my aesthetic opinions are valid. But, I am confident there are at least some serious logistical concerns:</p><h2>The Logistical Concerns</h2><p>Problems you'll run into with candles:</p><p><strong>1. You often need each candle to light other candles.</strong> This works much better if they are thin/long taper candles rather than fatter votive, tealight or pillar candles. <i>It is basically impossible if the candle is in a jar.</i></p><p><strong>2. Because you need thin candles, you need candle holders so they stay upright. </strong>Woe to the person who buys a bunch of taper candles and then realizes they have to awkwardly jury-rig candeholders out of aluminum foil at the last minute.</p><p><strong>3. You need candles to not run out, so you want fairly large and/or slow-burning ones. </strong>Woe to the person's important candle symbolizing human civilization flickers out halfway through the Petrov Day ceremony.</p><p><strong>4. Candles can drip wax, which can be annoying. </strong>They work better with a candleholder that can catch the wax as it falls. (Candleholders vary in size, some catch a lot of wax, some basically none). There are also some candles designed to drip less wax (marketed as ""dripless, though my experience is that they drip non-zero)</p><ul><li>Note: this also means you shouldn't put candles on a carpet or other hard-to-clean surface</li><li>Also note that you'll drip less wax if, rather than take Candle #1 and use it to light Candle #2, you take Candle #2 and light if off candle #1 (because candle #1 will have accumulated a bunch of liquid wax which will spill when you tip it sideways)<ul><li>(this option trades off against ""sometimes it feels more symbolically potent for Candle #1 to be the one lighting Candle #2, rather than vice-versa. Make good choices based on how much you hate cleaning up wax)</li></ul></li></ul><h2>Aesthetics</h2><p>If you believe me that Nice Looking Candles Are Better, and/or you want them to show up nicely in photographs, you probably also want large candles. They feel heftier in your hand.&nbsp;</p><p>On the flipside: if you're in a global pandemic and you're doing rituals over Zoom, there actually is such a thing as Too Big A Candle. If you're sitting in a small space where the camera must be very close to the candles, it don't be able to see 8 different tall taper candles. (You can either solve this by getting somewhat shorter taper candles, or by arranging in advance to be able to put the camera a bit farther away)</p><h2>Recommendation:</h2><p>This all adds up to: you probably want slow burning, dripless taper candles with candleholders. In situations where I don't care about lighting one candle off another, I also enjoy large pillar candles.</p><p><a href=""https://www.amazon.com/gp/product/B01AVEODA2/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1"">Here are the candles I currently use</a> – they're around $30 for 30 slow-burning candles. If you only do a few rituals a year, this will basically last you forever. And if you end up using candles all the time... well, $1 a candle isn't too bad. I chose these over some more expensive options that didn't feel that much better, and over cheaper options that constantly dripped wax everywhere.</p><p><a href=""https://www.amazon.com/Hosley-Candle-Holders-Weddings-Meditation/dp/B085GLYYCB/ref=sr_1_2?dchild=1&amp;keywords=Taper+Candle+Holder&amp;qid=1601242403&amp;sr=8-2"">Here are the candle holders I use</a> with them (about $20).</p><p>I also sometimes use larger pillar candles. You can either get a <a href=""https://www.amazon.com/Stonebriar-Unscented-Lanterns-Hurricanes-Centerpieces/dp/B075X3CCGW/ref=sr_1_8?dchild=1&amp;keywords=pillar+candle+set+of+6&amp;qid=1601243410&amp;sr=8-8"">set of 6 medium ones</a> or <a href=""https://www.amazon.com/Light-Dark-Vanilla-Pillar-Variety/dp/B076NXP3J3/ref=sr_1_6?dchild=1&amp;keywords=pillar+candle+set+of+3&amp;qid=1601243361&amp;sr=8-6"">this variety pack of small/medium/large</a>. I use these to light pathways through dark tunnels during some more, um, adventurous rituals that I've led.</p><p>You can see them in a variety of stages of burning here:</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_410 410w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_820 820w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_1230 1230w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_1640 1640w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_2050 2050w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_2460 2460w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_2870 2870w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_3280 3280w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_3690 3690w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8be815e45a40fb8aadb4d994fc1aec91635eee13ebab3440.png/w_4032 4032w""><figcaption>On the right, my Petrov Day ""Flourishing Future"" Candle - never actually lit, only reflected on.</figcaption></figure>",Raemon,raemon,Raemon,
LAR2ajpFDueNg45Mk,"What to do with imitation humans, other than asking them what the right thing to do is?",what-to-do-with-imitation-humans-other-than-asking-them-what,https://www.lesswrong.com/posts/LAR2ajpFDueNg45Mk/what-to-do-with-imitation-humans-other-than-asking-them-what,2020-09-27T21:51:36.650Z,10,3,6,False,True,,"<p>This question is about whether you have clever ideas about how to use AI imitations of humans for AI safety. The two main ideas I'm familiar with only seem to interface with these imitations as if they're humans.</p><ul><li>The most obvious thing one might do with a good predictor of a human is just to write software that queries the imitation human about what the right thing to do is, and then does it.</li><li>The less obvious thing to do is to try and amplify it - e.g. use teams of them working together to try to choose good actions. Or maybe even an IDA loop - use your learner that learned to imitate a human, and train it to imitate the teams working together. Then make teams of teams, etc.</li></ul><p>But can we use human imitations to increase the effectiveness of value learning in a way other than amplification/distillation? For example, is there some way of leveraging queries to human imitations to train a non-human AI that has a human-understandable way of thinking about the world?</p><p>Keep in mind the challenge that these are only imitation humans, not oracles for the best thing to do, and not even actual humans. So we can't give them problems that are too weird, or heavily optimized by interaction with the imitation humans, because they'll go off-distribution.</p><p>Another possible avenue is ways to ""look inside"" the imitation humans. One analogy would be how if you have an image-generating GAN, you can increase the number of trees in your image by finding the parameters associated with trees and then turning them up. Can you do the same thing with human-imitating GAN, but turning up ""act morally"" or ""be smart?""</p>",Charlie Steiner,charlie-steiner,Charlie Steiner,
gJR4StLkutHvcA6tc,What are good rationality exercises?,what-are-good-rationality-exercises,https://www.lesswrong.com/posts/gJR4StLkutHvcA6tc/what-are-good-rationality-exercises,2020-09-27T21:25:24.574Z,54,15,25,False,True,,"<p>I want to know what are good rationality exercises.</p><p>I was just on a call with Liron and PhilH, hanging out after the weekly LessWrong weekend event, and we discussed exercises that could happen on LessWrong.</p><p>Here is the list we generated:</p><ul><li>Thinking Physics</li><li>Fermi Estimates</li><li>Project Euler</li><li>Calibration Training</li><li>Basic probabilistic reasoning</li><li>Basic have-you-read-the-sequences knowledge test (e.g. ""Which of the following is an example of 'belief as attire'?"")</li></ul><p>Another user on the call (whose name I forget) suggested it could be fun to have a daily Fermi Estimate on LessWrong, where everyone submits their number and the model they used to reach the number. I think this would be quite exciting.</p><p>Please write answers with other exercises that you think are or might be great for rationality training, some explanation of why you think it could be good, and a suggestion of how it could be incorporated into LessWrong. I'll probably add some of the above myself.</p>",Benito,benito,Ben Pace,
4KATQFzniqDpHo6ck,Puzzle Games,puzzle-games,https://www.lesswrong.com/posts/4KATQFzniqDpHo6ck/puzzle-games,2020-09-27T21:14:13.979Z,57,21,70,False,False,,"<p>I like puzzle games. I have aesthetic opinions about puzzle games. This post is meant to be a resource for people whose preferences about puzzle games have a lot in common with mine. It includes a list of game recommendations.</p><h1>Rules about Spoilers</h1><p>This post will be very spoiler-free. The only things information in this post about individual games will be about how much I like them. Exceptions to this rule are Tetris, Sudoku, and Sokoban. I also combine puzzle games with sequels/downloadable content/other games by the same developer, only if I think the sequels are similar, so I might spoil what games have sequels, and whether sequels are similar to the originals. Additionally one game in the list is a physical book, and one game is not primarily a puzzle game, but has a puzzle mode, and I say that in the list.&nbsp;</p><p>I also intend to enforce comments avoiding spoilers. The things you can say about individual games without spoiler tags are:</p><ul><li>How much you like the game</li><li>Who developed it/What other things they developed</li><li>What platforms it is on/When it came out</li><li>If there are (good) Sequels/Prequels/DLC</li><li>If you believe you finished it</li><li>If you believe you reached the end of the main part of the game (e.g. the credits screen)</li><li>How long is the game (how long it took you, NOT the number of puzzles)</li><li>How difficult is the game, in general (you can compare to other games)</li><li>How the puzzles fit into the Deduction/Efficiency/Technical/Linchpin categories</li></ul><p>Everything else about individual games should use the spoiler black boxes. You may NOT say specifics about whether a specific game violates the rules of the genre I outline, unless you use spoiler box. For spoilers about individual games, please list what games might be spoiled outside of the spoiler box.</p><p>I encourage mods to help me enforce spoiler rules, and encourage others to send me a message if they think a comment violates the rules and should be deleted.</p><h1>Defining the Genre</h1><p>I will define the genre of games I am pointing to in a way similar to Berlin interpretation of the roguelike genre. There will be a list of properties of the pure form of the puzzle game. Many great puzzle games will violate some of these properties. We can use this list to measure how close a game is to a pure puzzle game.</p><p>Puzzle games under my definition usually: are deterministic, are self-contained, have a clear end, have no time elements, have simple rules, give complete information, and are factored into puzzles.</p><h2>Deterministic</h2><p>Puzzle Games are deterministic. Tetris (which is often called a puzzle game) is therefore not a puzzle game by my definition. This property of puzzle games is pretty important. Non-determism in a game might be forgivable if it is only in a small part of the game, or is not central to the main puzzle solving.&nbsp;</p><h2>Self-Contained</h2><p>Puzzle Games should have a clear boundary, and should not require information outside of that boundary. Any puzzle that requires you to know some piece of trivia such that it is conceivable that someone starts playing the game without that trivia is pretty strongly violating my definition. Part of the reason this rule is important is because it creates a clear line that allows players to simultaneously know that the puzzles are solvable and know that they are not cheating. If a puzzle game requires trivia, then you might have to look things up, but it is hard to draw a clear line between looking up trivia and looking up the solutions to the puzzle.&nbsp;</p><p>Some of my favorite puzzle-like-things that include trivia get around this rule by declaring the internet to be inside the boundaries of the puzzle, and then release the puzzle as a one time competition, so the solution to the puzzle can be known to not exist on the internet.</p><p>A very forgivable violation of this rule is when games have rules written in english (or another common language). This is forgivable because people tend to recognize english and know that they do not know it. A violation of being self-contained can be made more forgivable if the trivia it uses really is very close to being known by everyone, or if it is very clear that this specific part of the puzzle game is not self-contained.</p><h2>Clear End</h2><p>Puzzle games should have a clear end. The reason for this is similar to the reason that puzzle games should be self-contained. The player should have an algorithm they can follow that allows them to play through the game without cheating. If the game does not have a clear end, then the player has to avoid spoilers for the game forever, because there might be more hidden content they do not know about.&nbsp;</p><p>Many games insufficiently signal the true end of the game, and violate this rule. I get around this by considering the steam achievements to be part of the game's canon. There are usually achievements for completing all of the bonus content of the game. Unfortunately, there are also sometimes achievements for basically guessing the developer's password.</p><h2>No Time Elements</h2><p>Puzzle games should not require the player to perform any actions quickly. Whether or not a sequence of button presses causes the player to solve the puzzle should be independent of the time between presses.&nbsp;</p><p>Some games violate this rule for a good reason. They explore a space of puzzles that really needed time-sensitive mechanics to be convenient. A violation of this rule is very forgivable when performing time sensitive actions is sufficiently easy.</p><h2>Simple Rules</h2><p>Puzzle games should have simple rules. Sometimes this is forgivably violated because more complex rules open up a design space that could not have been reached otherwise, and very good puzzles can be found in this design space.</p><h2>Complete Information</h2><p>Puzzle games should have rules that are fully understood by the player. This means that the player should (after they understand the rules of the puzzle) be able to make a plan, and verify that their plan works without actually interacting with the game.&nbsp;</p><p>There are two ways that this can be violated in a forgivable way.&nbsp;</p><p>First, the game can have places where the player knows that they do not know what will happen. Maybe the player has been mixing various colors of potions, which have various effects, but has never managed to mix a red and blue potion. The player can then have flagged uncertainty, and maybe go out of his way to figure out how to get a red and blue potion together. However, there should still be large (and growing) domains in which the player does know what will happen, and can make plans.</p><p>Second, the game can have simple rules, but not tell them directly to the player. This can make it such that the player has to simulate science to figure out what the rules might be.&nbsp;</p><p>Both of these forgivable violations can add a lot to the game that you cannot get otherwise, so I really do not think of it as a point against the game when done well (Although it still feels like a point against being a pure puzzle game). If you have to search a large dungeon to find a key that was hidden under a slightly different colored rock, I consider this rule to be violated in a bad way.</p><h2>Factored into Puzzles</h2><p>Puzzle games should be factored into individual puzzles that do not interact. You can learn things from solving one puzzle that help you solve others, but you should not have to use the extra literal key that you have because you solved the previous puzzle especially efficiently.</p><p>This rule again can be violated in a very good way to reach a new design space of puzzles, and violating this rule is not necessarily a point of the game in my opinion.&nbsp;</p><p>As a weaker version of this rule, progress through the game should be factored. If a player solves a puzzle, they should be further through the game. If a player can solve a puzzle that will make a future puzzle impossible unless they start the entire game over, this is bad. If they can reset the first puzzle, and then do the second puzzle, this is more forgivable.</p><h1>Categories of Puzzles</h1><p>Now I want to talk about four different categories of puzzles. These are categories of puzzles, rather than categories of puzzle games, although games often have puzzles that fall into the same category or pair of categories.</p><h2>Deduction Puzzles</h2><p>Deduction puzzles are like Sudoku, in that you can collect information about the solution to the puzzle, and use that to deduce more information, in a way that combines and can be tracked. Not many puzzle games have puzzles that fall into this category, as these are more often thought about as individual puzzles, rather than games.&nbsp;</p><h2>Efficiency Puzzles</h2><p>Efficiency puzzles have some function, like a spent resource, which you are trying to minimize. Maybe you are trying to complete a task in a small number of steps, maybe you are trying to design a machine using a small amount of space, or maybe you are trying to build something spending a small amount of money. You might have a concrete goal to get the function below a specific value to solve the puzzle, or it might be more open-ended.</p><h2>Technical Puzzles</h2><p>Technical puzzles is the name I am giving to the default type of puzzle you see in puzzle games. For example, most puzzles in sokoban-like games fall in this category. You are trying to accomplish some task using some limited set of things you can do, and you have to figure out how to do them in exactly the right way to get the task done. Most puzzle games are filled with games like this.</p><h2>Linchpin Puzzles</h2><p>Linchpin puzzles often cause the player to struggle with a puzzle that might feel impossible, until they have one key realization that makes the puzzle easy, or at least feel possible. Linchpin puzzles are hard to design, especially with very simple rules. When a puzzle game has complicated rules that feel justified to me, it is usually because it enables them to build linchpin puzzles.</p><h2>Partial Progress</h2><p>These four classes of puzzles can be thought of as tracking the sense in which partial progress is possible. In deduction puzzles, you are building up a bunch of partial progress. In efficiency puzzles, you can often determine that one way to solve part of the puzzle dominates another because it uses less of the resource. In technical puzzles, you can collect more and more types of things that you can do that might be helpful. In linchpin puzzles, you usually go from feeling like you have made no progress to the puzzle being practically solved.&nbsp;</p><h1>A List of Games</h1><p>Here is a list of some puzzle games that I like, divided into tiers (and alphabetical within tiers). I might come back and modify or add to this list later. Some of these games may (or may not) violate some of the rules in the definition of puzzle game above. I group puzzles together with their sequels (assuming the sequels are similar).</p><p>Tier 1:</p><ul><li><a href=""https://store.steampowered.com/app/1052990/A_Monsters_Expedition/"">A Monster's Expedition</a></li><li><a href=""https://store.steampowered.com/app/736260/Baba_Is_You/"">Baba Is You</a></li><li><a href=""https://store.steampowered.com/app/26800/Braid/"">Braid</a></li><li><a href=""https://store.steampowered.com/app/497780/Recursed/"">Recursed</a> (<a href=""https://recursed-ice-palace.github.io"">+The Ice Palace</a>)</li><li><a href=""https://store.steampowered.com/app/353540/Stephens_Sausage_Roll/"">Stephen's Sausage Roll</a></li></ul><p>Tier 2:</p><ul><li><a href=""https://store.steampowered.com/app/316610/A_Good_Snowman_Is_Hard_To_Build/"">A Good Snowman Is Hard to Build</a></li><li>DROD (<a href=""https://store.steampowered.com/bundle/1847/DROD_The_Original_Trilogy/"">KDD, JtRH, TCB</a>, <a href=""https://store.steampowered.com/app/314330/DROD_Gunthro_and_the_Epic_Blunder/"">GatEB</a>, <a href=""https://store.steampowered.com/app/351320/DROD_The_Second_Sky/"">TSS</a>, and <a href=""https://store.steampowered.com/bundle/1846/DROD_All_the_Smitemasters_Selections/"">Smitemaster's Selections</a>)</li><li><a href=""https://store.steampowered.com/app/357300/Snakebird/"">Snakebird</a></li><li><a href=""https://store.steampowered.com/app/210970/The_Witness/"">The Witness</a></li><li><a href=""https://twisty-little-passages.backerkit.com/hosted_preorders"">Twisty Little Passages</a> (This is a physical book)</li></ul><p>Tier 3:</p><ul><li><a href=""https://store.steampowered.com/app/351330/DROD_RPG_Tendrys_Tale/"">DROD RPG: Tendry's Tale</a></li><li><a href=""https://avorobey.github.io/jelly/"">Jelly No Puzzle</a></li><li><a href=""https://www.kinegame.com/kine-home"">Kine</a></li><li><a href=""https://store.steampowered.com/app/558990/Opus_Magnum/"">Opus Magnum</a> (and other <a href=""https://store.steampowered.com/bundle/2925/The_Zachtronics_Puzzle_Pack/"">Zachtronics games</a>)</li><li><a href=""https://store.steampowered.com/app/721390/Pipe_Push_Paradise/"">Pipe Push Paradise</a></li></ul><p>Tier 4:</p><ul><li><a href=""https://smestorp.itch.io/corrypt"">Corrypt</a></li><li><a href=""https://store.steampowered.com/bundle/234/Portal_Bundle/"">Portal+Portal 2</a></li><li><a href=""https://store.steampowered.com/app/490220/Prismata/"">Prismata</a> (This is not primarily a puzzle game, but has puzzle content)</li><li><a href=""https://store.steampowered.com/app/1180540/Puddle_Knights/"">Puddle Knights</a></li><li><a href=""https://store.steampowered.com/app/290260/Sokobond/"">Sokobond</a></li><li><a href=""https://store.steampowered.com/app/235980/Tetrobot_and_Co/"">Tetrobot and Co.</a></li></ul><p>Please let me know if there are some other great puzzle games I am missing out on.</p>",Scott Garrabrant,scott-garrabrant,Scott Garrabrant,
QxdNxQoNfAceXKwFF,What hard science fiction stories also got the social sciences right?,what-hard-science-fiction-stories-also-got-the-social,https://www.lesswrong.com/posts/QxdNxQoNfAceXKwFF/what-hard-science-fiction-stories-also-got-the-social,2020-09-27T20:37:44.256Z,15,7,30,False,True,,"<p>Robin Hanson says that those stories are rare.</p>
<blockquote>
<p>Most discussion of the non-immediate future seems to take place in science fiction, and the small subset of science fiction where authors try hard to remain realistic is called hard science fiction. Loosely associated with hard science fiction is an intellectual community of people who try to make projections which are true to our best understanding of the world. They work out the broad science and engineering of plausible future space colonies, starships, virtual reality, computer networks, survellience, software assistants, genetically engineered people, tiny machines made to atomic accuracy, and much more.</p>
<p>Unfortunately, few if any people of these people know much social science. So their projections often combine reasonable physics or computers with laughable economic assumptions. This often seriously compromises their ability to make useful projections.
(source: <a href=""http://mason.gmu.edu/~rhanson/econofsf.html"">The Economics of Science Fiction</a>)</p>
</blockquote>
<p>This is different from <a href=""https://en.wikipedia.org/wiki/Social_science_fiction"">social science fiction</a>, which is deliberate exploration of other possible forms of sociological organisation.</p>
",MathieuRoy,mathieuroy,Mati_Roy,
DEe5cvpaTQeF9kkru,Tips for the most immersive video calls,tips-for-the-most-immersive-video-calls,https://www.lesswrong.com/posts/DEe5cvpaTQeF9kkru/tips-for-the-most-immersive-video-calls,2020-09-27T20:36:51.422Z,65,22,9,False,False,,"
    <p>I spend a lot of my day on video calls. Wave is a distributed company, so they’re the main way we communicate.
        But compared to talking in person, they feel unnatural:</p>
    <ul>
        <li>Most people have low-quality microphones and webcams that make them look and sound bad.</li>
        <li>There’s a lag between when you say something and when the other person hears it, making it hard to navigate
            conversational <a href=""https://en.wikipedia.org/wiki/Turn-taking"" target=""_blank"">turn-taking</a>.</li>
        <li>If you’re using headphones, you can’t hear your own voice very well.</li>
        <li>Because of <a href=""https://en.wikipedia.org/wiki/Echo_suppression_and_cancellation"" target=""_blank"">echo
                cancellation</a>, you often can’t talk when someone else is also talking, which makes the conversation
            flow less well.</li>
    </ul>
    <p>I started wondering how much nicer video calls would feel if I fixed these problems. So I spent way too much time
        fiddling with gear and software. This post summarizes what I’ve learned. Collectively, I think these
        recommendations have a pretty big impact: when talking one-on-one to friends with equally good setups, I’ve been
        able to go 4+ hours without feeling fatigued.</p>
    <p><em><small>Epistemic status: best guess; not a professional; almost certainly contains wrong bits. Tell me which
                ones by comment or <a href=""https://www.benkuhn.net/contact/"">email</a>!</small></em></p>
    <h2 id=""omg-ben-dont-make-me-read-your-4500-word-doorstopper-just-tell-me-what-to-do"">omg ben don’t make me read
        your 4500 word doorstopper, just tell me what to do</h2>
    <p>Here’s how I would stack-rank my advice for my past self. (Of course, your personal ranking might be different
        depending on your situation.)</p>
    <ol>
        <li>
            <p>($depends) Don’t work in a space where your noise can bother other people, or vice versa.</p>
        </li>
        <li>
            <p>($10-30) If you ever have network issues, run <a href=""https://amzn.to/3luTVdV"" target=""_blank"">a
                    cable</a> between your computer and router. You’ll probably need an <a href=""https://amzn.to/2YOFis9"" target=""_blank"">adapter</a>. (Contrary to popular belief that a bad
                connection is your ISP’s fault, it’s <a href=""https://www.benkuhn.net/wireless/"">more likely to be flaky wifi</a>.)</p>
        </li>
        <li>
            <p>(~$100) Buy <a href=""https://amzn.to/3bULynH"" target=""_blank"">open-back headphones</a>, which let you
                hear your own voice normally and are extremely comfortable.</p>
        </li>
        <li>
            <p>(~$30) Switch from your built-in computer mic to a <a href=""https://amzn.to/3cg3JUN"" target=""_blank"">headset mic</a> (and <a href=""https://amzn.to/3hOU5JU"" target=""_blank"">pop
                    filter</a>), which will sound much better and pick up less noise. Note this requires a headset with
                detachable cable, like the one I linked above.</p>
            <p>You can now leave yourself unmuted! If the other person also has headphones, you can also talk at the
                same time. Both of these will make your conversations flow better.</p>
        </li>
        <li>
            <p>($0) Prefer Zoom to most alternatives; it has higher sound quality, better echo cancellation, and fewer
                silly behaviors. If you have headphones and a good mic, enable “original sound” to turn off some
                unnecessary audio filtering.</p>
        </li>
        <li>
            <p>(~$200) Get a second monitor for notes so that you can keep Zoom full-screen on your main monitor. It’s
                easier to stay present if you can always glance at people’s faces. (I use an iPad with <a href=""https://support.apple.com/en-us/HT210380"" target=""_blank"">Sidecar</a> for this; for a
                dedicated device, the right search term is <a href=""https://amzn.to/3iZRUVz"" target=""_blank"">“portable
                    monitor”</a>. Also, if your meetings frequently involve presentations or screensharing, consider
                getting a third monitor too.)</p>
        </li>
        <li>
            <p>($0?) Arrange your lighting to cast lots of diffuse light on your face, and move any lights that shine
                directly into your camera. Lighting makes a bigger difference to image quality than what hardware you
                use!</p>
        </li>
        <li>
            <p>(~$20-80 if you have a nice camera) Use your camera as a webcam. There’s software for <a href=""https://www.usa.canon.com/internet/portal/us/home/support/self-help-center/eos-webcam-utility"" target=""_blank"">Canon</a>, <a href=""https://fujifilm-x.com/en-us/support/download/software/x-webcam/"" target=""_blank"">Fujifilm</a>, <a href=""https://downloadcenter.nikonimglib.com/en/download/sw/176.html"" target=""_blank"">Nikon</a>, and
                <a href=""https://support.d-imaging.sony.co.jp/app/webcam/en/"" target=""_blank"">Sony</a> cameras
                (Windows-only for Nikon and Sony); for others, if they can output clean HDMI (check <a href=""https://www.elgato.com/en/gaming/cam-link/camera-check"" target=""_blank"">this list</a>), you
                can buy an <a href=""https://amzn.to/3kgunjj"" target=""_blank"">HDMI capture card</a>. You will also want
                to be able to plug your camera into a power source, for which you may need a “dummy battery.”</p>
        </li>
        <li>
            <p>(~$40 if you have a smartphone with a good camera) Use that as a webcam via <a href=""https://reincubate.com/camo/"" target=""_blank"">Camo</a>.</p>
        </li>
        <li>
            <p>(~$350) If you don’t own a nice camera but want one, you can get a used entry-level mirrorless camera +
                lens + dummy battery + boom arm. See <a href=""#camera-buying"">buying tips below</a>.</p>
        </li>
    </ol>
    <p>More detailed recommendations and justifications follow.</p>
    <h2 id=""network"">Network</h2>
    <p>Connection problems are the thing that makes video calls suck the most. They do this in three different ways:</p>
    <ol>
        <li>
            <p>If your connection ever gets really bad, your audio will break up, which is exhausting to listen to and
                ruins the flow.</p>
        </li>
        <li>
            <p>Even if it doesn’t get that bad, a poor connection will increase <em>latency</em>, or the time between
                when you speak and when the other person hears you. High latency is what causes the dreaded “you first,
                no <em>you</em> first” dance.</p>
        </li>
        <li>
            <p>Finally (and least importantly), a bad connection limits the amount of data you can exchange, forcing you
                to use lower-quality video. This doesn’t really matter if you’re using a webcam, but by the end of this
                post, you might have a good enough camera that it matters.</p>
        </li>
    </ol>
    <p>I wrote a whole post of its own on how to troubleshoot your home network for video calls, but realistically, most
        connection problems are because <a href=""https://www.benkuhn.net/wireless/"">wifi sucks</a> and you can avoid them by not using wifi.
        So, first try running an <a href=""https://amzn.to/3luTVdV"" target=""_blank"">Ethernet cable</a> between your
        computer and your router. If you still notice high latency or your connection dropping, or if you really can’t
        run a cable for some reason, <a href=""https://www.benkuhn.net/vcnet/"">check the guide</a> for more troubleshooting advice.</p>
    <h2 id=""audio"">Audio</h2>
    <p>Video improvements are flashy and noticeable, but audio is the reason you’re having the call, thus ultimately
        more important. So audio comes first.</p>
    <h3 id=""get-away-from-other-people"">Get away from other people</h3>
    <p>This is a basic prerequisite for everything below. Coworking spaces and cafés are nice if you plan to be silent
        all day, but will make natural-feeling meetings impossible due to your crippling self-consciousness about noise
        levels. If you’re going to be on a call for more than 5 minutes, get your own space.</p>
    <p>(If you are committed to taking your meetings in a crowded and noisy space, ignore the rest of the audio section.
        You’re mostly just doomed to crappy calls in this case, though you might be able to limit the damage by getting
        <a href=""https://amzn.to/3cg3JUN"" target=""_blank"">a nice headset mic</a> and installing <a href=""https://krisp.ai/"" target=""_blank"">krisp.ai</a>.)</p>
    <p>If you’re talking to someone else who’s in a noisy environment, you can apparently also use krisp.ai to filter
        their audio yourself, though I haven’t tried this.</p>
    <h3 id=""get-full-duplex-audio-with-no-echo"">Get full-duplex audio with no echo</h3>
    <p>One key ingredient to making voice conversations feel “natural” is that both participants need be able to talk
        and hear the other person talking at the same time (“full-duplex audio”). Full-duplex audio is important because
        it allows you to talk simultaneously (“overlap”) with the other person.</p>
    <p>You might think that overlap should be rare, because interrupting someone else is rude. While that’s true of
        large-scale overlaps, we often use small-scale overlaps to <a href=""https://en.wikipedia.org/wiki/Turn-taking#Overlap"" target=""_blank"">negotiate conversational
            turn-taking</a> (e.g. starting talking when the speaker is trailing off but hasn’t finished), or to signify
        that we’re paying attention (“uh-huh,” “yeah”).</p>
    <p>The hard problem of full-duplex audio is that if someone else is talking, their voice is going to come out of
        your computer’s speakers and go back into your microphone. If your computer leaves the microphone on, in that
        case, it’ll end up playing back an “echo” of their own voice to them, which is extremely annoying. So video call
        tries to filter out feedback from your speakers into your microphone, which is called <a href=""https://en.wikipedia.org/wiki/Echo_suppression_and_cancellation"" target=""_blank"">echo
            cancellation</a>.</p>
    <p>Unfortunately, removing <em>only</em> the speaker echo from your microphone stream is really hard to do. So
        instead, the software often ends up completely muting your mic if someone else is talking. If you’ve ever tried
        to micro-overlap with someone and noticed that their audio cut out briefly, that’s what’s going on.</p>
    <p>If your listeners can’t overlap with you, it’s harder to tell whether they’re following along, and it’s harder to
        negotiate whose turn it is to speak. This makes the conversation feel less natural, especially in larger groups.
    </p>
    <p>To get full-duplex audio, you need to (a) have an audio setup that doesn’t produce echoes, then (b) convince your
        video call app not to try to suppress echoes.</p>
    <p>(a), “an audio setup that doesn’t produce echoes,” means that your microphone should not pick up any sound from
        your speakers. In practice this means that your “speakers” must be headphones.</p>
    <p>(b), “convince your video call app not to try to suppress echoes,” seemed surprisingly tricky when I tried to
        research it, because each video call app has its own heuristics for when to engage echo-cancellation.</p>
    <p>So I did my own tests of echo cancellation Zoom, Skype, and Hangouts in Chrome and Firefox. I started a chat
        between two computers, both with headphones attached—a setup that should have required no echo cancellation. I
        then played music into the microphone of one computer. On the other, I spoke into the microphone and listened
        for whether the music got quieter.</p>
    <p>Zoom, Skype and Hangouts in Firefox all seemed to slightly decrease the audio volume when I spoke, indicating
        light echo cancellation. For Hangouts in Chrome, the audio cut out <em>completely</em> every time I said
        anything. In Zoom, I was able to eliminate all echo cancellation by selecting the “use original audio” option,
        which you can also permanently enable for particular audio devices—I’d recommend doing this.</p>
    <h3 id=""throw-your-wireless-headset-in-the-trash"">Throw your wireless headset in the trash</h3>
    <p>The gear I recommend in this guide is all wired, not Bluetooth. While Bluetooth seems like it should be great, in
        practice it has <a href=""https://www.benkuhn.net/wireless/"">horrible problems with audio latency, quality and reliability</a>. Also, I
        don’t think wireless open-back headphones (see below) exist.</p>
    <p>If you finished the previous paragraph and still think you can get away with using wireless audio gear, read the
        post at the link :)</p>
    <h3 id=""hear-yourself-clearly-with-open-back-headphones"">Hear yourself clearly with open-back headphones</h3>
    <p>Most headphones are <em>closed-back</em>, which means they form an acoustic seal over your ear that attenuates
        outside sound. This is good for “noise isolation” when you’re listening to music. But it’s bad in calls because
        it also isolates you from your own voice, making you sound muffled and unnatural to yourself. (The same thing
        also happens with any earbuds that form a seal, i.e. pretty much everything except EarPods or non-Pro AirPods.)
    </p>
    <p>Personally, without the feedback from hearing myself, I also tend to start speaking louder or shouting on calls.
        This tires out my voice, and can get stressful for whoever I’m <del>shouting at</del> talking to.</p>
    <p>To avoid this, you can buy <em>open-back headphones</em>, which have mesh instead of a closed covering over your
        ears. I bought the <a href=""https://amzn.to/3bULynH"" target=""_blank"">Philips SHP9500</a>, which I like a lot; I
        haven’t tested any other pairs. (I chose a low-end pair because for video calls, sound quality will mostly be
        limited by people’s microphones; if you want to use the same ones to listen to music, you might want a
        higher-end pair.)</p>
    <p>As an extra bonus, open-back headphones are <em>way</em> more comfortable because they get less hot. I didn’t
        realize beforehand how much difference this would make, but it’s amazing to be able to wear headphones all day
        without my ears overheating!</p>
    <p>Note that open-back headphones “leak” sound, so anyone near you will hear the other side of the conversation as
        well. This isn’t a problem if you have your own space, but they’re not suitable for shared spaces. You might
        think the sound could leak into your own microphone and cause an echo, but I tested and it’s too quiet for that
        unless you set the volume uncomfortably high.</p>
    <h3 id=""dont-mute"">Don’t mute</h3>
    <p>This isn’t about equipment per se, but it has implications for your equipment choices. Quoting <a href=""https://ma.tt/2020/03/dont-mute-get-a-better-headset/"" target=""_blank"">Matt Mullenweg</a>, founder of
        one of the earliest and largest fully-distributed companies:</p>
    <blockquote>
        <p>One heterodox recommendation I have for audio and video calls when you’re working in a distributed fashion is
            not to mute, if you can help it. When you’re speaking to a muted room, it’s eerie and unnatural — you feel
            alone even if you can see other people’s faces. You lose all of those spontaneous reactions that keep a
            conversation flowing. If you ask someone a question, or they want to jump in, they have to wait to unmute. I
            also don’t love the “unmute to raise your hand” behavior, as it lends itself to meetings where people are
            just waiting their turn to speak instead of truly listening.</p>
    </blockquote>
    <p>I strongly agree with this and prefer for the people I’m talking with to stay unmuted unless they have a crappy
        mic that picks up a lot of noise. Which won’t be your problem as long as you…</p>
    <h3 id=""get-a-better-microphone"">Get a better microphone</h3>
    <p>Most non-standalone computer microphones, including ones on fancy headsets, sound ear-bleedingly bad. (The 2020
        MacBook Pro microphone is okay.) You can sound way more pleasant to your colleagues by getting a nicer one. For
        instance, compare me reading <a href=""https://www.nonsenselit.org/Lear/BoN/bon010.html"" target=""_blank"">Edward
            Lear</a> on the following mics:</p>
    <style>
        audio {
            width: 100px
        }
    </style>
    <table>
        <thead>
            <tr>
                <th>Mic</th>
                <th>Recording</th>
                <th>Commentary</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>2014 iMac</td>
                <td><audio controls="""">
                        <source src=""imac internal.mp3"" type=""audio/mp3""></audio></td>
                <td>Sounds like a tin can, because it is</td>
            </tr>
            <tr>
                <td>Jabra Evolve 70</td>
                <td><audio controls="""">
                        <source src=""jabra evolve.mp3"" type=""audio/mp3""></audio></td>
                <td>Wirecutter rec; sounds like a bad head cold</td>
            </tr>
            <tr>
                <td>2020 MacBook Pro 13”</td>
                <td><audio controls="""">
                        <source src=""macbook 13in 2020.mp3"" type=""audio/mp3""></audio></td>
                <td>“Studio quality” my foot; try “moderate head cold”</td>
            </tr>
            <tr>
                <td><a href=""https://amzn.to/3cg3JUN"" target=""_blank"">V-Moda BoomPro</a></td>
                <td><audio controls="""">
                        <source src=""boompro foam.mp3"" type=""audio/mp3""></audio></td>
                <td>Stupid name, $30, actually sounds ok</td>
            </tr>
        </tbody>
    </table>
    <p>The best “can’t mess up” microphone option is the last one in the table, the <a href=""https://amzn.to/3cg3JUN"" target=""_blank"">V-Moda BoomPro</a> (with a <a href=""https://amzn.to/3hOU5JU"" target=""_blank"">foam
            windscreen</a>), which attaches to your headphones in place of a standard 3.5mm audio cable.<sup id=""fnref:1""><a href=""#fn:1"" class=""footnote-ref"" role=""doc-noteref"">1</a></sup> It sounds much clearer and
        less muffled than any headset’s built-in mic. It’ll also pick up less background noise (e.g. typing) than
        <em>any</em> mic, by virtue of being closer to your mouth, which makes it easier to follow “don’t mute” above.
        However, it won’t sound quite as natural as a non-headset mic.</p>
    <p>If you want something that sounds even more realistic, your best bet is something like an <a href=""https://amzn.to/3kh95SM"" target=""_blank"">AT2005</a> positioned less than six inches from your face
        using a <a href=""https://amzn.to/3bVaDP5"" target=""_blank"">boom arm</a> that’s <a href=""https://amzn.to/3kb57uP"" target=""_blank"">clamped</a> to your desk. If you don’t want your mic to be visible, it may require some
        zooming/cropping of your camera setup to get it that close. Compare it to the BoomPro (you’ll need headphones to
        hear the difference well):</p>
    <table>
        <thead>
            <tr>
                <th>BoomPro</th>
                <th>AT2005</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><audio controls="""">
                        <source src=""https://www.benkuhn.net/vc/boompro foam.mp3"" type=""audio/mp3""></audio></td>
                <td><audio controls="""">
                        <source src=""https://www.benkuhn.net/vc/at2005usb 6in.mp3"" type=""audio/mp3""></audio></td>
            </tr>
        </tbody>
    </table>
    <p>I tested a few different microphones, and listened to recordings of many more. I haven’t trained myself to detect
        small quality differences, but my tentative conclusions are:</p>
    <ol>
        <li>
            <p>Distance to your mouth dominates nearly everything. For the mics I tested, 6 inches vs 12 inches made as
                big of a difference as switching mics.</p>
            <p>(That’s because from the microphone’s perspective, doubling the distance makes your voice 4x quieter, or
                equivalently, makes room noise 4x louder. It also makes louder e.g. the echoes of your voice—leading the
                microphone to produce a “boomy” sound.)</p>
            <p>Here’s two fairly different-sounding microphones at 6 inches vs 12:</p>
        </li>
    </ol>
    <table>
        <thead>
            <tr>
                <th>Model</th>
                <th>6 inches</th>
                <th>12 inches</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Blue Yeti</td>
                <td><audio controls="""">
                        <source src=""https://www.benkuhn.net/vc/yeti 6in.mp3"" type=""audio/mp3""></audio></td>
                <td><audio controls="""">
                        <source src=""https://www.benkuhn.net/vc/yeti 14in.mp3"" type=""audio/mp3""></audio></td>
            </tr>
            <tr>
                <td>AT2005USB</td>
                <td><audio controls="""">
                        <source src=""https://www.benkuhn.net/vc/at2005usb 6in.mp3"" type=""audio/mp3""></audio></td>
                <td><audio controls="""">
                        <source src=""https://www.benkuhn.net/vc/at2005usb 12in.mp3"" type=""audio/mp3""></audio></td>
            </tr>
        </tbody>
    </table>
    <ol start=""2"">
        <li>
            <p>Low-end condenser microphones sound somewhat “fuller” or more natural than dynamics, which can sound a
                little “muffled.” For example, the Blue Yeti above is a condenser while the AT2005USB is a dynamic.</p>
            <p>(Dynamic mics are sometimes said to “reject noise” better than condensers, but I couldn’t find anyone
                making this claim who explained why.<sup id=""fnref:2""><a href=""#fn:2"" class=""footnote-ref"" role=""doc-noteref"">2</a></sup>)</p>
        </li>
        <li>
            <p>Mics under $50, and headset mics, sound noticeably bad even when close to your mouth. Outside of that, it
                seemed like sound quality depended as much or more on things other than microphone quality, like how
                much your space echoes. (There’s a surprisingly large genre of YouTubers demoing expensive mics with
                bad-sounding setups.) So it seemed like higher-end microphones would probably be wasted in most video
                call setups.</p>
        </li>
    </ol>
    <p>Based on this, I suggest the AT2005 as a widely-recommended mic in the lowest “doesn’t sound noticeably bad”
        price tier, that can be stuck on a boom arm with minimal ceremony.</p>
    <p>Other microphone comparisons for further reading:</p>
    <ul>
        <li>
            <p><a href=""https://soundcloud.com/wirecutter-audio/sets/2019-usb-microphone-testing"" target=""_blank"">Wirecutter USB mic comparisons</a></p>
        </li>
        <li>
            <p><a href=""https://marco.org/podcasting-microphones"" target=""_blank"">Marco Arment on podcasting
                    microphones</a></p>
        </li>
        <li>
            <p><a href=""https://ma.tt/2020/03/dont-mute-get-a-better-headset/"" target=""_blank"">Matt Mullenweg “Don’t
                    mute, get a better headset”</a></p>
        </li>
        <li>
            <p><a href=""https://www.rtings.com/headphones/tests/microphone/recording-quality"" target=""_blank"">rtings.com
                    headset recording quality tests</a></p>
        </li>
    </ul>
    <h3 id=""listen-to-yourself"">Listen to yourself</h3>
    <p>In video calls, unlike real life, what you hear is not the same as what’s heard by the person you’re talking to.
        And mics are fiddly enough that your particular setup and mic technique matters a lot. So it’s really useful to
        listen to how your audio sounds. You can do this with a web app like <a href=""https://www.miccheck.me/"" target=""_blank"">miccheck.me</a>. The most common mic problems are <a href=""https://en.wikipedia.org/wiki/Pop_filter"" target=""_blank"">plosive pops</a> and <a href=""https://en.wikipedia.org/wiki/De-essing"" target=""_blank"">harsh sibilants</a>—pick sentences that will
        cover those. The <a href=""https://www.cs.columbia.edu/~hgs/audio/harvard.html"" target=""_blank"">Harvard
            Sentences</a> are a good starting point.</p>
    <p>If those consonants sound bad, you might need a better windscreen, or to change how your mic is positioned. For
        instance, if you have a headset mic, you should position it just beside the corner of your mouth—<em>not</em>
        directly in front—so that you’re not breathing/spitting into it.</p>
    <h2 id=""video"">Video</h2>
    <h3 id=""use-a-dedicated-monitor"">Use a dedicated monitor</h3>
    <p>I recently started putting my active call in full-screen mode on my primary (27”) monitor, and using a second
        monitor for notes or other activity. This turned out to make a surprisingly big difference to how immersive the
        call felt. (Maybe I should have been clued in by the fact that this feature is called “immersive mode”?) It’s
        amazing for keeping me focused and present. Possible reasons why:</p>
    <ul>
        <li>
            <p>In windowed mode, Zoom keeps your preview at the top of the screen in a bar of its own, but in fullscreen
                mode there’s no bar, just a floating preview window (which you can also hide). That means there’s a lot
                more screen available for other people’s video.</p>
        </li>
        <li>
            <p>Hiding the window bezel, task bar, etc. make it much less salient that you’re talking through a computer,
                and the user interface is less likely to distract you.</p>
        </li>
        <li>
            <p>In windowed mode, I’d sometimes end up tabbing away to look at another window, and lazily forget to tab
                back. Then I’d spend a lot of the call not looking at people and feeling less connected.</p>
        </li>
    </ul>
    <p>I use an iPad for this, but you can also buy dedicated “portable monitors” for under $200 that would serve this
        purpose well.</p>
    <h3 id=""improve-your-lighting"">Improve your lighting</h3>
    <p>The best way to get a sharper image on any camera is to put more light into the sensor. Laptop webcams have
        terrible image quality, but a laptop webcam with good lighting will look better than a fancy camera with bad
        lighting:</p>
    <figure>
        <div><a class=""img-wrapper"" href=""https://www.benkuhn.net/vc/imac%20cam%20bad.jpg"" title=""fullsize"" style=""max-width:33%""><img width=""211"" height=""140"" src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/wkkjnncfokn5r5it4md2"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/wkkjnncfokn5r5it4md2 2x, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/ue7onpunevy5xiwwzgup""></a><a class=""img-wrapper"" href=""https://www.benkuhn.net/vc/sony%20bad.jpg"" title=""fullsize"" style=""max-width:33%""><img width=""211"" height=""140"" src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/evimiass2fkymru5wkil"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/evimiass2fkymru5wkil 2x, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/xb6gdvnwibzxmlabryqe""></a><a class=""img-wrapper"" href=""https://www.benkuhn.net/vc/imac%20cam%20good.jpg"" title=""fullsize"" style=""max-width:33%""><img width=""211"" height=""140"" src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/kiunzoz94qtqdolnzlit"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/kiunzoz94qtqdolnzlit 2x, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/z2cunel4mft09xawj2jj""></a>
        </div>
        <figcaption>Left: 2014 iMac webcam destroyed by MAXIMUM BACKLIGHT. Middle: Fancy Sony A6000 struggles too.
            Right: iMac webcam gloriously lit by a <a href=""https://www.benkuhn.net/lux/"">lumenator</a> bounced off the wall in front of me.
            Click for fullsize.</figcaption>
    </figure>
    <p>The two basic rules of lighting are:</p>
    <ol>
        <li>
            <p>Cast lots of diffuse light on your face to make sure it’s brighter than the background. (Also, the more
                light that’s hitting the scene, the less grainy your image will appear.)</p>
            <p>The easiest way to do this is to put your desk in front of a window; second-easiest is to bounce
                artificial lights off of a light-colored surface behind you. If neither of those is enough, you can also
                use a “ring light” or “softbox” (no particular recommendations as I’ve never tried one).</p>
        </li>
        <li>
            <p>Eliminate light sources in the camera’s field of view. Compared to the human eye, cameras have lower
                “dynamic range” (the ability to faithfully capture variation in brightness)—which is why, for instance,
                your phone can’t take good pictures of trees against the sky on a bright day. If there’s a bright window
                behind you, or a light fixture in your camera’s field of view, that part will look “blown out” and
                everything else will look dark by comparison.</p>
        </li>
    </ol>
    <h3 id=""use-your-real-background"">Use your real background</h3>
    <p>Probably controversial. I’m speaking strictly from the point of view of immersiveness here—not e.g. expressing
        your individuality, making your coworkers laugh, or hiding the pile of laundry behind you. Those are all valid
        reasons to want to use backgrounds! Just be aware that you’re sacrificing immersiveness when you do.</p>
    <p>Why? Zoom’s background detection software is not very accurate, and it’ll periodically delete parts of your
        hair/body, make the background show through your eyeballs, etc. Plus, it’s really bad at detecting boundaries so
        some of the real background will show through your hair.</p>
    <p>If you have a decent camera (as below), and a space of your own (see <a href=""#get-away-from-other-people"">get
            away from other people</a> above), you’ll look less distracting and more real if you don’t use a fake
        background.</p>
    <h3 id=""dont-bother-with-webcams"">Don’t bother with webcams</h3>
    <p>It’s probably obvious that laptop webcams suck. Even if they’re not <a href=""https://www.reddit.com/r/Dell/comments/5pkpa7/who_thought_this_is_the_perfect_place_to_put_a/"" target=""_blank"">inexplicably positioned so that they look up your nose</a>, they will be grainy, blurry and
        have a tiny dynamic range.</p>
    <p>Maybe less obviously, external webcams aren’t that much better. This surprised me, since a high-end webcam like
        the <a href=""https://www.logitech.com/en-us/product/brio"" target=""_blank"">Logitech Brio</a> costs 2/3 as much as
        a used interchangeable-lens camera and has <em>one job</em>. (To be clear, the Brio <a href=""https://randsinrepose.com/archives/good-meetings-are-jazz/"" target=""_blank"">looks a lot better</a>
        than most other webcams—just still a lot worse than a real camera.) For instance, I bought a Logitech C920,
        Wirecutter’s pick for “best webcam,” but it wasn’t obviously better than my six-year-old iMac webcam, mostly due
        to <em>really</em> questionable exposure / color balance settings.<sup id=""fnref:3""><a href=""#fn:3"" class=""footnote-ref"" role=""doc-noteref"">3</a></sup></p>
    <figure>
        <div><a class=""img-wrapper"" href=""https://www.benkuhn.net/vc/imac%20cam%20good.jpg"" title=""fullsize"" style=""max-width:50%""><img width=""320"" height=""213"" src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/nc9lzc3qpd6mdzhzuxwp"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/nc9lzc3qpd6mdzhzuxwp 2x, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/y4arr7x2pyepgnpuoiyw""></a><a class=""img-wrapper"" href=""https://www.benkuhn.net/vc/c920.jpg"" title=""fullsize"" style=""max-width:50%""><img width=""320"" height=""213"" src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/pbcpktwu1lgl5632axk3"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/pbcpktwu1lgl5632axk3 2x, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/qyo2ax4hwe66i9teysdw""></a>
        </div>
        <figcaption>Left: 2014 iMac camera; right: Logitech C920. I am pretty white, but I'm not THAT white.
        </figcaption>
    </figure>
    <h3 id=""use-your-smartphone"">Use your smartphone…</h3>
    <p>(I haven’t used a smartphone as a webcam extensively because I jumped straight to using a full camera, so these
        are weakly held. I’ll update as I learn more.)</p>
    <p>I expected webcams to be better than smartphones, because they wouldn’t have much reason to exist otherwise. But
        it turns out I was wrong: webcams do not, in fact, have much reason to exist. Smartphones beat them on sensor
        size and quality of materials. For example, the iPhone 11 camera <a href=""https://www.cultofmac.com/658650/iphone-11-pro-max-production-cost-bill-materials-bom/"" target=""_blank"">costs $73.50 in materials</a>, and has a <a href=""https://photographylife.com/reviews/iphone-11-pro-camera"" target=""_blank"">1/2.55” sensor</a>, while
        the Logitech C920 <em>retails</em> for $80 and has a <a href=""https://www.techtravels.org/modifying-logitech-c920/"" target=""_blank"">1/3” sensor</a>.<sup id=""fnref:4""><a href=""#fn:4"" class=""footnote-ref"" role=""doc-noteref"">4</a></sup> So, if you want an external
        webcam, use a smartphone.</p>
    <figure>
        <div><a class=""img-wrapper"" href=""https://www.benkuhn.net/vc/c920.jpg"" title=""fullsize"" style=""max-width:50%""><img width=""320"" height=""213"" src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/pbcpktwu1lgl5632axk3"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/pbcpktwu1lgl5632axk3 2x, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/qyo2ax4hwe66i9teysdw""></a><a class=""img-wrapper"" href=""https://www.benkuhn.net/vc/camo%20phone.jpg"" title=""fullsize"" style=""max-width:50%""><img width=""320"" height=""213"" src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/mxzs25l4ucmjqkmisyen"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/mxzs25l4ucmjqkmisyen 2x, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/w1vwkwnlu8sn2mgjwhmi""></a>
        </div>
        <figcaption>Left: Logitech C920; right: iPhone XR via Reincubate Camo (crop).</figcaption>
    </figure>
    <p>I briefly tried two apps for using your smartphone as a webcam, <a href=""https://reincubate.com/camo/"" target=""_blank"">Camo</a> and <a href=""https://www.kinoni.com/"" target=""_blank"">EpocCam</a>. EpocCam is
        cheaper ($8 vs $40) but seemed somewhat buggier than Camo (both had occasional issues). Both of them have free
        trials that watermark your video, so suitable for testing but not actual calls.</p>
    <p>The easiest way to mount a smartphone seems to be via a <a href=""https://amzn.to/3hvwyOf"" target=""_blank"">gooseneck holder like this</a>. It seemed like it should be possible to find a much smaller
        device that attached the phone to my monitor, but the best I could find was <a href=""https://amzn.to/2RkLO5E"" target=""_blank"">this clip thing</a>, which obscures part of the screen on laptops, doesn’t adjust in small
        enough increments to get the right field of view, and doesn’t work on external monitors with curved backs like
        my iMac. Let me know if you have a better suggestion.</p>
    <h3 id=""or-a-real-camera"">…or a real camera</h3>
    <p>Even the lowest-end “real” cameras will trounce most smartphones on image quality. You’ll get a much sharper
        image, with a pleasingly blurred background to subtly draw attention toward your face and away from your piles
        of dirty laundry.</p>
    <figure>
        <div><a class=""img-wrapper"" href=""https://www.benkuhn.net/vc/camo%20phone.jpg"" title=""fullsize"" style=""max-width:50%""><img width=""320"" height=""213"" src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/mxzs25l4ucmjqkmisyen"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/mxzs25l4ucmjqkmisyen 2x, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/w1vwkwnlu8sn2mgjwhmi""></a><a class=""img-wrapper"" href=""https://www.benkuhn.net/vc/sony%20good.jpg"" title=""fullsize"" style=""max-width:50%""><img width=""320"" height=""213"" src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/m9s9fkqq67t43dvk9gou"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/m9s9fkqq67t43dvk9gou 2x, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DEe5cvpaTQeF9kkru/q1qrlogu2ezyjnhpxnbk""></a>
        </div>
        <figcaption>Left: iPhone XR via Camo; right: Sony A6000. This one makes more sense at full resolution; the Sony
            has a bit worse dynamic range, but the image is <strong>much</strong> sharper.</figcaption>
    </figure>
    <p>This is probably the most noticeable improvement in the list: I started using a camera when I gave a virtual
        conference talk at <a href=""http://bangbangcon.com/speakers.html#ben-kuhn"" target=""_blank"">!!con</a> and got
        compliments from ~20 different coworkers and conference speakers. (Being this noticeable is not necessarily an
        advantage, but I figured my coworkers already mostly knew that I was extremely vain, and so there was no point
        in trying to hide it.)</p>
    <p>Unfortunately, these cameras are also… pretty user-unfriendly and can take a bit of work to set up. Some
        non-obvious tips if you decide to replicate my setup (Sony A6000 + Elgato CamLink 4k):</p>
    <ul>
        <li>
            <p>Get a non-power-zoom lens, otherwise your zoom setting will get reset every time you turn the camera off
                and on again.</p>
        </li>
        <li>
            <p>If you have a Sony A6000, make sure you turn the top dial to “video” mode. Otherwise the HDMI output will
                be lower quality and the continuous focus won’t work right.</p>
        </li>
        <li>
            <p>Use the widest aperture possible to minimize graininess and get a nice blurry background effect.</p>
        </li>
    </ul>
    <h3 id=""a-note-on-camera-buying"">A note on camera buying</h3>
    <p>I’m not sure which camera model is best now that so many different manufacturers have webcam drivers. Before the
        webcam drivers came out, the Sony A6000 was the default recommendation for a camera for streaming, and it’s also
        generally well-recommended as an entry-level mirrorless camera; but if you have a Mac it currently requires a
        capture card while a Canon/Fujifilm wouldn’t. Then again, Sony is promising to release their Mac webcam driver
        in “Autumn 2020.” Basically, do your own research.</p>
    <p>It’s best to buy from a reputable used camera dealer (e.g. <a href=""https://www.keh.com/"" target=""_blank"">Keh</a>, <a href=""https://www.bhphotovideo.com/"" target=""_blank"">B&amp;H</a>, <a href=""https://www.lensauthority.com/"" target=""_blank"">Lens Authority</a>) since random resellers on Amazon /
        etc. might not be good about checking that the camera works well.</p>
    <p>For completeness, here’s what I bought though I don’t particularly endorse it:</p>
    <ul>
        <li>Sony A6000</li>
        <li>16-50mm ƒ/3.5–5.6 PZ OSS lens (as mentioned above, I’d recommend the non-PZ 18–55mm ƒ/3.5-5.6 OSS instead)
        </li>
        <li>Elgato Camlink 4k (may be out of stock; cheaper options available from no-name vendors)</li>
        <li>HDMI to Micro HDMI cable (took me a surprisingly long time to figure out what type of HDMI port the A6000
            has!)</li>
        <li>Random off-brand USB dummy battery (sometimes gives “battery depleted” errors so maybe not the best idea)
        </li>
    </ul>
    <h2 id=""conclusion"">Conclusion</h2>
    <p>The main thing I learned from this adventure is that, much like <a href=""https://www.benkuhn.net/wireless/"">wireless</a>, video calls
        still basically don’t work. Every piece of equipment has tricky subtleties that make it super hard to select the
        right gear and use it correctly. Software does incredibly stupid things that you’ll never notice unless you know
        what to look for. You never hear your own audio, so if it sucks, you’ll never know. Many of the problems you do
        notice will be near-impossible to debug because there are no good diagnostics.</p>
    <p>If there was a $2000 device that eliminated the need for this post to exist, it would be an automatic purchase
        for my employer and probably every other distributed company. But there isn’t, so here we are.</p>
    <p><em><small>Thanks to <a href=""https://danluu.com/"" target=""_blank"">Dan Luu</a>, <a href=""https://sdll.io/"" target=""_blank"">Sasha Illarionov</a>, David Coletta, <a href=""https://guzey.com/"" target=""_blank"">Alexey Guzey</a>, <a href=""https://www.lincolnquirk.com/"" target=""_blank"">Lincoln
                    Quirk</a>, <a href=""https://evebigaj.com/"" target=""_blank"">Eve Bigaj</a> and Jessica Gambirasi for
                commenting on a draft of this post.</small></em></p>
    <section class=""footnotes"" role=""doc-endnotes"">
        <hr>
        <ol>
            <li id=""fn:1"" role=""doc-endnote"">
                <p>This means the BoomPro requires headphones with a detachable cable. If you’re wedded to a different
                    pair, the <a href=""https://amzn.to/2Zypzhq"" target=""_blank"">Antlion ModMic</a> is a more expensive
                    option that works with any headphones, at the cost of a second cable. There’s also a wireless
                    version, although you shouldn’t use wireless audio equipment for video calls due to latency and
                    quality concerns. <a href=""#fnref:1"" class=""footnote-backref"" role=""doc-backlink"">↩︎</a></p>
            </li>
            <li id=""fn:2"" role=""doc-endnote"">
                <p>It is easier to put a dynamic microphone right next to your mouth, which is sort of like rejecting
                    noise, but not relevant if you want to place your mic outside the frame of a video. Dynamic mics
                    also have less high-frequency and “transient” response, which can make some types of room noise less
                    obtrusive. <a href=""#fnref:2"" class=""footnote-backref"" role=""doc-backlink"">↩︎</a></p>
            </li>
            <li id=""fn:3"" role=""doc-endnote"">
                <p>The C920 does allow you to tune these settings, but it took me about 2 hours to figure out how, and I
                    ended up having to buy a crappy third-party app. Manually tuning the settings would also require you
                    to change them whenever your lighting changes throughout the day, which is pretty annoying. <a href=""#fnref:3"" class=""footnote-backref"" role=""doc-backlink"">↩︎</a></p>
            </li>
            <li id=""fn:4"" role=""doc-endnote"">
                <p>Obviously, if you have a lower-end smartphone, the C920 might have nicer materials than your
                    smartphone. But it seems like there’s a very small region of tradeoff-space where “buy a webcam” is
                    a good idea relative to “buy a nicer smartphone” or “buy an entry-level camera.” <a href=""#fnref:4"" class=""footnote-backref"" role=""doc-backlink"">↩︎</a></p>
            </li>
        </ol>
    </section>
    <div class=""disclaimer"">
        <p><em>Some links in this post are <a href=""https://www.benkuhn.net/aff/"">affiliate links</a>; proceeds go to <a href=""https://www.givewell.org/"">GiveWell</a>.</em></p>
    </div>",benkuhn,benkuhn,benkuhn,
RQ8znEREG5dGsPKLr,A long reply to Ben Garfinkel on Scrutinizing Classic AI Risk Arguments,a-long-reply-to-ben-garfinkel-on-scrutinizing-classic-ai,https://www.lesswrong.com/posts/RQ8znEREG5dGsPKLr/a-long-reply-to-ben-garfinkel-on-scrutinizing-classic-ai,2020-09-27T17:51:31.712Z,17,7,6,False,False,,"<p>In July, Ben Garfinkel scrutinized the classic AI Risk arguments in a 158 minute-long <a href=""https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/"">interview</a> with 80000 hours, which I strongly recommend.</p>
<p>I have formulated a reply, and recorded 80 minutes of video, as part of two presentations in the <a href=""http://AISafety.com"">AISafety.com</a> Reading Group:</p>
<p><a href=""https://youtu.be/_kNvExbheNA"">196. Ben Garfinkel on Scrutinizing Classic AI Risk Arguments</a></p>
<p><a href=""https://youtu.be/j-_FvJ-XbWA"">197. Ben Garfinkel on Scrutinizing Classic AI Risk Arguments 2</a></p>
<p>I strongly recommend turning subtitles on. Also consider increasing the playback speed.</p>
<hr>
<p><em>""I have made this longer than usual because I have not had time to make it shorter.""</em><br>
-Blaise Pascal</p>
<p>The Podcast/Interview format is less well suited for critical text analysis, compared to a formal article or a LessWrong post, for 3 reasons:</p>
<ol>
<li>
<p><strong>Lack of precision.</strong> It is a difficult skill to place each qualifier carefully and deliberately when speaking, and at several points I was uncertain if I was parsing Ben's sentences correctly.</p>
</li>
<li>
<p><strong>Lack of references.</strong> The ""Classic AI Risk Arguments"" are expansive, and critical text analysis require clear pointers to the specific arguments that are being criticized.</p>
</li>
<li>
<p><strong>Expansiveness.</strong> There are a lot of arguments presented, and many of them deserve formal answers. Unfortunately, this is a large task, and I hope you will forgive me for replying in the form of a video.</p>
</li>
</ol>
<p>tl;dw: A number of the arguments Ben Garfinkel criticize are in fact not present in ""Superintelligence"" and ""The AI Foom Debate"". (This summary is incomplete.)</p>
",soren-elverlin-1,soren-elverlin-1,Søren Elverlin,
HzDcLf2LJg4x66fcH,Not all communication is manipulation: Chaperones don't manipulate proteins,not-all-communication-is-manipulation-chaperones-don-t,https://www.lesswrong.com/posts/HzDcLf2LJg4x66fcH/not-all-communication-is-manipulation-chaperones-don-t,2020-09-27T16:45:52.172Z,35,14,14,False,False,,"<p><i>Epistemic status: Original work, explanation of a mental model that I developed for a few years that brings together knowledge from existing fields.</i></p><p>Is all communication manipulation? I hear this sentiment frequently expressed and want to explain in this article that there’s nonmanipulative communication by using protein folding as an intuition pump.&nbsp;</p><p>It is common knowledge within molecular biology that proteins fold into their native state. That native state is the folded shape that possesses a minimum of free energy. Finding global minima is however a hard problem. For bigger proteins, it's at the time of writing - still impossible to calculate the shape.</p><p>Even in vivo protein folding is a hard problem. Cells are densely packed with many different molecules that push against each other. Frequently, resources are wasted when a protein misfolds into a shape that's not its native state.</p><figure style=""width:624px;""><img src=""https://lh5.googleusercontent.com/9FUALHgJOAzv9_sownd0GhrrzGpPTqAYP1V1KxfsibLP35FbXfmT6Op-IiZUovelM4b2eT0gMeB8nInWZ4wDtQG4MxvgojHBGdMqhEXRDxPBhYv9jA1y6_x_UY9Yr94IYFKI_lVO""></figure><p>Nature is clever and developed a way to help proteins fold into their native state. Cells produce chaperones. A chaperone surrounds an unfolded protein to protect it from outside influences to help the protein to fold into its native state. A chaperone doesn't need to know the native state of a protein to help the protein fold into that state. Instead of manipulating the protein like a sculpture, it holds space for a protein to be safe from outside influences, while it folds into its native form.</p><p>This allows a chaperone that works in an uncomplicated way to achieve a result that very complex machine learning algorithms currently don't achieve. The machine learning algorithm tries to figure out the best way for the protein to fold while the chaperone just lets the protein find this way by itself.</p><p>The psychologist Carl Rogers advocated that good psychologists act in the same way <i>nonmanipulative </i>with their patients. In his view, it's not the job of the therapist to solve the problem of their patient by manipulating the patient into a healthy form. A good therapist isn’t like ta sculptor sculpts a sculpture. The job of the therapist is rather to hold a space for the patient in which the patient is safe from certain forces that prevent the patient from finding their healthy authentic <i>native state</i>.&nbsp;&nbsp;</p><p>I don't intend to argue for <i>nonmanipulative communication</i> from a moral perspective. In cases where you know how to fix the problem of the person you are talking with and are confident that the other person will follow your advice, <a href=""https://www.youtube.com/watch?v=Ow0lr63y4Mw""><u>go ahead</u></a>. If you don't know what will help a person, taking a nonmanipulative approach is often more effective than giving the person advice that they have already heard a dozen times.</p><p>If you tell an obese person that they should lose weight <i>again</i>, you add additional stress which can make it harder for them to think about the issue. In the Rogerian model effective change isn't about creating enough pressure by telling the obese to lose weight till they finally get it. For an obese person who feels shame for being obese, it can be hard to clearly think about the issue when they are alone. Providing the person a space where they can speak about their challenges in a way where they aren't feeling judged can help them to make progress for themselves.</p><p>There's a mystic quality to being <i>nonmanipulative</i>. Even Carl Rogers, who proposed the ideal, that all interactions should be nonmanipulative, sometimes fell short of it. For practical purposes it's often more useful to do what makes sense in the moment and what helps the other than to live up to an ideal of being perfectly nonmanipulative.</p><p>On the other hand, having a mental model of what it means to be <i>nonmanipulative </i>can be very helpful to understand communication practices like Rogerian psychotherapy, Gestalt Therapy and Circling.</p><p>I invite you to explore communicating in a way that holds the space for others to find themselves.&nbsp;</p>",ChristianKl,christiankl,ChristianKl,
4nkSD3SJpitfrMBM5,Numeracy neglect - A personal postmortem,numeracy-neglect-a-personal-postmortem,https://www.lesswrong.com/posts/4nkSD3SJpitfrMBM5/numeracy-neglect-a-personal-postmortem,2020-09-27T15:12:45.307Z,81,38,29,False,False,,"<h1>My failed enlightenment</h1><p>I&apos;ve been thinking about my intellectual education, and what I wish had gone differently. </p><p>I am 26 years old. I&#x2019;ve been reading books and going to school since I was 8. This puts my career as a learner at about 19 years. Honestly? I feel a bit disappointed. I&apos;ve had a predominantly &quot;humanistic&quot; education, which is a nice way of saying that my gaps in scientific subjects are embarrassing. Meanwhile, I ended up interacting with people who&apos;ve invested their formative years in getting a solid foundation in mathy and sciency subjects. Inevitably, I found myself envying their skills and wondering where <em>my </em>study time has gone, and what do I have to show for it. </p><p>In particular, I have diagnosed myself with a condition I call <em>numeracy neglect.</em> When I reflect on my education, I find that: (1) I was a bright and precocious kid. (2) I was always very curious and had a strong motivation to understand the world. (3) Despite this, and despite all the resources that society invested in me, I managed to go at least 15 years without learning much about mathematics, physics, chemistry, and computer science (to mention just the basics). </p><p>This contradiction pains me, but it also makes me curious. How does something like this happen?  </p><h1><strong>Numeracy neglect</strong></h1><p>I will focus on mathematics, since it&apos;s a subject that most people are taught, but it&apos;s typically misunderstood and unappreciated. </p><p>Reflecting on my experience, I can identify two problems.</p><p>1. <em>Aesthetic insensitivity</em>. The inability to experience the beauty of mathematics, and to apply one&#x2019;s general curiosity to it.</p><p>2. <em>Epistemic ignorance</em>. The inability to see or accept the fact that mathematics is the language of science, and if you don&apos;t understand mathematics, you won&apos;t understand most science. In general, the inability to understand mathematics&apos; relevance and usefulness in life. </p><p>Another way to put it is that the first is a failure to grasp the intrinsic value of mathematics [1], while the second is a failure to understand its instrumental value. </p><p>How does this apply to my experience?</p><h2>Aesthetic insensitivity</h2><p>I was not born with a natural aversion for mathematics. I remember enjoying arithmetic and geometry in elementary school. Today, I feel a deep curiosity for mathematical subjects; I&#x2019;ve also developed, or perhaps rediscovered, an aesthetic appreciation for mathematical concepts.</p><p>Yet something went amiss in the age 11 to 23. My grades in mathematics, physics and chemistry were low. I felt little curiosity for these subjects and would only study for the tests. I did no better when I started university. In my first year, I showed little desire to understand statistics, and passed the exam with the minimum grade. It was only later that I (slowly) began to wake up.</p><p>Part of it was due to laziness. I was a fast reader and had an excellent memory. This allowed me to excel in most subjects without much work. In contrast, numerate subjects required more dedication and systematic study. </p><p>Perhaps it was also a self-esteem issue. Mathematics is <em>hard</em>. Studying it forces me to confront failure on a regular basis. It&#x2019;s humiliating to constantly fail on the simplest problems. (I recently downloaded the app Brilliant. It makes me feel anything but.) I was typically praised for intelligence rather than effort. Although some studies have <a href=""https://www.tandfonline.com/doi/full/10.1080/01443410.2019.1625306"">challenged the mindset hypothesis</a>, my experience confirms the trope (at least in hindsight). I had a lot of self-esteem invested in my intelligence, and it was much easier to feel brilliant while repeating philosophy, than to face my struggles with logarithms.&#xA0;</p><p>But there was a deeper issue at work. After all, I wasn&#x2019;t lazy when it came to subjects that I cared about. And there were things that I valued more than my self-esteem, such as the need for knowledge.&#xA0;</p><p>Well, I can&#x2019;t quite put my finger on it, but I would say that at that time, mathematics was not <em>really</em> <em>real</em> to me.&#xA0;</p><p>Many students complain that maths is too abstract, too detached from real life. They cannot find enjoyment in it (which is why I speak of &#x2018;aesthetic insensitivity&#x2019;). However, I&apos;m not sure that abstraction is the real problem. (In my case, I spent a lot of energy on philosophy, which can be very abstract.)&#xA0;Rather, the problem may be one of <em>failing to see the referents</em>. </p><p>When I read philosophy, I felt that the concepts written on the page were referring to something <em>real </em>&#x2014; something the words <em>stood for</em>. We could call them &apos;ideas&apos;, or &apos;<a href=""https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace"">objects in ideaspace</a>&apos;. You don&apos;t read philosophy to see how the writer combines words on a page. You read it because you are interested in the ideas that the words point to. If you understand the words, you can explore the ideas, play with them, break them apart or combine them. This makes philosophy enjoyable and even beautiful. </p><p>In contrast, when I was studying mathematics, I wasn&apos;t able to <em>really</em> see the referents. I was blind to the reality of mathematical structures. It seemed like a purely syntactical game: we had numbers and symbols, and were taught how to combine them. Of course, I knew that numbers were &apos;real&apos; in some sense. And I felt that mathematics was generally discovered rather than invented. But I did not get the glorious feeling that I was soaring in ideaspace, stretching my mind to think the unthinkable, and gazing at the fundamental structure of the multiverse. The signifier was there, but the signified was hidden. </p><p>It was programming that opened my eyes. As I started learning Python, I understood <a href=""https://www.lesswrong.com/posts/np3tP49caG4uFLRbS/the-quotation-is-not-the-referent"">the difference between the label and the thing</a>. When coding, one works on two levels: the namespace, which contains the labels for the objects, and the objects themselves. You manipulate objects through their names, but the two levels must be kept apart. As a beginner, I didn&apos;t understand this. If you are new to coding, there might come a moment when you feel the need to access a variable name at the object level. Imagine you are saving the weight of various dogs, so you declare &lt; terrier = 22 &gt;. Then you want to print &lt;&quot;the weight of {terrier} is {22}&quot;&gt;. You can get 22 by calling the variable &lt;terrier&gt;. But how do you print the <em>name</em> of the variable? Now you start looking for a function that takes the label down to the object level, such that the function &lt;get_varname(terrier)&gt; would return &quot;terrier&quot;. This is a bad idea, because you&apos;re mixing up labels and objects, referents and referees. In your code, the variable &lt;terrier&gt; is merely an address for the object &lt;22&gt;. It has nothing to do with the object &lt;&quot;terrier&quot;&gt;, unless you explicitly point it there.</p><p>At some point, I realized that doing maths is not so different. You are manipulating names that refer to objects. It&apos;s true, you cannot touch the objects directly; cannot see them except through their names. Still, the objects <em>exist</em>; it isn&apos;t a purely syntactical game. </p><p>There really <em>is</em> a thing like the number 17. Somewhere out there, in ideaspace. You can call it &quot;17&quot; or &quot;seventeen&quot; or &quot;diecisiete&quot; or &quot;xyz123&quot;. You might be unaware of its existence, but that won&apos;t make it disappear; you may go around saying it isn&apos;t prime, but that won&apos;t alter its primeness one bit. </p><p>When you do maths, you&apos;re not just shuffling symbols on the blackboard. You arrange labels meaningfully, and lo and behold &#x2014; this gives you access to <em>real mathematical objects</em>! You can explore them, play with them, break them apart and combine them. You can discover the number 17! You can prove its primeness! Prove that primes are infinite! And this <em>can</em> be fun. </p><p>To take the coding analogy further, you can imagine a Universal Mathematical Compiler that inputs your notation and translates it into real mathematical operations on real mathematical objects (provided your syntax makes sense). If you understand mathematics, it&apos;s like having a virtual machine in your brain that simulates the operations and returns an actual output. This output is not something you invented, or could have predicted in advance. You send a query to the universe, and the universe answers. It&apos;s like a message coming from the <em>other side</em>. It&apos;s your own private window on the inner workings of the multiverse. </p><p>Is this enough to start feeling that mathematics is beautiful, and to develop a passion for it? Perhaps not. But it should at least get one beyond the point where mathematics feels boring and empty. </p><h2>Epistemic ignorance</h2><p>Even if you don&apos;t like mathematics for its own sake, you should eventually realize that without it you cannot understand science. Donald Knuth said: &quot;Science is what we understand well enough to explain to a computer.&quot; Like many aphorisms, this goes too far; Darwin&apos;s understanding of evolution was &apos;scientific&apos; in my book, although his science was not advanced enough that he could have specified a faithful simulation (for instance, he didn&apos;t know about DNA). However, having a complete mathematical description of a system and being able to predict its behavior and simulate it on a computer, at least theoretically, is probably as far as scientific understanding can take you. </p><p>So why didn&apos;t I study more science?</p><p>If I could meet my eight-year old self, this is what I&apos;d tell him: &quot;You are curious about the world. To understand the world, you need to understand science. To understand science, you need to understand mathematics. Life is short, and the Art is long. Don&apos;t waste your time on dialectical philosophy. Don&apos;t get enmeshed in &apos;critical theory&apos;. Don&apos;t think you&apos;re smart because you read <a href=""https://www.lesswrong.com/posts/ndGYn7ZFiZyernp9f/the-beauty-of-settled-science"">science news</a>. Acquire at least a fundamental grasp of mathematics, then get some <a href=""https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject"">respected textbooks</a> and study the fundamental sciences. Make sure you have a basic knowledge of physics, chemistry and biology, as well as the theory of probability, so that you won&apos;t be completely ignorant of the nature of the universe, and you&apos;ll be less likely to fall prey to supernatural beliefs, psychologisms and <a href=""https://en.wikipedia.org/wiki/Mind_projection_fallacy"">mind-projections</a>. Then you can focus on the disciplines that most interest you.&quot; </p><p>Why did my younger self fail to grasp this? It wasn&apos;t a problem of worldview. Early on, I embraced atheism, the scientific worldview, physical reductionism, the whole package. Yet how great was the mismatch between my professed values and my actual choices! </p><p>ME: &quot;I believe physics describes the fundamental laws of the universe.&quot; </p><p>NOBODY: &quot;So... you&apos;re studying physics?&quot; </p><p>ME: &quot;Gosh, no! I can&apos;t even tell you what thermodynamics is.&quot; </p><p>NOBODY: &quot;Oh. So you don&apos;t care about understanding the universe?&quot; </p><p>ME: &quot;How dare you! Of course I do! I thirst for knowledge, truth and understanding!&quot; </p><p>NOBODY: &quot;So what are you doing to increase your knowledge?&quot; </p><p>ME: &quot;I&apos;m studying Kant. Did you know that space and time are <em>a priori </em>forms of experience?&quot; </p><p>NOBODY: We need to have a talk. </p><p>It all seems a bit absurd today. I have to make an effort of imagination to understand what was going on in my mind. This part is still not clear to me, but for now I can think of three factors.</p><p>The first is <a href=""https://en.wikipedia.org/wiki/Affect_heuristic"">affect heuristic</a>. I didn&apos;t choose which subjects to study based on a ranking of usefulness. Nor was I reflecting on the expected ROI of my study time. I was just going for what felt interesting or titillating or particularly mysterious at any point in time. And if that meant choosing Adorno&apos;s <em>Negative dialectics </em>over sitting down and doing physics problems, damn the world. This point ties in to aesthetic insensitivity: I felt that mathematics was neither exciting nor beautiful (at least for me) so I didn&apos;t take pains to study it. It was much too late when it occurred to me that the way I  <em>feel</em> about a subject has no bearing on its importance or usefulness. </p><p>The second factor is that I had an implicit faith in conceptual, dialectical &apos;knowledge&apos;. The kind of knowledge that makes you feel smart when you say that <a href=""https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password"">light is made out of waves</a>, even though you have no mathematical understanding of what a wave is. I confused true understanding for being able to recite a great number of facts about different subjects.</p><p>The third factor is that I had no practical application for my knowledge. I didn&apos;t make predictions. I didn&apos;t give myself the chance to be mistaken. I didn&apos;t have a mission that forced me to either <em>learn </em>or <em>fail</em>. At the end of the day, all I did with most of my knowledge was think about it verbally and sometimes talk about it with other people. </p><h1>Use computers!</h1><p>If you could change just one thing in how education works today, what would it be? </p><p>I will throw my own suggestion, the most direct and effective I can think of: <em>use computers.</em> </p><p>No, I don&apos;t mean giving the students free tablets so they can watch YouTube videos. I mean putting the computer at the <em>center </em>of your pedagogical system and teaching mathematics and the other exact sciences <em>through it</em>. Currently, the principal medium for doing maths in schools is pen and paper. What if instead people learned theorems and models by reproducing them in code? </p><p>After all, computers are a much more <em>natural </em>medium for doing that. <a href=""https://en.wikipedia.org/wiki/Floating-point_arithmetic"">Despite their limits</a>,  computers can actually simulate and run formal systems, as opposed to... Forlorn students scribbling symbols on their notebooks, trying to make the answer come right?  </p><p>As soon as children can reasonably learn to read and write in natural language, they should be taught the rudiments of programming. This would show them, for starters, that logic and mathematics are <em>really real </em>&#x2014; not mere syntactical games. It would also empower them to grow up as shapers, rather than mere users, of technology. </p><p>Later, you could have them simulate the models of physics, chemistry and biology. They could engage in competitive or cooperative games which reward curiosity and stimulate them to think. You could have them design the games themselves, or send them to gather data and test theories. The possibilities are endless. This would not be a replacement for theory and the classical blackboard exercises. But it would provide a practical, engaging field of application which may awaken at least <em>some students </em>from chronic boredom and apathy (though it may create special difficulties for others). </p><p>Of course, this would require reshaping the whole educational system and turning most teachers into programmers. I didn&apos;t say it was easy, or currently feasible. But neither is it beyond the touch of human capacity, I think. Two centuries ago, only <a href=""https://ourworldindata.org/literacy"">12% of people could read</a>. Today the numbers are basically reversed, with an estimated 14% of the world being illiterate. Yes, some children have serious difficulties with reading, but most can master it to an acceptable degree. </p><p>Will something similar happen with programming? I don&apos;t know; I can only hope. The spread of literacy was accelerated by cheap newspapers and print books. Personal computers have been around for fifty years, but only in the past two decades they became cheap enough to enter most households. And cheap smartphones are even younger. At least today most people know how to <em>use </em>a computer, which is a start. </p><p>Considering the rate at which educational institutions evolve, it might take a few decades before programming becomes a basic subject in most schools. In my opinion, it would be worth it to expend some effort in accelerating the process; the payoffs may be very large. </p><hr class=""dividerBlock""><p>[1] This Quora post provides a nice description of the aesthetic value of mathematics (unfortunately I haven&apos;t been able to find the author): &quot;The Surreal numbers are useful for broadening our minds, filling us with a sense of awe and marvel at what our own minds are capable of and what things exist in our imagination even if they don&apos;t fit in our accidental physical universe.&quot;</p>",vlad.proex,vlad-proex,vlad.proex,
ZiPaJjgfDFETuXCjF,Sander Verhaegh on Quine's Naturalism,sander-verhaegh-on-quine-s-naturalism,https://www.lesswrong.com/posts/ZiPaJjgfDFETuXCjF/sander-verhaegh-on-quine-s-naturalism,2020-09-27T10:58:59.845Z,8,2,0,False,False,http://www.sanderverhaegh.nl/Interview%203AM%20Magazine.pdf,"<p>Quine is famous for his naturalistic stance on philosophy which aligns heavily with the stance of people on Less Wrong. In fact, I've seen him mentioned a few times, but I wanted to learn more about his views. This interview gives a pretty good overview.</p>
",Chris_Leong,chris_leong,Chris_Leong,
Zw5STvhmGNzuQYM5B,The whirlpool of reality,the-whirlpool-of-reality,https://www.lesswrong.com/posts/Zw5STvhmGNzuQYM5B/the-whirlpool-of-reality,2020-09-27T02:36:34.276Z,9,3,2,False,False,,"<p>This post is mostly to share this quote that presents a metaphor describing <a href=""https://www.lesswrong.com/posts/EReZtCsGg2giRTZP3/zen-and-rationality-map-and-territory"">the dance</a> between <a href=""https://www.lesswrong.com/tag/map-and-territory"">map and territory</a> (and the key insight of <a href=""https://www.lesswrong.com/tag/embedded-agency"">embeddedness</a>) that was new to me, so let me share it first:</p><blockquote><p>Consider a whirlpool. We can observe it, we can admire it, and we can talk about it. But we don't readily appreciate that what we glibly call ""the whirlpool"" is not a particular thing. The whirlpool, shifting in the lake or on the river, doesn't hold its form. It becomes shallower; the molecules of water are in continuous change. In no two moments do we find the same configuration of water molecules defining the surface or shape of the whirlpool. Yes we view ""the whirlpool"" as a thing[.]</p><p>We imagine enduring things, even as we think of ""them"" as fleeting. We hold to this notion at the expense of realizing that there's no actual thing there at all.</p><p>[...]</p><p>If you attend very carefully, it becomes apparent that everything is like the whirlpool—not a thing at all, but change itself. If we could speed up our perspective enough, even a mass of stone would appear, over the course of millennia, to change and disappear, just like a whirlpool. (And if we alter our perspective in the opposite way, we'd see the stone's constant molecular and atomic changes.)</p><p>But there's a far more subtle point here. It's only out of convention that I used the terms <i>stone</i> and <i>whirlpool</i>. Because, in fact, there is <i>only</i> change[.] There's no abiding thing called ""stone"" or ""whirlpool"" that changes. There's really no particular thing that's stone or whirlpool or you or me at all.</p></blockquote><p>This is from chapter 40, ""Ice Forming in Fire"", of <a href=""https://www.harpercollins.com/products/buddhism-is-not-what-you-think-steve-hagen?variant=32123164983330""><i>Buddhism Is Not What You Think: Finding Freedom Beyond Beliefs</i></a> by <a href=""https://en.wikipedia.org/wiki/Steve_Hagen"">Steve Hagen</a>. The title of the chapter is from a perhaps less easily relatable <a href=""https://dogeninstitute.wordpress.com/2017/01/29/beyond_thought/"">metaphor from Dogen</a> that Hagen liberally translates as ""What is Reality? An icicle forming in fire"".</p><p>I like the whirlpool metaphor because it makes clear how it is our minds bringing things into existence by finding useful patterns in the soup of reality and reifying those patterns into more persistent-seeming forms. Our minds readily trick us into thinking our maps, models, and ideas of what reality is like are more solid than they actually are. It's a quick and easy jump from observation to reification and then from reification to detachment of the map from the territory, paying more attention to our beliefs about what reality is than reality itself. And then we're quickly off behaving not much better than <a href=""https://www.lesswrong.com/tag/aixi"">AIXI</a>, willing to metaphorically <a href=""https://www.lesswrong.com/tag/anvil-problem"">drop anvils on our heads</a> because we fail to realize <a href=""https://www.lesswrong.com/posts/WJzsTmsDctYCCyMfy/humans-are-embedded-agents-too"">how we're embedded in reality</a>.</p>",gworley,gordon-seidoh-worley,Gordon Seidoh Worley,
hB2vEA4aKin2JBJQv,Blog posts as epistemic trust builders,blog-posts-as-epistemic-trust-builders,https://www.lesswrong.com/posts/hB2vEA4aKin2JBJQv/blog-posts-as-epistemic-trust-builders,2020-09-27T01:47:07.830Z,18,7,7,False,False,,"<p>I've really been enjoying Zvi's weekly posts on the coronavirus. Keeping up with what's going on is something I want to do, but not badly enough to put in the time myself. I'm not even sure how capable I would be even if I wanted to put in the time myself; it seems difficult to sift through all of the information out there.</p><p>Reading Zvi's posts works out perfectly for me though. 20-30 minutes a week and I get just what I need.</p><p>But all of this only works because I trust Zvi.</p><hr><p>Like most people nowadays, I spend a lot of time online. In particular, reading blog posts. LessWrong, Overcoming Bias, Slate Star Codex, Hacker News, FiveThirtyEight, etc. When I need a break, I have my little routine of websites that I click through.</p><p>Sometimes I reflect on how much value I get out of reading all of these blog posts. Nothing against the authors, but when I finish an article, I usually am not left with the feeling that I've gained much. I see it as a numbers game: most of the time I don't gain much, but once in a while I come across something that really influences me.</p><p>But even when I'm not left off feeling particularly inspired by a post, I think that there is something more subtle that I gain by reading it: epistemic trust.</p><p>By reading the same authors over and over again, I start to get a feel for how much I can trust their reasoning ability. The more I trust them, the more I update in response to what they say. And when I reflect on the updates I perform, a surprisingly large proportion of them are of the (rough) form ""I'll take your word for it"".</p><hr><p>The ultimate example of this is probably with respect to AI safety. I think AI safety is a huge deal, but the reason why I think so largely comes from me saying ""I'll take your word for it"". I have a very amateurish understanding of it all and wouldn't really be able to come to the conclusion ""this is by far the most important thing in the world"" via gears level reasoning.</p><p>But fortunately, I have a very high level of epistemic trust for the rationalist community. I've been able to cash in on this trust and update my beliefs about something that is very, very important.</p><hr><p>I want to be careful about what I'm implying here. I'm not trying to imply that epistemic trust building is the <i>main</i> purpose of blog posts. I'm not even trying to be at all precise about how important I think that function is. My point is just that I think it's a function important enough to take note of.</p>",adamzerner,adamzerner,Adam Zerner,
krSHSDYjSrihivmwD,Distributed public goods provision,distributed-public-goods-provision,https://www.lesswrong.com/posts/krSHSDYjSrihivmwD/distributed-public-goods-provision,2020-09-26T21:20:05.352Z,27,10,3,False,False,,"<p>Most people benefit significantly from privately funded public goods (e.g. Wikipedia).</p>
<p>If we all contribute to such public goods, then we can all end up better off. But as an individual it’s almost never a good return on investment. I think of supporting such public goods as being a good citizen, but that leaves open the question <em>what is a good amount to contribute</em>? I can make most decisions by balancing costs and benefits, but I think that basically never leads to making small contributions to public goods in the name of being a “good citizen.”</p>
<p>This post doesn’t aim to answer that full question, but it sets up one simple formal model for the situation and a very natural “wish list” for public goods funding norms. It then proposes the following norm that meets this wish list:</p>
<blockquote><p>My contribution to public good X should be 100 times larger than the amount I’d personally benefit if public good X received 1% more funding.</p></blockquote>
<p>For example, suppose I’m considering how much to contribute to Wikipedia. Maybe I get $1000/year of value from Wikipedia, and I’d get $1000.10 of value if Wikipedia had 1% more funding. This rule says I should contribute $10/year (= 10 cents x 100).<span></span></p>
<p>(In practice I think that labor is often a more important contribution than money, but a similar principle can apply. I’d also prefer that most donors to public goods use something like a <a href=""https://forum.effectivealtruism.org/posts/WvPEitTCM8ueYPeeH/donor-lotteries-demonstration-and-faq"">donor lottery</a> so that they can make a more informed decision about a larger amount of money when they do decide to give. This post is trying to bite off a tiny bit of the conceptual problem.)</p>
<h3>Formal model</h3>
<p>Assume there are n people and k projects. Person i makes a contribution x_ij to public good j. Let y_j = sum x_ij be the total funding for public good j.</p>
<p>Each person has a utility function U_i(y_1, …, y_k) expressing how much they benefit if the public goods are funded at levels y_1, …, y_k. The total utility for person i is U_i(y_1, …, y_k) – sum_j x_ij.</p>
<p>For simplicity I’ll assume that everyone’s utility is monotone in public goods: I’m never <em>unhappy</em> when one of the projects receives more funding.</p>
<p>I’ll also assume that the utility functions U are convex: increases in funding only ever <em>decrease</em> the marginal value of more funding. Convexity is often realistic and it makes it much easier to prove theorems or do computations, so it seems like a good starting point. Relaxing it seems important but not something I’m going to get into here.</p>
<p>I’m also assuming that everyone has linear utility in money which is only realistic if the amounts involved are very small. When utility is linear it is natural to measure utility in $ and compare it freely across people. Relaxing this assumption (along with the associated normative assumptions) is a whole can of worms that’s also important but I’m also not going to get into here.</p>
<p>In this setting there is a single “optimal” funding for public goods, namely the allocation that maximizes the convex aggregate utility function U_1(y_1, …, y_k) + … + U_n(y_1, …, y_k) – (y_1 +… + y_k). This leaves unspecified <em>who</em> provides the funding.</p>
<h3>Wishlist</h3>
<p>By “norm” I mean a rule that individuals can use for deciding how much to fund each public good.</p>
<p>Here are two plausible desiderata for a norm:</p>
<ul>
<li>If everyone always follows the norm, then we end up with the optimal levels of funding for the public goods.</li>
<li>If you start with a community that follows the norm and add a bunch of new people who behave manipulatively, they can never make the original community worse off.</li>
</ul>
<p>These are not the conventional desiderata in mechanism design—they say nothing at all about incentives.</p>
<p>All these desiderata say is that <em>if</em> a bunch of people choose to follow this norm, <em>then</em> they will do relatively well (even if many people try to manipulate their generosity). You can probably do even better by defecting. Following the norm in this post would be a kind of act of generosity. It’s also not verifiable so it can’t be enforced in any straightforward way.</p>
<p>Overall it’s not at all clear that you want a norm with these properties (since they can be in tension with other desirable properties like incentive compatibility), but I do think there’s a plausible case to use such norms as a kind of “cooperative default.”</p>
<p>Participating in this kind of cooperative norm involves being generous along an orthogonal axis to utilitarianism. Ten utilitarians with big enough irreconcilable differences would find themselves in exactly the same bind as 10 selfish people unwilling to fund public goods that would benefit the group.</p>
<h3>The norm</h3>
<p>I’ll first describe a centralized implementation of funding decisions since it’s easier to analyze. In the next section i’ll describe centralizing it.</p>
<p>Everyone publishes their utility function U_i. Choose the socially optimal level of funding y_j given those utility functions. Person j contributes y_j * dU_i/dy_j to public good j. In light of the following lemma, this is equivalent to the norm quoted at the top of the post:</p>
<p><strong>Lemma [efficiency]</strong>: This norm balances the budget, i.e. y_j is in fact equal to the sum of x_ij.</p>
<p><strong>Proof</strong>: at optimal funding levels, the sum of DU_i/dy_j must be equal to 1 for every j, otherwise we could improve utility by either increasing or decreasing the level of funding. So sum x_ij = sum y_j * dU_i/dy_j = y_j * sum dU_i/dy_j = y_j * 1 = y_j.</p>
<p><strong>Lemma [robustness]</strong>: adding new people never harms an honest community who follows this norm.</p>
<p><strong>Proof</strong>: let x_ij and y_j be the funding levels before adding new people and x_ij*, y_j* be the levels after adding people.</p>
<p>Adding new dishonest people doesn’t change the utility functions of any of the honest people, it just adds new (monotone) terms U_i to the sum that we are maximizing. This results in increasing the marginal value of every public good, which results in increasing the level of funding y_j* for each public good, which results (by convexity) in decreasing every derivative dU_i/dy_j. I’ll write dU_i/dy_j* for the derivative of U evaluated at y* and dU_i/dy_j for the derivative evaluated at y.</p>
<p>We’ll show that the increase in utility U_i(y*) – U_i(y) is always greater than the increase in person i’s payment:</p>
<ul>
<li>By convexity, U_i(y*) – U_i(y) is at least sum_j (y_j* – y_j) dU_i/dy_j*</li>
<li>Since derivatives are decreasing and y_j &gt;= 0, this is at least sum_j ( y_j* dU_i / dy_j*) – (y_j dU_i / dy_j)</li>
<li>By definition, this is equal to sum_j x_ij* – x_ij, i.e. the total amount that we increased person i’s payment.</li>
</ul>
<h3>Relaxing assumptions</h3>
<p><strong>Centralization</strong>. To implement this norm need to find the unique allocation that satisfies x_ij = y_j * dU_i/dy_j. We could do this in an iterative process where everyone iteratively adjusts their contributions x_ij until we get to equilibrium, basically implementing a distributed convex optimization algorithm—we don’t actually need to have anyone ever write down their utility function or have any communication beyond calculating the combined funding for each public good y_j = sum_i x_ij.</p>
<p><strong>Linear utility</strong>. In reality my value for money increases as I spend more of it. I think the basic approach in this post still works, and the iterative algorithm is basically unchanged as long as you just convert between $ and utility using your current preferences at each step. The guarantee becomes more complex because it’s no longer so simple to talk about a social optimum. The best we can say is that we’ll get something Pareto efficient (even including the possibility of transfers).</p>
<p><strong>Convexity</strong>. If utility is non-convex then an iterative approach won’t work and you certainly need to have some kind of coordination to get to an efficient allocation (you also need to solve a potentially intractable optimization problem). I believe the norm in this post may also become vulnerable to manipulation but I haven’t checked for an example. It’s plausible to me that my two desiderata are unachievable when utility is non-convex. (They are obviously unachievable when utility is non-monotone.)</p>
<p><strong>Uniqueness?</strong> It seems like everything had to work out surprisingly nicely for this norm to achieve the two desiderata. I’m not sure whether there are many other norms that would work or if this one is essentially unique.</p>",paulfchristiano,paulfchristiano,paulfchristiano,
KafZRToDv9a5Ehi9F,Notebook for generating forecasting bets,notebook-for-generating-forecasting-bets,https://www.lesswrong.com/posts/KafZRToDv9a5Ehi9F/notebook-for-generating-forecasting-bets,2020-09-26T20:36:05.878Z,25,13,0,False,False,,"<p>We made a <a href=""https://colab.research.google.com/drive/1YfCIhOJ6v0BcX-RB47ziJAvXj6lkr2Ay?usp=sharing&amp;utm_source=lw&amp;utm_medium=colab&amp;utm_campaign=lwbettingcolab"">colab notebook</a> that lets you generate a bet from two people's <a href=""https://elicit.ought.org/"">Elicit</a> distributions. You can edit the notebook to generate your bet (the changes won't be saved). Here's an example of a suggested bet between Ben Pace and SDM on AI timelines:</p><h2>Comparison of predictions:</h2><p><a href=""https://elicit.ought.org/builder/xul7srb37"">Snapshot link</a></p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_230 230w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_460 460w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_690 690w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_920 920w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_1150 1150w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_1380 1380w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_1610 1610w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_1840 1840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_2070 2070w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ea00123107ad2f6e1cbc99bff7f18fc43f9e9e12b4d5e65.png/w_2246 2246w""></figure><p>&nbsp;</p><h2>What the notebook outputs:</h2><p><strong>If the event occurs between 24 Oct 2051 and 17 Sep 2059, Ben Pace should pay SDM $87. Otherwise, SDM should pay Ben Pace $13.&nbsp;</strong></p><p><strong>Disagreement: </strong>You disagree most between 24 Oct 2051 and 17 Sep 2059.&nbsp;</p><p><strong>Probabilities: </strong>Ben Pace thinks the probability the event occurs in this range is 8%, while SDM thinks it's 18%.&nbsp;</p><p><strong>Odds: </strong>You should make a bet with 87:13 odds.&nbsp;</p><p><strong>Expected Value: </strong>Given your beliefs, you each win $5 in expectation</p><p>&nbsp;</p><h2>What this is actually doing:</h2><ul><li>We search across the question range to find the interval (with a width of 10% of the total range) where your probabilities differ the most, including the probabilities outside of the bounds</li><li>We determine odds that create equal and positive expected value for each person using the method outlined in <a href=""https://statmodeling.stat.columbia.edu/2010/07/10/creating_a_good/"">this blog post</a></li></ul>",Amandango,amandango,Amandango,
evDZoYG4p6ZkQQkDw,Surviving Petrov Day,surviving-petrov-day,https://www.lesswrong.com/posts/evDZoYG4p6ZkQQkDw/surviving-petrov-day,2020-09-26T16:40:03.169Z,35,22,8,False,False,,"<p>Sorry, but I can't let this metaphorical world be destroyed.</p>
<p>In <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020"">Honoring Petrov Day on LessWrong, in 2020</a>, <a href=""https://www.lesswrong.com/users/benito"">Ben Pace</a> explains that during Petrov's Day (ie. today), 270 people will have received an email giving them a launch code they could use to bring down LessWrong for a day. Here's how the website looked like:</p>
<p><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_1800"" alt=""""></p>
<p>At 7:37 EST, Neel Nanda posted that the site was already down.</p>
<p><img src=""https://i.imgur.com/qT7XWG8.jpg"" alt=""""></p>
<p>Chris Leong writes:</p>
<blockquote>
<p>Sorry, I got tricked:</p>
<p>petrov_day_admin_account September 26, 2020 11:26 AM Hello Chris_Leong,</p>
<p>You are part of a smaller group of 30 users who has been selected for the second part of this experiment. In order for the website not to go down, at least 5 of these selected users must enter their codes within 30 minutes of receiving this message, and at least 20 of these users must enter their codes within 6 hours of receiving the message. To keep the site up, please enter your codes as soon as possible. You will be asked to complete a short survey afterwards.</p>
</blockquote>
<p>We failed at preventing the launch, but it's not over.</p>
<h1>Stopping the threat</h1>
<p>There's an image preventing access to the <a href=""https://lesswrong.com"">main page</a>. To remove it, create a bookmark with any name, and put as the bookmarked URL:</p>
<blockquote>
<p>javascript:var element = document.getElementsByClassName(""PetrovDayLossScreen-root"")[0]; element.parentNode.removeChild(element);</p>
</blockquote>
<p>Then go on <a href=""https://lesswrong.com"">https://lesswrong.com</a> and click the bookmark to remove the image.</p>
<p><em>Metaphor for <a href=""https://en.wikipedia.org/wiki/Anti-ballistic_missile"">anti-ballistic missiles</a>.</em></p>
<h1>Living through the winter</h1>
<p>Here are alternative places where LessWrongers gather:</p>
<ul>
<li><a href=""https://forum.effectivealtruism.org/"">Effective Altruism Forum</a></li>
<li><a href=""https://www.alignmentforum.org/"">AI Alignment Forum</a></li>
<li><a href=""https://www.facebook.com/EffectiveGroups/"">Effective Altruism Group Directory</a></li>
<li><a href=""https://www.reddit.com/r/LessWrong/"">LessWrong</a></li>
<li><a href=""https://www.reddit.com/r/TheMotte/"">TheMotte</a></li>
<li><a href=""https://forum.radicalxchange.org/"">RadicalxChange Forum</a></li>
</ul>
<p>I would say <a href=""https://slatestarcodex.com/"">SlateStarCodex</a>, but it already got its real world Petrov Day caused by the New York Times. People from SlateStarCodex now hangout at <a href=""https://www.reddit.com/r/slatestarcodex/"">https://www.reddit.com/r/slatestarcodex/</a>.</p>
<p><em>Metaphor for <a href=""https://allfed.info/"">ALLFED</a> researching food security.</em></p>
<h1>Recovering from a backup</h1>
<p>Use <a href=""https://www.greaterwrong.com/"">https://www.greaterwrong.com/</a>, a mirror of LessWrong. If this also gets down, use the <a href=""https://archive.org/"">Wayback Machine</a> to retrieve URLs from the GreaterWrong. If it also gets down, use <a href=""https://www.webcitation.org/"">WebCitation</a>. If it also gets down, email me at <a href=""mailto:contact@matiroy.com"">contact@matiroy.com</a> for a local backup (if this post gets taken down, I'll post it on <a href=""https://matiroy.com"">https://matiroy.com</a>). If I get offline, ask <a href=""https://www.lesswrong.com/posts/mMgiGy6i55iZbMzyx/rationalist-sites-worth-archiving"">other rationalists</a> that have made a local backup.</p>
<p>Here's how Gwern archives websites: <a href=""https://www.gwern.net/Archiving-URLs"">https://www.gwern.net/Archiving-URLs</a>. I used to use <a href=""http://www.webcitation.org/comb"">http://www.webcitation.org/comb</a> to massively archive a list of scraped article URLs, but I'm not sure if it still works.</p>
<p>You can also just go to the <a href=""https://www.lesswrong.com/allPosts"">All Posts</a> tab.</p>
<p><em>Metaphor for: a backup on Mars (h/t <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020?commentId=7DQFZQL8xkdw99xpb"">Vanessa Kosoy</a>) and a <a href=""https://www.sciencedirect.com/science/article/abs/pii/S009457651830119X"">moon-based backup</a></em></p>
<p><em>And making your own back-up is a metaphor for <a href=""https://www.facebook.com/groups/rationalpreppers/"">prepping</a>.</em></p>
<h1>Managing information flow</h1>
<p>It seems hard to avoid admins seeing this post today, but we can still try. The LessWrong team is: Ben Pace, Jim Babcock, Oliver Habryka, Raymond Arnold, Ruby Bloom. When sharing on Facebook, choose the sharing option ""Friends except..."". (EtA: Raymond said it wasn't them that sent the email, so nevermind that part.)</p>
<p>But overall, better to overshare this post than undershare it.</p>
<p>Most people failing to go on LessWrong will be redirected to the Petrov Day post, so upvote <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020?commentId=LHNmdkDrdAbumMPRt"">my comment</a> there so it gets at the top for people to see.</p>
<p><em>Metaphor for: <a href=""https://concepts.effectivealtruism.org/concepts/information-hazards/"">Information Hazards</a>.</em></p>
<h1>Improving the incentives</h1>
<p>Increase the social penalty for pressing the button.</p>
<p><em>Metaphor for: <a href=""https://causeprioritization.org/Moral_economics"">Moral economics</a>.</em></p>
<p>For the in-person events, I don't re-invite button-pressers to future Petrov ceremonies. That's something I had decided after it happened. Similarly, I will also decide after the fact here.</p>
<p>--</p>
<p>I'll set a reminder to prepare more for next year ^_^</p>
",MathieuRoy,mathieuroy,Mati_Roy,
rZJvrf2DzsXcqnp6t,Petrov Day is not about unilateral action,petrov-day-is-not-about-unilateral-action,https://www.lesswrong.com/posts/rZJvrf2DzsXcqnp6t/petrov-day-is-not-about-unilateral-action,2020-09-26T13:41:02.481Z,25,15,0,False,False,,"<p>It's about the dangers of unrestrained escalation. It's a failure of humanity that we even got to the point where a few people's small and difficult decisions decided the fate of so much. This post is a friendly reminder to...</p><ul><li>Please avoid escalating socio-economic disagreements to nuclear levels.</li><li>Please avoid escalating market competitions to Moloch levels.</li><li>Please avoid escalating paperclippers to superintelligent levels.</li></ul><p>Learn to recognize signs of unrestrained escalation. You may be participating in unrestrained escalation...</p><ul><li>If your information landscape becomes increasingly narrowed by a centralized source or by recommender algorithms.</li><li>If you attempt to fight over-escalated issues in a way that encourages opposing parties to fight back. For example, if you try to overtly starve the object of escalation of its resources and ability to act.</li><li>If your actions become increasingly coordinated with large and powerful groups.</li><li>If your judgment becomes increasingly dictated by stories told from a single point of view or single optimization criterion.</li></ul><p>When you do recognize signs of unrestrained escalation, please take actions to de-escalate. You may do this...</p><ul><li>By learning and teaching others to recognize unrestrained escalations.</li><li>By refusing to participate in group actions that encourage further escalation.</li><li>By starving the participants of their motivation (rather than of their resources and ability to act).</li><li>By using every escalation as a teaching opportunity for how to recognize and reduce future escalations.</li></ul><p>And, of course, when considering whether a contentious situation warrants escalation, please do not divorce your opinions from the nuances of the current situation. In high-energy, high-pressure scenarios, small-scale effects often have large-scale consequences, and dually large-scale consequences are often driven by small-scale effects.</p><p>Thank you for your attention. Escalate responsibly.</p>",sen,sen,sen,
xSqM8E2Q7HqKx8nMi,What is complexity science? (Not computational complexity theory) How useful is it? What areas is it related to?,what-is-complexity-science-not-computational-complexity,https://www.lesswrong.com/posts/xSqM8E2Q7HqKx8nMi/what-is-complexity-science-not-computational-complexity,2020-09-26T09:15:50.446Z,7,6,11,False,True,,"<p>I've been reading ""Playing with movement - Hargrove 2019"". This book told me about ""complexity science"", which is a field which supposedly studies complex systems and it does it not by reductionism, but by looking at the system as a whole. It seems that some key concepts used in complexity science are: complex system, emergence, adaptivity, nonlinearity, self-organization, constraints, attractors, feedback loops. This book pitched an idea that <em>with many complex adaptive systems, if you want the system to achieve a certain goal, it's bad to specify a specific plan for it and instead it's better to specify or build constraints under which the system, being adaptive and all, will achieve the goal on its own, supposedly in a more optimal way</em>. Examples:</p>
<ul>
<li>System: my body. Goal: get healthy by physical exercise. Specific plan: do strength training by doing specified sets of reps, using specific instructions on how to position my body. Alternative (constraints): live in such an environment that a lot of physical activity will happen naturally.</li>
<li>System: economics of a country. Goal: make it productive. Specific plan: central planning of what to produce, in what quantities and how. Alternative: free market economy, the organizations will behave however they want, and hopefully the whole system will act in an optimal way.</li>
<li>System: my body. Goal: eat healthily. Specific plan: write down a schedule of specific meals for the next year and eat them. A potential problem is I might miss something and that might lead to a lack of some nutrients. If instead I eat whatever I want, I will probably crave those other nutrients and hence I'll go and eat them. Obviously, this has downsides.</li>
</ul>
<p>This idea seems plausible and very important. However, I've never heard of complexity science before. I've been following all kinds of links about complexity science trying to figure out what it is, what fields it's related to and what subareas it has. Also, I want to know how much of it is correct and how to apply it in the real world. Please help me figure out these questions. So, I guess I want a primer. Except I know that if I find a primer written by a complexity scientist, it'll claim that complexity science is the greatest invention ever. Instead, I want a sceptical primer which specifies the domain of applicability of complexity science and the domain of applicability of theory of complex adaptive systems. Below, I list some more specific questions.</p>
<ul>
<li>It seems complexity science is also sometimes called complexity theory (different from computational complexity theory) and is somehow related to systems theory and complex systems science. Systems theory is supposedly a synonym for cybernetics. And maybe systems theory is related to systems thinking. How are all these areas related and to which does the study of complex adaptive systems belong?</li>
<li>Complexity science is somehow related to a bunch of math areas: chaos theory, differential equations, fractals, and cellular automata. But at the same time, it seems related to sociology, the study of human organizations, and has some applications to medicine and biology.</li>
<li>I want to read or study something to better understand the idea described earlier. I want to understand when that idea is true and when it's false. Any recommendations or thoughts? Is this idea also studied in other scientific fields?</li>
<li>In general, how awesome is complexity science? Is it almost entirely correct like math or physics? Or is it full of incorrect information like psychology and other social sciences? It is at least not 100% crackpottery, since some books are published by Princeton university press and Oxford university press.</li>
<li>Complexity science seems useful for rationality. Why isn't it popular on Lesswrong?</li>
</ul>
<p>And here are some links and sources I found about complexity science:</p>
<ul>
<li>Wikipedia pages for <a href=""https://en.wikipedia.org/wiki/Systems_theory"">Systems theory</a>, <a href=""https://en.wikipedia.org/wiki/Complex_system"">complex system</a>, <a href=""https://en.wikipedia.org/wiki/Complex_adaptive_system"">complex adaptive system</a>.</li>
<li>Some courses/primers/introductions: <a href=""https://www.classcentral.com/provider/ce"">complexity explorer courses</a>, <a href=""http://gen.lib.rus.ec/book/index.php?md5=F7756C4A4F8878CCEB9FC03AE1C687C5"">Simply complexity - a clear guide to complexity theory - Johnson 2011</a>, <a href=""https://complexityexplained.github.io/"">https://complexityexplained.github.io/</a>, <a href=""https://libgen.lc/ads.php?md5=0ef662692369146779127bcf471a12a0&amp;key=6100"">Complex Adaptive Systems: An Introduction to Computational Models of Social Life - Miller, Page 2007</a>, <a href=""https://press.princeton.edu/series/primers-in-complex-systems"">Princeton university press - primers in complex systems</a>, Understanding complexity by Scott E. Page - the book contains lectures and is published by Princeton university press (not available on libgen, available as <a href=""magnet:?xt=urn:btih:fd683f719e6439d4ea1935e978552fb60bd58562&amp;dn=TTC%20VIDEO%20-%20Understanding%20Complexity&amp;tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&amp;tr=udp%3A%2F%2F9.rarbg.to%3A2920%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&amp;tr=udp%3A%2F%2Ftracker.internetwarriors.net%3A1337%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.pirateparty.gr%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.cyberia.is%3A6969%2Fannounce"">videolectures</a>, <a href=""http://gen.lib.rus.ec/book/index.php?md5=43A2E54F1C5F5BDD4C50AE07ACCF19AD"">Thinking in systems: a primer - Meadows 2008</a> - I am not sure if it's about complexity science or systems theory or what, <a href=""http://gen.lib.rus.ec/book/index.php?md5=97330E8F0F60737D46108E5103C6F00E"">Complexity: a guided tour - Mitchell 2011</a> published by Oxford university press.</li>
<li><a href=""https://en.wikipedia.org/wiki/Complex_Systems_(journal)"">Complex systems peer reviewed journal founded by Wolfram</a></li>
<li><a href=""https://www.lesswrong.com/posts/DshBToGnNbTBD7BSw/systems-theory-terms"">A list of systems theory terms on LW</a></li>
</ul>
",crabman,philip_b,philip_b,
XfHXQPPKNY8BXkn72,"Honoring Petrov Day on LessWrong, in 2020",honoring-petrov-day-on-lesswrong-in-2020,https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020,2020-09-26T08:01:36.838Z,117,45,100,False,False,,"<figure style=""width:100%;""><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_360 360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_720 720w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_1080 1080w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_1440 1440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_1800 1800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_2160 2160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_2520 2520w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_2880 2880w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_3240 3240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/799ef6e81dcb17e4bb8b64afc197b94e927f4323fc1a203a.png/w_3584 3584w""><figcaption>The Petrov Day Red Button.</figcaption></figure><p>Just after midnight last night, 270 LessWrong users received the following email.</p><blockquote><p><i>Subject Line: <strong>Honoring Petrov Day: I am trusting you with the launch codes</strong></i></p><p>Hello {username},</p><p>On Petrov Day, we celebrate and practice not destroying the world.</p><p>It's difficult to know who can be trusted, but today I have selected a group of (270) LessWrong users who I think I can rely on in this way. You've all been given the opportunity to not destroy LessWrong.</p><p>This Petrov Day, if you, {username}, enter the launch codes below on LessWrong, the Frontpage will go down for 24 hours, removing a resource thousands of people view every day. Each entrusted user has personalised launch codes, so that it will be clear who nuked the site.&nbsp;</p><p>Your personalised codes are: {codes}</p><p>I hope to see you in the dawn of tomorrow, with our honor still intact.</p><p>–Ben Pace &amp; the LessWrong Team</p><p>P.S. Here is the <a href=""https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020"">on-site announcement</a>.</p></blockquote><h2>Not Destroying the World</h2><p>Stanislav Petrov once chose not to destroy the world.</p><p>As a Lieutenant Colonel of the Soviet Army, Petrov manned the system built to detect whether the US government had fired nuclear weapons on Russia. On September 26th, 1983, the system reported five incoming missiles. Petrov’s job was to report this as an attack to his superiors, who would launch a retaliative nuclear response. But instead, contrary to the evidence the systems were giving him, he called it in as a false alarm, for he did not wish to instigate nuclear armageddon. (He later turned out to be correct.)</p><p>During the Cold War, many other people had the ability to end the world – presidents, generals, commanders of nuclear subs from many countries, and so on. Fortunately, none of them did. As humanity progresses, the number of people with the ability to end the world increases, and so too does the standard to which we must hold ourselves. We lived up to our responsibilities in the cold war, but barely. (The Global Catastrophic Risks Institute has compiled this <a href=""https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3137081"">list of 60 close calls</a>.)</p><p>In 2007, <a href=""https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day"">Eliezer named September 26th Petrov Day</a>, and the rationality community has celebrated the holiday ever since. We celebrate Petrov's decision, and we ourselves practice not destroying things, even if it is pleasantly simple to do so.</p><h2>The Big Red Button</h2><p>Raymond Arnold has <a href=""https://www.lesswrong.com/posts/XJxwFMSL5TPN2usC6/modes-of-petrov-day"">suggested many ways</a> of observing Petrov Day.&nbsp;</p><p>You can discuss it with your friends.</p><p>You can hold a quiet, dignified ceremony with candles and <a href=""http://petrovday.com/"">the beautiful booklets</a> Jim Babcock created.</p><p>And you can also play on hard mode: <i>""During said ceremony, unveil a large red button. If anybody presses the button, the ceremony is over. Go home. Do not speak.""</i></p><p>This has been a common practice at Petrov Day celebrations in Oxford, Boston, Berkeley, New York, and in other rationalist communities. It is often done with pairs of celebrations, each whose red button (when pressed) brings an end to the partner celebration.</p><p>So for the <a href=""https://www.lesswrong.com/posts/vvzfFcbmKgEsDBRHh/honoring-petrov-day-on-lesswrong-in-2019"">second year</a>, at midnight, I emailed personalized launch codes to 270 LessWrong users. This is over twice the number of users I sent codes to <a href=""https://www.lesswrong.com/posts/vvzfFcbmKgEsDBRHh/honoring-petrov-day-on-lesswrong-in-2019"">last year</a> (which was 125), and includes a lot more users who use a pseudonym and who I've never met. If any users do submit a set of launch codes, then (once the site is back up) we'll publish their username, and whose unique launch codes they were.&nbsp;</p><p>During Saturday 26th September (midnight to midnight Pacific Time), we will practice the skill of sitting together and not pressing harmful buttons.</p><h2>Relating to the End of Humanity</h2><p>Humanity could have gone extinct many times.&nbsp;</p><p>Petrov Day is a celebration of the world not ending. It's a day where we come together to think about how one man in particular saved the world. We reflect on the ways in which our civilization is fragile and could have ended already, we feel grateful that it has not, and we ask ourselves how we could also save the world.</p><p>If you would like to participate in the tradition of Petrov Day on LessWrong this year, and if you feel up to talking directly about it, then you're invited to write a comment and share your own feelings about humanity, extinction, and how you relate to it. There's a few prompts below to help you figure out what to say. Note that not all people are in a position in their lives to focus on preventing an existential catastrophe.</p><ol><li><strong>What's </strong><a href=""https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect""><strong>at stake</strong></a><strong> for you?</strong> <i>What are the things you're grateful for, and that you look forward to? What are the things you'd mourn if humanity perished?</i></li><li><strong>How do you relate to the extinction of humanity?</strong> <i>What’s your story of coming to engage with the fragility of a world </i><a href=""https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god""><i>beyond the reach of god</i></a><i>, and how do you connect to it emotionally?</i></li><li><strong>Are you taking actions to</strong><a href=""https://www.lesswrong.com/posts/F2DZXsMdhGyX4FPAd/on-saving-the-world""><strong> protect it</strong></a><strong>?</strong> <i>What are you taking responsibility for in the world, and in what ways are you </i><a href=""https://www.lesswrong.com/tag/heroic-responsibility""><i>taking responsibility</i></a><i> for the future?</i></li></ol><p>Finally, if you’d like to participate in a Petrov Day Ceremony today, check out Ray’s <a href=""https://www.lesswrong.com/posts/JtG3y8GBC2dTot339/petrov-event-roundup-2020"">Petrov event roundup</a>, especially the online New York mega-meetup.</p><p>To all, I wish you a safe and stable Petrov Day.</p>",Benito,benito,Ben Pace,
Byh69WmkzJduDdnqr,[Link] Where did you get that idea in the first place? | Meaningness,link-where-did-you-get-that-idea-in-the-first-place-or,https://www.lesswrong.com/posts/Byh69WmkzJduDdnqr/link-where-did-you-get-that-idea-in-the-first-place-or,2020-09-25T15:38:00.092Z,7,4,4,False,False,,"<p><a href=""https://meaningness.com/eggplant/no-new-ideas"">Where did you get that idea in the first place? | Meaningness</a></p>
",Kenny,kenny,Kenny,
royfAunky4feDsz4G,Up close and personal with the world,up-close-and-personal-with-the-world,https://www.lesswrong.com/posts/royfAunky4feDsz4G/up-close-and-personal-with-the-world,2020-09-25T06:52:28.302Z,12,10,2,False,False,,"<p>Explicitly building models of the world is often done through understanding the mechanics of how things work, the relations between them, causal effects and more. But I don't put enough emphasis on <em>quantifying</em>. I'm not talking about doing Fermi estimates because I do them and my rationalist(-adjacent) friends do them. There is a difference between Fermi-estimating quantities and knowing them from heart. I think there is value in the latter.</p>
<p>It is good to know, on an intuitive level, how a force of 100 newtons <em>feels</em> like. To have an intuitive grasp of how much a volt is, and how some number of amperes feels like. Take acceleration and velocity, for example. You have a very intuitive grasp of them because of the experience of driving and looking at the speedometer. The idea is to expand the same intuition to other units of measurement. You want to <em>feel</em> what the numbers mean. When you do this, you get very up close and personal to your model of the world. Things are not abstract, black boxes anymore.</p>
<p>Tech-savvy dads were right all along when they were impressed by some specific number related to fuel consumption, or torque, or clamping pressure, or whatever, and I failed for not being interested in what seemed like random numbers and units. I understood the mechanism and the underlying idea, wasn't that enough? But it wasn't, because I was not fluent, I did not <em>own</em> the model in my head. It didn't yet get that <a href=""https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why"">quality of ordinariness</a> that I now seek.</p>
",dominicq,dominicq,dominicq,
wfbgihbRzieD73d5p,Human Biases that Obscure AI Progress,human-biases-that-obscure-ai-progress,https://www.lesswrong.com/posts/wfbgihbRzieD73d5p/human-biases-that-obscure-ai-progress,2020-09-25T00:24:45.451Z,46,23,2,False,False,,"<p>There are some common biases that are often used to discount AI progress. We should keep these in mind, as they can prevent us from having an objective understanding of progress in the field.</p><p>I'm going to use AI here instead of ML because usually these biases are relevant to any AI technique, not just ML. But in practice, ML is usually what I'm referring to.</p><p><strong>1. AI uses the easiest solution it can find.&nbsp;</strong>Often people argue that a system isn't intelligent because it did some task in a simpler way than humans would have done it. This is especially applicable if there is some simple heuristic the AI found that did well enough. If finding the word ""death"" in a sentence is sufficient to do perfect classification, the AI will probably only learn to do that, no matter how intelligent it is. If your test set includes a case where that rule isn't sufficient, but the training set does not, the AI will probably fail on the test set case, because it has no way of knowing that ""looking for the word death"" wasn't the rule you wanted. AI will only do more complicated things once that simple thing is no longer sufficient, and it'll keep around the dumb heuristics whenever they work.</p><p><strong>2. AI is different than us.&nbsp;</strong>Sometimes people argue that a system isn't intelligent because it does something in a more complicated way than is necessary (or just in a different way than what humans would have done, but not necessarily more or less complicated). Just because a specific rule is intuitive to us humans doesn't mean it's the easiest for an AI to find. A more complicated rule that also works and is easier for the AI to find will be found and used, no matter how intelligent an AI is. Regularization could in theory help with this, but only if regularization pushes towards the natural ""human"" approach, and only if the researchers continue training it after it's performance is very good. It's also not guaranteed that the rules that seem sensible to us are as good as what the AI does, our approach could be worse.<br><br><strong>3. Many models will be better than humans in some ways and worse in others, and which aspects those are often won't be correlated with what we view as ""difficult"".</strong>&nbsp;</p><p>There are certain things that are easy for us to do (continuing to use the same name to refer to someone over a long story, preserving world details like the size of a horse) and things that are difficult for us to do (advanced reasoning, logic involving detailed steps, stories that involve 30 or more characters, etc.). We have a bias that the things that are easy for us to do should be easy for an AI to do, and the things that are hard for us to do should be hard for an AI to do. This leads to two important assumptions, both of which are&nbsp;<i>false</i>:</p><p>If an AI cannot do those easy things, it cannot do those difficult things.</p><p>If an AI can do those difficult things, it can easily do the easy things.</p><p>These days this bias is more apparent: visual recognition is much more difficult than playing chess, or solving complicated arithmetic expressions. But this point has deeper implications that I feel like some people miss. A model may become superhuman in all ways but one, and in that way it is still subhuman. If that subhuman aspect is something that is easy for humans to do, we will write off the model as ""not intelligent"".&nbsp;<strong>The key bias here is that our internal intuition for the difficulty of tasks should not be used to judge how intelligent something is, it can only be used to determine how ""humanlike"" something is in its way of thinking</strong>. We can still use the intuition as a rough guiding pole for being surprised when AI does well at some task, but the point is that intelligence is multifaceted and the ordering of tasks solved by AI is not a given. A model could be better at theorem proving than any mathematician on earth, and simultaneously struggle to be consistent about the name of a character for more than a few paragraphs.<br>&nbsp;</p><p>This is a really really important point. Superintelligent agents will probably still be dumb about&nbsp;<i>something&nbsp;</i>for a long time, even after they are dangerous. It's unreasonable to expect them to converge to our way of thinking: they function differently, so different things are easy and hard for us than they are for them.</p><p><strong>4. AI models can be capable of being dumb or smart depending on the prompt.</strong>&nbsp;Generative models need to model&nbsp;<i>all&nbsp;</i>of human behaviour, including the stupid mistakes we all sometimes make. I've seen this one most concisely stated as ""<strong>sampling cannot be used to prove the absence of knowledge or ability, only the presence of it</strong>"". If your prompt doesn't give you what you want, it's possible that&nbsp;<a href=""https://www.blogger.com/u/1/blog/post/edit/7079330866239822858/1007680846105520470#"">PEBCAK</a>, and consider trying a different prompt, or trying different sampling settings.</p><p><strong>5. World knowledge isn't intelligence.&nbsp;</strong>A&nbsp;superintelligent alien that landed on earth would not know whether a horse is larger than a toaster. Testing world knowledge is an interesting task, and being able to pick up world knowledge is an important sign of intelligence. But lack of world knowledge should not be used to discount intelligence or reasoning ability.&nbsp;</p><p><strong>6. Exponential growth is counterintuitive.</strong>&nbsp;Somehow AI progress seems to be exponential in the same way that Moore's law is. This means that everything always seems too early to do anything about, right up until it is too late. In practice, most things are probably S curves that level off eventually, but where they level off may be far past the relevant danger points, so we should still try and keep this in mind.</p><p><strong>7. Advertising/Celebrities</strong>. Significant progress made by small labs or independent researchers may not get nearly as much attention as progress made by big organizations or well known researchers. This is a difficult problem caused in part by the large amount of papers. In theory it could be helped by recommendation and <a href=""https://www.connectedpapers.com/"">exploration</a> software to improve paper discovery, but either way the bias is important to keep in mind.</p><p>Let me know if you think of other biases and I'll add them to the list.</p><p>PS: When people claim that an AI is ""memorizing and pattern matching"" and not ""truly understanding"", in practice I find it usually comes down to either them referring to point 1 or 2, or when they see a model make a mistake a human wouldn't normally make (3).</p><p><br>&nbsp;</p>",phylliida-dev,danielle-ensign,Danielle Ensign,
2xjWjnubhs5Bsa3P5,Losing the forest for the trees with grid drawings,losing-the-forest-for-the-trees-with-grid-drawings,https://www.lesswrong.com/posts/2xjWjnubhs5Bsa3P5/losing-the-forest-for-the-trees-with-grid-drawings,2020-09-24T21:13:35.180Z,20,11,1,False,False,,"<p>Last night I learned about grid drawings and spent some time trying to work on them. In doing so, I ended up having a lightbulb moment where I really internalized what it means to lose the forest for the trees.</p><p>I'll explain what I mean. To start, let's back up and talk about what grid drawings are.</p><figure><img src=""https://www.free-for-kids.com/wp-content/uploads/2020/01/flower-drawing-grid-thumbnail.jpg"" srcset=""https://www.free-for-kids.com/wp-content/uploads/2020/01/flower-drawing-grid-thumbnail.jpg 415w, https://www.free-for-kids.com/wp-content/uploads/2020/01/flower-drawing-grid-thumbnail-212x300.jpg 212w""><figcaption>https://www.free-for-kids.com/drawing-grid-with-picture.shtml</figcaption></figure><p>The top image is the reference image, the bottom is your version. You draw grid lines in the reference image. Then you draw grid lines on your version. Then you go square-by-square and try to copy what you see.</p><p>For example, let's say that rows are labeled A through H and columns are labeled 1 through 11. For the flower, maybe we start at square A6. We look at what A6 looks like in the reference image and try to copy it into our version of A6. Then we move on to A5 and do the same thing. Then maybe B5. So on and so forth until we have gone through every square.</p><p>I think the idea is that there's less going on inside an individual square than there is when you zoom out and look at the image as a whole, so by approaching it this way it ends up being easier for the artist.</p><p>At first this felt like it made a ton of sense. I was successfully using the technique to draw easy things. But then once I moved on to a harder image, I ran into a weird problem. I was going square-by-square, and I felt like each individual square I drew was pretty close to the one in the reference image. But then I zoomed out and looked at what I had... and it looked terrible!</p><p>Huh? How could that be? So I went through each of the squares I drew and compared them to the ones in the reference image. Maybe I screwed one of them up or something.</p><p>Nope. Each one seemed pretty good. And yet... the zoomed-out perspective still looked like garbage!</p><p>I was confused. Reductionism is a thing, right?</p><p>And then I realized: small errors in each individual square can <i>compound</i> and make the zoomed-out version look terrible.</p><p>And furthermore, I was paying too much attention to the individual squares and not enough attention to the zoomed-out version. To the trees instead of the forest. Let me explain what I mean by that.</p><p>Say that in cell E21 the line in the reference image starts about 25% down from the top and goes at an angle of about 45°. I was just trying to imitate that. But if my version of the adjacent square, E20, is screwed up, I should factor that in to my E21 square. <i>E20 should ""flow smoothly"" into E21.</i> But I lost sight of that. I just kinda moved from square-to-square without factoring in the adjacent squares too much.</p><p>And that's when it hit me that this is <i>exactly</i> what it means to lose sight of the forest for the trees.</p><p>It honestly felt like a very powerful moment, and I'd recommend that you go through the grid drawing exercise and try to reproduce that moment yourself. I've always had an intellectual understanding of ""losing the forest for the trees"", but something about last night really hit home and helped me <i>internalize</i> it. It's like the difference between reading The Lean Startup and spending two years of your life failing at a startup because you weren't agile enough. Internalizing is different from intellectual understanding.</p>",adamzerner,adamzerner,Adam Zerner,
JtG3y8GBC2dTot339,Petrov Event Roundup 2020,petrov-event-roundup-2020,https://www.lesswrong.com/posts/JtG3y8GBC2dTot339/petrov-event-roundup-2020,2020-09-24T21:07:38.757Z,43,12,0,False,False,,"<p>Petrov Day is this weekend. Various LessWrong folk have taken the opportunity to reflect on the <a href=""https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day"">Day that Stanislav Petrov Didn't Destroy the World</a>. Some people have employed <a href=""https://www.lesswrong.com/posts/fr8MEigHzJeFQkctL/petrov-day-is-september-26"">ritual</a>, and others parties. Some employ <a href=""https://www.lesswrong.com/posts/XJxwFMSL5TPN2usC6/modes-of-petrov-day"">Big Red Buttons</a> of some sort.</p><p>This year, many of the traditional manners people have commemorated the event are a bit different.&nbsp;</p><p><strong>East Coast Petrov Megameetup</strong></p><p>The New York Rationality community will be holding an <a href=""https://www.lesswrong.com/posts/bh76S7YhiHYufjpk9/petrov-day-2020-virtual-celebration"">Online Petrov Megameetup</a>, which includes:</p><ul><li>3:00pm - 3:15am Welcome</li><li>3:15pm - 4:15pm Ice Breakers</li><li>4:15pm - 5:30pm Lightning Talks &amp; Activities</li><li>5:30pm - 6:00pm Snack/Water Break</li><li>6:00pm - 7:00pm Ritual</li></ul><p>It's recommended that you get 8 candles/candleholders for the ritual if you can, but if not you can listen along, and maybe download a Candle App for your phone that you can use for a few key moments.</p><p><strong>Austin Petrov Day</strong></p><p>The folks in Austin are holding an <a href=""https://www.lesswrong.com/events/7TMap3isj36wY9Rdz/austin-petrov-day-6-30pm-9-26"">in person, masked, outdoor Petrov Day</a>.</p><blockquote><p>We have <a href=""https://www.lesswrong.com/posts/jPSJTh5ctffyzFNB8/socially-distanced-outdoor-petrov-day-ceremonial-manual"">modified</a> the ceremonial manual to accommodate the outdoor setting. If possible, please bring a printed copy of the <a href=""https://github.com/GeneralAntilles/PetrovDay/releases/download/1.3.COVID-19/PetrovDay-1.3-COVID-19-DoubleSidedBooklet.pdf"">double-sided booklet version</a> (or a mobile device on which you can read the <a href=""https://github.com/GeneralAntilles/PetrovDay/releases/download/1.3.COVID-19/PetrovDay-1.3-COVID-19-MobileFriendly.pdf"">mobile-friendly version</a>), and a pen/pencil.</p></blockquote><p><strong>Littleton, CO Petrov Day</strong></p><p>Littleton is having an <a href=""https://www.lesswrong.com/events/oYbztGevs9iT6LLMW/petrov-day-celebration-1"">outdoor potluck</a>:</p><blockquote><p>Come join us as we celebrate the world <a href=""https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day"">not having been destroyed</a>, and raise a glass of Vodka to Stanislav Petrov.</p><p>We have chosen an outdoor venue for pandemic concerns, and will have a potluck for anyone willing to break bread during these troubled times. Masks are encouraged if you are not eating or drinking, and social distancing is recommended for anyone not already spending time together (some attendees have created quarantine circles, so don't be surprised if you see people ignoring typical precautions).</p></blockquote><p>If you're hosting a Petrov Day celebration of some kind that people are welcome to join, please list them in the comments below!</p>",Raemon,raemon,Raemon,
REnYhKSWRpCxtNkuc,Visualizing the textbook for fun and profit,visualizing-the-textbook-for-fun-and-profit,https://www.lesswrong.com/posts/REnYhKSWRpCxtNkuc/visualizing-the-textbook-for-fun-and-profit,2020-09-24T19:37:47.932Z,26,9,4,False,False,,"<p>Here we go again. It's the beginning of my undergraduate o-chem series. It's going to be a year of prose like this:</p><blockquote><p>Recall the electron configuration of carbon (Figure <a href=""https://edugen.wileyplus.com/edugen/courses/crs10264/ebook/c01/klein9781119110477c01xlinks.xform?id=c01-fig-0018""><u>1.18</u></a>). This electron configuration cannot satisfactorily describe the bonding structure of methane (CH<sub>4</sub>), in which the carbon atom has four separate C─H bonds, because the electron configuration shows only two atomic orbitals capable of forming bonds (each of these orbitals has one unpaired electron). This would imply that the carbon atom will form only two bonds, but we know that it forms four bonds. We can solve this problem by imagining an excited state of carbon (Figure <a href=""https://edugen.wileyplus.com/edugen/courses/crs10264/ebook/c01/klein9781119110477c01xlinks.xform?id=c01-fig-0019""><u>1.19</u></a>): a state in which a 2<i>s</i> electron has been promoted to a higher energy 2<i>p</i> orbital.</p></blockquote><p>Scintillating. I genuinely enjoy my subjects, but let's be honest. Right in the moment, there are many other things I would rather be doing than reading my chemistry textbook.</p><p>Recently, <a href=""https://www.lesswrong.com/posts/e5sKhCsQJmjos994J/rationality-and-playfulness"">I explored</a> whether it's possible to feel a sense of playfulness while you're alone. I found that it was. It it possible to feel playfulness while reading a chemistry textbook? Or is it better to optimize for an efficient, if dry, learning experience, and find ways to reward yourself afterward? If it is possible to feel playfulness while reading an o-chem textbook, it would be worth trading off at least some efficiency in favor of positive feeling. The question is how much. Of course, the best case scenario is where it's both possible to feel playfulness while reading, <i>and</i> it makes you learn more efficiently.</p><p>In my experiments of feeling playful while alone, I took away a few key insights.</p><ul><li>Starting with an open, unfocused, non goal-oriented mindset was crucial.</li><li>The feeling of playfulness was based on connecting physical objects with memories, a sense of profound personal meaning, and possibility. There was a poetic, metaphorical, associative quality to the experience. These associations were neither purely spontaneous and involuntary, nor deliberately constructed. It was more a sense of resting my attention and gaze on an object, and then gently asking my mind to remember or imagine a psychological association.</li><li>I remembered the act of remembering afterward, and it made those memories more available to me in the future.</li></ul><p>Playfulness while reading a chemistry textbook might take a similar form, something quite different, or simply be unavailable.</p><p>A particular difference is that my playfulness-around-the-house experiment was entirely visual. This will be, if anything, textual first and visual only secondarily.</p><p><strong>Experiment 1</strong></p><p>I read a couple review paragraphs about hybridized orbitals. I try to get out of the mindset of reading the text, and instead try to visualize it. My visual imagination is fairly poor, so mostly that entails looking at the diagrams and trying to ""echo"" them in my visual imagination. By practicing this a bit, I can see them in my mind's eye, and compare them with each other. I can directly see the point my textbook is making in words, but directly represented.</p><p>It's like the difference between having somebody explain different cuts in a diamond, or styles of brushwork in a gallery of paintings, and being able to directly perceive them yourself.</p><p>This doesn't feel ""playful,"" but seems to be a step in the right direction.</p><p><strong>Experiment 2</strong></p><p>As I read another couple paragraphs, I notice that some sentences add something new to the visual. Others call attention to some feature that I'd ignored. Others don't affect the visual at all. By focusing my attention on the visual I've been generating and developing, rather than on the verbiage, my experience of reading this textbook feels different than it did before - more relaxed, more meditative. I'm having an interesting experience of what my visual imagination is capable of, rather than doing the labor of reading a certain number of paragraphs.</p><p>Certain words seem to suggest a visual representation, even though I'm not confident that it accurately depicts them. For example, the textbook says:</p><blockquote><p>The larger front lobe enables hybridized atomic orbitals to be more efficient than <i>p</i> orbitals in their ability to form bonds.</p></blockquote><p>What does that word ""efficient"" mean in a visual sense? Hybridized orbitals aren't a real physical process, just a mathematical description that more accurately predicts empirical data. I try representing this fact by imagining two hybridized methane molecules. One has shorter symmetrical orbitals. The other reaches out one end of its orbitals toward the surrounding hydrogen atoms, like an octopus extending four tentacles to grab on to some clams.</p><p>... Well, that's a fun image. Maybe I'll try to imagine bonds as octopuses and clams this year! And here's a little bit of genuine playfulness entering the picture.</p><p>This is all starting to remind me of <a href=""https://www.lesswrong.com/posts/Rd8LTPFmygWpddJCq/using-a-memory-palace-to-memorize-a-textbook"">my experiment</a> with using a memory palace to try to memorize my general chemistry textbook.</p><p><strong>Experiment 3</strong></p><blockquote><p>The shapes of small molecules can often be predicted if we presume that all electron pairs (whether bonding or nonbonding) repel each other, and as such, they arrange themselves in three-dimensional space so as to achieve maximal distance from each other.</p></blockquote><p>I try to imagine this. At first, I get a picture that simply represents ""small molecules,"" like a dot in my mind. Then my mind thinks of how squirrels will run around a tree, trying to stay on the opposite side of the trunk from you. Then it spits up a representation of yellow dots chasing each other around a spherical nucleus, in a similar style to the chemistry textbook. These imagines come up rapidly.</p><p>I'm starting to get out of the mindset of scrutinizing the words of these paragraphs, as if the language itself held the key to understanding. Instead, I rapidly ""upload"" the text into a visual image. This takes far less time. It's an attitude shift. Reading isn't about diligently shoving an imaginative aural simulation of the words themselves through your brain, or dragging your eyeballs across every line of text on the page. It's about activating your visual imagination and allowing the text to help you manipulate and notice features of those images. Sometimes that results in playful visuals, other times in geometrical reductions.</p><p>This is part of Valence Shell Electron Pair Repulsion theory. Last year, I'd have tried to remember this phrase by stringing together the sounds in my mind, like a jingle. Now, I just picture an atom, see the electrons in the valence shell, see them repelling each other, and the name just feels natural. It's the equivalent of naming a dock the ""Place Where Cargo is Unloaded From Ships."" The name isn't mysterious or hard to remember at all once you've visited, created a visual memory, and understood what's going on there.</p><p><strong>Experiment 4</strong></p><p>My imagination is starting to incorporate sound effects. I'm able to create transitions between simulated molecular shapes that feel sophisticated, sort of like a video game. There's a sense of being able to ""click"" a certain orbital to transition it back and forth between a bond with hydrogen and a lone pair of electrons. The sound effect and expansion as the bond with hydrogen turns into a lone pair, pushing the other bonds with hydrogen closer together, makes the significance of the different geometries more clear.</p><p>There's a physical satisfaction in being able to hold and expand on these images. It's a bit like the feeling of strength that comes after you've gotten into a long run and are no longer resisting the exercise, but instead enjoying the feeling of power residing in your body.</p><p>These visuals dramatically improve my ability to remember. The difference between an sp3 and sp2 hybridization looks insignificant when it's just these abstract abbreviations on paper. &nbsp;But these two types of geometries <i>look</i> very different in their diagrams, and being able to quickly ""flip"" back and forth between imagining one and then the other is helpful.</p><p>The visual image contains all the information from the preceding paragraph. Thus, I no longer feel the anxiety I typically do about having forgotten the words in the first sentence by the time I get to the last sentence. It's all contained within the increasingly elaborate visual image I've been developing as I go along. A picture is worth a thousand words.</p><p>I'm not sure if this is taking longer than it would to just read the text. I suspect so. I'm also not sure if I'll retain the information better than by merely reading the text. I do feel as if I'm understanding it on an intuitive level much better, and to me, that's worth it. This is all review material, and I'm very curious to know whether this visualization practice will be a useful compliment, or even a replacement, for spaced repetition with flashcards.</p><p><strong>Experiment 5</strong></p><p>Why is fluorine the most electronegative element on the periodic table? The answer is that it has the greatest ratio of positive charge from the protons in its nucleus to the amount of shielding it gets from the electrons in its orbit. Picture those protons as black holes in the center of the atom, sucking in any electrons nearby. In a big fat molecule like iodine, there's a big huge cluster of protons in the center, but there are just as many electrons surrounding it ""absorbing"" that ""suction,"" and what's more, they surround it in layer after layer, protecting electrons in nearby molecules from the force. In fluorine, there's not as many protons, but they have far fewer layers of electron ""fat"" around their core, so nearby electrons are much more exposed to their ""suction.""</p><p>Picture iodine as a big, content Roman patrician after devouring a feast. Fluorine is a skeletal, starving Windigo, ravenous and ready to devour the electrons of any atoms it can get its hands on.</p><p><strong>Conclusion</strong></p><p>Occasionally, the effort I put into visualizing the text produced funny or striking images, and felt like a form of poetic, imaginative mental play. Most of the time, it was a more calm and focused experience. Overall, though, making an effort to visualize makes the experience of reading a textbook much more satisfying. Now that I'm doing it, it's almost inconceivable that I would go back to doing things the old way - reading paragraphs and hoping that the verbiage somehow sticks.</p><p>Does this connect with playfulness, somehow?</p><p>When kids are playing, they're transforming the world with their visual imagination. A stick becomes a sword or a laser gun. Putting your arm up overhead like Superman and running around the room becomes flying. Telling a story creates a world in your mind.</p><p>Kids hurt themselves and fail in their efforts all the time partly because their imaginative world is more real to them than physical reality. When we decide we want to master physical reality, we start to focus on the world of our senses at the expensive of our imagination. This isn't bad, but the visual imagination appears to me to become incredibly important once again when we circle back around and want to explore the world in the greatest possible detail through science and mathematics.</p><p>My long-term perception of myself has been that I lack almost entirely the ability to visualize. Reading novels has tended to be an auditory and intellectual experience. This experience and <a href=""https://www.lesswrong.com/posts/z7b67r7MhshovHLwG/visual-babble-and-prune"">others that I've written about</a> over the last few months have convinced me that my visual imagination can be trained, and that this training is the number one method I have available to improve my general intelligence.</p>",AllAmericanBreakfast,directedevolution,DirectedEvolution,
sK5EF97oZZMXQj2RC,Shittests are actually good,shittests-are-actually-good,https://www.lesswrong.com/posts/sK5EF97oZZMXQj2RC/shittests-are-actually-good,2020-09-24T17:20:29.002Z,-11,11,23,False,False,,"<p>Epistemic status: Exploring new area, making bold claim</p>
<h2>What is a shittest?</h2>
<p>In conventional usage, a shit-test is a behavior strategy in which a woman challenges a man's status/value/loyaly/ by observing his response. Some common variants include: giving a man an unreasonable task to see if he does it (where negotiating or refusing the request would signal high status), insulting a suitor, or behaving in a particularly unpleasant way to test for loyalty. The phrase ""If you can't handle me at my worst, you don't deserve me at my best"" is sometimes the idea. I won't link to any PUA or romance blogs becuase most of them suck: as always I recommend Geoffrey Miller's Mate for practical advice.</p>
<h2>What are they good for</h2>
<p>Despite the bad reputations, I think shit-testing is instrumentally rational for the tester. In the right situation, more men should do it.</p>
<p>I recently moved from a female-minority mating market to a male-minority mating market in a US coastal city. I'm also a better match for people's preferences here. Because of the new market, it takes me far fewer hours of bumbling/socializing to get a date on average. It looks like I can have 1 +/- .5 dates a week at the cost of only a 4-5 hours of texting/week. I will also live in this city for a much longer time. Therefore its worthwhile to take my time in mate selection and meet a few preferences. Shittesting helps me sort people.</p>
<p>I have two main behavioral preferences I shit test for.</p>
<h2>How do they argue?</h2>
<p>I want partners with:</p>
<ul>
<li>low need for closure (NFC) - they should have be willing to change ideas on new evidence and accept that there is not one definitive answer</li>
<li>are able to supply arguments I find compelling</li>
<li>evaluate arguments I make in a compelling way</li>
</ul>
<p>I want low NFC partners because they are much more fun to talk to. Talking to someone who hears one argument for a position then adopts that position and ignores all future arguments is just really really boring. Also I have found people with high NFC have more one-dimensional, less nuanced views (anecdotal). Furthermore, I seek partners who can supply compelling arguments because I don't want to have to fake finding their arguments compelling. If she can rip apart my arguments, that's the cherry on top because I'll improve my arguments every time we hang out.</p>
<p>Shit-testing for this is easy to do on a first date. I wait for my date to make an interesting proposition, then I supply contradictory evidence or state that I am unconvinced. This test gets diverse responses. The responses I dislike are: repeating the assertion/evidence, arguing to authority, ad hominem, avoiding the disagreement, a non-compelling critique of my evidence. The low NFC responses are: supplying an additional argument, motte-and-bailey[^1], a compelling critique of my evidence, supplying multiple new arguments, clarifying the original position. I've given the shit-test 4 times and had 1 pass and 2 failures and 1 tie.</p>
<p>For example, last night my date asserted that the Belgian racial policies caused the Rwandan genocide. I countered that ethnic divisions are common and mostly nonviolent, positing a food insecurity explanation. She supplied an argument about the unusually disorganized quality of the violence and the targeting of the killings. Iirc, she also pointed to the partition of India. She pointed out that while ethnic division is common, equal size ethnic groups are much less common. HOT.</p>
<h2>How do they train me?</h2>
<p>I accept basically accept <a href=""https://www.youtube.com/watch?v=Jre_xN2HSrk"">Diana Fleischman's argument</a> that people subconsciously reinforce and punish behaviors to shape their partners. She is giving a SSC online meetup talk this Sunday, which I am excited about. People both subconsciously and consciously train their partners by punishing behaviors they dislike and rewarding behaviors they prefer. Fleischman argues that women have evolved to train more effectively than man.</p>
<p>My preference is for a woman that</p>
<ul>
<li>Trains me in traits I myself want to change</li>
<li>Trains me more with rewards and less with punishment</li>
<li>Is willing to watch the Diana Fleishman lectures and think about the best relationship for both of us</li>
</ul>
<p>So  I need a shit test for that, but I'm not sure how. The problem is that people probably don't start punishing partners during the ""honeymoon"" period. I need behaviors I can observe or test in the honeymoon period which are strongly correlated with my preferences. Ideas welcome!</p>
<p>[^1] Fallacies are fine. I want a smart women with an open mind, not an ideal Bayesian Homonculus.</p>
",snog toddgrass,snog-toddgrass,snog toddgrass,
kJFSydfyAxzgvkqzE,Scheduling Algorithm for a PhD Student,scheduling-algorithm-for-a-phd-student,https://www.lesswrong.com/posts/kJFSydfyAxzgvkqzE/scheduling-algorithm-for-a-phd-student,2020-09-24T16:10:12.177Z,7,4,2,False,False,,"<p>This post was inspired by Christian and Griffiths <a href=""http://library.lol/main/ACD444B31B7583EC257E53C5442FA862""><em>Algorithms to live by</em></a>. Unlike other self-help books, it argues that the optimal scheduling strategy depends on your goals. If you want to minimize lateness of your single latest output, you should use Earliest Due Date. If you want to minimize the number of late items, use Moore's Algorithm. If you want to maximize value for time, select by value-weighted processing time (value/processing time). And so on and so forth. Based on their work, here is my scheduling plan for different items.</p>
<h1>Coursework</h1>
<p>As a phd student, I cannot afford to submit any coursework late. A high maximum lateness is bad for my reputation. Therefore, for coursework I want to use Earliest Due Date. However, I only want to devote a portion of my time to coursework: my primary goal is to become a <strong>producer</strong> of research, not a consumer. Therefore I should limit the amount of time I spend on each assignment (this also helps w/ focus). I have devised the following heuristics (and usually set timers for each activity)</p>
<ul>
<li>Reading an article: 20 min</li>
<li>Reading an article relevant to my interests: 40 min</li>
<li>Reading a book: 2 hrs <sup class=""footnote-ref""><a href=""#fn-imfn8wxb3f7myWPQL-1"" id=""fnref-imfn8wxb3f7myWPQL-1"">[1]</a></sup></li>
<li>Reading a book relevant to my interests: 4 hrs (in two sessions)</li>
<li>A problem set: until completion</li>
<li>A coding problem: until completion</li>
<li>Writing a reading response: 20 min</li>
</ul>
<p>After coursework is done, I switch immediately to career development.</p>
<h3>Career development</h3>
<p>These tasks include every behavior that increase my chance of becoming a professor, producing good research, and producing juicey QALY's. The most common tasks are pitching articles, analysing data, writing, editing, writing, communicating w/ coauthors, and writing more.</p>
<p>These tasks should be ordered by value per weight. I have more ideas than I have instrumentalized time to explore them. Any tasks which will take a long time to complete but give only marginal academic value can be thrown out.</p>
<p>The real problem is assigning values to the different tasks. How valuable is planning out my course schedule for the next year? Evaluating a specific research agenda? Exploring the papers in a new field? Unfortunately, I lack good answers for these questions. I'm vaguely aware that I should have &gt;5 articles published when I enter the job market. But the weightings for quality, quantity, prestige and coherence of these publications are unclear. This is a vital area for further research.</p>
<h3>Thesis ideas</h3>
<p>This is the one task for which I drop any activity to write it down. If I am reading a paper and I come across a new question or if the authors assumption lacks depth, I go straigth to markdown and write a description on my github. It's okay if the description but refers to the literture that gave the intuition.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-imfn8wxb3f7myWPQL-1"" class=""footnote-item""><p>I know this sounds crazy, but in my discipline you summarize lots of info fast. <a href=""#fnref-imfn8wxb3f7myWPQL-1"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",rockthecasbah,tim-liptrot,Tim Liptrot,
E8fp7BxpuzcwaMGtz,Covid 9/24: Until Morale Improves,covid-9-24-until-morale-improves,https://www.lesswrong.com/posts/E8fp7BxpuzcwaMGtz/covid-9-24-until-morale-improves,2020-09-24T15:40:02.594Z,95,40,16,False,False,,"<p>Last Two Weeks: <a href=""https://thezvi.wordpress.com/2020/09/17/covid-9-17-its-worse/"">Covid 9/17: It’s Worse</a>, <a href=""https://thezvi.wordpress.com/2020/09/10/covid-9-10-vitamin-d/"">Covid 9/10: Vitamin D</a></p>



<p>There is a pandemic. It is not going anywhere. </p>



<span></span>



<p>The news, on the other hand, is ready to move on. There is a supreme court seat open. There is an election coming soon, complete with a first debate. There are protests and presumably riots flaring up once again, this time regarding there no direct charges to any police officers in the death of Breonna Taylor. </p>



<p>Oh, and the President of the United States said yesterday, <a href=""https://www.cnn.com/2020/09/24/politics/donald-trump-election-democracy/index.html""><em>and I quote</em></a><em>:</em></p>



<p>“Well, we’re going to have to see what happens. You know that I’ve been complaining very strongly about the ballots and the ballots are a disaster,” Trump said, when asked if he could commit to the peaceful transition.</p>



<p>“(G)et rid of the ballots and you’ll have a very … there won’t be a transfer, frankly. There’ll be a continuation.”</p>



<p>If you are going to vote for Biden, vote in person. Make sure your vote counts and reduce the chances of chaos breaking loose before that happens.</p>



<p>If you’re going to vote for Trump despite everything, <em>at least have the decency to vote by mail. </em>That way, to count your vote, we’ll have to… count the votes. </p>



<p>Who cares about the pandemic? No one, it seems. That’s old news. The pandemic is now merely one more prop, albeit a big one, in the political game.</p>



<p>I can’t even argue. I too am more worried about the election than the pandemic, at this point. </p>



<p>We all grow tired of this pandemic. The damage accumulates. Economies and lives and reputations and credibility get destroyed week in and week out, and nothing good seems to come of any of it. Solutions get ignored, and the situation fails to improve. The same scolds keep saying we need to keep holding out, while scolding anyone attempting to solve the problem. Same covid time, same covid channel. If I didn’t have people thanking me every week for doing these, it would be difficult to keep going.</p>



<p>As usual, most of the Covid news falls into our existing patterns. When no one cares enough to do anything, but they do care enough to make sure no one else does anything, each week’s news is going to look like the previous week’s news. </p>



<p>And yet, if we look, there is always something worth talking about. Including one piece of genuinely great news.</p>



<p>Let’s run the numbers.</p>



<h2>Positive Test Counts</h2>



<figure><table><tbody><tr><td>Date</td><td>WEST</td><td>MIDWEST</td><td>SOUTH</td><td>NORTHEAST</td></tr><tr><td>July 30-Aug 5</td><td>91002</td><td>64462</td><td>212945</td><td>23784</td></tr><tr><td>Aug 6-Aug 12</td><td>93042</td><td>61931</td><td>188486</td><td>21569</td></tr><tr><td>Aug 13-Aug 19</td><td>80887</td><td>63384</td><td>156998</td><td>20857</td></tr><tr><td>Aug 20-Aug 26</td><td>67545</td><td>66540</td><td>132322</td><td>18707</td></tr><tr><td>Aug 7-Sep 2</td><td>55000</td><td>75401</td><td>127414</td><td>21056</td></tr><tr><td>Sep 3-Sep 9</td><td>47273</td><td>72439</td><td>106408</td><td>21926</td></tr><tr><td>Sep 10-Sep 16</td><td>45050</td><td>75264</td><td>115812</td><td>23755</td></tr><tr><td>Sep 17-Sep 23</td><td>54025</td><td>85381</td><td>127732</td><td>23342</td></tr></tbody></table></figure>



<figure><img src=""https://lh6.googleusercontent.com/f8jsJovtZBPEFq03_Yb16N7rBVKZwH0dTTH64_JK4BoN_xHC_H9R2Fh89K5M-MR3RD_Dz-lA-2RpNxwSoki-cSgwSMQyVqhQu5Ve8xx6DLoxI_dtdcfd1C0QlRcMGLra-co9fpIM"" /></figure>



<p>That looks bad, but the change is more than all accounted for by increased testing. It’s actually fine.</p>



<h3>Deaths</h3>



<figure><table><tbody><tr><td>Date</td><td>WEST</td><td>MIDWEST</td><td>SOUTH</td><td>NORTHEAST</td></tr><tr><td>July 16-July 22</td><td>1469</td><td>674</td><td>3106</td><td>524</td></tr><tr><td>July 23-July 29</td><td>1707</td><td>700</td><td>4443</td><td>568</td></tr><tr><td>July 30-Aug 5</td><td>1831</td><td>719</td><td>4379</td><td>365</td></tr><tr><td>Aug 6-Aug 12</td><td>1738</td><td>663</td><td>4554</td><td>453</td></tr><tr><td>Aug 13-Aug 19</td><td>1576</td><td>850</td><td>4264</td><td>422</td></tr><tr><td>Aug 20-Aug 26</td><td>1503</td><td>745</td><td>3876</td><td>375</td></tr><tr><td>Aug 27-Sep 2</td><td>1245</td><td>759</td><td>3631</td><td>334</td></tr><tr><td>Sep 3-Sep 9</td><td>1141</td><td>771</td><td>2717</td><td>329</td></tr><tr><td>Sep 10-Sep 16</td><td>1159</td><td>954</td><td>3199</td><td>373</td></tr><tr><td>Sep 17-Sep 23</td><td>1016</td><td>893</td><td>2695</td><td>399</td></tr></tbody></table></figure>



<figure><img src=""https://lh5.googleusercontent.com/A6gNI834tcF3HQfHllZdUFhhQ5TqzXwcvdHk44kBjnil0UFiiqSsE9ljIuhJiV3TEUn9Vwo1FEIZAWMauCY5X3L_ZscMf33shN1uVrbCmZv8dIv1Ux1-PZB-tXcEyLOiltHJcjvv"" /></figure>



<p>A worrisome uptick continues in the Northeast region. We see improvement in the other three. There was a large jump last week in the Midwest, so the modest pullback still isn’t great. But the South and West numbers are definitely encouraging. New York’s was as well, down to a new low of 27 deaths all week for the whole state. </p>



<h3>Positive Tests by Region</h3>



<figure><table><tbody><tr><td>Percentages</td><td>Northeast</td><td>Midwest</td><td>South</td><td>West</td></tr><tr><td>7/30 to 8/5</td><td>2.58%</td><td>7.26%</td><td>12.35%</td><td>6.68%</td></tr><tr><td>8/6 to 8/13</td><td>2.30%</td><td>5.67%</td><td>14.67%</td><td>6.98%</td></tr><tr><td>8/13 to 8/20</td><td>2.06%</td><td>5.62%</td><td>9.41%</td><td>6.47%</td></tr><tr><td>8/20 to 8/26</td><td>1.86%</td><td>5.78%</td><td>9.93%</td><td>5.88%</td></tr><tr><td>8/27 to 9/2</td><td>1.87%</td><td>6.37%</td><td>9.38%</td><td>4.78%</td></tr><tr><td>9/3 to 9/9</td><td>1.97%</td><td>6.02%</td><td>8.48%</td><td>4.13%</td></tr><tr><td>9/10 to 9/16</td><td>2.41%</td><td>5.99%</td><td>11.35%</td><td>4.49%</td></tr><tr><td>9/17 to 9/23</td><td>2.20%</td><td>5.96%</td><td>7.13%</td><td>4.11%</td></tr></tbody></table></figure>



<figure><img src=""https://lh3.googleusercontent.com/ToHMaZZAtuKTQ3HzEcQZLx7R4XU3LKBt2HZYOUTK5BeSANbRmkPxedRFl67HcmvZtw39K3htQbLfz-0OSYPjKWFspIdJHb7sARliPKugRYeyRWz4GDGuSbrxTNW71mekpCfSw1I7"" /></figure>



<p>Last week looks very scary here. This week looks good. The South is now doing better than it has in some time as is the West, and the Northeast is pulling back from the dramatic rise last week. I suspect some amount of negative tests were held back last week for whatever reason, and got dumped into this week, so things should look smoother than they do. </p>



<p>Still this is a great relief. Last week looked like we might be about to ramp up once again, potentially quite rapidly. Now it seems much less likely that is going to happen. That assumes of course that we want less cases now rather than more cases – I actually think it’s reasonable at this point to want to get it over with, if it wouldn’t cause additional lockdowns, but I do think it would case additional lockdowns, so I’d prefer to avoid it.</p>



<h3>Test Counts</h3>



<figure><table><tbody><tr><td>Date</td><td>USA tests</td><td>Positive %</td><td>NY tests</td><td>Positive %</td><td>Cumulative Positives</td></tr><tr><td>July 17-July 29</td><td>5,746,056</td><td>7.9%</td><td>452,889</td><td>1.0%</td><td>1.34%</td></tr><tr><td>July 30-Aug 5</td><td>5,107,739</td><td>7.8%</td><td>484,245</td><td>1.0%</td><td>1.46%</td></tr><tr><td>Aug 6-Aug 12</td><td>5,121,011</td><td>7.3%</td><td>506,524</td><td>0.9%</td><td>1.58%</td></tr><tr><td>Aug 13-Aug 19</td><td>5,293,536</td><td>6.2%</td><td>548,421</td><td>0.8%</td><td>1.68%</td></tr><tr><td>Aug 20-Aug 26</td><td>4,785,056</td><td>6.0%</td><td>553,369</td><td>0.7%</td><td>1.77%</td></tr><tr><td>Aug 27-Sep 2</td><td>5,042,113</td><td>5.5%</td><td>611,721</td><td>0.8%</td><td>1.85%</td></tr><tr><td>Sep 3-Sep 9</td><td>4,850,253</td><td>5.3%</td><td>552,624</td><td>0.9%</td><td>1.93%</td></tr><tr><td>Sep 10-Sep 16</td><td>4,632,005</td><td>5.8%</td><td>559,463</td><td>0.9%</td><td>2.01%</td></tr><tr><td>Sep 17-Sep 23</td><td>5,719,327</td><td>5.2%</td><td>610,137</td><td>0.8%</td><td>2.10%</td></tr></tbody></table></figure>



<p>Welcome back, testing, to levels seen in mid-July. Hopefully this is the start of a trend or at least a new normal, rather than a blip. The positive rate is clearly an improvement as well, assuming the extra tests aren’t negative tests are new tests, rather than being dumped onto the system from previous weeks. </p>



<p>It is telling that this week we saw a bunch of headlines about the ‘rising number of cases’ when that is <em>obviously </em>not what this chart says at all. The previous week was a scary increase. This week’s headline numbers are obviously good news. Yet even now, in September, the whole ‘do less testing so it looks less bad’ strategy <em>is the correct strategy </em>if you want good media. And what we incentivize, we are likely to get. Hate the player if you wish, but first hate the game.</p>



<p>By the way, if you’re in New York City and need a rapid test, a friend <a href=""https://www.drconnornyc.com/"">reports this as a good source</a>.</p>



<h3>For Relief That Lasts</h3>



<p>In a surprise to few who read this column, the newest potential solution to Covid-19 is essentially an attempt at regulatory arbitrage.</p>



<p><a href=""https://www.upi.com/Health_News/2020/09/17/Nasal-solution-may-stop-spread-of-COVID-19-study-finds/1881600350075/"">Nasal solution may stop spread of COVID-19,</a> <a href=""https://jamanetwork.com/journals/jamaotolaryngology/fullarticle/2770785"">study finds</a>. </p>



<p>It’s still early and the results were in a lab, so not that great a chance it works. But we’re all excited, because <em>if it works the Very Serious People might be forced to let us use it.</em></p>



<p>Every now and then, ask what regulatory behaviors would look like if they wanted us all to lose our jobs, starve to death, get sick and die. </p>



<p>And be thankful they have not yet closed all their loopholes.</p>



<h3><a href=""https://blogs.sciencemag.org/pipeline/archives/2020/09/21/the-vaccine-protocols"">The Vaccine Protocols</a></h3>



<p>They look good. The trials are way too small, of course. Why do 30,000 when you can do 300,000? It’s not like these vaccines are that dangerous. But given size limitations and the standards we live by, these seem… fine?</p>



<p>They don’t look remotely optimized, of course. The actual sensible thing would be to <em>continuously </em>check the results via computer and report if thresholds are ever crossed (and have higher thresholds to compensate for that). It’s also not clear to me what kind of logic is behind the thresholds involved here. The p-values necessary for a positive finding keep getting larger as the study continues. There’s no evidence of any attempt to do a Bayesian analysis. So what’s really going on here?</p>



<p>There’s some psychological story behind this design. I’m curious what it is, but I have at best small pieces of that puzzle. </p>



<p>In any case, how fast we get results will depend on the rate of infections in the trials. The better the vaccine, the slower we get the results, but only by a factor of 2-3 at most since there’s a control group, and results are expected this year. Each trial has at least 30,000 people, who they presumably are testing continuously. They need roughly 150 infections to finish, so a 1% infection rate in the control group would do it, or 0.5% from the combined group. How fast will we get there? </p>



<p>If you go by actual positive tests in the USA, that’s ten weeks to get 1% infected. Those who volunteer for a trial could be more or less at risk modulo the vaccine itself, and it’s not obvious to me which way that goes. Nothing we can do but wait.  </p>



<h3>The Correct Vaccine Protocols</h3>



<p>Challenge trial! <a href=""https://uk.reuters.com/article/uk-health-coronavirus-vaccine-uk-idUKKCN26E2DR"">Actual first world government funded challenge trial!</a></p>



<p><a href=""https://www.youtube.com/watch?v=3GwjfUFyY6M&amp;ab_channel=KoolAndTheGangVEVO"">Celebrate, good times, come on!</a></p>



<p>We need to do it bigger, faster, better. But something is <em>so so much better </em>than nothing. </p>



<p><a href=""https://www.youtube.com/watch?v=_lAEO07SISU&amp;ab_channel=BrianEgan"">Woo-hoo!</a></p>



<h3>CDC Accidentally Tells Truth, Profusely Apologizes</h3>



<p>This column <em>strongly </em>believes in good incentives and in redemption. If the CDC <em>or anyone else </em>comes around way too late to the truth and/or the right strategy, we will applaud them for doing so sooner rather than later. The ‘what took you so long’ approach helps no one. </p>



<p>Unfortunately, that is not this story. </p>



<p>This week, the CDC came out with new guidelines that said the Covid-19 virus is airborne. You can be infected by breathing in air that contains the virus.</p>



<p>No. Shirt. Sherlock.</p>



<p>Let it sink in that now, <em>in September, </em>the CDC finally got around to admitting <em>the most important single fact </em>about Covid-19 – that the virus is airborne. That’s the primary mode of infection. </p>



<p>Then the WHO called, and asked what the CDC was doing saying the virus was airborne. Because the WHO has yet to admit this one. They are continuing to lie to all of us.</p>



<p>The CDC, being called out for accidentally speaking truth to the public, moved quickly to correct the situation. They declared that the new guidelines were only a draft, and rescinded them. </p>



<p>Without saying anything remotely like: Yes, obviously the Covid-19 virus is airborne. We’re a bureaucracy and we haven’t gotten the wording right yet, so those guidelines aren’t official yet, but </p>



<p>So the headlines are now that the CDC retracts guidelines saying Covid-19 is airborne. </p>



<p>This is like if Google Maps had people driving into a river, then changed to a new map that didn’t do that, but realized the new map misspelled the town’s name, so they restored the old version of the map until they could fix the problem. Which doesn’t seem realistic, so to fix that, let’s substitute Apple Maps.</p>



<p>What is the game, at this point? Why hold to this lie? </p>



<p>Because (if my explanations in previous columns and posts haven’t made this clear yet, if they have you can skip this) it’s the foundational lie behind the idea that there are ‘safe’ and ‘unsafe’ configurations and actions. That if you stay six feet away, and Sacrifice to the Gods with frequent ‘deep cleaning,’ you’ll be safe. So in order to protect arbitrary guidelines, to get people to latch onto simple heuristics that they think will cause the public to do what’s good for them, the science has to say that which does not correspond to physical reality. I can’t even say “to say that which is not” because that implies they know the difference. They certainly don’t much care.</p>



<p>So. Yeah. CDC Delenda Est. </p>



<p>That’s the shot. Now for the chaser. </p>



<h3>CDC Announces They Have Antibody Results, Provides #Analysis</h3>



<p><a href=""https://www.wsbradio.com/news/politics/jamie-dupree/cdc-chief-less-than-10-percent-americans-have-had-virus/TSBNZWPAKRB3ZC6ZLE2B5GV23Y/"">Here’s a news report on the latest announcement</a>. I encourage reading it before reading my take on the new information.</p>



<p>The first thing to note is that the CDC director is directly conflating <em>has not yet had the virus </em>with <em>susceptible, </em>and also conflating <em>tested positive on the serology test </em>with <em>has had the virus:</em></p>



<p>The head of the Centers for Disease Control told Congress on Wednesday that preliminary data from nationwide testing shows that over 90 percent of Americans have not contracted the Coronavirus, as top federal health officials continued to implore Americans to take precautions in order to slow the virus outbreak.</p>



<p>“More than 90 percent of the population remains susceptible,” Dr. Robert Redfield told a Senate hearing, saying serology tests have shown clear trends in the spread of the virus.</p>



<p>This makes three implicit claims which imply a fourth claim. </p>



<p><strong>Claim 1: If and only if you have had Covid-19, your serology test will be positive.</strong> Modulo small false positive and false negative rates due to measurement error, which I agree are small enough we can mostly ignore both of them once we get to the 10% level.</p>



<p><strong>Claim 2: If and only if you have a positive serology test, you are not susceptible to Covid-19. </strong>Again, modulo a false result that can mostly be ignored in both directions.</p>



<p><strong>Claim 3: Before having Covid-19, everyone is susceptible to Covid-19.</strong></p>



<p><strong>Claim 4 (From Claim 1 + Claim 2): Having Covid-19 and recovering makes you permanently immune to Covid-19 reinfection. </strong>Or, at a minimum, makes you immune for a period of no less than seven months.</p>



<p>(There’s also the tidbit that &gt;25% of infections are ages 18-25. That’s quite a narrow range, and one at very low risk.)</p>



<p>So first off, I’d like to congratulate Redfield for acknowledging that immunity is long lasting. It’s not like there was any real doubt, but scaring people with ‘who knows how long it lasts and it’s gonna end Real Soon Now’ remains a key part of the Very Serious People scare tactic policy portfolio.</p>



<p>Then I’d challenge the other two claims rather strongly.</p>



<p>There are some strong claims out there that lots of people are not susceptible to Covid-19, or are highly varying degrees of susceptible. Those claims may or may not be right. But the idea that <em>no one </em>is effectively immune without first gathering enough direct antibodies to test positive on the serology test seems like obvious nonsense. That’s not how the immune system works. </p>



<p>Second, we have lots of screaming that ‘immunity fades’ because antibody levels fall over time. So people who would have tested positive on serology, stop testing positive, yet these cases have near-zero reinfection rates. It seems obvious we both have <em>some </em>people who are immune without being infected and producing antibodies in the first place, because of cross-immunity or other lines of defense, and that we also have <em>some </em>people who no longer show antibodies but are still immune because of other lines of defense and the ability to remember and produce the antibodies again if needed.</p>



<p>The question is whether there are only a few such people, or there are lots of such people.</p>



<p>If we took the CDC’s data at face value, what would that mean? They say ‘Over 90%’ so that’s a claim that under 10% tested positive. How far under 10% should we assume? Given the incentives and other actions of the CDC, let’s assume this means something like 9%, versus a nationwide 2.1% rate for positive test results from PCR tests.</p>



<p>The peak of rates in the study was 24%, presumably for New York City but perhaps not. That’s higher than previous measured antibody results, but lower than plausible infection rates – my model has New York State at around 25%, and the city is far higher than the rest of the state. Our <a href=""https://covid19-projections.com/"">go-to outside machine learning</a> plausible guess puts USA overall rate at 16% (which implies a measured death rate of roughly 0.4%), but has only 21% for New York State. That still is substantially higher than 24% being the highest observed local rate would imply. Note that the machine learning model has Arizona at 24.6%, substantially higher than New York, and many southern states in the 21% range. </p>



<p>On its own these are more data points, and they are useful parts of the puzzle. The way it is presented, however, strikes me as another attempt to have it both ways, and describe everyone as in danger as often as possible to try and get people to behave the way the CDC wants. </p>



<p>This week’s other interesting serology test news is this study from Japan: <a href=""https://www.medrxiv.org/content/10.1101/2020.09.21.20198796v1"">Dynamic Change of COVID-19 Seroprevalence among Asymptomatic Population in Tokyo during the Second Wave</a>. Those numbers are rather boggling. I don’t know what to make of them and am curious what others think. To avoid anchoring too much, I won’t say more.</p>



<h3>In Other Sort-Of News: Links That Tell The Same Old Story</h3>



<p><a href=""https://www.nj.com/coronavirus/2020/09/nursing-homes-have-new-covid-19-tests-that-are-fast-and-cheap-so-why-wont-nj-allow-them-to-be-used.html"">Nursing homes have new COVID-19 tests that are fast and cheap. So why won’t N.J. allow them to be used?</a> </p>



<p>Because we’ve decided that every test needs to be as effective as our best test, or we can’t use it, even though that is mind-bogglingly murderously dumb. I’d explain, but I’d only be repeating myself.</p>



<p><a href=""https://www.businessinsider.com/airline-workers-covid-rates-is-flying-safe-2020-9"">Airline workers have lower rates of COVID-19 than the general population — and airline CEOs say it’s proof that flying is safe</a></p>



<p>This keeps happening. There’s an activity that it seems plausible would have unacceptably high risk of Covid-19 infection. For example, you might want to play professional sports, or you might want to treat patients in a hospital. Safety protocols are implemented, including masks and frequent testing and hyper-awareness of the dangers. </p>



<p>In the case of airlines, lots of attention is also paid to air flow.</p>



<p>Then the tests come in, and <em>despite more vigilance and testing, and therefore much higher probabilities of finding each case, </em>the case rates end up lower than the general population. In this case, the positive rate for flight attendants is under 1%, whereas the confirmed positive test rate for the USA is above 2%.</p>



<p>The pattern is clear. Risky things are risky, <em>but risky things done with a real dedication to safety, and a willingness to endure major costs to get that safety, are actually net </em><strong><em>safer </em></strong><em>than the background risk level.</em></p>



<p>Does this mean it is ‘safe to fly’? </p>



<p>No. It means that if one takes professional-level precautions, flying can be put into the ‘safe’ bucket. But that’s true of most things that aren’t about packing people into tight spaces so they can share an experience with each other. </p>



<p><a href=""https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0239252"">SARS-CoV-2 positivity rates associated with circulating 25-hydroxyvitamin D levels</a></p>



<p>Yet another correlational study about Vitamin D. The correlational verdict is so beyond in that, as the person who linked me to this put it, there really isn’t much point. Figured I’d throw it in, but it doesn’t change anything.</p>



<p><a href=""https://twitter.com/donmoyn/status/1306614873502224385"">USPS Had Plan to Send Everyone Face Masks</a></p>



<p>Plan was rejected by the Trump Administration in order to ‘not cause panic.’ Also perhaps to avoid giving people the impression that the Postal Service was competent and important and could do other things like let people vote. Either way, I consider this a <em>positive </em>update. Our civilization did figure out a good plan to do something important. Yes, it didn’t happen because someone actively decided to stop it, but I’d rather be competent and up against malice than be incapable at all.</p>



<p><a href=""https://marginalrevolution.com/marginalrevolution/2020/09/our-dna-our-antigens-our-selves.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+marginalrevolution%2Ffeed+%28Marginal+Revolution%29"">Our Antigens, Our Selves – Marginal REVOLUTION</a></p>



<p>Another story from Marginal Revolution about how the FDA puts insane restrictions on how people might gain information about themselves and their health, because if someone had information they might change their behavior based on that information, and information can sometimes be wrong or misinterpreted, so information should be regulated as a high-risk medical device.</p>



<p>I violently agree that all of that is bonkers – FDA is the original member and still captain of the Delenda Est Club. I only have one bone to pick, which is with the term ‘Unintended’ Consequences.</p>



<p>Unintended?</p>



<p>I say intended. One hundred percent intended. The intention was to prevent people from having access to the information they need to make decisions to protect their health. People dying <em>is not an unintended consequence of that. </em></p>



<p>You stop systems from functioning, they stop working. You refuse to tell people when they’re sick or how to get better, and they die more often. This isn’t complicated or hard. </p>



<p>I wasn’t <em>intending </em>to get fat, I just wanted to eat lots of giant meals and delicious desserts every day and never exercise. Don’t look at me like that.</p>



<p>I mean, officer, I only intended him to stop moving so I could take his wallet. That’s why I shot him in the face. I didn’t <em>intend</em> to kill him.</p>



<p>There are frequently unintended consequences of regulation. They are <em>usually </em>but not always destructive. There are also frequently fully intended, and fully predictable, destructive consequences of regulation. When you regulate something and it gets more expensive, lower quality, less innovative, subject to an oligopoly or monopoly and increasingly nonsensical and <a href=""https://thezvi.wordpress.com/2020/05/23/mazes-sequence-summary/"">maze-like</a>, and those subject to the regulations capture the regulatory body and sculpt it to their own ends… <em>You do not get to say these consequences are unintended.</em></p>



<p>You can say those consequences are <em>worth it. </em>You can say that the world with the regulations is better than the world without them, <em>in spite </em>of those consequences. You can’t call them unintended anymore. </p>



<p>See everyone next week. Now more than ever, I hope things stay quiet.</p>",Zvi,zvi,Zvi,
zyyh7c3tey2HCmFyP,Spoiler-Free Review: The Stanley Parable,spoiler-free-review-the-stanley-parable,https://www.lesswrong.com/posts/zyyh7c3tey2HCmFyP/spoiler-free-review-the-stanley-parable,2020-09-24T14:40:01.655Z,27,8,4,False,False,,"<p>At Ben Pace’s recommendation, I recently played The Stanley Parable. Ben considers the game Tier 1.5. On reflection I consider this Tier 2. If you can spare the $15 for a short game, it’s Worth It to play. </p>



<p>If there’s one game I’ve played that needs to be played blind, this is it, so that’s all I’ll say. </p>",Zvi,zvi,Zvi,
cxEeHeBaW3NQ6ez9r,Should it be a research paper or a blog post?,should-it-be-a-research-paper-or-a-blog-post,https://www.lesswrong.com/posts/cxEeHeBaW3NQ6ez9r/should-it-be-a-research-paper-or-a-blog-post,2020-09-24T08:09:08.179Z,15,10,9,False,True,,"<p><i>The answer to this question may seem obvious to some, but let's see.</i></p><p>&nbsp;</p><p>My impression is that some people write long blog posts about things that they perceive as innovative new ideas in philosophy, AI research, or whatever. So how do you decide that your idea should be a blogpost (here or on some other site) instead of submitting it to a journal?</p><p>Is the university research system just too closed? Or do you think it is superfluous?</p><p>Or do you think that you can do it faster for a blog? (This may not be unambiguously good. My impression is that there is less literature research for some blog articles than I would like to see, which often creates noise.)</p><p>Or do you want to have feedback for a blogpost first and submit later?</p><p>(Note that the question can be generalized to other media and forms, e.g. magazine essay vs. journal article.)</p>",Sherrinford,sherrinford,Sherrinford,
HDyePg6oySYQ9hY4i,David Deutsch on Universal Explainers and AI,david-deutsch-on-universal-explainers-and-ai,https://www.lesswrong.com/posts/HDyePg6oySYQ9hY4i/david-deutsch-on-universal-explainers-and-ai,2020-09-24T07:50:23.615Z,4,4,9,False,True,,"<p>In <a href=""https://www.amazon.co.uk/Beginning-Infinity-Explanations-Transform-World/dp/0143121359"">The Beginning of Infinity</a> David Deutch claims that the world is explicable and that human beings can explain anything that can be explained (Chapter 3):</p><blockquote><p>The astrophysicist Martin Rees has speculated that somewhere in the universe ‘there could be life and intelligence out there in forms we can’t conceive. Just as a chimpanzee can’t understand quantum theory, it could be there are aspects of reality that are beyond the capacity of our brains.’ But that cannot be so. For if the ‘capacity’ in question is mere computational speed and amount of memory, then we can understand the aspects in question with the help of computers – just as we have understood the world for centuries with the help of pencil and paper. As Einstein remarked, ‘My pencil and I are more clever than I.’ In terms of computational repertoire, our computers – and brains – are already universal (see Chapter 6). But if the claim is that we may be qualitatively unable to understand what some other forms of intelligence can – if our disability cannot be remedied by mere automation – then this is just another claim that the world is not explicable. Indeed, it is tantamount to an appeal to the supernatural, with all the arbitrariness that is inherent in such appeals, for if we wanted to incorporate into our world view an imaginary realm explicable only to superhumans, we need never have bothered to abandon the myths of Persephone and her fellow deities.</p><p>So human reach is essentially the same as the reach of explanatory knowledge itself. An environment is within human reach if it is possible to create an open-ended stream of explanatory knowledge there. That means that if knowledge of a suitable kind were instantiated in such an environment in suitable physical objects, it would cause itself to survive and would then continue to increase indefinitely. Can there really be such an environment? This is essentially the question that I asked at the end of the last chapter – can this creativity continue indefinitely? – and it is the question to which the Spaceship Earth metaphor assumes a negative answer.</p></blockquote><p>Deutsch claims that an AI would be a universal explainer (Chapter 7):</p><blockquote><p>There is a deeper issue too. AI abilities must have some sort of universality: special-purpose thinking would not count as thinking in the sense Turing intended. My guess is that every AI is a person: a general-purpose explainer. It is conceivable that there are other levels of universality between AI and ‘universal explainer/constructor’, and perhaps separate levels for those associated attributes like consciousness. But those attributes all seem to have arrived in one jump to universality in humans, and, although we have little explanation of any of them, I know of no plausible argument that they are at different levels or can be achieved independently of each other. So I tentatively assume that they cannot. In any case, we should expect AI to be achieved in a jump to universality, starting from something much less powerful. In contrast, the ability to imitate a human imperfectly or in specialized functions is not a form of universality. It can exist in degrees. Hence, even if chatbots did at some point start becoming much better at imitating humans (or at fooling humans), that would still not be a path to AI. Becoming better at pretending to think is not the same as coming closer to being able to think.</p></blockquote><p>So according to Deutsch there is no qualitative distinction between an AI and a human being. Comments?</p>",alanf,alanf,alanf,
rT2HwDS3ew2Bq9T2G,I have discovered a new kind of unemployment.,i-have-discovered-a-new-kind-of-unemployment,https://www.lesswrong.com/posts/rT2HwDS3ew2Bq9T2G/i-have-discovered-a-new-kind-of-unemployment,2020-09-24T01:04:18.266Z,12,6,4,False,False,https://seekingalpha.com/article/4361570-skill-stalagmites-technology-stalactites,"<p>Hello everybody, I have an article that explains why since 2000: business investment has been weak; the fall in the U.S net labor share; the decline in the prime age U.S labor participation rate vs large gains elsewhere; the rise in deaths of despair. The article is called Skill Stalagmites, Technology Stalactites and can be found&nbsp;<a href=""https://seekingalpha.com/article/4361570-skill-stalagmites-technology-stalactites"">here</a>. I have split the piece into two parts: a 1500 word article for the general reader and a longer piece for the more sophisticated reader. There is a link to the latter at the end of the first piece.</p><p>The punchline to the article is that the 4-5% gap in the lfpr between the U.S and peer&nbsp;economies is a form of disguised unemployment. And this is a novel kind of unemployment, which is not caused by a fall in aggregate demand.&nbsp;</p><p>The actual cause is that firms are imposing higher effort levels on workers. I can summarize the argument you will find in the main article; it goes like this:</p><ol><li>Firms impose higher effort demands on workers; workers have to complete more tasks (for a higher wage) or be fired.</li><li>The higher wage does not compensate workers for their lost work leisure; thus workers look for less demanding job positions (or refuse to move up to more senior roles).</li><li>If one imagines a skill ladder, then all workers attempt to drop down a rung. This is easy for higher skilled workers, but what happens to workers at the bottom?</li><li>The lowest skilled workers compete for job openings with somewhat more skilled workers. Firms prefer to hire the more skilled worker, resulting in the lowest skilled workers being pushed out of employment altogether.</li><li>This assumes that employers can always identify the highest skilled worker from their pool of applicants. This won't always be the case; if the higher skilled worker has a bad interview or the weaker candidate has positive chemistry with the interviewer, then the objectively weaker candidate can win a job offer.</li><li>Thus provided the lowest skill workers are willing to keep searching for jobs they will eventually obtain a job offer and regain employment.</li><li>This means though that workers on the second lowest skill rung will be unable to drop down to the lowest rung unless they also increase their job search activity. And in turn this forces the workers above them to increase their job search.</li><li>Any person wanting a job now has to apply to many more job positions before they can get their first job offer. But after a string of failures, job seekers become discouraged and temporarily withdraw from the search process. It is this temporary withdrawal that is responsible for the drop in lfpr. For those who are the main breadwinners, the period of withdrawal will be short - perhaps only a few months. But for workers who are more marginally attached to the labor force, it could be years or forever.</li><li>Evidence for higher effort in the U.S can be found in the higher U.S productivity growth since 2000 vs peer economies.</li><li>Evidence of higher job search can be found in the elevated duration of unemployment, which in 2019 was still equal to recessionary levels. The American Time Use Survey also shows higher than normal time spent on job search.</li></ol><p>The questions of why this is happening post 2000 and not before, and why only in the U.S and not elsewhere, are&nbsp;taken up in the full article.</p><p>If you have any questions of&nbsp;your&nbsp;own, please ask away.</p><p>Best,&nbsp;</p><p>Nathan.</p><p>P.S The article is published on Seeking Alpha, but don't&nbsp;let that put you off. Though I don't have a formal background in economics, I do keep up with the relevant literature.</p>",Nathan Brooks,nathan-brooks,Nathan Brooks,
FghubkDy6Dp6mnxk7,The rationalist community's location problem,the-rationalist-community-s-location-problem,https://www.lesswrong.com/posts/FghubkDy6Dp6mnxk7/the-rationalist-community-s-location-problem,2020-09-23T18:39:26.278Z,169,78,142,False,False,,"<h1>The Problem</h1><p>Basically ever since the first rationalists settled in Berkeley, people have been saying, “Why do you live in Berkeley, Berkeley sucks! You should all move to Location X instead, it’s so much better.” The problem has always been that no one agrees on what Location X is. Some common candidates for Location X:</p><ul><li>A smaller, cheaper, friendlier US city</li><li>NYC</li><li>Australia</li><li>Canada</li><li>Somewhere with very low cost of living (often in Southeast Asia or Latin America)</li><li>London</li><li>Oxford</li><li>Blackpool</li><li>Prague</li><li>A castle</li><li>A private island</li></ul><p>and of course</p><ul><li>Wherever the speaker is from</li></ul><p>In the past I've brushed off all such suggestions, because it was just too hard a coordination problem to get multiple hundreds of rationalists to leave Berkeley, where they've gotten jobs, rented or even bought houses, established organizations, enrolled in schools, and established social circles.&nbsp;</p><p>But we're in a unique time! Due to the pandemic, there's far less reason to stay in any one place - work and school are remote, expensive leases can be terminated, and you can't see your friends anyway. Most of the rationalist houses I know have moved or dissolved, and the former Berkeley rationalists are flung across all corners of the globe (yeah I know globes don't have corners). A fair number of us have stayed, but I think for most of us it's just because our friends are here, we're hoping that someday the rest of our friends come back, and we're not sure where else to go.&nbsp;</p><p>So, if ever there were a time when we <i>actually </i>had the chance to move the physical locus of the rationalist community, it's now. Below, I'll lay out what I believe to be some of the most important general considerations for deciding on a new location. <strong>I encourage people to make their case for a specific location,</strong> either in comments or in their own posts. (Looking at you, Mikk!)&nbsp;</p><hr><h1>Considerations for Location X</h1><h2>Potential dealbreakers</h2><p><strong>Visas</strong></p><p>In order to settle in a location, you have to be able to legally live there long-term. Most Berkeley rationalists are US citizens, and those who aren't have already paid the steep cost of acquiring US visas and learning US immigration law. This feels like a strong argument in favor of staying in the US somewhere, although it's possible there are places where this wouldn't actually be that much of an issue. In any case, it's certainly an argument against countries with strict immigration laws, like Switzerland.</p><p>Relatedly, organizations such as MIRI, CFAR, Open Phil, BERI, etc are registered in the US. I don't know how hard it would be for them to operate elsewhere and am unfamiliar with this domain in general.</p><p><strong>Language&nbsp;</strong></p><p>Given that basically all rationalists speak English (since it's pretty hard to read the relevant material otherwise), we should settle somewhere English-speaking; it would be very costly if everyone had to deal with a language barrier every single day (or learn a new language).&nbsp;</p><p>Notably this doesn't automatically disqualify all locations in e.g. continental Europe - Habryka points out that you can get along just fine in Berlin if you only know English. But somewhere like e.g. Japan looks like a much worse prospect on this metric.</p><p><strong>National political environment / culture</strong></p><p>The rationality community often attracts controversy, so it's important that we settle somewhere that protects freedom of thought and speech, and is generally friendly to weird ideas. We should definitely not move somewhere where political dissidents can be abducted willy nilly.</p><p>Some people are worried about unrest in the US, which might be reasonable, but on that metric it's still better to live here than, say, Mali or Afghanistan.</p><p><strong>Local</strong> <strong>political environment / culture</strong></p><p>Same basic considerations as the above. California may be an increasingly hostile environment for our community, but it's almost certainly still better to live here than in a town where people fly Confederate flags and openly carry guns.&nbsp;</p><p>It's also really valuable to be near Silicon Valley. The Bay Area has a general culture of ambition and intellectual curiosity that's hard to find.</p><p><strong>General infrastructure</strong></p><p>People talk wistfully about private islands or about founding our own town, but my guess is that most of those people haven't actually thought those ideas through. A place needs SO MANY THINGS to sustain a modern human population: roads, electricity, water, laws, buildings, police, medicine, commerce, trash collection... and those are just the basic necessities! Despite the appeal of building something from the ground up and thus controlling every aspect of its development, it just seems way better to move to a place that already has this basic infrastructure in place.</p><h2>Other important considerations</h2><p><strong>Cost of living</strong></p><p>A major complaint about the Bay Area is rental prices, and justifiably so. Obviously cost of living interacts with a lot of other factors, but on the whole, it would feel pretty silly to leave the Bay only to move somewhere with equally high rent.&nbsp;</p><p><strong>Occupancy laws</strong></p><p>Many municipalities, at least in the US, have laws prohibiting unrelated adults from sharing a home. This would render most group houses illegal.</p><p><strong>Modern conveniences</strong></p><p>Berkeley has fiber internet, 2-day Amazon delivery, a myriad of quick restaurant and grocery delivery options, and excellent coverage by Lyft, Uber, and bikeshares. I expect many would be reluctant to give up this level of convenience. This is a strike against private islands, remote castles, and developing countries, among others.</p><p><strong>Walkability (/ bikeability / public transit)</strong></p><p>Sparse suburban areas are <i>terrible </i>places to build community. In addition, driving is dangerous and owning a car is super annoying. We should settle somewhere where it's possible to all live close enough together that we can visit each other on foot, and also ideally where the city center is within walking distance of our homes.&nbsp;</p><p>(Being able to bike safely and easily between homes and city center would also work. Sufficiently good public transit might also do the trick.)</p><p><strong>Medical care</strong></p><p>It's really important to have quick access to modern medicine – rationalists may largely be healthy 20-somethings, but healthy 20-somethings can still die of sepsis if they can't get antibiotics quickly. This is an argument against many locations in developing countries. It could also be construed as an argument against the US, where medical care is theoretically available but often avoided due to expense.</p><h2>Additional things to consider</h2><p><strong>Crime</strong></p><p>All else equal, less crime seems better. If that's not possible, property crime is better than violent crime. It's really unpleasant to have your bike or laptop stolen, but it's a lot worse when it happens at gunpoint (which happened to some of my friends when I lived in Chicago).&nbsp;</p><p>(Aside: High-trust environments are great, but I would guess that in general they're also more insular, which might make it hard to pick up our ~300-person community, plop it down in an existing high-trust town, and have everyone maintain those high trust levels. No real action item here and I'm confused.)</p><p><strong>Schools</strong></p><p>Rationalists may be less likely than average to want kids, but that doesn't mean none of us are having them. I don't know if there's anywhere in the world that has truly non-terrible schools, but at least some schools are a lot less terrible than others.</p><p><strong>Weather</strong></p><p>A lot of people who live in California really hate extreme weather. A lot of people have SAD and don't want to live in a place that has winters. Natural disasters are bad too.</p><hr><h1>Call to Action</h1><p>As I said above, <strong>I'd be excited for people to pitch their own favorite Location X! </strong>Write an essay making your case, or even just a bullet-pointed comment.</p><p>And please also let me know if there are additional considerations I missed.</p>",mingyuan,mingyuan,mingyuan,
8eX8DJctsACtR2sfX,"[AN #118]: Risks, solutions, and prioritization in a world with many AI systems",an-118-risks-solutions-and-prioritization-in-a-world-with,https://www.lesswrong.com/posts/8eX8DJctsACtR2sfX/an-118-risks-solutions-and-prioritization-in-a-world-with,2020-09-23T18:20:04.779Z,15,6,6,False,False,,"<p>Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter <strong><a href=""http://rohinshah.com/alignment-newsletter/"">resources here</a></strong>. In particular, you can look through <strong><a href=""https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing"">this spreadsheet</a></strong> of all summaries that have ever been in the newsletter.</p><p>Audio version <strong><a href=""http://alignment-newsletter.libsyn.com/alignment-newsletter-118"">here</a></strong> (may not be up yet). 			  </p><h1>HIGHLIGHTS </h1><p><strong><a href=""https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact"">AI Governance: Opportunity and Theory of Impact</a></strong> <em>(Allan Dafoe)</em> (summarized by Rohin): What is the theory of change for work on AI governance? Since the world is going to be vastly complicated by the broad deployment of AI systems in a wide variety of contexts, several <em>structural risks</em> will arise. AI governance research can produce &#x201C;assets&#x201D; (e.g. policy expertise, strategic insights, important networking connections, etc) that help humanity make better decisions around these risks. Let&#x2019;s go into more detail.</p><p>A common perspective about powerful AI is the &#x201C;superintelligence&#x201D; perspective, in which we assume there is a single very cognitively powerful AI agent. This leads people to primarily consider &#x201C;accident&#x201D; and &#x201C;misuse&#x201D; risks, in which either the AI agent itself &#x201C;wants&#x201D; to harm us, or some bad actor uses the AI agent to harm us.</p><p>However, it seems likely that <strong>we should think of an ecology of AI agents, or AI as a general purpose technology (GPT)</strong>, as in e.g. <strong><a href=""https://www.fhi.ox.ac.uk/reframing/"">CAIS</a></strong> (<strong><a href=""https://mailchi.mp/b649f32b07da/alignment-newsletter-40"">AN #40</a></strong>) or <strong><a href=""https://ageofem.com/"">Age of Em</a></strong>. In this case, we can examine the ways in which narrow AI could transform social, military, economic, and political systems, and the <em>structural risks</em> that may arise from that. Concrete examples of potential existential structural risks induced by AI include nuclear instability, geopolitical turbulence, authoritarianism, and value erosion through competition.</p><p>A key point about the examples above is that the relevant factors for each are different. For example, for nuclear instability, it is important to understand nuclear deterrence, first strike vulnerability and how it could change with AI processing of satellite imagery, undersea sensors, cyber surveillance and weapons, etc. In contrast, for authoritarianism, relevant processes include global winner-take-all-markets, technological displacement of labor, and authoritarian surveillance and control.</p><p>This illustrates a general principle: unlike in the superintelligence perspective, <strong>the scope of both risks and solutions in the ecology / GPT perspectives is very broad</strong>. As a result, we need a broad range of expertise and lots of connections with existing fields of research. In particular, <strong>&#x201C;we want to build a metropolis -- a hub with dense connections to the broader communities of computer science, social science, and policymaking -- rather than an isolated island&#x201D;</strong>.</p><p>Another important aspect here is that in order to <em>cause better decisions to be made</em>, we need to focus not just on generating the right ideas, but also on ensuring the right ideas are in the right places at the right time (e.g. by ensuring that people with the right tacit knowledge are part of the decision-making process). Instead of the &quot;product model&quot; of research that focuses on generating good ideas, we might instead want a &#x201C;field-building model&#x201D;, which also places emphasis on improving researcher&#x2019;s competence on a variety of issues, bestowing prestige and authority on those who have good perspectives on long-term risks, improving researcher&#x2019;s networks, and training junior researchers. However, often it is best to focus on the product model of research anyway, and get these benefits as a side effect.</p><p>To quote the author: &#x201C;I think there is a lot of useful work that can be done in advance, but most of the work involves us building our competence, capacity, and credibility, so that when the time comes, we are in position and ready to formulate a plan. [...] Investments we make today should increase our competence in relevant domains, our capacity to grow and engage effectively, and the intellectual credibility and policy influence of competent experts.&#x201D;</p><p><strong>Rohin&apos;s opinion:</strong> See the next summary. Note also that the author is organizing the <strong><a href=""https://www.cooperativeai.com/"">Cooperative AI Workshop</a></strong> (<strong><a href=""https://mailchi.mp/d31663e4d330/an-116-how-to-make-explanations-of-neurons-compositional"">AN #116</a></strong>) to tackle some of these issues.</p><p><strong><a href=""https://futureoflife.org/2020/09/15/andrew-critch-on-ai-research-considerations-for-human-existential-safety/?utm_source=feedly&amp;utm_medium=rss&amp;utm_campaign=andrew-critch-on-ai-research-considerations-for-human-existential-safety"">Andrew Critch on AI Research Considerations for Human Existential Safety</a></strong> <em>(Lucas Perry and Andrew Critch)</em> (summarized by Rohin): This podcast discusses the recent <strong><a href=""http://acritch.com/papers/arches.pdf"">ARCHES</a></strong> (<strong><a href=""https://mailchi.mp/60475c277263/an-103-arches-an-agenda-for-existential-safety-and-combining-natural-language-with-deep-rl"">AN #103</a></strong>) document, and several thoughts surrounding it. There&#x2019;s a lot in here that I won&#x2019;t summarize, including a bunch of stuff that was in the summary of ARCHES. I&#x2019;m going to focus primarily on the (substantial) discussion of how to prioritize within the realm of possible risks related in some way to AI systems.</p><p>Firstly, let&#x2019;s be clear about the goal: ensuring existential safety, that is, making sure human extinction never happens. Note the author means literal extinction, as opposed to something like &#x201C;the loss of humanity&#x2019;s long-term potential&#x201D;, because the former is clearer. While it is not always clear whether something counts as &#x201C;extinction&#x201D; (what if we all become uploads?), it is a lot clearer than whether a scenario counts as a loss of potential.</p><p>Typical alignment work focuses on the &#x201C;single-single&#x201D; case, where a single AI system must be aligned with a single human, as in e.g. <strong><a href=""https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment"">intent alignment</a></strong> (<strong><a href=""https://mailchi.mp/b6dc636f6a1b/alignment-newsletter-33"">AN #33</a></strong>). However, this isn&#x2019;t ultimately what we care about: we care about multi-multi existential safety, that is, ensuring that when multiple AI systems act in a world with multiple humans, extinction does not happen. There are pretty significant differences between these: in particular, it&#x2019;s not clear whether multi-multi &#x201C;alignment&#x201D; even has meaning, since it is unclear whether it makes sense to view humanity as an agent to which an AI system could be &#x201C;aligned&#x201D;.</p><p>Nonetheless, single-single alignment seems like an important subproblem of multi-multi existential safety: we will be delegating to AI systems in the future; it seems important that we know how to do so. How do we prioritize between single-single alignment, and the other subproblems of multi-multi existential safety? A crucial point is that single-single work will not be neglected, because companies have strong incentives to solve single-single alignment (both in the sense of optimizing for the right thing, and for being robust to distributional shift). In contrast, in multi-multi systems, it is often the case that there is a complex set of interacting effects that lead to some negative outcome, and there is no one actor to blame for the negative outcome, and as a result it doesn&#x2019;t become anybody&#x2019;s job to prevent that negative outcome.</p><p>For example, if you get a huge medical bill because the necessary authorization forms hadn&#x2019;t been filled out, whose fault is it? Often in such cases there are many people to blame: you could blame yourself for not checking the authorization, or you could blame the doctor&#x2019;s office for not sending the right forms or for not informing you that the authorization hadn&#x2019;t been obtained, etc. Since it&#x2019;s nobody&#x2019;s job to fix such problems, they are and will remain neglected, and so work on them is more impactful.</p><p>Something like transparency is in a middle ground: it isn&#x2019;t profitable yet, but probably will be soon. So, if someone were indifferent between a bunch of areas of research, the author would advise for e.g. multi-stakeholder delegation over transparency over robustness. However, the author emphasizes that it&#x2019;s far more important that people work in some area of research that they find intellectually enriching and relevant to existential safety.</p><p>The podcast has lots of other points, here is an incomplete quick selection of them:</p><p>- In a multi-multi world, without good coordination you move the world in a &#x201C;random&#x201D; direction. There are a lot of variables which have to be set just right for humans to survive (temperature, atmospheric composition, etc) that are not as important for machines. So sufficiently powerful systems moving the world in a &#x201C;random&#x201D; direction will lead to human extinction.</p><p>- One response to the multi-multi challenge is to have a single group make a powerful AI system and &#x201C;take over the world&#x201D;. This approach is problematic since many people will oppose such a huge concentration of power. In addition, it is probably not desirable even if possible, since it reduces robustness by creating a single point of failure.</p><p>- Another suggestion is to create a powerful AI system that protects humanity (but is still uncontrollable in that humanity cannot stop its operation). The author does not like the solution much, because if we get it wrong and deploy a misaligned uncontrollable AI system, then we definitely die. The author prefers that we instead always have control over the AI systems we deploy.</p><p><strong>Rohin&apos;s opinion:</strong> Both this and the previous summary illustrate an increasingly common perspective:</p><p>1. The world is not going to look like &#x201C;today&#x2019;s world plus a single AGI agent&#x201D;: instead, we will likely have a proliferation of many different AI systems specialized for different purposes.</p><p>2. In such a world, there are a lot of different challenges that aren&#x2019;t standard intent alignment.</p><p>3. We should focus on these other challenges because [a variety of reasons].</p><p><strong>If you have technical CS skills</strong>, how should you prioritize between this perspective and the more classical intent alignment perspective?</p><p><strong>Importance.</strong> I&#x2019;ve <strong><a href=""https://aiimpacts.org/conversation-with-rohin-shah/"">estimated</a></strong> (<strong><a href=""https://mailchi.mp/b3dc916ac7e2/an-80-why-ai-risk-might-be-solved-without-additional-intervention-from-longtermists"">AN #80</a></strong>) a 10% chance of existential catastrophe via a failure of intent alignment, absent intervention from longtermists to address intent alignment. Estimates vary quite a lot, even among people who have thought about the problem a lot; I&#x2019;ve heard as low as &lt; 1% and as high as 80% (though these usually don&#x2019;t assume &#x201C;no intervention from longtermists&#x201D;).</p><p>It&#x2019;s harder to estimate the importance of structural risks and extinction risks highlighted in the two summaries above, but the arguments in the previous two posts seem reasonably compelling and I think I&#x2019;d be inclined to assign a similar importance to it (i.e. similar probability of causing an existential catastrophe).</p><p>Note that this means I&#x2019;m disagreeing with Critch: he believes that we are far more likely to go extinct through effects unique to multi-multi dynamics; in contrast I find the argument less persuasive because we do have governance, regulations, national security etc. that would already be trying to mitigate issues that arise in multi-multi contexts, especially things that could plausibly cause extinction.</p><p><strong>Neglectedness.</strong> I&#x2019;ve already taken into account neglectedness outside of EA in estimating the probabilities for importance. Within EA there is already a huge amount of effort going into intent alignment, and much less in governance and multi-multi scenarios -- perhaps a difference of 1-2 orders of magnitude; the difference is even higher if we only consider people with technical CS skills.</p><p><strong>Tractability.</strong> I buy the argument in Dafoe&#x2019;s article that for AI governance due to our vast uncertainty we need a &#x201C;metropolis&#x201D; model where field-building is quite important; I think that implies that solving the full problem (at today&apos;s level of knowledge) would require a lot of work and building of expertise. In contrast, with intent alignment, we have a single technical problem with significantly less uncertainty. As a result, I expect that currently in expectation a single unit of work goes further to solving intent alignment than to solving structural risks / multi-multi problems, and so intent alignment is more tractable.</p><p>I also expect technical ideas to be a bigger portion of &quot;the full solution&quot; in the case of intent alignment -- as Dafoe argues, I expect that for structural risks the solution looks more like &quot;we build expertise and this causes various societal decisions to go better&quot; as opposed to &quot;we figure out how to write this piece of code differently so that it does better things&quot;. This doesn&apos;t have an obvious impact on tractability -- if anything, I&apos;d guess it argues in favor of the tractability of work on structural risks, because it seems easier to me to create prestigious experts in particular areas than to make progress on a challenging technical problem whose contours are still uncertain since it arises primarily in the future.</p><p>I suspect that I disagree with Critch here: I think he is more optimistic about technical solutions to multi-multi issues themselves being useful. In the past I think humanity has resolved such issues via governance and regulations and it doesn&#x2019;t seem to have relied very much on technical research; I&#x2019;d expect that trend to continue.</p><p><strong>Personal fit.</strong> This is obviously important, but there isn&#x2019;t much in general for me to say about it.</p><p>Once again, I should note that this is all under the assumption that you have technical CS skills. I think overall I end up pretty uncertain which of the two areas I&#x2019;d advise going in (assuming personal fit was equal in both areas). However, if you are more of a generalist, I feel much more inclined to recommend choosing some subfield of AI governance, again subject to personal fit, and Critch agrees with this.</p><h1>TECHNICAL AI ALIGNMENT </h1><br><h2>HANDLING GROUPS OF AGENTS </h2><p><strong><a href=""https://www.alignmentforum.org/posts/cYsGrWEzjb324Zpjx/comparing-utilities"">Comparing Utilities</a></strong> <em>(Abram Demski)</em> (summarized by Rohin): This is a reference post about preference aggregation across multiple individually rational agents (in the sense that they have <strong><a href=""https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem"">VNM-style</a></strong> utility functions), that explains the following points (among others):</p><p>1. The concept of &#x201C;utility&#x201D; in ethics is somewhat overloaded. The &#x201C;utility&#x201D; in hedonic utilitarianism is very different from the VNM concept of utility. The concept of &#x201C;utility&#x201D; in preference utilitarianism is pretty similar to the VNM concept of utility.</p><p>2. Utilities are not directly comparable, because affine transformations of utility functions represent exactly the same set of preferences. Without any additional information, concepts like &#x201C;utility monster&#x201D; are type errors.</p><p>3. However, our goal is not to compare utilities, it is to aggregate people&#x2019;s preferences. We can instead impose constraints on the aggregation procedure.</p><p>4. If we require that the aggregation procedure produces a Pareto-optimal outcome, then Harsanyi&#x2019;s utilitarianism theorem says that our aggregation procedure can be viewed as maximizing some linear combination of the utility functions.</p><p>5. We usually want to incorporate some notion of fairness. Different specific assumptions lead to different results, including variance normalization, Nash bargaining, and Kalai-Smorodinsky.</p><h2>FORECASTING </h2><p><strong><a href=""https://www.openphilanthropy.org/blog/new-report-brain-computation"">How Much Computational Power It Takes to Match the Human Brain</a></strong> <em>(Joseph Carlsmith)</em> (summarized by Asya): In this blog post, Joseph Carlsmith gives a summary of his longer report estimating the number of floating point operations per second (FLOP/s) which would be <em>sufficient</em> to perform any cognitive task that the human brain can perform. He considers four different methods of estimation.</p><p>Using <em>the mechanistic method</em>, he estimates the FLOP/s required to model the brain&#x2019;s low-level mechanisms at a level of detail adequate to replicate human task-performance. He does this by estimating that ~1e13 - 1e17 FLOP/s is enough to replicate what he calls &#x201C;standard neuron signaling&#x201D; &#x2014; neurons signaling to each other via using electrical impulses (at chemical synapses) &#x2014; and learning in the brain, and arguing that including the brain&#x2019;s other signaling processes would not meaningfully increase these numbers. He also suggests that various considerations point weakly to the adequacy of smaller budgets.</p><p>Using <em>the functional method</em>, he identifies a portion of the brain whose function we can approximate with computers, and then scales up to FLOP/s estimates for the entire brain. One way to do this is by scaling up models of the human retina: Hans Moravec&apos;s estimates for the FLOP/s of the human retina imply 1e12 - 1e15 FLOP/s for the entire brain, while recent deep neural networks that predict retina cell firing patterns imply 1e16 - 1e20 FLOP/s.</p><p>Another way to use the functional method is to assume that current image classification networks with known FLOP/s requirements do some fraction of the computation of the human visual cortex, adjusting for the increase in FLOP/s necessary to reach robust human-level classification performance. Assuming somewhat arbitrarily that 0.3% to 10% of what the visual cortex does is image classification, and that the EfficientNet-B2 image classifier would require a 10x to 1000x increase in frequency to reach fully human-level image classification, he gets 1e13 - 3e17 implied FLOP/s to run the entire brain. Joseph holds the estimates from this method very lightly, though he thinks that they weakly suggest that the 1e13 - 1e17 FLOP/s estimates from the mechanistic method are not radically too low.</p><p>Using <em>the limit method</em>, Joseph uses the brain&#x2019;s energy budget, together with physical limits set by Landauer&#x2019;s principle, which specifies the minimum energy cost of erasing bits, to upper-bound required FLOP/s to ~7e21. He notes that this relies on arguments about how many bits the brain erases per FLOP, which he and various experts agree is very likely to be &gt; 1 based on arguments about algorithmic bit erasures and the brain&apos;s energy dissipation.</p><p>Lastly, Joseph briefly describes <em>the communication method</em>, which uses the communication bandwidth in the brain as evidence about its computational capacity. Joseph thinks this method faces a number of issues, but some extremely preliminary estimates suggest 1e14 FLOP/s based on comparing the brain to a V100 GPU, and 1e16 - 3e17 FLOP/s based on estimating the communication capabilities of brains in traversed edges per second (TEPS), a metric normally used for computers, and then converting to FLOP/s using the TEPS to FLOP/s ratio in supercomputers. </p><p>Overall, Joseph thinks it is more likely than not that 1e15 FLOP/s is enough to perform tasks as well as the human brain (given the right software, which may be very hard to create). And he thinks it&apos;s unlikely (&lt;10%) that more than 1e21 FLOP/s is required. For reference, an NVIDIA V100 GPU performs up to 1e14 FLOP/s (although FLOP/s is not the only metric which differentiates two computational systems.)</p><p><strong>Read more:</strong> <strong><a href=""https://www.openphilanthropy.org/brain-computation-report"">Full Report: How Much Computational Power Does It Take to Match the Human Brain?</a></strong></p><p><strong>Asya&apos;s opinion:</strong> I really liked this post, although I haven&apos;t gotten a chance to get through the entire full-length report. I found the reasoning extremely legible and transparent, and there&apos;s no place where I disagree with Joseph&apos;s estimates or conclusions. See also <strong><a href=""https://jack-clark.net/2020/09/14/import-ai-214-nvidias-40bn-arm-deal-a-new-57-subject-nlp-test-ai-for-plant-disease-detection/"">Import AI&apos;s summary</a></strong>.</p><h2>MISCELLANEOUS (ALIGNMENT) </h2><p><strong><a href=""https://www.alignmentforum.org/posts/qEjh8rpxjG4qGtfuK/the-backchaining-to-local-search-technique-in-ai-alignment"">The &quot;Backchaining to Local Search&quot; Technique in AI Alignment</a></strong> <em>(Adam Shimi)</em> (summarized by Rohin): This post explains a technique to use in AI alignment, that the author dubs &#x201C;backchaining to local search&#x201D; (where local search refers to techniques like gradient descent and evolutionary algorithms). The key idea is to take some proposed problem with AI systems, and figure out mechanistically how that problem could arise when running a local search algorithm. This can help provide information about whether we should expect the problem to arise in practice.</p><p><strong>Rohin&apos;s opinion:</strong> I&#x2019;m a big fan of this technique: it has helped me notice that many of my concepts were confused. For example, this helped me get deconfused about wireheading and inner alignment. It&#x2019;s an instance of the more general technique (that I also like) of taking an abstract argument and making it more concrete and realistic, which often reveals aspects of the argument that you wouldn&#x2019;t have previously noticed.</p><h1>NEWS </h1><p><strong><a href=""https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship"">The Open Phil AI Fellowship</a></strong> (summarized by Rohin): We&#x2019;re now at the fourth cohort of the <strong><a href=""https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship"">Open Phil AI Fellowship</a></strong> (<strong><a href=""https://mailchi.mp/c8ea4a5e842f/an-66-decomposing-robustness-into-capability-robustness-and-alignment-robustness"">AN #66</a></strong>)! Applications are due October 22.</p><p><strong><a href=""https://nbiair.com/"">Navigating the Broader Impacts of AI Research</a></strong> (summarized by Rohin): This is a workshop at NeurIPS; the title tells you exactly what it&apos;s about. The deadline to submit is October 12.</p><h4><strong>FEEDBACK</strong></h4><p> I&apos;m always happy to hear feedback; you can send it to me, <strong><a href=""https://rohinshah.com/"">Rohin Shah</a></strong>, by <strong>replying to this email</strong>.                         </p><h4><strong>PODCAST</strong></h4><p>An audio podcast version of the <strong>Alignment Newsletter</strong> is available. This podcast is an audio version of the newsletter, recorded by <strong><a href=""http://robertskmiles.com/"">Robert Miles</a></strong>.</p>",rohinmshah,rohinmshah,Rohin Shah,
PAG3z2ZPne33sC6tQ,Petrov Day Ritual: Coronavirus Edition,petrov-day-ritual-coronavirus-edition,https://www.lesswrong.com/posts/PAG3z2ZPne33sC6tQ/petrov-day-ritual-coronavirus-edition,2020-09-23T16:41:39.856Z,6,1,0,False,False,https://thingofthings.wordpress.com/2020/09/18/petrov-day-ritual-coronavirus-edition/,"<p>Ozy adapts <a href=""https://www.lesswrong.com/posts/fr8MEigHzJeFQkctL/petrov-day-is-september-26"">the rationalist Petrov Day ritual</a> for 2020.</p>",mingyuan,mingyuan,mingyuan,
SfNwpyL7o49ohYyWB,Dehumanisation *errors*,dehumanisation-errors,https://www.lesswrong.com/posts/SfNwpyL7o49ohYyWB/dehumanisation-errors,2020-09-23T09:51:53.091Z,13,3,0,False,False,,"<p>In response to my post <a href=""https://www.lesswrong.com/posts/LkytHQSKbQFf6toW5/anthropomorphisation-vs-value-learning-type-1-vs-type-2"">contrasting value learning with anthropomorphisation</a>, steve2152 brought up the fact that <a href=""https://www.lesswrong.com/posts/LkytHQSKbQFf6toW5/anthropomorphisation-vs-value-learning-type-1-vs-type-2?commentId=4PcxAHE9Cm82wvkoW"">dehumanisation can be seen as the opposite of anthropomorphisation</a>.</p>
<p>I agree with this insight, but only when dehumanisation causes <em>errors</em> of interpretation. I was using empathy in the sense of ""insight into the other agent"", rather than ""sympathy with the other agent"".</p>
<p>In practice, dehumanisation does tend to cause errors. We see outgroups as <a href=""https://en.wikipedia.org/wiki/Out-group_homogeneity"">more homogeneous</a>, coherent, and organised than they actually are. Despite the suave psychopaths depicted in movies, psychopaths tend to be less effective at achieving their goals (as evidenced by the large number of psychopaths in prison). Torturers are <a href=""https://en.wikipedia.org/wiki/Effectiveness_of_torture_for_interrogation"">less effective</a> at extracting true information than classical interrogators.</p>
<p>Now, it's not a universal law by any means, but it does seem that dehumanisation can often lead to errors, and from that perspective can be seen as a failure of value learning.</p>
<h1>The meaning of errors</h1>
<ul>
<li>""Objection! Hold on just a minute!"" screams the convenient strawman I have just constructed.</li>
</ul>
<p><img src=""https://www.dropbox.com/s/q8r2v496nl82d0k/xbox_ban_objection-thumb-550x404-28829.jpg?raw=1"" alt=""""></p>
<ul>
<li>
<p>""You've claimed that 'agent's goals' are <a href=""https://youtu.be/1M9CvESSeVc?t=810"">interpretations by the outside observer</a>; that you can model a human as perfectly rational, <a href=""https://arxiv.org/abs/1712.05812"">without being wrong</a>. You've <a href=""https://www.lesswrong.com/posts/9rjW9rhyhJijHTM92/learning-human-preferences-black-box-white-box-and"">claimed</a> that this is 'structured white box knowledge', which can't be deduced from the agent's policy or its algorithm.""</p>
</li>
<li>
<p>""Given that, how can you claim that anyone 'fails' at interpreting the goals of others, or that they make 'errors'?""</p>
</li>
</ul>
<p>This a very valid point, strawman, but I've also <a href=""https://www.lesswrong.com/posts/LkytHQSKbQFf6toW5/anthropomorphisation-vs-value-learning-type-1-vs-type-2"">pointed out</a> that human theory of mind/empathy is very similar from human to human, and tends to agree with how we interpret our own goals. Because of this, there is a rough ""universal human theory of mind"", ie a universal way of going from human policy to human preferences.</p>
<p>When I'm talking about errors, I'm talking about deviations from this ideal<sup class=""footnote-ref""><a href=""#fn-ANGWprQ5r5BzzMPbw-1"" id=""fnref-ANGWprQ5r5BzzMPbw-1"">[1]</a></sup>.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-ANGWprQ5r5BzzMPbw-1"" class=""footnote-item""><p>Because human theories of mind do <em>not</em> agree perfectly, there will always be an irreducible level of uncertainty in this ideal, but there is agreement on the broad strokes of it. <a href=""#fnref-ANGWprQ5r5BzzMPbw-1"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
BisQ5ZNxAS4mPjhcw,The new Editor,the-new-editor,https://www.lesswrong.com/posts/BisQ5ZNxAS4mPjhcw/the-new-editor,2020-09-23T02:25:53.914Z,58,19,24,False,False,,"<figure><table><tbody><tr><td><strong>Look</strong></td><td><strong>at</strong></td><td><strong>this</strong></td><td><strong>glorious</strong></td><td><strong>table</strong></td></tr></tbody></table></figure><h2><strong>Celebrations! The new editor is finally here!&nbsp;</strong></h2><p>Starting from today, all desktop users will by-default use the new visual editor that we've been testing for a while. While the primary goal of this is to have a better foundation on which to build future editor features, here are a number of things you can do starting from today:&nbsp;</p><ul><li>Insert tables! A heavily requested feature.</li><li>Copy-paste LaTeX without everything breaking!</li><li>Nest bullet lists and other block elements in blockquotes! (still no nested blockquotes though, though if enough people want that, it would be easy to change)</li><li>Image Uploads! (Just drag-and-drop images into the editor, and things should work out naturally. You can also copy-paste, though beware that copy-pasting from other websites means we link to the copies of those images from other websites, and won't reupload them.)</li><li>Much less jankyness and brokenness!</li></ul><p>Let us know what you think about the new editor. We've been testing it for a while and have been pretty happy with it (and users who had opted into beta features also had predominantly positive feedback). You can also use the old editor if you run into any problems by checking the ""Restore the previous WYSIWYG editor"" checkbox in your user settings.</p>",habryka4,habryka4,habryka,
zaGsZ5uSzCseTmCFu,New Tagging Power Tools: Dashboard + Upgraded Tag Editing Experience,new-tagging-power-tools-dashboard-upgraded-tag-editing,https://www.lesswrong.com/posts/zaGsZ5uSzCseTmCFu/new-tagging-power-tools-dashboard-upgraded-tag-editing,2020-09-23T01:49:14.600Z,20,5,0,False,False,,"<p>Following the strong response of taggers to calls for help, plus lessons learned from our earlier designs, we are launching new and improved power tools for maintaining and improving the tag/wiki (twiki?) corpus.</p><p>We are first applying these tools to completing the underway <a href=""https://www.lesswrong.com/posts/ELN6FpRLoeLJPgx8z/the-wiki-is-dead-long-live-the-wiki-help-wanted""><strong>Wiki Import campaign</strong></a>, and when that's done we'll reconfigure them for long-term use.</p><p>There are several connected parts to the new tools:</p><ol><li><strong>Tag Flags</strong></li><li><strong>New Twiki Dashboard</strong><a href=""https://www.lesswrong.com/tags/dashboard""><strong> (www.lesswrong.com/tags/dashboard)</strong></a></li><li><strong>Upgraded Tag Editing</strong></li><li><strong>Tag Discussion Sections</strong></li></ol><h1>Tag Flags</h1><p>At the heart of the new Tagging Power Tools are <i>tag flags. </i>These are a limited and generally fixed set of flags that people can set on tags when they require work, and then remove when the work is done.&nbsp;</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_90 90w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_180 180w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_270 270w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_360 360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_450 450w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_540 540w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_630 630w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_720 720w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_810 810w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/89f705e8ced32d586b473039573f91274557dd5042ffcc4b.png/w_857 857w""><figcaption>The Special Tag Flags for the Wiki Import&nbsp;<br>The numbers indicate how many tags have this flag set.</figcaption></figure><p>Temporarily, the tag flags are designed specifically for the import (those from the deprecated spreadsheet). Long-term they'll be general things like ""needs a description"", ""should have more posts tagged"", ""is out of date"". The tag flags will be managed by admins, of course, with input from taggers. This is largely because we want to keep the number to something 5 main ones and maybe another 5-10 minor ones. Keep it manageable.</p><p>We are deprecating the <a href=""https://www.lesswrong.com/tag/tag-grading-scheme"">Tag Grading Scheme</a>.</p><h1>New Twiki Dashboard</h1><p>You can get lists of tags with each kind of tag-flag on the new dashboard. By default, you get a list of all tags with any flag applied. If you click on a flag in the header list, the page will be filtered for only tags with that tag. The selected Tag Filter displays in <strong>black</strong>. You can remove the filter with the <i>Reset Filter </i>button.</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a1b6f26897d0f8a0ca3da6696d3a9ae62a0d50df006e7d46.png/w_983 983w""></figure><p>If you hover of the flag, you'll see a description describing what it means and how to address it.</p><p>&nbsp;</p><figure style=""width:72.49%;""><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_120 120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_360 360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_720 720w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_840 840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_960 960w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_1080 1080w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/53bb65f69b4740fc2d9b1fc10edd22e391356900119e8407.png/w_1152 1152w""><figcaption>Hover-Over explains the Tag Flag</figcaption></figure><h2>Editing Tags</h2><p>Once you've found tags you want to edit, you have two options for how to do that.</p><ol><li><strong>Twiki Dashboard On-Page Editing</strong></li><li><strong>Full-Edit on Tag Page</strong></li></ol><p>On the Twiki Dashboard page, if you click the <i>Edit</i> button for a tag, it will open it up for editing on-page.</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_190 190w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_380 380w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_570 570w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_760 760w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_950 950w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_1140 1140w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_1330 1330w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_1520 1520w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_1710 1710w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/caad5f6cbc5c8885eec5bcda9f3d29a06be5acf2d44b2008.png/w_1862 1862w""><figcaption>On-page editing.</figcaption></figure><p>As you see, here you can toggle the flags. Don't forget to click submit! If a removed flag was part of your filter, the tag probably won't disappear from the list until you refresh.</p><h1>Upgraded Tag Editing</h1><p>Continuing from the above, you can alternatively start editing tags in full-edit mode by clicking on the text body on the dashboard (rather the <i>Edit </i>button).</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_170 170w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_340 340w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_510 510w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_680 680w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_850 850w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_1020 1020w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_1190 1190w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_1360 1360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_1530 1530w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/925d9c7eb4b5c0b514954183c1fb710c1e785022ae30fcd8.png/w_1666 1666w""><figcaption>Click anywhere this region on a tag to open the tag page in editing mode.</figcaption></figure><p><strong>The new full tag-editing experience looks like this:</strong></p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_180 180w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_360 360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_540 540w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_720 720w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_1080 1080w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_1260 1260w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_1440 1440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_1620 1620w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4daf5eafcc6f91a94e9897868c66502748767318b215caac.png/w_1748 1748w""><figcaption>The Editing Mode for Tags looks just like Published Page</figcaption></figure><p>Things to note:</p><ul><li>Opens in edit mode, ready to go. Remember to click submit!</li><li>Tag flags can be set.</li><li>""Next Tag"" [in filter list] button.</li><li>Tag Discussion Section</li><li>Posts List (can add posts from here)</li><li>For imported old wiki pages, there are links to the original on the old site, and the latest revision of the import.<ul><li>These can be used to compare with the pre-import page for formatting issues, etc., and for copying/merging text. Copy from the revision page rather than the old wiki site since it has the links fixed up.</li></ul></li></ul><p>It's everything you need to edit and improve tags with minimal clicks.</p><h2>Tag Flag Playlists</h2><p>If you've clicked through to editing a tag via a filter on the dashboard, instead of going back to the dashboard, you can move onto the next tag with a handy <i>next tag </i>button. This takes you to the next from the tag flag list you've selected.</p><figure style=""width:48.11%;""><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png/w_104 104w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png/w_184 184w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png/w_264 264w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png/w_344 344w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png/w_424 424w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png/w_504 504w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png/w_584 584w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png/w_664 664w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7c3f5e35cf1cba5df59ac13036b3c6176e3f7bfa36795ec.png/w_744 744w""></figure><p>&nbsp;</p><h1>Tag Discussion</h1><p>A week or two ago, we added a Discussion section for each tag/wiki. You can get to them by clicking the <i>Discussion </i>button on the top right of tag pages.</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_640 640w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_960 960w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_1120 1120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_1280 1280w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_1440 1440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png/w_1540 1540w""></figure><p><i>We've also </i>placed the same Discussion section beneath the tag text when editing. This is so people can leave notes and ask questions about a tag while editing it, super conveniently. We'll work on further improving the visibility of these comments.</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_180 180w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_360 360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_540 540w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_720 720w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_1080 1080w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_1260 1260w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_1440 1440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_1620 1620w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e2a32ed03948e03f2b4b9f4a15461a80113f9ba1f3d989d9.png/w_1730 1730w""><figcaption>The Tag Discussion section, now visible when you're editing a tag.</figcaption></figure><h1>Feedback, please!</h1><p>That's it, folks, I hope you like it. Please try it out and let us know what you think.</p><p>As always, you can join the <a href=""https://join.slack.com/t/lwtaggers/shared_invite/zt-gvrubehu-fRnVK9hH_7SQcXmFXYB87A"">Tagger Slack</a> to chat live with other taggers and the LW team.</p>",Ruby,ruby,Ruby,
QWHeLizdEaHFktsko,AI Advantages [Gems from the Wiki],ai-advantages-gems-from-the-wiki,https://www.lesswrong.com/posts/QWHeLizdEaHFktsko/ai-advantages-gems-from-the-wiki,2020-09-22T22:44:36.671Z,22,10,7,False,False,https://www.lesswrong.com/tag/ai-advantages,"<p><i>During the </i><a href=""https://www.lesswrong.com/posts/ELN6FpRLoeLJPgx8z/the-wiki-is-dead-long-live-the-wiki-help-wanted""><i>LessWrong 1.0 Wiki Import</i></a><i> we (the LessWrong team) discovered a number of great articles that most of the LessWrong team hadn't read before. Since we expect many others to also not have have read these, we are creating a series of the best posts from the Wiki to help give those hidden gems some more time to shine.</i></p><p><i>The original wiki article was fully written by </i><a href=""https://www.lesswrong.com/users/kaj_sotala""><i>Kaj Sotala</i></a><i>, who I've added as a coauthor to this post. Thank you for your work on the wiki!</i></p><hr><p><strong>AI advantages</strong> are various factors that might favor AIs in case there was ever a conflict between them and humans. These can be classified as hardware advantages, self-improvement capabilities, co-operative advantages, and human handicaps.</p><h2>Hardware advantages</h2><ul><li><strong>Superior processing power:</strong> Having more serial processing power would let an AI think faster than humans, while having more parallel processing power and more memory would let it think about more things at once.</li></ul><h2>Self-improvement capabilities</h2><p>An AI with access to its source code may directly modify the way it thinks, or create a modified version of itself. An AI can intentionally be built in a manner that is easy to understand and modify, and may even read its own design documents. Self-improvement capabilities may enable <a href=""https://www.lesswrong.com/tag/recursive-self-improvement"">recursive self-improvement</a> to occur, thereby triggering an <a href=""https://www.lesswrong.com/tag/intelligence-explosion"">intelligence explosion</a>.</p><ul><li><strong>Improving algorithms:</strong> An AI may modify its existing algorithms, e.g. making them faster, to consume less memory, or to rely on fewer assumptions.</li><li><strong>Designing new mental modules:</strong> A mental module is a part of a mind that specializes in processing a certain kind of information. An AI could create entirely new kinds of modules, custom-tailored for specific problems.</li><li><strong>Modifiable motivation systems:</strong> Humans frequently suffer from problems such as procrastination, boredom, mental fatigue, and burnout. A mind which did not become bored or tired with its work would have a clear advantage over humans.</li></ul><h2>Co-operative advantages</h2><ul><li><strong>Copyability:</strong> A digital mind can be copied very quickly, and doing so has no cost other than access to the hardware required to run it.</li><li><strong>Perfect co-operation:</strong> Minds might be constructed to lack any self-interest. Such entities minds could share the same goal system and co-operate perfectly with one another.</li><li><strong>Superior communication:</strong> AIs could communicate with each other at much higher bandwidths than humans, and modify themselves to understand each other better.</li><li><strong>Transfer of skills:</strong> To the extent that skills can be modularized, digital minds could create self-contained skill modules to be shared with others.</li></ul><h2>Human handicaps</h2><p>Humans frequently reason in <a href=""https://www.lesswrong.com/tag/bias"">biased</a> ways. AIs might be built to avoid such biases.</p><ul><li><strong>Biases from computational limitations or false assumptions</strong>: Some human biases can be seen as assumptions or heuristics that fail to reason correctly in a modern environment, or as satisficing algorithms that do the best possible job given human computational resources.</li><li><strong>Human-centric biases</strong>: People tend to think of the capabilities of non-human minds, such as God or an artificial intelligence, as if the minds in question were human. This tendency persists even if humans are explicitly instructed to act otherwise.</li><li><strong>Biases from socially motivated cognition</strong>: It has also been proposed that humans have evolved to acquire beliefs which are socially beneficial, even if those beliefs weren't true.</li></ul><h2>References</h2><ul><li><strong>Kaj Sotala (2012):</strong> <a href=""https://stuff.kajsotala.fi/Papers/DigitalAdvantages.pdf"">Advantages of Artificial Intelligences, Uploads, and Digital Minds</a>. <i>International Journal of Machine Consciousness 4</i> (1), 275-291.</li></ul>",habryka4,habryka4,habryka,
DNN4xwqfTDKdmuNyi,"Sunday September 27, 12:00PM (PT) — talks by Alex Flint, Alex Zhu and more",sunday-september-27-12-00pm-pt-talks-by-alex-flint-alex-zhu,https://www.lesswrong.com/posts/DNN4xwqfTDKdmuNyi/sunday-september-27-12-00pm-pt-talks-by-alex-flint-alex-zhu,2020-09-22T21:59:56.546Z,11,2,0,False,False,,"<p>This Sunday at 12pm (PT), we're running another session of ""lightning talks"" by curated LessWrong authors (see <a href=""https://www.lesswrong.com/tag/lesswrong-events"">here</a> for previous weeks' transcripts).</p><ul><li>For the first hour, we will have a series of lightning talks each lasting about 5 minutes followed by discussion. The talks will be short and focus on presenting one core idea well, rather than rushing through a lot of content.</li><li>From 1PM to 2PM, we will hangout in Gather.town. Link will be sent out closer to the event.</li><li>We want to give top LessWrong writers an interesting space to discuss their ideas, and have more fruitful collaboration between users. Think of it like a cross between an academic colloquium and some friends chatting by a whiteboard.</li></ul><p><i>If you're a curated author and interested in giving a 5-min talk at a future event, which will then be transcribed and edited, sign up </i><a href=""https://forms.gle/iwFatbhys9muPmQA7""><i>here</i></a><i>.</i></p><h2>Speakers</h2><ul><li><strong>Alex Flint:</strong> ""Why go on retreat?""<ul><li><strong>Curated posts: </strong><a href=""https://www.lesswrong.com/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1""><strong>The Ground of Optimization</strong></a>, <a href=""https://www.lesswrong.com/posts/A9vvxguZMytsN3ze9/reply-to-paul-christiano-on-inaccessible-information""><strong>Reply to Paul Christiano on Inaccessible Information</strong></a>, <a href=""https://www.lesswrong.com/posts/qPoaA5ZSedivA4xJa/our-take-on-chai-s-research-agenda-in-under-1500-words""><strong>Our take on CHAI's research agenda in under 1500 words</strong></a></li></ul></li><li><strong>Other speakers: TBA</strong></li></ul><h2>Details</h2><p><strong>When? </strong>Sunday September 27, 12:00PM (PT)</p><p><strong>Where? </strong><a href=""https://us02web.zoom.us/j/87259855821"">https://us02web.zoom.us/j/87259855821</a></p>",habryka4,habryka4,habryka,
YthENnKAxnAKnd3ZJ,Comparative Advantage is Not About Trade,comparative-advantage-is-not-about-trade,https://www.lesswrong.com/posts/YthENnKAxnAKnd3ZJ/comparative-advantage-is-not-about-trade,2020-09-22T18:43:11.496Z,88,41,26,False,False,,"<p><a href=""https://www.amazon.com/Civilization-Capitalism-15th-18th-Century-Vol/dp/0520081145""><u>Braudel</u></a> is probably the most impressive historian I have read. His quantitative estimates of premodern populations and crop yields are exactly the sort of foundation you’d think any understanding of history would be based upon. Yet reading his magnum opus, it became steadily clearer as the books progressed that Braudel was missing some fairly fundamental economic concepts. I couldn’t quite put my finger on <i>what</i> was missing until a section early in book 3:</p><blockquote><p>... these deliberately simple tautologies make more sense to my mind than the so-called ‘irrefutable’ pseudo-theorem of David Ricardo (1817), whose terms are well known: that the relations between two given countries depend on the “comparative costs” obtaining in them at the point of production</p></blockquote><p>Braudel, apparently, is not convinced by the principle of <a href=""https://en.wikipedia.org/wiki/Comparative_advantage""><u>comparative advantage</u></a>. What is his objection?</p><blockquote><p>The division of labor on a world scale (or on world-economy-scale) cannot be described as a concerted agreement made between equal parties and always open to review… Unequal exchange, the origin of the inequality in the world, and, by the same token, the inequality of the world, the invariable generator of trade, are longstanding realities. In the economic poker game, some people have always held better cards than others…</p></blockquote><p>It seems Braudel is under the impression that comparative advantage is only relevant in the context of “equal” exchange or “free” trade or something along those lines.</p><p>If an otherwise impressive economic historian is that deeply confused about comparative advantage, then I expect other people are similarly confused. This post is intended to clarify.</p><p>The principle of comparative advantage does not require that trade be “free” or “equal” or anything of the sort. When the Portugese or the British seized monopolies on trade with India in the early modern era, those trades were certainly not free or equal. Yet the monopolists would not have made any profit whatsoever unless there were some underlying comparative advantage.</p><p>For example, consider an oversimplified model of the salt trade. People historically needed lots of salt to preserve food, yet many inland areas lack local sources, so salt imports were necessary for survival. <a href=""https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/4s2gbwMHSdh2SByyZ""><u>Transport by ship was historically orders of magnitude more efficient than overland</u></a>, so a government in control of a major river could grab a monopoly on the salt trade. Since the people living inland could not live without it, the salt monopolist could charge quite high prices - a “trade” arguably not so different from threatening inland farmers with death if they did not pay up. (An exaggeration, since there were other ways to store food and overland smuggling became viable at high enough prices, but I did say it’s an oversimplified example.)</p><p>Notice that, in this example, there is a clear underlying comparative advantage: the inland farmers have a comparative disadvantage in producing salt, while the ultimate salt supplier (a salt mine or salt pan) has a comparative advantage in salt production. If the farmer could produce salt with the same opportunity cost as the salt mine/pan, then the monopolist would have no buyers. If the salt mine/pan had the same opportunity cost for obtaining salt as the farmers, then the monopolist would have no supplier. Absent some underlying comparative advantage between two places, the trade monopolist cannot make any profit.</p><p>Another example: suppose I’m a transatlantic slave trader, kidnapping people in Africa and shipping them to slave markets in the Americas. It’s easy to see how the kidnapping part might be profitable, but why was it profitable to move people across the Atlantic? Why not save the transportation costs, and work the same slaves on plantations <i>in Africa</i> rather than plantations in the Americas? Or why not use native American slaves entirely, rather than importing Africans? Ultimately, the profits were because the Americas had a lot lower population density - there was more land, and fewer people to work it. Thus, labor was worth more in the Americas (and that same comparative advantage drove not just the slave trade, but also immigration and automation). Without a comparative advantage, enslaving people might still have been profitable, but there would be no reason to ship them across the Atlantic.</p><p>Let’s take it a step further. This argument need not involve any trade at all.</p><p>Suppose I’m the dictator of some small archipelago. I have total ownership and control over the country’s main industries (bananas and construction), and there’s an international embargo against trade with my little country, so there’s no trade to worry about either internally or externally. Let’s say I just want to maximize construction output - although I will still need to order <i>some</i> banana-growing in order to keep my construction workers fed.</p><p>The question is: who and where do I order to grow bananas, and who and where do I order to build things? To maximize construction, I will want to order people with the largest comparative advantage in banana-growing to specialize in banana-growing, and I will want to order those bananas to be grown on the islands with the largest comparative advantage in banana-growing. (In fact, this is not just relevant to maximization of construction - it applies to pareto-optimal production in general.) There’s no trade; I’m just using comparative advantage to figure out how best to deploy my own resources.</p><p>Takeaway: comparative advantage is not a principle of <i>trade</i>, it’s a principle of <i>optimization</i>. Pareto-optimal production means specialization by comparative advantage.</p>",johnswentworth,johnswentworth,johnswentworth,
sShC4ajY3xarMpbJa,The Haters Gonna Hate Fallacy,the-haters-gonna-hate-fallacy,https://www.lesswrong.com/posts/sShC4ajY3xarMpbJa/the-haters-gonna-hate-fallacy,2020-09-22T12:20:06.050Z,47,29,6,False,False,,"<p>Occasionally I see people doing what I think of as the &#x201C;Haters Gonna Hate Fallacy&#x201D;.</p><p>The HGHF says something like: &#x201C;People are going to misinterpret you no matter how carefully you word things. Therefore, there&#x2019;s no point wasting time wording things carefully.&#x201D;</p><p>An example:</p><p>&#x201C;I think [term X] in your post is going to cause misunderstandings, I&#x2019;d suggest phrasing it differently.&#x201D;<br> &#x201C;Oh, haters are gonna hate, there&#x2019;s no amount of rephrasing that&#x2019;s going to prevent this from being misinterpreted if people want to.&#x201D;</p><p>Now there&#x2019;s obviously a grain of truth in this. It <em>is</em> impossible to phrase something in a way that would always be interpreted correctly, and for pretty much any message there <em>are</em> people who are hostile to it and who will twist it in the most uncharitable possible way.</p><p>The fallacy is in assuming that if you cannot avoid <em>all</em> misunderstandings, there is no point in avoiding <em>any</em> misunderstandings. Maybe 5% of your audience will dismiss the message no matter what, but 30% will dismiss the old phrasing while being receptive to the new phrasing.</p><p>This is most obvious if you take it to an extreme:</p><p>&#x201C;Hey maybe you shouldn&#x2019;t start your essay by saying that all of your readers are idiots who deserve to be shot.&#x201D;<br> &#x201C;Eh, if that upsets them then they wouldn&#x2019;t like me explaining the theory of general relativity anyway.&#x201D;</p><p>Communication is hard and &#x2013; importantly &#x2013; contextual. Most of your readers will be reasonable people and assume you to use words to mean things they&#x2019;re used to them meaning. If they&#x2019;re used to word X being used differently than how you mean it, that doesn&#x2019;t make them haters.</p><p>When I&#x2019;ve fallen into something like the fallacy myself, it has often been motivated by an unwillingness to put in work. Other people should just understand me right away! &#x201C;It&#x2019;s beneath me to waste my time on doing other people&#x2019;s interpretative work for them!&#x201D; It&#x2019;s dangerous to psychoanalyze others, but I have seen at least one person communicate unclearly, have that pointed out to them, then argue for why it was <em>right</em> for them to be unclear&#x2026; only to later on admit that they were <em>enjoying</em> the frustration of being misunderstood.</p><p>Now avoiding misunderstandings <em>is</em> a lot of work, and it&#x2019;s totally valid not to bother! It&#x2019;s alright to just focus on a particular target audience who understands you. I&#x2019;m not saying that you <em>should</em> always put in maximal effort into being understood &#x2013; I certainly don&#x2019;t.</p><p>But I do suggest owning up to it if you are <em>choosing</em> to write something in a way that is going to cause misunderstandings that could have been avoided.</p><p><em>Cross-posts: <a href=""https://twitter.com/xuenay/status/1308376151732215810"">Twitter</a>, <a href=""https://www.facebook.com/Xuenay/posts/10160240681193662"">Facebook</a>.</em></p>",Kaj_Sotala,kaj_sotala,Kaj_Sotala,
LkytHQSKbQFf6toW5,Anthropomorphisation vs value learning: type 1 vs type 2 errors,anthropomorphisation-vs-value-learning-type-1-vs-type-2,https://www.lesswrong.com/posts/LkytHQSKbQFf6toW5/anthropomorphisation-vs-value-learning-type-1-vs-type-2,2020-09-22T10:46:48.807Z,16,5,10,False,False,,"<p>The <a href=""https://arxiv.org/abs/1712.05812"">Occam's razor paper</a> showed that one cannot deduce an agent <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span>'s reward function (<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""R_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">R</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> - using the notation from that paper) or their level of rationality (<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""p_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>) by observing their behaviour or even by knowing their policy (<span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\pi_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.003em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">π</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>). Subsequently, in a <a href=""https://www.lesswrong.com/posts/9rjW9rhyhJijHTM92/learning-human-preferences-black-box-white-box-and"">LessWrong post</a>, it was demonstrated that even knowing the agent's full algorithm (call this <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""a_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>) would not be enough to deduce either <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""R_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">R</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> or <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""p_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> individually.</p>
<p>In an <a href=""https://www.youtube.com/watch?v=1M9CvESSeVc"">online video</a>, I argued that the reason humans can do this when assessing other humans, is because we have an empathy module/theory of mind <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""E_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.026em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, that allows us to model the rationality and motives of other humans. These <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""E_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.026em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> are, crucially, quite similar from human to human, and when we turn them on ourselves, the results are similar to what happens when others assess us. So, roughly speaking, there is an approximate 'what humans want', at least in typical environments<sup class=""footnote-ref""><a href=""#fn-hgHKt4wQMdnxJQdpa-1"" id=""fnref-hgHKt4wQMdnxJQdpa-1"">[1]</a></sup>, that most humans can agree on.</p>
<p>I struggled to convince people that, without this module, we would fail to deduce the motives of other humans. It is hard to imagine what we would be like if we were fundamentally different.</p>
<p>But there is an opposite error that people know very well: <a href=""https://en.wikipedia.org/wiki/Anthropomorphism"">anthropomorphisation</a>. In this situation, humans attribute motives to the behaviour of the wind, the weather, the stars, the stock market, cute animals, uncute animals...</p>
<p><img src=""https://www.dropbox.com/s/sxppdk7lfheceml/the_north_wind_and_the_sun_-_wind_-_project_gutenberg_etext_19994.jpg?raw=1"" alt=""""></p>
<p>So the same module that allows us to, somewhat correctly, deduce the motivations of other humans, also sets us up to fail for many other potential agents. If we started 'weakening' <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""E_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.026em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span>, then we would reduce the number of anthropomorphisation errors we made, but we'd start making more errors about actual humans.</p>
<p>So our <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""E_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.026em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> can radically fail at assessing the motivations of non-humans, and also <a href=""https://www.researchgate.net/publication/16462204_Culture_and_the_Development_of_Everyday_Social_Explanation"">sometimes fails</a> at assessing the motivations of humans. Therefore I'm relatively confident in arguing that <span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""E_H""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.026em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;"">E</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;"">H</span></span></span></span></span></span></span></span> is not some ""a priori"" object, coming from pure logic, but is contingent and dependent on human evolution. If we met an alien race, they we would likely assess their motives in ways they would find incorrect - and they'd assess our motives in ways we would find incorrect, no matter how much information either of us had.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-hgHKt4wQMdnxJQdpa-1"" class=""footnote-item""><p>See <a href=""https://www.lesswrong.com/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps"">these</a> <a href=""https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1"">posts</a> for how we can and do extend this beyond typical environments. <a href=""#fnref-hgHKt4wQMdnxJQdpa-1"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",Stuart_Armstrong,stuart_armstrong,Stuart_Armstrong,
87MeyLu8Fy4xTj8d5,Emptiness and Form,emptiness-and-form,https://www.lesswrong.com/posts/87MeyLu8Fy4xTj8d5/emptiness-and-form,2020-09-22T04:20:49.986Z,9,5,0,False,False,,"<p><strong>Translation note:</strong> There is no English equivalent to the Sanskrit words शून्यता <em>śūnyatā</em> and रूप <em>rūpa</em>. By convention, शून्यता is translated ""emptiness"" and रूप is translated ""form"". I follow this convention. <strong>My use of the words ""emptiness"" and ""form"" in this post have little to do with the English words ""emptiness"" and ""form"";</strong> they are placeholders for Sanskrit.</p>
<hr>
<p>Consider a cat. From the perspective of fundamental physics, the cat is a collection of particles no more special than any other collection of particles. There is no clear line between ""cat"" and ""non-cat"". Everything is quantum fields. The ""cat"" is a representation created by the human mind. It is a trick of human perspective. From the perspective of an omniscient unbiased observer, the cat is just a scoop of water in a limitless ocean.</p>
<p>Cats are real.</p>
<p>The perspective ""cats are real"" is called ""form"". The perspective ""cats are an arbitrary ontology with no well-defined meaning amongst the fundamental laws of the universe"" is called ""emptiness"". There is no conflict between form and emptiness just as there is no conflict between quantum mechanics and classical mechanics. They are different ways interpreting the same thing at different scales.</p>
<p>Classical mechanics can be more practical than quantum mechanics even though quantum mechanics is more fundamental than classical mechanics. Similarly, emptiness is more fundamental than form yet form is a more useful model of the world than emptiness. Emptiness and form are neither equally true nor equally practical.</p>
<h1>Maps ≠ Form &amp; Emptiness ≠ Territory</h1>
<p>You could say ""form"" roughly corresponds to ""maps"" and ""emptiness"" roughly corresponds to ""territory"". That would constitute a better translation from the original Sanskrit than ""form"" and ""emptiness"". But the form-emptiness dichotomy draws its line in a slightly different place than the map-territory dichotomy.</p>
<p>The map-territory dichotomy draws the line between reality and models of reality. The map-territory dichotomy distinguishes between reality and one's simplified models of reality. In this way, the map-territory dichotomy is a <strong>materialist</strong> perspective.</p>
<p>The form-emptiness dichotomy is an <strong>informatic</strong> perspective. If there is no difference between a map and a territory then—mathematically—the map and the territory are isomorphic respresentations of the same group.</p>
<h1>Ontologies</h1>
<p>""Emptiness"" describes a shared quality between the reductionist nature of objective reality and the raw sensory data coming into a mind. In both cases, our Bayesian priors bucket high-dimensional data into into an ontology called ""form"".</p>
<p>In other words, form is a byproduct of subjectivity. All ontologies dissolve under the scrutiny of theoretical physics.</p>
<p>The duality between emptiness and form is fundamental to general intelligence.</p>
<h1>Discreteness and Differentiability</h1>
<p>Big data is easy. The hard problem of general intelligence concerns <a href=""https://www.lesswrong.com/posts/xkWEJPwEArWFKH5jM/small-data"">small data</a>. Small data is all about transfer learning. Transfer learning is all about ontologies.</p>
<p>An intelligent system with hard-coded ontologies is conceptually unadaptable and therefore not a general intelligence. A general intelligence's ontologies must be emergent from its input data. But ontologies are <strong>discrete</strong> and the only way to navigate a high-dimensional input data is via the gradient descent algorithm. But the gradient descent algorithm requires a <strong>continuous</strong> representation. Can a representation be both continuous and discrete?</p>
<p>In theory, no. In practice, yes.</p>
<p>Consider the sigmoid function in the multilayer perceptron.</p>
<p><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""\text{output}=\tanh(\text{input})""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.519em;"">output</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">tanh</span></span><span class=""mjx-mo""><span class=""mjx-char""></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.519em;"">input</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span></p>
<p><img src=""https://www.lsusr.com/images/sigmoid-zoom-medium.png"" alt=""sigmoid zoom medium""></p>
<p>If we zoom in on this function we can see it is continuously differentiable.</p>
<p><img src=""https://www.lsusr.com/images/sigmoid-zoom-in.png"" alt=""sigmoid zoom in""></p>
<p>But when we zoom out it appears as a discrete step function.</p>
<p><img src=""https://www.lsusr.com/images/sigmoid-zoom-out.png"" alt=""sigmoid zoom out""></p>
<p>The sigmoid function illustrates the scale-dependence of emptiness and form. When we zoom in we see continuity (emptiness), which is a prerequisite for gradient descent. When we zoom out, we see a discrete system (form), which is necessary for the emergence of ontologies. Emptiness and form work together to produce emergent ontologies.</p>
",lsusr,lsusr,lsusr,
6x9rJbx9bmGsxXWEj,Forecasting Thread: Existential Risk,forecasting-thread-existential-risk-1,https://www.lesswrong.com/posts/6x9rJbx9bmGsxXWEj/forecasting-thread-existential-risk-1,2020-09-22T03:44:28.622Z,43,17,39,False,True,,"<p>This is a thread for displaying your probabilities of an existential catastrophe that causes extinction or the destruction of humanity’s long-term potential.</p><p>Every answer to this post should be a forecast showing your probability of an existential catastrophe happening at any given time.</p><p>For example, here is <a href=""https://elicit.ought.org/builder/dOk1mqBw2""><u>Michael Aird’s timeline</u></a>:</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_230 230w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_460 460w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_690 690w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_920 920w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_1150 1150w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_1380 1380w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_1610 1610w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_1840 1840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_2070 2070w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5e52648012b725a03b9e51d4f0ab4921fba179a1b7eaaec6.png/w_2210 2210w""></figure><p>The goal of this thread is to create a set of comparable, standardized x-risk predictions, and to facilitate discussion on the reasoning and assumptions behind those predictions. The thread isn’t about setting predictions in stone – you can come back and update at any point!</p><p>&nbsp;</p><p><strong>How to participate</strong></p><ol><li><strong>Go to </strong><a href=""https://elicit.ought.org/builder/idUcY9sgM""><strong><u>this page</u></strong></a></li><li><strong>Create your distribution</strong><ul><li>Specify an interval using the Min and Max bin, and put the probability you assign to that interval in the probability bin.</li><li>You can specify a cumulative probability by leaving the Min box blank and entering the cumulative value in the Max box.</li><li>To put probability on <i>never</i>, assign probability above January 1, 2120 using the edit button to the right of the graph. Specify your probability for <i>never</i> in the notes, to distinguish this from putting probability on existential catastrophe occurring after 2120.</li></ul></li><li><strong>Click 'Save snapshot' to save your distribution to a static URL</strong><ul><li>A timestamp will appear below the 'Save snapshot' button. This links to the URL of your snapshot.</li><li>Make sure to copy it before refreshing the page, otherwise it will disappear.</li></ul></li><li><strong>Click ‘Log in’ to automatically show your snapshot on the Elicit question page</strong><ul><li>You don’t have to log in, but if you do, Elicit will:<ul><li>Store your snapshot in your account history so you can easily access it.</li><li>Automatically add your most recent snapshot to the <a href=""https://elicit.ought.org/builder/idUcY9sgM"">x-risk question page</a> under ‘Show more’. Other users will be able to import your most recent snapshot from the dropdown, shown below.</li></ul></li><li>We’ll set a default name that your snapshot will be shown under – if you want to change it, you can do so on your <a href=""https://elicit.ought.org/profile"">profile page</a>.</li><li>If you’re logged in, <i>your snapshots for this question will be publicly viewable.</i></li></ul></li><li><strong>Copy the snapshot timestamp link and paste it into your LessWrong comment</strong><ul><li>You can also add a screenshot of your distribution in your comment using the instructions below.</li></ul></li></ol><p>Here's an example of how to make your distribution:</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_120 120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_360 360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_720 720w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_840 840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_960 960w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_1080 1080w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/cac970ef0865928b8d05055b7291980d37bea37a3ed5068c.gif/w_1112 1112w""></figure><p>&nbsp;</p><p><strong>How to add an image to your comment</strong></p><ol><li>Take a screenshot of your distribution</li><li>Then do one of two things:<ol><li>If you have beta-features turned on in your account settings, drag-and-drop the image into your comment</li><li>If not, upload it to an image hosting service like <a href=""https://www.lesswrong.com/posts/sg4P4PrkKTJjXQRDx/imgur.com"">imgur.com</a>, then write the following markdown syntax for the image to appear, with the url appearing where it says ‘link’: ![](link)</li></ol></li><li>If it worked, you will see the image in the comment before hitting submit.</li></ol><p>&nbsp;</p><p>If you have any bugs or technical issues, reply to Ben from the LW team or Amanda (me) from the Ought team in the comment section, or email me at <a href=""mailto:amanda@ought.org""><u>amanda@ought.org</u></a>.</p><p>&nbsp;</p><p><strong>Questions to consider as you're making your prediction</strong></p><ul><li>What definitions are you using? It’s helpful to specify them.</li><li>What evidence is driving your prediction?</li><li>What are the main assumptions that other people might disagree with?</li><li>What evidence would cause you to update?</li><li>How is the probability mass allocated amongst x-risk scenarios?</li><li>Would you bet on these probabilities?</li></ul><p>&nbsp;</p><p><strong>Comparisons and aggregations</strong></p><p><a href=""https://elicit.ought.org/builder/Fc-tiWcXy"">Here's</a> a comparison of the 8 predictions made so far (last updated 9/26/20).</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_230 230w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_460 460w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_690 690w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_920 920w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_1150 1150w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_1380 1380w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_1610 1610w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_1840 1840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_2070 2070w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3b85c4846fd53e655ff6c7f1cbd9aad3bf84221f30e36eaf.png/w_2226 2226w""></figure><p>&nbsp;</p><p><a href=""https://elicit.ought.org/builder/kGWFrGAt2"">Here's</a> a distribution averaging all the predictions (last updated 9/26/20). The averaged distribution puts <strong>19.3% probability before 2120</strong> and <strong>80.7% after 2120.</strong> The year within 2021-2120 with the greatest risk is <strong>2040</strong>.</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_220 220w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_440 440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_660 660w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_880 880w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_1100 1100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_1320 1320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_1540 1540w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_1760 1760w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_1980 1980w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a8243a2e57af1d265cd48db26cc255a2c60d56a6259fe304.png/w_2198 2198w""></figure><p>Here's a CDF of the averaged distribution:</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_130 130w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_260 260w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_390 390w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_520 520w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_650 650w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_780 780w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_910 910w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_1040 1040w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_1170 1170w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/395dfe1c1f9f30e953728470b707954d50135c643c4f0f5d.png/w_1288 1288w""></figure>",Amandango,amandango,Amandango,
nvBgAEJvkTHignTQP,What AI companies would be most likely to have a positive long-term impact on the world as a result of investing in them?,what-ai-companies-would-be-most-likely-to-have-a-positive,https://www.lesswrong.com/posts/nvBgAEJvkTHignTQP/what-ai-companies-would-be-most-likely-to-have-a-positive,2020-09-21T23:41:24.281Z,11,6,2,False,True,,"<p>Ever since GPT-3 was unveiled, I&apos;ve been thinking pretty heavily about increasing my investment in AI-related companies. My first thoughts were to invest in Microsoft and Alphabet (Google) - Microsoft because they are partnered with OpenAI, and Alphabet since they have big AI reseach projects of their own. But in the process of thinking about investing in these companies, I started wondering about the long-term impacts such investments would have on the world - investing in the right or wrong company could dramatically change how the world looks 20 years from now, and whether it is a place I&apos;d want to live in - the worst case scenario would be all humans dead, or even worse; best case scenario is... too amazing to put into words. And then there&apos;s plenty of room in between those for how things can go, depending on who makes the important decisions, and how good the decisions they make will be. (While I&apos;m only a single person with modest funds to invest in companies, I also consider that my actions are acausally correlated with those of others sufficiently similar to me, which means the acausal results of any investment I make will be multiplied by an amount that makes my decisions have non-trivial impact on the world).</p><p>So the important question is, do I expect Microsoft and Alphabet to do better or worse, in regards to alignment and ethical issues, compared to other actors who will develop AGI in their lieu? (I do expect someone <em>will</em> develop AGI in their lieu) I can think of actors who I expect will likely do worse than Microsoft or Alphabet - the government of basically any country, or firms based in a country with more totalitarian ethics than the US - wheras I can only think of alternative actors who I expect to do roughly as good as Microsoft or Alphabet, but not neccesarily to do better than them. I trust MIRI, but I also don&apos;t perceive MIRI as being actively involved in the development of working AI systems; it seems to me that they are laying the important theoretical groundwork for getting things right, but aren&apos;t in position to be the ones who actually do the work that needs to be gotten right.</p><p>So my main problem here is a lack of knowledge - there almost certainly are other firms who, if I had the relevant information, I would expect would do better on alignment and ethical issues than Microsoft or Alphabet, but I also don&apos;t know who those firms are, or why I should expect them to do so. So my question is, <u>for an investor looking to make a AGI-sized profit off of AGI, but also cares about what the future looks like as a result of such investment, what companies will be most likely to result in a good long-term future for humanity?</u></p><p><em>Note that I&apos;m not asking which company will make the most profit - as long as I reasonably expect that a company will make an AGI-sized profit, that&apos;s all I care about on that front. What matters is the impact it has on the desirablity of the future world it will lead to. I&apos;m also not asking about organizations to donate to, because while that is important, that&apos;s not the problem I&apos;m chewing over right now.</em></p>",mikkel-wilson,mikkel-wilson,MikkW,
2tNtcTYNmger4Tm4m,"Prepare for COVID-19 Human Challenge Trials — A Petition in Canada (and soon, the UK)",prepare-for-covid-19-human-challenge-trials-a-petition-in,https://www.lesswrong.com/posts/2tNtcTYNmger4Tm4m/prepare-for-covid-19-human-challenge-trials-a-petition-in,2020-09-21T19:38:04.158Z,6,4,0,False,False,,"<p><a href=""https://petitions.ourcommons.ca/en/Petition/Details?Petition=e-2845"">Canadians&#x2014; sign the petition here. </a></p><h1><strong>TL;DR</strong></h1><p>COVID-19 human challenge trials could save tens of thousands of lives by quickly narrowing the field of promising candidates, and there is strong reason to believe that signaling clear public support for these trials via <a href=""https://petitions.ourcommons.ca/en/Petition/Details?Petition=e-2845"">an official petition</a> could meaningfully accelerate preparation. </p><h1><strong>Why COVID-19 Human Challenge Trials? </strong></h1><p>In a COVID-19 human challenge trial, willing participants would receive the vaccine candidate and, once the vaccine takes effect, be deliberately exposed to live coronavirus.  The ability to observe participants closely and gather samples while tracing the progress of infection in real time, knowing exactly when they were infected and with what dose, and being able to follow up over a long period, would offer an unprecedented level of scientific and medical insight into an unfamiliar virus. It would also help us test vaccines far faster. If a challenge trial brings us one day closer to the development of an additional vaccine that could avert just 25% of daily COVID-19 deaths, it would save 1,250 lives. If a challenge trial brings us a month closer, it&#x2019;d save 37,500 lives. </p><p>To learn more about COVID-19 human challenge trials:</p><ul><li>Watch this <u><a href=""https://www.youtube.com/watch?v=r2fxJI_cP58"">Vox video</a></u></li><li>Read this <u><a href=""https://www.telegraph.co.uk/global-health/science-and-disease/deliberately-infecting-volunteers-coronavirus-can-answer-vital/"">piece</a></u> by Dr. Sayantan Banerjee in The Telegraph </li><li>Read this <u><a href=""https://academic.oup.com/cid/advance-article/doi/10.1093/cid/ciaa935/5868014"">paper </a></u>in the Journal of Clinical Infectious Diseases</li></ul><h1><strong>Why Is It Cost-Effective To Sign The Petition? </strong></h1><p><strong>For one, it is remarkably easy (takes around 20 seconds), so even a very small chance that your signature makes a difference tips the scale in any cost-effectiveness calculation on the margin. </strong></p><p>More broadly, though, signaling a groundswell of public support for COVID-19 human challenge trials has directly led to faster preparation for these trials. In May, a NIH document <u><a href=""https://www.ft.com/content/0e7f1aff-9323-4d82-93ff-bbc5c20514d0"">noted</a></u> that their consideration of challenge trials &#x201C;has been driven almost entirely by the altruism of potential volunteer advocates and the intense considerations of bioethicists.&#x201D; </p><p>1Day Sooner, which has worked systematically to include volunteers in the public conversation about challenge trials, launched an <u><a href=""https://1daysooner.org/openletter"">open letter</a></u> in support of COVID-19 challenge trials on July 15 that was signed by over 100 academics and experts as well as 2,000 potential challenge trial volunteers. A week later, the Washington Post Editorial Board <u><a href=""https://www.reuters.com/article/us-health-coronavirus-vaccine-challenge/exclusive-u-s-to-make-coronavirus-strain-for-possible-human-challenge-trials-idUSKCN25A1EL"">wrote</a></u> in favor of challenge trial preparation. Within a few weeks, Reuters <u><a href=""https://www.reuters.com/article/us-health-coronavirus-vaccine-challenge/exclusive-u-s-to-make-coronavirus-strain-for-possible-human-challenge-trials-idUSKCN25A1EL"">reported</a></u> that the National Institutes of Health were preparing a coronavirus strain for a COVID-19 challenge trial, in part due to &#x201C;pressure from advocacy groups such as 1Day Sooner.&#x201D; </p><p>The logic behind the effectiveness of public advocacy for challenge trials is that vaccine developers want assurance that their decision to deliberately infect people with a dangerous virus won&#x2019;t prompt public backlash. By making clear that the public is actually on board with these trials, stakeholders have a safety net to move forward.</p><p>We are now launching a Canada and UK petition campaign because <a href=""https://www.nbcnews.com/nightly-news/video/inside-oxford-s-push-for-challenge-trials-amid-race-for-coronavirus-vaccine-89676357631"">Oxford&#x2019;s Jenner Institute </a>and <a href=""https://www.thestar.com/opinion/contributors/2020/08/24/it-is-worth-the-risk-to-accelerate-covid-19-vaccine-testing.html"">several Canadian MPs </a>have signaled<a href=""https://www.theguardian.com/science/2020/jul/16/coronavirus-vaccine-oxford-team-volunteers-lab-controlled-human-challenge-trial""> </a>interest<a href=""https://www.theguardian.com/science/2020/jul/16/coronavirus-vaccine-oxford-team-volunteers-lab-controlled-human-challenge-trial""> </a>in conducting a COVID-19 human challenge trial. By showing broad support for these trials, we hope to make it easier for more stakeholders to come out in favor of these trials. </p>",abierohrig,abierohrig,abierohrig,
5Rc5P68ZmGjcJQcA9,How often do series C startups fail to exit?,how-often-do-series-c-startups-fail-to-exit,https://www.lesswrong.com/posts/5Rc5P68ZmGjcJQcA9/how-often-do-series-c-startups-fail-to-exit,2020-09-21T19:37:33.671Z,14,5,15,False,True,,"<p>How often do series C startups really fail? By fail I mean never have an acquisition or IPO. Internet says 80% (see https://medium.com/journal-of-empirical-entrepreneurship/dissecting-startup-failure-by-stage-34bb70354a36) but this seems very high to me. <br><br>Most Series C companies are worth in the 100-200M range, the one I&apos;m at is worth 270M. How does all the value just evaporate? What happens to the companies that &quot;fail&quot;?<br><br>Asking to decide whether to exercise my options. I only need my company to exit at 41M to break even. I am bearish on the company but with around 40M in ARR it is hard to imagine it not exiting.</p>",jessica-shu,jessica-shu,Jessica Shu,
3D3DsX5rMbk3jEZ5h,Needed: AI infohazard policy,needed-ai-infohazard-policy,https://www.lesswrong.com/posts/3D3DsX5rMbk3jEZ5h/needed-ai-infohazard-policy,2020-09-21T15:26:05.040Z,68,26,16,False,False,,"<p>The premise of AI risk is that AI is a danger, and therefore research into AI might be dangerous. In the AI alignment community, we're trying to do research which makes AI safer, but occasionally we might come up with results that have significant implications for AI capability as well. Therefore, it seems prudent to come up with a set of guidelines that address:</p>
<ul>
<li>Which results should be published?</li>
<li>What to do with results that shouldn't be published?</li>
</ul>
<p>These are thorny questions that it seems unreasonable to expect every researcher to solve for themselves. The inputs to these questions involve not only technical knowledge about AI, but also knowledge about the behavior of progress, to the extent we can produce such using historical record or other methods. AI risk organizations might already have internal policies on these issues, but they don't share them and don't discuss or coordinate them with each other (that I know of: maybe some do it in private channels). Moreover, coordination might be important even if each actor is doing something reasonable when regarded in isolation (avoiding bad Nash equilibria). We need to have a public debate on the topic inside the community, so that we arrive at some consensus (that might be updated over time). If not consensus, then at least a reasonable spectrum of possible policies.</p>
<p>Some considerations that such a policy should take into account:</p>
<ul>
<li>Some results might have implications that shorten the AI timelines, but are still good to publish since the distribution of outcomes is improved.</li>
<li>Usually we shouldn't even start working on something which is in the should-not-be-published category, but sometimes the implications only become clear later, and sometimes dangerous knowledge might still be net positive as long as it's contained.</li>
<li>In the midgame, it is unlikely for any given group to make it all the way to safe AGI by itself. Therefore, safe AGI is a broad collective effort and we should expect most results to be published. In the endgame, it might become likely for a given group to make it all the way to safe AGI. In this case, incentives for secrecy become stronger.</li>
<li>The policy should not fail to address extreme situations that we only expect to arise rarely, because those situations might have especially major consequences.</li>
</ul>
<p>Some questions that such a policy should answer:</p>
<ul>
<li>What are the criteria that determine whether a certain result should be published?</li>
<li>What are good channels to ask for advise on such a decision?</li>
<li>How to decide what to do with a potentially dangerous result? Circulate in a narrow circle? If so, which? Conduct experiments in secret? What kind of experiments?</li>
</ul>
<p>The last point is also related to a topic with independent significance, namely, what are reasonable precautions for testing new AI algorithms? This has both technical aspects (e.g. testing on particular types of datasets or particular types of environments, throttling computing power) and procedural aspects (who should be called to advice/decide on the manner). I expect to have several tiers of precautions, s.t. a tier can be selected according to our estimate of the new algorithm's potential, and guidelines for producing such an estimate.</p>
<p>I emphasize that I don't presume to have good answers to these questions. My goal here was not to supply answers, but to foster debate.</p>
",vanessa-kosoy,vanessa-kosoy,Vanessa Kosoy,
hmuKkZbLCYfx8NjQ3,Has anyone written stories happening in Hanson's em world?,has-anyone-written-stories-happening-in-hanson-s-em-world,https://www.lesswrong.com/posts/hmuKkZbLCYfx8NjQ3/has-anyone-written-stories-happening-in-hanson-s-em-world,2020-09-21T14:37:11.150Z,16,5,8,False,True,,"<p>EtA: Robin Hanson complains that while most hard sci-fi may get the hard tech right, they often get the economics wrong. I'm interested in stories that get both right.</p>
",MathieuRoy,mathieuroy,Mati_Roy,
AaMPY9ddiAejMPw2v,Zen and Rationality: Just This Is It,zen-and-rationality-just-this-is-it,https://www.lesswrong.com/posts/AaMPY9ddiAejMPw2v/zen-and-rationality-just-this-is-it,2020-09-20T22:31:56.338Z,36,22,6,False,False,,"<p><i>This is post 4/? about the intersection of my decades of LW-style rationality practice and my several years of Zen practice.</i></p><p><i>In today's installment, I look at ""just this is it"" from a rationalist perspective.</i></p><p>When Dongshan, the co-founder of what would become the Soto Zen school within which I practice, was preparing to leave his teacher Yunyan and go out in the world, he asked Yunyan how he might summarize his teaching. Yunyan replied, ""<a href=""https://www.ancientdragon.org/just-this-is-it/"">just this [is it]</a>"". Because the more we say the more we move into <a href=""https://www.lesswrong.com/posts/EReZtCsGg2giRTZP3/zen-and-rationality-map-and-territory"">the world of words and away from reality as it is on its own prior to conception</a>, this is often shorted in various ways to ""just this"" or ""just is"" or ""this is"" or ""it is"" or, perhaps best of all short of saying nothing and letting reality stand on its own, ""is"".</p><p>This is arguably the core teaching of Soto Zen and maybe all of Buddhism, to <a href=""https://www.lesswrong.com/posts/vXsQWi7LMm59LMt9L/zen-and-rationality-don-t-know-mind"">perceive</a> and <a href=""https://www.lesswrong.com/posts/fFidzSC5dK8zKMvAT/zen-and-rationality-trust-in-mind"">accept</a> reality just as it is. Yet I see it all over the place in the LessWrong corpus, too. I'll mention a few of these.</p><p><a href=""https://wiki.lesswrong.com/wiki/Egan%27s_law"">Egan's Law</a> posits that ""it all adds up to normality"". In ""<a href=""https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation"">A Technical Explanation of Technical Explanation</a>"", Eliezer phrased a similar sentiment as ""since the beginning, not one unusual thing has ever happened"". Both point at the way that reality is just as it is, and the only way we can be confused or surprised is because we had an idea about how reality is rather than simply looking and seeing how it is.</p><p>I think this is a hard thing to remember, because to the kind of people that are attracted to Less Wrong, better models of reality are very attractive. I know they are to me! Yet it's very easy to go from accepting reality as it is and trying to better predict it to getting lost in the model that does the predicting and confusing it for the real thing. Thus, while at the same time we look for models with better <a href=""https://www.lesswrong.com/tag/gears-level"">gears</a> that more precisely <a href=""https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries"">carve reality at its joints</a>, we also have to remember those <a href=""https://www.lesswrong.com/posts/8gLEnEwm2g257vqyx/fuzzy-boundaries-real-concepts"">boundaries are fuzzy</a> and that <a href=""https://www.lesswrong.com/posts/ZGXtHCMhpuDzX2vvG/the-one-mistake-rule"">all models are ultimately wrong</a> even and especially when they are useful. It's perhaps the great koan of Less Wrong to build better models while simultaneously accepting that all models are somewhere wrong.</p><p>To help us deal with this koan, we have a poem to help us. You might think I mean the <a href=""https://www.lesswrong.com/posts/3nZMgRTfFEfHp34Gb/the-meditation-on-curiosity"">Litany of Tarski</a>, but you would be wrong, because that poem is about having beliefs correspond to reality, but ""just this is it"" is all about getting under those beliefs and just seeing what's actually being perceived. For that, we turn to the <a href=""https://www.lesswrong.com/posts/HYWhKXRsMAyvRKRYz/you-can-face-reality"">Litany of Gendlin</a>:</p><blockquote><p>What is true is already so.<br>Owning up to it doesn’t make it worse.<br>Not being open about it doesn’t make it go away.<br>And because it’s true, it is what is there to be interacted with.<br>Anything untrue isn’t there to be lived.<br>People can stand what is true,<br>for they are already enduring it.</p></blockquote><p>This was said by Eugene Gendlin of <a href=""https://www.lesswrong.com/posts/PXqQhYEdbdAYCp88m/focusing-for-skeptics"">Focusing</a> fame, a technique for helping you reconnect to your perceptions just as they are without judgement or modeling. The <a href=""https://www.lesswrong.com/posts/noXTkjP45BAZqKJxM/focusing"">method is simple</a>, yet its impact can be profound for many to get out of their ideas about how things are and to get back to what evidence they are actually getting about the world. Zen asks us to over and over again come back to this fundamental point that reality is just as we perceive it, not what we believe about it, and that belief is just a useful mechanism for helping us better live our lives, if only we don't get tripped up into mistaking the map for the territory.</p><p>Finally, to return to ""<a href=""https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation"">A Technical Explanation of Technical Explanation</a>"", it contains one other phrase that neatly captures the spirit of ""just this"": ""joy in the merely real"". If we can take joy in what actually is, if that can be enough for us, then all else becomes the playground in which we live our lives.</p>",gworley,gordon-seidoh-worley,Gordon Seidoh Worley,
v6Q7T335KCMxujhZu,Clarifying “What failure looks like”,clarifying-what-failure-looks-like,https://www.lesswrong.com/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like,2020-09-20T20:40:48.295Z,97,46,14,False,False,,"<p><em>Thanks to Jess Whittlestone, Daniel Eth, Shahar Avin, Rose Hadshar, Eliana Lorch, Alexis Carlier, Flo Dorner, Kwan Yee Ng, Lewis Hammond, Phil Trammell and Jenny Xiao for valuable conversations, feedback and other support. I am especially grateful to Jess Whittlestone for long conversations and detailed feedback on drafts, and her guidance on which threads to pursue and how to frame this post. All errors are my own.</em></p>
<p><strong>Epistemic status:</strong>  <a href=""https://www.lesswrong.com/posts/Hrm59GdN2yDPWbtrd/feature-idea-epistemic-status"">My Best Guess</a></p>
<p><strong>Epistemic effort:</strong> ~70 hours of focused work (mostly during FHI’s <a href=""https://www.fhi.ox.ac.uk/summer-research-fellowship/"">summer research fellowship</a>), talked to ~10 people.</p>
<h1>Introduction</h1>
<p><a href=""https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like"">“What failure looks like”</a> is the one of the most comprehensive pictures of what failure to solve the AI alignment problem looks like, in worlds without discontinuous progress in AI. I think it was an excellent and much-needed addition to our understanding of AI risk. Still, if many believe that this is a main source of AI risk, I think it should be fleshed out in more than just one blog post. The original story has two parts; I’m focusing on part 1 because I found it more confusing and nebulous than part 2.</p>
<p>Firstly, I’ll summarise part 1 (hereafter “WFLL1”) as I understand it:</p>
<ul>
<li>
<p>In the world today, it’s easier to pursue easy-to-measure goals than hard-to-measure goals.</p>
</li>
<li>
<p>Machine learning is differentially good at pursuing easy-to-measure goals (assuming that we don’t have a satisfactory technical solution to the <a href=""https://www.lesswrong.com/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment"">intent alignment</a> problem<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-1"" id=""fnref-5fCXGsgmJ9pureYYt-1"">[1]</a></sup>).</p>
</li>
</ul>
<ul>
<li>
<p>We’ll try to harness this by designing easy-to-measure proxies for what we care about, and deploy AI systems across society which optimize for these proxies (e.g. in law enforcement, legislation and the market).</p>
</li>
<li>
<p>We’ll give these AI systems more and more influence (e.g. eventually, the systems running law enforcement may actually be making all the decisions for us).</p>
</li>
<li>
<p>Eventually, the proxies for which the AI systems are optimizing will come apart from the goals we truly care about, but by then humanity won’t be able to take back influence, and we’ll have permanently lost some of our ability to steer our trajectory.</p>
</li>
</ul>
<p>WFLL1 is quite thin on some important details:</p>
<ul>
<li>
<p>WFLL1 does not envisage AI systems directly causing human extinction. So, to constitute an existential risk in itself, the story must involve the lock-in of some suboptimal world.<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-2"" id=""fnref-5fCXGsgmJ9pureYYt-2"">[2]</a></sup> However, the likelihood that the scenario described in part 1 gets locked-in (especially over very long time horizons) is not entirely clear in the original post.</p>
</li>
<li>
<p>It’s also not clear how bad this locked-in world would actually be.</p>
</li>
</ul>
<p>I’ll focus on the first point: how likely is it that the scenario described in WFLL1 leads to the lock-in of some suboptimal world. I’ll finish with some rough thoughts on the second point - how bad/severe that locked-in world might be - and by highlighting some remaining open questions.</p>
<h1>Likelihood of lock-in</h1>
<p>The scenario described in WFLL1 seems very concerning from a longtermist perspective if it leads to humanity getting stuck on some suboptimal path (I’ll refer to this as “lock-in”). But the blog post itself isn't all that clear about why we should expect such lock-in --- i.e. why we won't be able to stop the trend of AI systems optimising for easy-to-measure things before it's too late -- a confusion which has been <a href=""https://fragile-credences.github.io/prioritising-ai/#questions-about-this-argument"">pointed out</a> before. In this section, I'll talk through some different mechanisms by which this lock-in can occur, discuss some historical precedents for these mechanisms occurring, and then discuss why we might expect the scenario described in WFLL1 to be more likely to lead to lock-in than for the precedents.</p>
<h2>The mechanisms for lock-in</h2>
<p><strong>Summary:</strong> I describe five complementary mechanisms by which the scenario described in WFLL1 (i.e. AI systems across society optimizing for simple proxies at the expense of what we actually want) could get locked-in permanently. The first three mechanisms show how humanity may increasingly depend on the superior reasoning abilities of AIs optimizing for simple proxies to run (e.g.) law enforcement, legislation and the market, despite it being apparent --- at least to some people --- that this will be bad in the long term. The final two mechanisms explain how this may eventually lead to a truly permanent lock-in, rather than merely temporary delays in fixing the problem.</p>
<p>Before diving into the mechanisms, first, let’s be clear about the kind of world in which they may play out. The original post assumes that we have not solved <a href=""https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6"">intent alignment</a> and that AI is “responsible for” a very large fraction of the economy.<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-3"" id=""fnref-5fCXGsgmJ9pureYYt-3"">[3]</a></sup> So we’ve made sufficient progress on alignment (and capabilities) such that we can deploy powerful AI systems across society that pursue easy-to-measure objectives, but not hard-to-measure ones.</p>
<h3>(1) Short-term incentives and collective action</h3>
<p>Most actors (e.g. corporations, governments) have some short-term objectives (e.g. profit, being reelected). These actors will be incentivised to deploy (or sanction the deployment of) AI systems to pursue these short-term objectives. Moreover, even if some of these actors are aware that pursuing proxies in place of true goals <a href=""https://en.wikipedia.org/wiki/Goodhart%27s_law"">is prone to failure</a>, if they decide <em>not</em> to use AI then they will likely fall behind in their short-term objectives and therefore lose influence (e.g. be outcompeted, or not reelected). This kind of situation is called a <a href=""https://en.wikipedia.org/wiki/Collective_action_problem"">collective action problem</a>, since it requires actors to coordinate on collectively limiting their use of AI - individual actors are better off (in the short term) by deploying AI anyway.</p>
<p><strong>Example:</strong> predictive policy algorithms used in the US <a href=""https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing"">are</a>  <a href=""https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423"">biased</a>  <a href=""http://epubs.surrey.ac.uk/852008/1/Biased%20Algorithm%20re%20Hispanics.pdf"">against</a> people of colour. We can’t debias these algorithms, because we don’t know how to design algorithms that pursue the hard-to-measure goal of “fairness”. Meanwhile, such algorithms continued to be used. Why? Given crime rate objectives and a limited budget, <a href=""https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/"">police departments do better</a> on these objectives by using (cheap) predictive algorithms, compared with hiring more staff to think through bias/fairness issues. So, individual departments are “better off” in the short term (i.e. more likely to meet their objectives and so keep their jobs) if they just keep using predictive algorithms. Even if some department chief realises that this minimization of reported crime rate produces this perverse outcome, they are unable to take straightforward action to fix the problem because this would likely result in increased reported crime rate for their department, impacting that chief’s career prospects.</p>
<h3>(2) Regulatory capture</h3>
<p>The second mechanism is that influential people will benefit from the AIs optimizing for easy-to-measure goals, and they will oppose attempts to put on the brakes. Think of a powerful CEO using AI techniques to maximize profit: they will be incentivised to <a href=""https://en.wikipedia.org/wiki/Regulatory_capture"">capture regulators</a> who attempt to stop the use of AI, for example via political donations or lobbying.</p>
<p><strong>Example:</strong> Facebook is aware of how user data protection and the spread of viral misinformation led to problems in the 2016 presidential election. Yet <a href=""https://www.washingtonpost.com/technology/2020/01/22/amazon-facebook-google-lobbying-2019/"">they spent</a> $17 million lobbying the US government to assuage regulators who were trying to introduce countervailing regulation in 2019.</p>
<h3>(3) Genuine ambiguity</h3>
<p>The third mechanism is that there will be genuine ambiguity about whether the scenario described in WFLL1 is good or bad. For a while, humans are overall better off in absolute terms than they are today.<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-4"" id=""fnref-5fCXGsgmJ9pureYYt-4"">[4]</a></sup> From the original post:</p>
<blockquote>
<p>There will be legitimate arguments about whether the implicit long-term purposes being pursued by AI systems are really so much worse than the long-term purposes that would be pursued by the shareholders of public companies or corrupt officials.</p>
</blockquote>
<p>This will be heightened by the fact that it’s easier to make arguments about things for which you have clear, measurable objectives.<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-5"" id=""fnref-5fCXGsgmJ9pureYYt-5"">[5]</a></sup> So arguments that the world is actually fine will be easier to make, in light of the evidence about how well things are going according to the objectives being pursued by AIs. Arguments that something is going wrong, however, will have no such concrete evidence to support them (they might only be able to appeal to a vague sense that the world just isn’t as good as it could be).</p>
<p>This ambiguity will make the collective action problem of the first mechanism even harder to resolve, since disagreement between actors on the severity of a collective problem impedes collective action on that problem.</p>
<p><strong>Example:</strong> genuine ambiguity about whether capitalism is “good” or “bad” in the long run. Do negative externalities become catastrophically high, or does growth lead to sufficiently advanced technology fast enough to compensate for these externalities?</p>
<h3>(4) Dependency and deskilling</h3>
<p>If used widely enough across important societal functions, there may come a time when ceasing to use AI systems would require something tantamount to societal collapse. We can build some intuition for this argument by thinking about electricity, one general purpose technology on which society already depends heavily. Suppose for the sake of argument that some research comes out arguing that our use of electricity will eventually cause our future to be less good than it otherwise could have been. How would humanity respond? I’d expect to see research on potential modifications to our electricity network, and research that tries to undermine the original study. But actually giving up electricity seems unlikely. Even if doing so would not imply total societal collapse, it would at least significantly destabilise society, reducing our ability to deal with other existential risks. This destabilisation would increase the chance of conflict, which would further erode international trust and cooperation and increase risks posed by a range of weapon technologies.<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-6"" id=""fnref-5fCXGsgmJ9pureYYt-6"">[6]</a></sup> And even if giving up electricity was actually the best strategy in expectation, we wouldn’t necessarily do so, due to the problems of short term incentives, collective action, regulatory capture and genuine ambiguity mentioned above.</p>
<p>Furthermore, if we increasingly depend on AIs to make the world work, then humans are unlikely to continue to learn the skills we would need to replace them. In a world where most businesspeople/doctors/lawyers are now AIs, we would likely cut costs by closing down most human business/medical/law schools. This deskilling is an additional reason to think we could be locked-in to a world where AI systems are filling these roles.</p>
<h3>(5) Opposition to taking back influence</h3>
<p>Whilst these four mechanisms may mean that our attempts at taking back influence from AIs will be delayed, and will come at some cost, surely we will <em>eventually</em> realise that something has gone wrong, and make a proper attempt to fix it, even if this involves some costly reskilling and destabilisation?</p>
<p>By way of answering this question in the negative, the original article imagines the following possibility:</p>
<blockquote>
<p>Eventually, large-scale attempts to fix the problem are themselves opposed by the collective optimization of millions of optimizers pursuing simple goals.</p>
</blockquote>
<p>This opposition could take two forms. The first can be seen as a continuation of the “genuine ambiguity” mechanism. Simply because the AIs are doing their jobs so well, we may be increasingly unlikely to realise that anything is going wrong. Reported sense of security, healthcare statistics, life satisfaction, GDP, etc. will look great, because it is precisely these proxies for which the AIs are optimizing. As the gap between how things are and how they appear grows, so too will the persuasion/deception abilities of AIs and the world’s incomprehensibility. Eventually, AIs will be able to manipulate human values and our ability to perceive the world in sophisticated ways (think: highly addictive video games, highly persuasive media or education; cf. the <a href=""https://www.lesswrong.com/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety"">human safety problem</a>).</p>
<p><strong>Example:</strong> recommender algorithms maximizing click-throughs <a href=""https://www.youtube.com/watch?v=iFTWM7HV2UI&amp;feature=youtu.be&amp;t=541"">feed</a> users more extreme content in order to keep them online for longer. Stuart Russell claims that this is an example of an algorithm making its users’ values more extreme, in order to better pursue its objective.<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-7"" id=""fnref-5fCXGsgmJ9pureYYt-7"">[7]</a></sup></p>
<p>Secondly, the AIs may explicitly oppose any attempts to shut them down or otherwise modify their objectives. This is because human attempts to take back influence probably will result in (short term) losses according to their objective functions (e.g. reported sense of security will go down if the systems that have been driving this down are switched off). Therefore, AIs will be incentivised to oppose such changes.</p>
<p>What this opposition looks like depends on how general the AIs are. In <a href=""https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as"">CAIS</a>-type scenarios, AIs would probably be limited to the narrow kinds of deception described above. For example, an AI police service with bounded resources minimizing the number of complaints before the end of the day (as a proxy for society’s actual safety) will not take long-term, large-scale actions to manipulate human values (e.g. producing advertising to convince the public that complaining is ineffectual). However, it could still take unintended short-term, small-scale actions, if they’re helpful for the task before the end of the bound (e.g. offer better protection to people if they don’t file complaints).</p>
<p>More general AI could oppose human attempts to take back influence in more concerning ways. For example, it could hamper human attempts at collective action (by dividing people’s attention across different issues), cut funding for research on AI systems that can pursue hard-to-measure objectives or undermine the influence of key humans in the opposition movement. Our prospects certainly seem better in CAIS-type scenarios.</p>
<h2>Historical precedents</h2>
<p>I think the existence of these mechanisms makes the case that it is <em>possible</em> that the scenario described in WFLL1 will get locked-in. But is it <em>plausible?</em> In particular, will we really fail to make a sufficient attempt to fix the problem before it is irreversibly locked-in? I’ll examine three historical precedents which demonstrate the mechanisms playing out, which positively update my credence that it will also play out in the case of WFLL1. However, this reasoning via historical precedents is far from decisive evidence, and I can imagine completely changing my mind if I had more evidence about factors like takeoff speeds and the generality of AI systems.</p>
<h3>Climate change</h3>
<p>Climate change is a recent example of how mechanisms 1-3 delayed our attempts to solve a problem until some irreversible damage was already done. However, note that the mechanism for the irreversible lock-in is different to WFLL1 (the effects of climate change are locked-in via irreversible physical changes to the climate system, rather than mechanisms 4 and 5 described above).</p>
<p>(1) <strong>Short-term incentives and collective action</strong></p>
<p>Most electricity generation companies maximize profit by producing electricity from fossil fuels. Despite the unequivocal scientific evidence that burning fossil fuels causes climate change and will probably make us collectively worse off in the long term, individual companies are better off (in the short term) if they continue to burn fossil fuels. And they will be outcompeted if they don’t. The result is a slow-rolling climate catastrophe, despite attempts at collective action like the <a href=""https://en.wikipedia.org/wiki/Kyoto_Protocol"">Kyoto Protocol</a>.</p>
<p>(2) <strong>Regulatory capture</strong></p>
<p>BP, Shell, Chevron, ExxonMobil and Total <a href=""https://www.theguardian.com/business/2019/oct/24/fossil-fuel-big-five-spent-251m-lobbying-european-union-2010-climate-crisis"">have spent</a> €251m lobbying the EU since 2010 in order to water down EU climate legislation.</p>
<p>(3) <strong>Genuine ambiguity</strong></p>
<p>Consensus among the scientific community that human-caused emissions were contributing to climate change <a href=""https://en.wikipedia.org/wiki/History_of_climate_change_science#:~:text=The%20history%20of%20the%20scientific,natural%20greenhouse%20effect%20first%20identified."">was not established until the 1990s</a>. Even today, some people deny there is a problem. This probably delayed attempts to solve the problem.</p>
<h3>The agricultural revolution</h3>
<p>The agricultural revolution is a precedent for mechanisms 1 and 4 leading to lock-in of technology <a href=""https://onlinelibrary.wiley.com/doi/full/10.1111/padr.12152"">that</a>  <a href=""https://www.discovermagazine.com/planet-earth/the-worst-mistake-in-the-history-of-the-human-race"">arguably</a>  <a href=""https://erenow.net/common/sapiensbriefhistory/20.php"">made</a> human life worse (on average) for thousands of years. (The argument that agriculture made human life worse is that increased population density enabled epidemics, farm labour increased physical stress, and malnutrition rose due to the replacement of a varied diet with fewer starchy foods.<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-8"" id=""fnref-5fCXGsgmJ9pureYYt-8"">[8]</a></sup>)</p>
<p>(1) <strong>Short-term incentives and collective action</strong></p>
<p>Humans who harnessed agricultural technology could increase their population relative to their hunter-gatherer peers. Despite the claimed lower levels of health among agriculture communities, their sheer advantage in numbers gave them influence over hunter-gatherers:</p>
<blockquote>
<p>The greater political and military power of farming societies since their inception resulted in the elimination and displacement of late Pleistocene foragers (<a href=""https://www.pnas.org/content/pnas/108/12/4760.full.pdf"">Bowles, 2011</a>).</p>
</blockquote>
<p>So, individual communities were incentivised to convert to agriculture, on pain of being eradicated by more powerful groups who had adopted agriculture.</p>
<p>(4) <strong>Dependency</strong></p>
<p>Once a community had been depending on agricultural technology for some generations, it would be difficult to regress to a hunter-gatherer lifestyle. They would have been unable to support their increased population, and would probably have lost some skills necessary to be successful hunter-gatherers.</p>
<h3>The colonisation of New Zealand</h3>
<p>The colonisation of New Zealand is a precedent for a group of humans permanently losing some influence over the future, due to mechanisms 1, 3 and 5. In 1769, the indigenous Māori were the only people in New Zealand, but by 1872, the British (with different values to the Māori) had a substantial amount of influence over New Zealand’s future (see <a href=""https://static3.stuff.co.nz/2-landloss-crown-purchases-b8801f0c.gif"">this animation</a> of decline in Māori land ownership for a particularly striking illustration of this). Despite the superficial differences, I think this provides a fairly close analogy to WFLL1.<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-9"" id=""fnref-5fCXGsgmJ9pureYYt-9"">[9]</a></sup></p>
<p>(1) <strong>Short-term incentives and collective action</strong></p>
<p>The British purchased land from the Māori, in exchange for (e.g.) guns and metal tools. Each tribe was individually better off if they engaged in trade, because guns and tools were economically and militarily valuable; tribes that did not obtain guns were devastated in the <a href=""https://en.wikipedia.org/wiki/Musket_Wars"">Musket Wars</a>. However, tribes became collectively worse off because the British charged unreasonable prices (e.g. in 1848, over 30% of New Zealand was purchased for around NZD 225,000 in today’s currency) and could use this land to increase their influence in the longer term (more settlers could arrive and dominate New Zealand’s agriculture-based economy).</p>
<p>(3) <strong>Genuine ambiguity</strong></p>
<p>British goals were initially somewhat aligned with Māori goals. Most early contact <a href=""https://en.wikipedia.org/wiki/History_of_New_Zealand#Early_European_exploration"">was peaceful and welcomed by</a> Māori. In absolute economic terms, the Māori were initially better off thanks to trade with the British. The Māori translation of the <a href=""https://en.wikipedia.org/wiki/Treaty_of_Waitangi"">Treaty of Waitangi</a>, which the Māori knew would bring more British settlers, was signed by around 540 Māori chiefs.</p>
<p>(5) <strong>Opposition to taking back influence</strong></p>
<p>However, once the British had established themselves in New Zealand, the best ways to achieve their goals ceased to be aligned with Māori goals. Instead, they turned to manipulation (e.g. breaking agreements about how purchased land would be used), confiscation (e.g. the <a href=""https://en.wikipedia.org/wiki/New_Zealand_land_confiscations"">New Zealand Settlements Act 1863</a>) and conflict (e.g. the <a href=""https://en.wikipedia.org/wiki/New_Zealand_Wars#Aftermath"">New Zealand Wars</a>). For the past 150 years, Māori values have sadly been just one of many determinants of New Zealand’s future, and not even a particularly strong one.</p>
<h2>How WFLL1 may differ from precedents</h2>
<p>These precedents demonstrate that each of the lock-in mechanisms have already played out, making it seem more plausible. The next section discusses how WFLL1 may differ from the precedents. I think these differences suggest that the lock-in mechanisms are a stronger force in WFLL1 than in the precedents, which also positively updates my credence that WFLL1 will be locked-in.</p>
<h3>AI may worsen the “genuine ambiguity” mechanism</h3>
<p>If AI leads to a proliferation of misinformation (e.g. via language models or deepfakes), then this will probably reduce our ability to reason and reach consensus about what is going wrong. This misinformation need not be sufficiently clever to convince people of falsehoods, it just has to splinter the attention of people who are trying to understand the problem enough to break our attempts at collective action.<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-10"" id=""fnref-5fCXGsgmJ9pureYYt-10"">[10]</a></sup></p>
<p>Another way in which AI may increase the amount of “genuine ambiguity” we have about the problem is the <a href=""https://www.cambridge.org/core/journals/episteme/article/echo-chambers-and-epistemic-bubbles/5D4AC3A808C538E17C50A7C09EC706F0"">epistemic bubble/echo chamber</a> phenomenon, supposedly aggravated by social media recommender systems. The claim is that (1) epistemic communities are isolated from each other via (accidental or deliberate) lack of exposure to (reasonable interpretations of) dissenting viewpoints, and (2) recommender systems, by virtue of maximising click-throughs, have worsened this dynamic. If this is true, and epistemic communities disagree about whether specific uses of AI (e.g. AI systems maximizing easy-to-measure goals replacing judges in courts) are actually serving society’s goals, this would make it even harder to reach the consensus required for collective action.</p>
<h3>High risk of dependency and deskilling</h3>
<p>WFLL1 assumes that AI is “responsible for” a very large fraction of the economy, making it the first time in human history where most humans are no longer required for the functioning of the economy. The agricultural and industrial revolutions involved some amount of deskilling, but humans were still required at most stages of production. However, in WFLL1 it seems likely that humans will heavily depend on AI for the functioning of the economy, making it particularly hard to put on the brakes.</p>
<h3>Speed and warning shots</h3>
<p>As AI gets more advanced, the world will probably start moving much faster than today (e.g. Christiano once <a href=""https://www.lesswrong.com/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage"">said</a> he thinks the future will be “like the Industrial Revolution but 10x-100x faster”). Naively, this would seem to make things less likely to go well because we’ll have less opportunity to identify and act on warning signs.</p>
<p>That said, some amount of speed may be on our side. If the effects of climate change manifested more quickly, it seems more likely that individual actors would be galvanised towards collective action. So faster change seems to make it more likely that the world wakes up to there being a problem, but less likely that we’re able to fix the problem if we do.</p>
<p>Another way of putting this might be: too fast, and the first warning shot spells doom; too slow, and warning shots don’t show up or get ignored. I’m very uncertain about what the balance will look like with AI. All things considered, perhaps faster progress is worse because human institutions move slowly even when they’re galvanised into taking action.</p>
<p>This discussion seems to carry an important practical implication. Since warning shots are only as helpful as our responses to them, it makes sense to set up institutions that are likely to respond effectively to warning shots if they happen. For example, having a clear, reputable literature describing these kinds of risks, which (roughly) predicts what early warning shots would look like, and argues persuasively that things will only get worse in the long run if we continue to use AI to pursue easy-to-measure goals, seems pretty helpful.</p>
<h1>Severity of lock-in</h1>
<p>The extent to which we should prioritise reducing the risk of a lock-in of WFLL1 also depends on how bad this world actually is. Previous discussion has seen <a href=""https://www.lesswrong.com/posts/Q8Z8yoG4tBaowBHwk/critiquing-what-failure-looks-like?commentId=SQML9vfycz9Tkp4Yd"">some</a>  <a href=""https://www.lesswrong.com/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai#fn-uN5mgNcRj7ghxhRAF-4"">confusion</a> about this question. Some possibilities include:</p>
<ul>
<li>
<p>The world is much worse than our current world, because humans eventually become vastly less powerful than AIs and slowly go extinct, in much the same way as <a href=""https://en.wikipedia.org/wiki/List_of_recently_extinct_insects"">insects that become extinct</a> in our world.</p>
</li>
<li>
<p>The world is worse than our current world, because (e.g.) despite curing disease and ageing, humans have no real freedom or understanding of the world, and spend their lives in highly addictive but unrewarding virtual realities.</p>
</li>
<li>
<p>The world is better than our current world, because humans still have some influence over the future, but our values are only one of many forces, and we can only make use of 1% of the cosmic endowment.</p>
</li>
<li>
<p>The world is much better than our current world, because humans lead fairly worthwhile lives, assisted by AIs pursuing proxies. We course-corrected these proxies along the way and they ended up capturing much of what we value. However, we still don’t make use of the full cosmic endowment.</p>
</li>
</ul>
<p><a href=""https://www.lesswrong.com/posts/Q8Z8yoG4tBaowBHwk/critiquing-what-failure-looks-like?commentId=y6GCetomMJr2H28zh"">It seems</a> that Christiano had something like the third scenario in mind, but it isn’t clear to me why this is the most likely. The question is: how bad would the future be, if it is at least somewhat determined by AIs optimizing for easy-to-measure goals, rather than human intentions? I think this is an important open question. If I were to spend more time thinking about it, here are some things I’d do.</p>
<h2>Comparison with precedents</h2>
<p>In the same way that it was helpful when reasoning about the likelihood of lock-in to think about past examples, then work out how WFLL1 may compare, I think this could be a useful approach to this question. I’ll give two examples: both involve systems optimizing for easy-to-measure goals rather than human intentions, but seem to differ in the severity of the outcomes.</p>
<p>CompStat: where optimizing for easy-to-measure goals was net negative?<sup class=""footnote-ref""><a href=""#fn-5fCXGsgmJ9pureYYt-11"" id=""fnref-5fCXGsgmJ9pureYYt-11"">[11]</a></sup></p>
<ul>
<li>
<p>CompStat is a system used by police departments in the US.</p>
</li>
<li>
<p>It’s used to track crime rate and police activity, which ultimately inform the promotion and remuneration of police officers.</p>
</li>
<li>
<p>Whilst the system initially made US cities much safer, it ended up leading to:</p>
</li>
<li>
<p>Widespread under/misreporting of crime (to push reported crime rate down).</p>
</li>
<li>
<p>The targeting of people of the same race and age as those who were committing crimes (to push police activity up).</p>
</li>
<li>
<p>In NYC one year, the reported crime rate was down 80%, but in interviews, officers reported it was only down ~40%.</p>
</li>
<li>
<p>It seems plausible that pressure on police to pursue these proxies made cities less safe than they would have been without CompStat: there were <a href=""https://en.wikipedia.org/wiki/CompStat#Critique"">many other</a> successful initiatives which were introduced alongside CompStat, and there were cases of substantial harm caused to the victims of crime underreporting and unjust targeting.</p>
</li>
</ul>
<p>“Publish or perish”: where optimizing for easy-to-measure goals is somewhat harmful but plausibly net positive?</p>
<ul>
<li>
<p>The pressure to publish papers to succeed in an academic career <a href=""https://en.wikipedia.org/wiki/Publish_or_perish#Disadvantages"">has some negative effects</a> on the value of academic research.</p>
</li>
<li>
<p>However, much important work continues to happen in academia, and it’s not obvious that there’s a clearly better system that could replace it.</p>
</li>
</ul>
<p>In terms of how WFLL1 may differ from precedents:</p>
<ul>
<li>
<p>Human institutions incorporate various “corrective mechanisms”, e.g. <a href=""https://en.wikipedia.org/wiki/Separation_of_powers"">checks and balances</a> in political institutions, and “common sense”. However, it’s not obvious that AI systems pursuing easy-to-measure goals will have these.</p>
</li>
<li>
<p>Most human institutions are at least somewhat interpretable. This means, for example, that humans who tamper with the measurement process to pursue easy-to-measure objectives are prone to being caught, as <a href=""https://cdn.theatlantic.com/assets/media/files/justice%20quarterly%20article%20Eterno%20Verma%20Silverman.pdf"">eventually happened</a> with CompStat. However, ML systems today are currently hard to interpret, and so it may be more difficult to catch interference with the measurement process.</p>
</li>
</ul>
<h1>Conclusion</h1>
<p>What this post has done:</p>
<ul>
<li>
<p>Clarified in more detail the mechanisms by which WFLL1 may be locked-in.</p>
</li>
<li>
<p>Discussed historical precedents for lock-in via these mechanisms and ways in which WFLL1 differs from these precedents.</p>
</li>
<li>
<p>Taken this as cautious but far from decisive evidence that the lock-in of WFLL1 is plausible.</p>
</li>
<li>
<p>Pointed out that there is confusion about how bad the future would be if it is partially influenced by AIs optimizing for easy-to-measure goals rather than human intentions.</p>
</li>
<li>
<p>Suggested how future work might make progress on this confusion.</p>
</li>
</ul>
<p>As well as clarifying this confusion, future work could:</p>
<ul>
<li>
<p>Explore the extent to which WFLL1 could increase existential risk by being a <em>risk factor</em> in other existential risks, rather than an existential risk in itself.</p>
</li>
<li>
<p>Search for historical examples where the mechanisms for lock-in <em>didn’t</em> play out.</p>
</li>
<li>
<p>Think about other ways to reason about the likelihood of lock-in of WFLL1, e.g. via a game theoretic model, or digging into <a href=""https://en.wikipedia.org/wiki/The_Age_of_Em"">The Age of Em</a> scenario where similar themes play out.</p>
</li>
</ul>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-5fCXGsgmJ9pureYYt-1"" class=""footnote-item""><p>I’m worried that WFLL1 could happen <em>even if</em> we had a satisfactory solution to the intent alignment problem, but I’ll leave this possibility for another time. <a href=""#fnref-5fCXGsgmJ9pureYYt-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-2"" class=""footnote-item""><p>WFLL1 could also increase existential risk by being a <em>risk factor</em> in other existential risks, rather than a mechanism for destroying humanity’s potential in itself. To give a concrete example: faced with a global pandemic, a health advice algorithm minimising short-term excess mortality may recommend complete social lockdown to prevent the spread of the virus. However, this may ultimately result in higher excess mortality due to the longer term (and harder to measure) effects on mental health and economic prosperity. I think that exploring this possibility is an interesting avenue for future work. <a href=""#fnref-5fCXGsgmJ9pureYYt-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-3"" class=""footnote-item""><p>The latter assumption is not explicit in the original post, but <a href=""https://www.lesswrong.com/posts/Q8Z8yoG4tBaowBHwk/critiquing-what-failure-looks-like?commentId=SQML9vfycz9Tkp4Yd"">this comment</a> suggests that it is what Christiano had in mind. Indeed, WFLL1 talks about AI being responsible for running corporations, law enforcement and legislation, so the assumption seems right to me. <a href=""#fnref-5fCXGsgmJ9pureYYt-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-4"" class=""footnote-item""><p>This isn’t clear in the original post, but is clarified in <a href=""https://www.lesswrong.com/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai#PAL_isn_t_in_tension_with_Christiano_s_story_and_isn_t_especially_informative"">this discussion</a>. <a href=""#fnref-5fCXGsgmJ9pureYYt-4"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-5"" class=""footnote-item""><p>I owe this point to Shahar Avin. <a href=""#fnref-5fCXGsgmJ9pureYYt-5"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-6"" class=""footnote-item""><p>These pathways by which conflict may increase existential risk are summarised in The Precipice (<a href=""https://theprecipice.com/"">Ord, 2020, ch. 6</a>). <a href=""#fnref-5fCXGsgmJ9pureYYt-6"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-7"" class=""footnote-item""><p>From <a href=""https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem-ebook/dp/B07N5J5FTS"">Human Compatible</a>: “... consider how content-selection algorithms function on social media. They aren’t particularly intelligent, but they are in a position to affect the entire world because they directly influence billions of people. Typically, such algorithms are designed to maximize click-through, that is, the probability that the user clicks on presented items. The solution is simply to present items that the user likes to click on, right? Wrong. The solution is to change the user’s preferences so that they become more predictable. A more predictable user can be fed items that they are likely to click on, thereby generating more revenue. People with more extreme political views tend to be more predictable in which items they will click on. (Possibly there is a category of articles that die-hard centrists are likely to click on, but it’s not easy to imagine what this category consists of.) Like any rational entity, the algorithm learns how to modify the state of its environment—in this case, the user’s mind—in order to maximize its own reward.8 The consequences include the resurgence of fascism, the dissolution of the social contract that underpins democracies around the world, and potentially the end of the European Union and NATO. Not bad for a few lines of code, even if it had a helping hand from some humans. Now imagine what a really intelligent algorithm would be able to do.” <a href=""#fnref-5fCXGsgmJ9pureYYt-7"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-8"" class=""footnote-item""><p>There is <a href=""https://www.reddit.com/r/AskHistorians/comments/5613ac/in_his_book_sapiens_yuval_noah_harari_states_that/"">some controversy</a> about whether this is the correct interpretation of the paleopathological evidence, but there seems to at least be consensus about the other two downsides (epidemics and physical stress increasing due to agriculture). <a href=""#fnref-5fCXGsgmJ9pureYYt-8"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-9"" class=""footnote-item""><p>I got the idea for this analogy from Daniel Kokotajlo’s work on <a href=""https://www.lesswrong.com/posts/kEtgXdjxA4oWjcLFQ/lessons-on-ai-takeover-from-the-conquistadors"">takeovers by conquistadors</a>, and trying to think of historical precedents for takeovers where loss of influence happened more gradually. <a href=""#fnref-5fCXGsgmJ9pureYYt-9"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-10"" class=""footnote-item""><p>I owe this point to Shahar Avin. <a href=""#fnref-5fCXGsgmJ9pureYYt-10"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-5fCXGsgmJ9pureYYt-11"" class=""footnote-item""><p>Source for these claims about CompStat: <a href=""https://gimletmedia.com/shows/reply-all/n8hwl7"">this podcast</a>. <a href=""#fnref-5fCXGsgmJ9pureYYt-11"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",Sam Clarke,sam-clarke,Sam Clarke,
NAzK9yqP9njr2Xp83,Are aircraft carriers super vulnerable in a modern war?,are-aircraft-carriers-super-vulnerable-in-a-modern-war,https://www.lesswrong.com/posts/NAzK9yqP9njr2Xp83/are-aircraft-carriers-super-vulnerable-in-a-modern-war,2020-09-20T18:52:29.270Z,18,8,11,False,True,,"<p>It seems like aircraft carriers are a good candidate for something that only exists because it made sense in WWII and it looks the part of “impressive military asset”, i.e. it’s all larping at this point. It seems vulnerable to attack relative to its huge cost because offense has an advantage over defense: I.e. can’t an enemy send tons of cheap drone planes and drone submarines to first hunt for its location and then swarm-attack it?</p>
<p>Note: I don’t know anything about this subject. The ideal answerer is someone with domain knowledge who has good epistemology; that’s why I wasn’t satisfied with a Google search and I’m asking here.</p>
",Liron,liron,Liron,
DTncBY8L5gnRsGtJW,"I'm Voting For Ranked Choice, But I Don't Like It",i-m-voting-for-ranked-choice-but-i-don-t-like-it,https://www.lesswrong.com/posts/DTncBY8L5gnRsGtJW/i-m-voting-for-ranked-choice-but-i-don-t-like-it,2020-09-20T18:40:04.410Z,32,18,12,False,False,,"<p><span>

This fall, </span>

<a href=""https://en.wikipedia.org/wiki/Instant-runoff_voting"">Ranked
Choice</a> / Instant Runoff Voting (IRV) will be 

<a href=""https://ballotpedia.org/Massachusetts_Question_2,_Ranked-Choice_Voting_Initiative_(2020)"">on
the ballot</a> in Massachusetts.  I'm voting for it, but only because
it's better than the status quo, not because I think it's a very good
voting system.



</p><p>

Massachusetts currently uses traditional majority (""first past the
post"") voting: whoever gets the most votes wins.  Unfortunately, this
only works well when you have two candidates. With more candidates,
the candidates tend to <a href=""https://en.wikipedia.org/wiki/Vote_splitting#Spoiler_effect"">hurt
their allies</a> by competing for the same pool of votes, making it
more likely that an opponent wins.

</p>

<p>


In IRV each voter lists their preferred candidates in order, and if
your first choice is eliminated then your vote goes to your next
favorite.  This mostly fixes the problem of minor spoiler candidates:
anyone who is not a serious contender will get eliminated and their
votes redistributed.

</p>

<p>

Unfortunately, IRV has major problems when you have more than two
serious candidates. For example, even if there is a candidate that a
<a href=""https://en.wikipedia.org/wiki/Condorcet_method"">majority of
voters prefer to every other</a>, they can still lose if their
competitors happen to be eliminated in the wrong order.  In <a href=""https://www.jefftk.com/p/why-ranked-choice-voting-isnt-great"">Why Ranked Choice Voting
Isn't Great</a> I give examples of realistic situations in which IRV
can give poor results.

</p>

<p>

While every voting method has cases it handles poorly, some are better
than others. One attempt to compare them is called <a href=""http://electionscience.github.io/vse-sim/VSEbasic/"">Voter
Satisfaction Efficiency</a> (<a href=""http://electionscience.github.io/vse-sim/VSE/"">more
details</a>).  The idea is, you run a large number of simulations and
see how different methods perform.  It turns out that IRV does
very poorly here, and if voters are highly strategic IRV does <a href=""http://electionscience.github.io/vse-sim/vse.html"">even
worse</a> than traditional plurality voting.

</p>

<p>

While I wish the voting method for us to consider were <a href=""https://en.wikipedia.org/wiki/Approval_voting"">Approval</a> (or
maybe <a href=""https://electowiki.org/wiki/3-2-1_voting"">3-2-1</a> or
<a href=""https://en.wikipedia.org/wiki/STAR_voting"">STAR</a>), I do
still think IRV is better than what we have today, and I'm planning on
voting for it.  One specific way in which IRV is an improvement is
that it mostly doesn't, in its failings, benefit one type of party.
This means that if we switch to IRV, and then as third-party
candidates become stronger contenders we start to run into IRV's
problems, it should be politically practical to switch to a better
system.  I do think there is some risk of setting back alternative
voting systems in general by implementing an inferior version, but on
balance I think the benefit of fixing the ""minor spoiler"" problem is
likely larger.

  </p>

<p><i>Comment via: <a href=""https://www.facebook.com/jefftk/posts/10100184817106872"">facebook</a></i></p>",jkaufman,jkaufman,jefftk,
SZ3jDHXHb4WF4jmbr,Where is human level on text prediction? (GPTs task),where-is-human-level-on-text-prediction-gpts-task,https://www.lesswrong.com/posts/SZ3jDHXHb4WF4jmbr/where-is-human-level-on-text-prediction-gpts-task,2020-09-20T09:00:28.693Z,27,14,19,False,True,,"<p>I look at graphs like these (From the GPT-3 paper), and I wonder where human-level is:</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_110 110w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_220 220w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_330 330w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_440 440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_550 550w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_660 660w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_770 770w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_880 880w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_990 990w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/585ac09b045f68460300c1c1b8121214fb18d82b2d7586a2.png/w_1008 1008w""></figure><p>Gwern seems to <a href=""https://www.gwern.net/newsletter/2020/05#gpt-3"">have the answer here</a>:&nbsp;</p><blockquote><p>GPT-2-1.5b had a cross-entropy validation loss of ~3.3 (based on the perplexity of ~10 in <a href=""https://www.gwern.net/images/ai/2019-radford-figure4-gpt2validationloss.png"">Figure 4</a>, and&nbsp;<span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\log_2(10)=3.32""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.519em;"">log</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char""></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">10</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mn MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3.32</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span></span>). GPT-3 halved that loss to ~1.73 judging from <a href=""https://www.gwern.net/images/ai/2020-brown-figure31-gpt3scaling.png"">Brown et al 2020</a> and using the scaling formula (<span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""2.57 \cdot (3.64 \cdot 10^3)^{-0.048}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.57</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.004em; padding-bottom: 0.298em;"">⋅</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3.64</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.004em; padding-bottom: 0.298em;"">⋅</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">10</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.048</span></span></span></span></span></span></span></span></span></span></span>). For a hypothetical GPT-4, if the scaling curve continues for another 3 orders or so of compute (100–1000×) before crossing over and hitting harder diminishing returns, the cross-entropy loss will drop, using to ~1.24 (<span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""2.57 \cdot (3.64 \cdot 10^6)^{-0.048}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2.57</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.004em; padding-bottom: 0.298em;"">⋅</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3.64</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.004em; padding-bottom: 0.298em;"">⋅</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base""><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">10</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">6</span></span></span></span><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-texatom"" style=""""><span class=""mjx-mrow""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">0.048</span></span></span></span></span></span></span></span></span></span></span>).</p><p>If GPT-3 gained so much meta-learning and world knowledge by dropping its absolute loss ~50% when starting from GPT-2’s near-human level, what capabilities would another ~30% improvement over GPT-3 gain? What would a drop to ≤1, perhaps using wider context windows or recurrency, gain?</p></blockquote><p>So, am I right in thinking that if someone took random internet text and fed it to me word by word and asked me to predict the next word, I'd do about as well as GPT-2 and significantly worse than GPT-3? If so, this actually lengthens my timelines a bit.<br>(Thanks to Alexander Lyzhov for answering this question in conversation)</p>",daniel-kokotajlo,daniel-kokotajlo,Daniel Kokotajlo,
K4RSZJd7J53ty8RDt,My (Mis)Adventures With Algorithmic Machine Learning,my-mis-adventures-with-algorithmic-machine-learning-1,https://www.lesswrong.com/posts/K4RSZJd7J53ty8RDt/my-mis-adventures-with-algorithmic-machine-learning-1,2020-09-20T05:31:19.745Z,16,6,4,False,False,,"<h2>Introduction</h2>
<p><em>This was originally posted <a href=""http://anthonylorenhart.com/2020-09-18-My-(Mis)Adventures-with-Algorithmic-Machine-Learning/"">here</a>.</em></p>
<p>I've been researching, for quite some time, the prospect of machine learning on a wider variety of data types than normally considered; things other than tables of numbers and categories. In particular, I want to do ML for program and proof synthesis which requires, at the very least, learning the structures of trees or graphs which don't come from a differentiable domain. Normal ML algorithms can't handle these; though some recent methods, such as graph neural networks and transformers, can be adapted to this domain with some promising results. However, these methods still rely on differentiation. Is this really required? Are we forever doomed to map all our data onto a differentiable domain if we want to learn with it?</p>
<p>An alternative approach that has been bandied about for a while is the utilization of compression. It's not hard to find articles and <a href=""https://www.lesswrong.com/posts/hAvGi9YAPZAnnjZNY/prediction-compression-transcript-1"">talks</a> about the relationship between compression and prediction. If you have a good predictor, then you can compress a sequence into a seed for that predictor and decompress by running said predictor. Going the other way is harder, but, broadly speaking, if you have a sequence that you want to make a prediction on and a good compressor, then whichever addition increases the compressed size the least should be considered the likeliest prediction. This approach is quite broad, applying to any information which can be represented on a computer and not requiring any assumptions whatsoever about the structure of our data beyond that. We could use this idea to, for example, fill in gaps in graphs, trees, sets of input-output pairs, etc.</p>
<p>It's important to understand what's actually required here. We don't actually need to compress our training data; we only need a way to estimate the change in minimal-compression-size as we add a prediction. This minimal-compression-size is called the Kolmogorov Complexity, denoted <code>K(X)</code>. The minimal-compression-size of a program which outputs <code>X</code> on an input <code>Y</code> is called the Conditional Kolmogorov Complexity, denoted <code>K(X|Y)</code>. The topic of Kolmogorov Complexity is quite broad, and I won't explain all its complexities here. A standard introduction is the textbook <a href=""https://www.springer.com/gp/book/9781489984456"">An Introduction to Kolmogorov Complexity and Its Applications</a> by Li, Ming, Vitányi, and Paul. If we have a good method for calculating <code>K</code>, then we don't need to actually make use of compression.</p>
<p>Making this practical is quite hard and under-researched, and there aren't many papers on the topic. But there is this;
<a href=""https://arxiv.org/abs/1910.02758"">Algorithmic Probability-guided Supervised Machine Learning on Non-differentiable Spaces</a>
which reproduces some standard ML applications using this approach. I want to understand how doing ML this way works, and this post will basically be a collection of the notes I made while reading the paper. If I refer to ""the paper"", ""this paper"", etc. in this post, this is what I'm referring to. These notes will digress quite often and I'm also quite critical of some aspects of the paper. This post was also written somewhat like a stream of consciousness, so I'll often say something which I correct later on. This post isn't intended to merely summarize the paper, but to describe what I learned and thought as I read it. Hopefully, you'll learn stuff too.</p>
<h2>Why Not Use Ordinary Compression?</h2>
<p>One of the most common suggestions for approximating <code>K(X)</code> is to simply use an already existing compression algorithm. The problem is that most ""optimal"" compression algorithms such as arithmetic encoding are only optimal up to the Shannon Entropy of the data. That is if we assume the data is sampled randomly from a distribution, the best we can do is estimate the shape of this distribution and give shorter encodings appropriately to more likely symbols. This is, asymptotically, about the same as counting substring occurrences to reduce redundancy. If our data is actually just randomly sampled, then this is great! But the real world isn't like this. Most real-world data can be construed as an essentially deterministic process with some added noise. Most compression potential comes from modeling this underlying process, not the noise.</p>
<p>Consider the sequence;</p>
<pre><code>1, 2, 3, 4, ..., 1000
</code></pre>
<p>This is, obviously, very compressible. An optimal (truly optimal, not Shannon entropy-optimal) compressor would be able to compress this into a program producing this output. Maybe <code>Range@1000</code>, or something even smaller, depending on what language it's using. But statistical compression will just try to find repetitive substrings. Even if we represent this list in binary and compress, statistical methods won't be able to compress this much better than a truly random string since there are few repetitious patterns.</p>
<p>There are lots of natural examples of this. Compressing the digits of π, compressing the coordinates of regular geometric figures, compressing a list of positions for a simple physical system simulation. It's obvious that these can have small algorithmic complexity, that they should be compressible into small programs that generate them, and yet statistical compression methods won't be able to take advantage of this.</p>
<p>As a consequence, we must use compression methods that do something more sophisticated than statistical compression. Unfortunately, essentially all general-purpose compression methods are like this. There are some ML-based methods that probably aren't. A lot of the text-compression algorithms which participated in the <a href=""http://mattmahoney.net/dc/text.html"">Large Text Compression Benchmark</a> use RNNs which are definitely doing something other than statistical compression.</p>
<p>Much of the paper is dedicated to explaining one method for approximating Kolmogorov Complexity. Kolmogorov Complexity isn't computable, and getting a good approximation is very hard. Some clever methods have been devised, but we can't get a good grasp of it as easily as we can perform statistical compression.</p>
<h2>Methods for Approximating Kolmogorov Complexity</h2>
<p>We, ultimately, need a way to approximate Kolmogorov complexity. The learning methods themselves should be largely independent of this, but choosing a method is essential for real-world applications. Here are a few methods I've found in the literature;</p>
<hr>
<h3>CTM - Coding Theorem Method</h3>
<p>This is the method the paper endorses so I'll talk about it in more detail later on.</p>
<p>The idea is to enumerate all strings of a given length and run them as programs for some chosen model. We collect all the input-output pairs to a database. We then use the coding theorem to justify using this to estimate <code>K</code>. In particular, if we add together <code>2^-l(p)</code>, where <code>l(p)</code> is the length of <code>p</code>, for all programs <code>p</code> which output <code>X</code>, that will get us an estimate for <code>K(X)</code>. This is basically what the coding theorem says, hence the name of the method.</p>
<h4>BDM - Block Decomposition Method</h4>
<p>This utilizes an existing CTM database to estimate Kolmogorov Complexity. It first tries finding algorithmically compressible substrings using CTM and then uses that information in conjunction with a Shannon entropy like calculation to estimate the complexity of the whole string. For small strings, BDM is close to the performance of CTM, for large strings its average-case performance is close to statistical compression. Many large strings in practice, however, tend to be compressed better than with statistical methods.</p>
<p>See:</p>
<ul>
<li><a href=""https://arxiv.org/abs/1101.4795"">Numerical Evaluation of Algorithmic Complexity for Short Strings</a></li>
<li><a href=""https://arxiv.org/abs/1609.00110"">A Decomposition Method for Global Evaluation of Shannon Entropy and Local Estimations of Algorithmic Complexity</a></li>
</ul>
<hr>
<h3>List Approximation</h3>
<p>List Approximation is based on optimizing a simple observation. While generating the smallest program generating <code>X</code> is not computable, generating a list guaranteed to contain the smallest program is. In particular, we can return a list enumerating all strings below and containing <code>X</code>. This will definitely have the smallest program generating <code>X</code>, but it will be exponentially large in the length of <code>X</code>. How small can this list be?</p>
<p><a href=""https://arxiv.org/abs/1301.1547"">Short lists with short programs in short time</a> (also an improved version in <a href=""https://arxiv.org/abs/1212.6104"">Short lists for shortest descriptions in short time</a>) show that this list can be made quadratically large (and asymptotically no smaller) in the length of the input while guaranteedly containing the smallest program. This makes searching for <code>K(X)</code> much more practical as we only need to run a number of programs quadratic in the size of <code>X</code>.</p>
<p>If we are willing to accept only approximating <code>K</code> with a list, we can ensure an <code>O(log(|X|))</code> penalty to our smallest generating program and make the list linear in the size of <code>X</code>, as shown in <a href=""https://arxiv.org/abs/1311.7278"">Linear list-approximation for short programs</a>.</p>
<p>These methods seem promising, but the algorithms themselves are quite abstruse and some require exponential space, making them impractical. However, improvements may be possible.</p>
<p>It's unclear how much labor is actually saved when using the approximation lists. It may be the case that both the smallest possible representations of programs and everything else in the list requires an absurd amount of work to normalize. It may remove those programs which were already easy to dismiss when using brute-force while exclusively keeping the ones that are hard to assess anyway. The lists may also only have the smallest program which is hard to assess. If there's no second-best approximation to <code>K</code>, then we're stuck having to find the actual smallest value with no backup if that's impractical to know. Without any practical demonstrations, it's hard to know if these are genuine problems.</p>
<hr>
<h3>Universal Almost Optimal Compression</h3>
<p>This method is based on a generic property of compression-decompression pairs. As it turns out, we can, while incurring polylogarithmic overhead in the size of the compressed string, replace a (potentially non-computable) compression algorithm and its decompressor with a pair consisting of an efficient compressor and a potentially inefficient decompressor. By fixing our compressor-decompressor pair to be <code>K</code> and <code>E</code> (the function that simply evaluates a program), we can get a new compression-decompression pair that will compress inputs to a length which differs, at most, polylogarithmically from <code>K</code>. This compressor would not get us smaller, equivalent programs, but, if our goal is to simply approximate the size of a hypothetical Kolmogorov-compressed program, this should work fine.</p>
<p>The basic idea is the following; rather than trying to find the actual smallest compressed string, instead only generate a small ""fingerprint"" which would allow you to identify what the smallest compressed string might be. The decompressor then just generates a list of candidates, perhaps exhaustively, and uses the fingerprint to find and run it by brute force.</p>
<p>Depending on how much work we're willing to put into making this fingerprint, we can get it down to a pretty small size. According to the paper, it can be made within <code>K(X) + O(log(X))</code> with only polynomial effort in the size of the string.</p>
<p>I don't fully understand how this technique works. It's tied up in a lot of the theory that List Approximation uses as well. It's concepts come from the theory of pseudorandomness; something I'll have to become more familiar with.
See</p>
<ul>
<li><a href=""https://arxiv.org/abs/1911.04268"">Universal almost optimal compression and Slepian-Wolf coding in probabilistic polynomial time</a></li>
</ul>
<hr>
<h3>Incremental compression</h3>
<p>Instead of calculating <code>K(X)</code> all at once, it can usually be done piecemeal. The idea is that, given some input <code>X</code>, we want to find a pair of functions <code>F</code>, <code>D</code>, such that <code>F(D(X)) = X</code> and <code>|F| + |D(X)| &lt; |X|</code>. Specifically, we want to find the smallest <code>F</code> meeting this requirement. The idea is that <code>D(X)</code> reduces the size of <code>X</code>, deleting whatever information is in <code>F</code> from <code>X</code>. <code>F</code> is then that information, isolated from <code>X</code>. By repeating this over and over again, we can decompose <code>X</code> into a series <code>F1(F2(F3(...(R))))</code>, where <code>R</code> is the residual which wasn't compressed. In the limit, <code>R</code> should basically consist of all the random information present in <code>X</code>, while the <code>F</code>s correspond to algorithmic ""features"" which can be isolated from <code>X</code>. So long as the <code>F</code>s are always as small as possible, this construction will approach the actual Kolmogorov complexity.</p>
<p>I think this line of work hints towards a rich theory of ""atomic"" algorithmic information, but it's not ready for practical applications as of yet. The incremental compression is not computable, but it should be much quicker to approximate, on average, than <code>K(X)</code> while still approaching <code>K(X)</code>.</p>
<p>See:</p>
<ul>
<li><a href=""https://arxiv.org/abs/1908.03781"">A theory of incremental compression</a></li>
</ul>
<hr>
<h3>Higher-order compression</h3>
<p>This is a method of compressing lambda expressions by observing a connection between grammar-based compression and lambda binding.</p>
<p>The procedure is very simple. Start with a lambda expression.</p>
<ul>
<li>
<p>Compress the expression using a tree-grammar (using re-pair, for instance). Convert this tree grammar back into a lambda expression.</p>
</li>
<li>
<p>Run a ""simplification procedure"" which performs</p>
<ul>
<li>eta-reduction</li>
<li>beta-reduction on linear lambda bindings</li>
<li>beta-reduction on applications to bound variables</li>
</ul>
</li>
<li>
<p>Repeat until the expression stops shrinking.</p>
</li>
</ul>
<p>I honestly have a hard time believing this works. I'll have to think about it more carefully. To me, this doesn't seem like it should perform better than statistical compression, but, according to the paper <a href=""http://www-kb.is.s.u-tokyo.ac.jp/~koba/papers/hosc-fpcd.pdf"">Functional Programs as Compressed Data</a>;</p>
<blockquote>
<p>our representation of compressed data in the form of λ-terms is optimal with respect to Kolmogorov complexity, up to an additive constant.</p>
</blockquote>
<p>I don't buy the argument given in the paper, though, which just seems to argue that optimal compression should be possible in theory; it doesn't even mention the specifics of the algorithm they present. None the less, I want to include this here since it makes a specific and relevant claim. Some followup work seems to be doing something more computationally interesting, such as <a href=""https://arxiv.org/abs/1706.10061"">Compaction of Church Numerals for Higher-Order Compression</a>, so a future version of this might be better suited for the task at hand.</p>
<p>Similar grammar-based methods should work for other structured models of computation. For example, using re-pair for graphs as presented in <a href=""https://arxiv.org/abs/1704.05254"">Grammar-Based Graph Compression</a>, a version should be possible for interaction nets.</p>
<h2>Approximating Conditional Kolmogorov Complexity using CTM</h2>
<p>While those methods allow approximating <code>K(X)</code>, we usually actually want to approximate <code>K(X|Y)</code>, the amount of information stored in <code>X</code> but not in <code>Y</code>; the amount of information <code>X</code> tells us if we already know <code>Y</code>. The paper tries giving a method for doing this, but the method seems very questionable. It says to do the following;</p>
<ul>
<li>Fix a ""computable relation"" <code>M : x → y</code>.
<ul>
<li>I don't know what the paper means by this, and the phrase ""computable relation"" is never clarified. I would assume that it means that a list of output <code>ys</code> can be enumerated on any input <code>x</code>, but I don't know. <code>M</code> is also described as being a ""Turing complete space"", in the typical case (e.g. when using CTM). <code>M</code> cannot be both a space and a relation, so clearly space is meant in some loose sense, but it's unclear what a ""Turing complete space"" is supposed to be. I interpret this as meaning that <code>M</code> is supposed to be a relation from programs to outputs in the typical case, which is a function, not a relation. But this framing implies that <code>M</code> could be broader. Perhaps <code>M</code> may be a relation in the case of a nondeterministic computation model, but this is not expanded upon in the paper.</li>
</ul>
</li>
<li>Fix a finite set <code>P</code>, such that <code>(y, x) ∈ P</code> iff <code>y ∈ M(x)</code>
<ul>
<li>In the ordinary case, <code>P</code> would be the database in CTM of output-input pairs.</li>
</ul>
</li>
<li>The paper then states that we can then approximate <code>K(X|Y)</code> by taking the <code>log₂</code> of the sum, for all <code>(Y, X) ∈ P</code>, of <code>1/|P|</code>.</li>
</ul>
<p>The problems start when we realize that, since <code>P</code> is a set, any pair occurs in <code>P</code> at most once, meaning that this value is <code>log₂ (1/|P|)</code> if <code>(Y, X)</code> is in the database or <code>-∞</code> if it isn't. This is obviously not what's intended, but I also can't glean from the context what is. Furthermore, the full expression given in the paper is;</p>
<pre><code>CTM(X|Y) = log₂ Σ{(Y, X) ∈ P} 1/|P|
</code></pre>
<p>Both <code>X</code> and <code>Y</code> are bound twice, once in defining CTM and once by the sum itself. It seems like the second bind is trying to reference the first, but that makes no sense, syntactically. Alternatively, if we interpret the binders as entirely separate, then CTM does nothing with its arguments, and just returns <code>0</code> on all inputs (since <code>log₂ |P|/|P| = 0</code>), which is obviously wrong.</p>
<p>The simplest fix is to simply make <code>P</code> a multiset which may contain multiple copies of any given pair. The calculation should then be;</p>
<pre><code>CTM(x|y) = log₂( |[ p ∈ P : p == (x, y) ]| / |P| )
         = log₂ Σ{p ∈ P} if p == (x, y) then 1/|P| else 0
</code></pre>
<p>This may be off, but this is the closest thing to the original paper I could think of. It just calculates the log-likelihood of a random program covered by <code>P</code> outputing <code>x</code> on input <code>y</code>. Except, there are a few problems with this. Firstly, this quantity will always be negative since <code>log(X) &lt; 0</code> when <code>0 &lt; X &lt; 1</code>. <code>K</code> can never be negative. Even if we fix this, there's still a bigger issue. This CTM definition assumes that all programs are uniformly random, but they aren't. Think of the procedure we'd go through when generating a random program. Assuming our choices are uniformly distributed, the programs we generate won't be. If we assume our programs are binary strings, then we will, at each point, make one of three choices; add a <code>0</code>, add a <code>1</code>, or end the string. If each choice is uniformly sampled, then there will be a one third chance of generating the empty string, a one in nine chance of generating <code>1</code>, and about a one in 60,000 chance of generating <code>001101001</code>. The chance of generating a program decays exponentially with the length of the program. This observation is built into algorithmic probability, and it's weird that the CTM measure, as described in this paper, ignores that.</p>
<p>Digressing a bit, I feel like the authors may have some over-familiarity with one specific model of computation. One approach to define <code>M</code> used by the Complexity Calculator project is to use an enumeration for Turing machines which, to my knowledge, was originally devised for investigating busy beaver numbers. I believe that the authors are imagining that <code>M</code> as a function that enumerates all Turing machines, runs <code>x</code> on them all, and outputs a stream of all the <code>ys</code> that each Turing machine outputs. This will certainly be a function rather than some generic relation, though.</p>
<p>Let's think of what this measure means for other models of computation. If we were using, say, lambda expressions instead, <code>M</code> should enumerate all lambda expressions, take another lambda expression as input, and output the normal forms of the input applied to all possible lambda expressions. This does seem like it makes sense for any model of computation, but I'm not sure it makes sense as a measure of algorithmic similarity.</p>
<p>The justification for this procedure is supposed to come from the coding theorem, which states that <code>K(X) + O(1) = -log₂(m(X))</code>, where</p>
<pre><code>m(X) = Σ{p | p ↓ X} 2 ^ -l(p)
</code></pre>
<p>where <code>p ↓ X</code> means <code>p</code> normalizes to <code>X</code> and <code>l(p)</code> is the length of <code>p</code>. There's that exponential decay I was talking about.</p>
<p>See:</p>
<ul>
<li><a href=""http://www.scholarpedia.org/article/Algorithmic_probability"">Scholarpedia: Algorithmic probability</a></li>
<li>Theorem 4.3.3 of ""An Introduction to Kolmogorov Complexity and Its Applications""</li>
</ul>
<p>Modifying this for lambda expressions,</p>
<pre><code>m(X) = Σ{l | l ↓ X} 2 ^ -I(l)
</code></pre>
<p>where <code>l ↓ X</code> means <code>l</code> normalizes to <code>X</code> and <code>I(l)</code> measures the bit information of <code>l</code>, essentially the number of binary decisions made when constructing <code>l</code>. <code>I(l)</code> would be calculated</p>
<pre><code>I(l) := I(l, 0)
I(λ x . y, b) := log₂(2 + b) + I(y, b + 1)
I(x y,     b) := log₂(2 + b) + I(x, b) + I(y, b)
I(x,       b) := log₂(2 + b)
</code></pre>
<p>Incidentally, the length of a binary string doesn't actually give the information content of that string. If a string's length isn't fixed beforehand, then each additional digit incurs one trit of information since at each stage of the construction we are choosing between one of three options; <code>0</code>, <code>1</code>, or stop constructing the string. From this, we can conclude that <code>l(s) = log₃(2 ^ -I(s)) - 1</code>; that is, the length of a string is one less than the number of trits in that string. If the string's length is fixed beforehand, if we cannot choose to end the construction of a string at our leisure, then each choice is actually binary and <code>l(s) = I(s)</code>.</p>
<p>I think that using <code>I(s)</code> to calculate the information rather than the length is more theoretically correct than the usual expression in terms of length. It doesn't seem to matter too much in the case of strings because the sum over all <code>2 ^ (-l(s)-1)</code> = the sum over all <code>2 ^ -I(s)</code> = <code>1</code>, so both are valid ways of making a probability distribution over all programs with a similar exponential decay. That <code>-l(p)-1</code> is there so that the empty string isn't given 100% of the distribution. The real problem is generalizability; the length calculation generally fails to make a coherent distribution if our computation model no longer accepts binary strings as inputs. The information, however, can always be adapted even if our computational model expects programs to be something esoteric, like graphs, such is the case with interaction nets.</p>
<p>As a side note, despite length being theoretically incorrect, it's been used in some papers for measuring the information of a lambda expression. See <a href=""https://arxiv.org/abs/1805.08592"">Computable Variants of AIXI which are More Powerful than AIXItl</a> for instance. But it seems like the theoretically wrong thing to do, especially since the actual information is so easy to calculate. I think many authors in this field don't think too carefully about the information content of the things they write about, which is quite ironic.</p>
<p>The conditional coding theorem states that;</p>
<pre><code>K(X|Y) + O(1) = -log₂(m(X|Y))
</code></pre>
<p>where</p>
<pre><code>m(X|Y) = Σ{p | p(Y) ↓ X} 2 ^ -l(p)
</code></pre>
<p>See:</p>
<ul>
<li>Theorem 4.3.4 and Definition 4.3.7 in ""An Introduction to Kolmogorov Complexity and Its Applications""</li>
</ul>
<p>This is definitely not what that CTM measure is approximating. Because the original in the paper is so obviously wrong, and the nature of <code>M</code> is so poorly explained, it's hard to patch it up to whatever the author's intended. In fact, I'm not sure this is actually possible. The conditional coding theorem relies on the length of the program, <code>p</code>, which <code>Y</code> is being fed into. This would require us to incorporate the complexity of the Turing machine itself, but <code>P</code> doesn't store that information.</p>
<p>Let me try to offer a more sensible formulation of the CTM idea. Assume a computing function <code>M : x → y</code> which simply evaluates an input program into an output using a fixed computational model. Let <code>P</code> be a finite subset of output-input pairs <code>(y, x)</code>. The input type should satisfy the smn theorem, so we can format programs like <code>f(x)</code>; have functions which can have variables substituted into them. For some Turing machines, application is often just done as list concatenation, though, this becomes squirley if we want to represent functions which take multiple arguments, nested function application, etc. For a more well-structured model of computation, such as the lambda calculus, application may be a fundamental operation. Regardless, we can then define our metric as;</p>
<pre><code>CTM(x|y) = - log₂( Σ{ p | (x, p(y)) ∈ P } 2 ^ -I(p) )
</code></pre>
<p>This would require us to be able to pattern match so as to detect <code>p(y)</code>. If application is just concatenation, then this is as simple as looking for the suffix <code>y</code>, which is pretty trivial.</p>
<p>This doesn't much resemble what's in the paper, but it makes much more sense.</p>
<p>The paper mentions that <code>CTM(x)</code>, which approximates non-conditional Kolmogorov complexity, can be defined as <code>CTM(x|"""")</code>, <code>x</code> conditioned on the empty string. Well, actually it says that <code>CTM(""""|x)</code> should do this, but that doesn't make any sense. It's unclear enough in the original, it should definitely be <code>CTM(x|"""")</code> in my modified version since it would just be summing for every program <code>p = p ++ """"</code> which outputs <code>x</code>; hence it's eminently compatible with concatenation-as-application. In general, a separate measure would need to be made for other models of computation since application-as-concatenation doesn't even make sense in general for Turing machines (do you really think application should be associative?), much less other models of computation. More generically, we'd define;</p>
<pre><code>CTM(x) = - log₂( Σ{ p | (x, p) ∈ P } 2 ^ -I(p) )
</code></pre>
<h2>Block Decomposition</h2>
<p>In the section on BDM (Block Decomposition Method), the authors keep calling things ""tensors"", but it never explains what a tensor is. It definitely isn't in the ordinary mathematical sense since the only things described as tensors are just binary strings. The <a href=""https://arxiv.org/abs/1609.00110"">original paper on BDM</a> talks about compressing tensors and vectors. However, neither of those two things are actually compressed in that paper. Instead, it seems like the authors think that any list is a vector and any list-of-lists is a tensor, which is what they actually compress. It's annoying when authors abuse terminology like this; it's just confusing. From that, I think ""tensor"" just means a 2-dimensional array of bits. Just call them arrays if that's what they are! This is a CS paper, after all.</p>
<p>I have a suspicion that the segment on block decomposition was copy-pasted from somewhere else without any copyediting. Tensors aren't mentioned outside the section on BDM.</p>
<p>Setting that aside, we're trying to approximate <code>K(X|Y)</code> using <code>BDM(X|Y)</code>. Assume a fixed ""partitioning strategy"". The paper never explains what this is, but I read other sources (which I'll talk about later on) which informed me that a ""partitioning strategy"" is simply a method of splitting up a string into (possibly overlapping) substrings which are already in our CTM database. What BDM tries to do is then devise a ""pairing strategy"" which minimizes a quantity. The paper doesn't state what a ""pairing strategy"" is either, and no other source I read clarifies. It only says the following;</p>
<p>A pairing strategy generates a set <code>P</code></p>
<ul>
<li>consisting of pairs of pairs <code>((rx, nx), (ry, ny))</code>
<ul>
<li>where <code>rx</code> and <code>ry</code> are partitions of <code>X</code> and <code>Y</code> made by our partitioning strategy
<ul>
<li>where each <code>rx</code> occurring in <code>P</code> must only occur once, though there is no similar restriction on <code>ry</code>.
This just means that <code>P</code>, treated as a relation, is (non-totally) functional.</li>
</ul>
</li>
<li>where  <code>nx</code> and <code>ny</code> are the occurrence counts of <code>rx</code> and <code>ry</code> within the partitionings of <code>X</code> and <code>Y</code>, respectively.</li>
</ul>
</li>
</ul>
<p>That's all it says on pairing strategies. As far as I can tell from this, a pairing strategy that pairs nothing and is just empty is valid, but I'm pretty sure it's not supposed to be.</p>
<p>Assuming we have an understanding of what additional constraints a pairing strategy should have, we want to find the pairing strategy which minimizes the following quantity;</p>
<pre><code>Σ{((rx, nx), (ry, ny)) ∈ P} CTM(rx|ry) + if nx == ny then 0 else log(nx)
</code></pre>
<p>The minimal value for this quantity will be <code>BDM(X|Y)</code>.</p>
<p>This quantity will always be nonnegative and we can always minimize it to zero by making <code>P</code> empty. This is obviously not intended. It also doesn't make much sense to me that we're taking the log of <code>nx</code> if <code>nx</code> is just the count of <code>rx</code>s rather than something involving the length of <code>rx</code>. And shouldn't that log term scale with the difference between <code>nx</code> and <code>ny</code> in some way? The paper offers no real intuition.</p>
<p>Maybe looking at the <a href=""https://arxiv.org/abs/1609.00110"">original BDM paper</a> can offer clarification. It gives a nice example which I'll reproduce here. Let's say we're applying BDM to the string</p>
<pre><code>010101010101010101  
</code></pre>
<p>We have a choice of partitioning strategy, but it gives the example of splitting the string into substrings of length 12 which may overlap by, at most, 11 digits. When we do this, we get 3 <code>101010101010</code>s and 4 <code>010101010101</code>s. According to CTM, both strings have a complexity of 26.99 bits (assuming we're using only 2-state binary Turing machines). This would indicate that the smallest program generating the 12 digit string is, at most, 26 digits long.  Also, there are no Turing complete 2-state binary Turing machines, so this choice seems doubly weird. We then calculate the BDM value as</p>
<pre><code>26.99 + log(3) + 26.99 + log(4) ≈ 57.565
</code></pre>
<p>... Okay, but shouldn't it be way smaller? The original string wasn't even twice as long as its partitions, and it should be almost as easy to generate as the partitions. I thought this might be an idiosyncrasy of the specific kind of Turing machine which the paper uses, but the <a href=""https://www.complexitycalculator.com/"">complexity calculator website</a> says almost the same thing, giving the ""BDM algorithmic complexity estimation"" as 57.5664 bits when we select a block size of 12 with an overlap of 11.</p>
<p>Let's digress a bit and think of <code>K</code> in the lambda calculus. Firstly, we need a way to represent binary strings. We'll just encode these as lists of bits. The type of bits will be defined as</p>
<pre><code>2 = ∀ X . X → X → X = {0, 1}
</code></pre>
<p>where</p>
<pre><code>0 = λ f . λ t . f
1 = λ f . λ t . t
</code></pre>
<p>Strings should have an appropriate elimination rule stating that, for any type family or predicate <code>P</code> over binary strings,</p>
<pre><code>∀ S : BinString . (∀ s : BinString . (b : 2) → P s → P (b :: s)) → P """" → P S
</code></pre>
<p>This is essentially the induction rule for binary strings. One form of it, anyway. We can replace that predicate with a polymorphic variable to get our representation.</p>
<pre><code>BinString = ∀ X . (2 → X → X) → X → X
</code></pre>
<p>Compare</p>
<pre><code>∀ S : BinString . ∀ P . (∀ s . (b : 2) → P s → P (b :: s)) → P """" → P S
                  ∀ X . (           2  → X   → X         ) → X    → X
</code></pre>
<p>For any particular string, <code>S</code>, we can realize the induction principal using</p>
<pre><code>λ c : ∀ s . (b : Bits) → P s → P (b :: s) . λ n : P """" . S c n
</code></pre>
<p>See</p>
<ul>
<li><a href=""https://homepage.divms.uiowa.edu/~astump/papers/cpp-2018.pdf"">Generic Derivation of Induction for Impredicative Encodings in Cedille</a>, I guess, since I don't know of a better source on this topic.</li>
</ul>
<p>I'll talk about alternate representations later on, but I think this is among the most natural representations given the mathematical structure of the data type. ""Most natural"" doesn't necessarily mean ""best"", though. Unary natural numbers are more natural than binary representations since pretty much every simple representation of the universal property of ℕ suggests a unary representation. However, they're horribly inefficient.</p>
<p>Using this representation, the original string would be encoded as</p>
<pre><code>λc . λn . 
  c 0 (c 1 (c 0 (c 1 (c 0 (c 1
  (c 0 (c 1 (c 0 (c 1 (c 0 (c 1
  (c 0 (c 1 (c 0 (c 1 (c 0 (c 1 n))))))
  )))))))))))
</code></pre>
<p>This representation has about 192.747 bits of information. That information count might seem like a lot, but the lambda calculus represents trees of any kind as efficiently as it can represent lists. In this case, the string is a list of bits, which is about as small as can be expected.</p>
<p>However, it can be compressed to;</p>
<pre><code>λc . λn . 
  (λ f . λ x . f (f x))
  (λ f . λ x . f (f (f x)))
  (λ x . c 0 (c 1 x))
  n
</code></pre>
<p>which has about 83.93 bits of info. One of the 12 character sub-partitions can be compressed into</p>
<pre><code>λc . λn . 
  (λ f . λ x . f (f x))
  ((λ f . λ x . f (f (f x))) (λ x . c 0 (c 1 x)))
  n
</code></pre>
<p>For the other, just swap <code>0</code> and <code>1</code>. This has about 83.93 bits; the exact same, in fact, as the full string. The reason why these are the same is that there are 9 repetitions of <code>01</code> in the full string and <code>9 = 3 ^ 2</code>. In the substrings, there are 6 repetitions of <code>01</code> or <code>10</code>, and <code>6 = 2 * 3</code>. The information in multiplication is the same as the information in exponentiation, so the representations end up having the same amount of info. Of course, I don't know if these are actually the smallest possible representations; these are just the smallest I could come up with. They do illustrate my point, however. The two strings should have about the same information; maybe the original should have a little more. It seems extremely suspicious to me that the two strings have such dramatically different bit-counts according to BDM.</p>
<p>This isn't the only way to represent binary strings. We can write the original string instead as;</p>
<pre><code>λ0 . λ1 . λ x .
  0 (1 (0 (1 (0 (1
  (0 (1 (0 (1 (0 (1
  (0 (1 (0 (1 (0 (1 x)))))))
  ))))))))))
</code></pre>
<p>To justify this representation we need to prove that its type is isomorphic to something satisfying the universal property of binary strings. Here, <code>1</code> and <code>0</code> are expected to take a function <code>X → X</code> and return another function of the same type. This means our new representation has type;</p>
<pre><code>∀ X . (X → X) → (X → X) → (X → X)
</code></pre>
<p>we can rewrite this using a bit of type algebra;</p>
<pre><code>  ∀ X . (X → X) → (X → X) → (X → X)
≅ ∀ X . (X → X) × (X → X) × X → X
≅ ∀ X . (X → X) × (X → X) × (1 → X) → X
≅ ∀ X . (X + X + 1 → X) → X
≅ ∀ X . (2 × X + 1 → X) → X
</code></pre>
<p>Note that any type of the form <code>∀ X . (F(X) → X) → X</code> is the (weakly) initial algebra over the endofunctor <code>F</code>. Binary strings are the initial algebra over the endofunctor <code>X ↦ 2 × X + 1</code>, a special case of initiality for lists in general. Continuing this calculation;</p>
<pre><code>  ∀ X . (2 × X + 1 → X) → X
≅ ∀ X . (2 × X → X) → (1 → X) → X
≅ ∀ X . (2 → X → X) → X → X
</code></pre>
<p>Which is the representation we started with. This justifies that the new representation is isomorphic to the old one and we can comfortably use it interchangeably. See also;</p>
<ul>
<li><a href=""https://homepages.inf.ed.ac.uk/wadler/papers/free-rectypes/free-rectypes.txt"">Recursive types for free!</a></li>
<li><a href=""https://www.youtube.com/watch?v=YScIPA8RbVE"">The Algebra of Algebraic Data Types (Youtube)</a></li>
</ul>
<p>Our new representation has about 90.49 bits of information, while the sub-strings have about 62.63 bits. We can compress our larger string to</p>
<pre><code>λ0 . λ1 . 
  (λ f . λ x . f (f x))
  (λ f . λ x . f (f (f x)))
  (λ x . 0 (1 x))
</code></pre>
<p>which has about 59.858 bits of information, nearly what BDM states the Turing machine representation should have. And the lambda calculus has to represent every tree-like datatype! What's the Turing machine representation doing with all that space below 57 bits if it can't even fit 9 repetitions of <code>01</code>? My point from before still stands; both strings have similar amounts of algorithmic information. It's suspicious that BDM would say otherwise.</p>
<p>...</p>
<p>Okay, I think I figured it out. I was confused about the partitioning strategy. It's up to us to find a good selection of a block-size and overlap. Going back to the complexity calculator, if I set the block-size to 2 and the overlap to 1, it estimates the complexity to be 12.82 bits. Doing the calculation myself, we have 9 repetitions of <code>01</code> and 8 repetitions of <code>10</code>. The calculation would then be;</p>
<pre><code>3.3274 + log(9) + 3.3274 + log(8) ≈ 12.8247
</code></pre>
<p>Going through all the options in the complexity calculator, the minimal is a block-size of 2 with an overlap of 0. This partition only has 9 <code>01</code>s, and nothing else. This gives the calculation as;</p>
<pre><code>3.3274 + log(9) ≈ 6.4973
</code></pre>
<p>The BDM paper states that</p>
<blockquote>
<p>[...] if <code>|Adj(X)|</code> is close to <code>1</code>, then <code>BDM(X) ≈ K(X)</code>.</p>
</blockquote>
<p>where <code>Adj(X)</code> is the set of pairs of substrings with their occurrence counts. The block-size of 2 with an overlap of 0 gives an <code>|Adj(X)|</code> of exactly 1 since it only has one partition; <code>Adj(X) = {(01, 9)}</code>. This should, presumably, be as close to the actual Kolmogorov complexity as BDM can get. I suppose that means we're trying to find the partition strategy which minimizes the number of substrings that are covered by the CTM database.</p>
<p>The appendix of the paper has a section on ""The Impact of the Partition Strategy"". It says the following;</p>
<blockquote>
<p>BDM better approximates the universal measure <code>K(X)</code> as the number of elements resulting from applying the partition strategy to <code>X</code>.</p>
</blockquote>
<p>as the number of elements... what? Was this paper not copyedited!</p>
<blockquote>
<p><code>BDM(X|Y)</code> is a good approximation to <code>K(X|Y)</code> when the <code>Adj(X)</code> and <code>Adj(Y)</code> share a high number of base tensors.</p>
</blockquote>
<p>I assume that this doesn't actually rely on our data being tensors (or 2-dimensional arrays).</p>
<blockquote>
<p>We conjecture that there is no general strategy for finding a best partition strategy [...] Thus the partition strategy can be considered an hyperparameter that can be empirically optimized from the available data.</p>
</blockquote>
<p>Hmm... this seems like a big gap in the whole approach.</p>
<p>So this clears up some things about the partitioning strategy, at any rate. But I wasn't worried about the partitioning strategy anyway; I wanted to know what a ""pairing strategy"" is! The original paper on BDM isn't any help since it doesn't describe conditional BDM at all.</p>
<p>Going back to the topic paper of this post, it does describe a ""coarse conditional BDM of <code>X</code> with respect to the tensor <code>Y</code>"". Again, tensors are not explained at all in the paper, and it's unclear if <code>Y</code> actually needs to be a tensor in any mathematical sense. As I stated before, I think the authors just mean a 2-dimensional array when they say ""tensor"", and it seems obvious that the construction doesn't rely on dimensionality at all. It defines <code>BDM(X|Y)</code> as</p>
<pre><code>BDM(X|Y) = (Σ{(rx, nx) ∈ Adj(X) &amp;&amp; rx ∉ Adj(Y)} CTM(rx) + log(nx))
         + (Σ{(rx, nx) ∈ Adj(X) ∩ Adj(Y)} log(nx))
</code></pre>
<p>This definition isolates the unique information in <code>X</code> while issuing additional penalties if the information shared between <code>X</code> and <code>Y</code> appears more or less often in <code>X</code> than in <code>Y</code>. I'm not sure if this makes sense. The paper says;</p>
<blockquote>
<p>[the second sum] is important in cases where such multiplicity dominates the complexity of the objects</p>
</blockquote>
<p>but, intuitively, it seems to me like the sum should only add a penalty if <code>nx &gt; ny</code>; because, otherwise, we're penalizing the conditional complexity of <code>X</code> for content that's in <code>Y</code> but not in <code>X</code>. I'll have to think about this a bit more.</p>
<p>The ""coarse"" BDM is, I guess, less accurate than the ""strong"" BDM that I first looked at; but, at least, it makes sense. It's weaker since it doesn't utilize conditional CTM. But without additional clarification on what a ""pairing strategy"" is, I just can't understand how the strong version works.</p>
<p>I've thought a lot about it and, while I'm not confident, I think I've figured out the two most reasonable fixes.</p>
<ul>
<li>
<p>If the pairing strategies must cover <code>X</code> then that solves the specific problem I pointed out.</p>
</li>
<li>
<p>If P is supposed to be totally functional over the partitions of <code>X</code>.</p>
</li>
</ul>
<p>Neither of these conditions is hinted at in the paper, but it's the best I've got. The paper does say;</p>
<blockquote>
<p>prior knowledge of the algorithmic structure of the objects can be used to facilitate the computation by reducing the number of possible pairings to be explored</p>
</blockquote>
<p>So, at the very least, the pairing strategy is supposed to be determined by some algorithm that isn't described in any detail. I'm frustrated by this whole thing.</p>
<h2>Spit-balling ways to improve BDM and CTM</h2>
<p>One of the thoughts I had was that there may be program patterns in the computational model that always uniformly evaluate. For example, it may always be the case that <code>100101X10101Y0</code>, for any strings <code>X</code> and <code>Y</code>, evaluate to <code>1100101</code>, or maybe <code>0Y1010</code>. In either case, we can make the replacement to get a smaller program without running the program. Or maybe it reduces to <code>01Y0Y01</code>. Depending on the length of <code>Y</code> this may or may not be a reduction in size. And there may be variations on this involving variable substrings being riffled and combined in nontrivial ways.</p>
<p>The CTM database only keeps track of string reductions, but it may be possible to search for patterns and perform block replacement via unification/pattern matching instead. This description was given in terms of first-order unification, but there may be a close link with higher-order unification; unification modulo computation.</p>
<p>This also seems similar to finding smaller extensionally equivalent expressions to certain programs. That is, finding programs that don't normalize to the same term but do normalize to the same term when given the same inputs. Programs that are not the same but are behaviourally indistinguishable. I wrote a program a while ago in Mathematica which enumerated SKI expressions, applied 30 or so variable arguments to them, and collected them into pairs which normalized to the same expression after application. In this way, I built up a database which I used to make expressions smaller by replacing expressions with smaller extensionally equivalent ones. In practice, this ran into some issues. Namely, two extensionally equivalent expressions are not guaranteed to have the same behavior since one may start evaluating after two arguments while the other will evaluate after three. For example, the following two expressions are extensionally equivalent, but they only normalize to the same term after two arguments are applied despite the first fully normalizing after only one application.</p>
<pre><code>λ f . f
λ f . λ x . f x
</code></pre>
<p>This only ceases to be a problem if you have some sort of typing discipline which can allow you to infer the number of arguments an expression expects. You can then assess extensional equivalence up to that number of arguments while also guaranteeing the preservation of expected behavior up to the type of the full expression you're compressing. This, of course, doesn't work on extensional equivalences which require some elimination principle to justify; e.g. the equivalence of merge sort with quicksort. This may be particularly relevant to incremental compression.</p>
<hr>
<p>Generating programs with specific, even simple, types is highly nontrivial. It's just the proof synthesis problem where you want to enumerate all proofs (encoding programs) of a given theorem (encoding a type). Restricting to certain typing disciplines, such as simple types without intersection types, certain polymorphic typing disciplines, some refinement type disciplines, some disciplines heavily leaning on algebraic data types, and some others, can be searched fairly efficiently, however. The following papers seem particularly relevant;</p>
<ul>
<li><a href=""https://dl.acm.org/doi/pdf/10.1145/2813885.2738007"">Type-and-Example-Directed Program Synthesis</a></li>
<li><a href=""http://www.jfrankle.com/refinements-popl-16.pdf"">Example-Directed Synthesis: A Type-Theoretic Interpretation</a></li>
<li><a href=""https://dl.acm.org/doi/pdf/10.1145/2980983.2908093"">Program Synthesis from Polymorphic Refinement Types</a></li>
<li><a href=""https://dspace.library.uu.nl/bitstream/handle/1874/383386/thesis.pdf?sequence=2"">Generating Constrained Test Data using Datatype Generic Programming</a></li>
</ul>
<p>This may be leverageable for some applications. In fact, I'd guess most applications could leverage this.</p>
<p>It's also worth noting that this whole problem is just the inductive programming problem + a desire to minimize the induced program's size. Inductive programming is a whole field unto itself. There are algorithms which can exhaustively produce programs which have a particular output sequence;</p>
<p>See;</p>
<ul>
<li><a href=""https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C3&amp;q=exhaustive+program+generation+inductive+programming&amp;btnG="">Google Scholar: ""exhaustive program generation inductive programming""</a></li>
</ul>
<p>These approaches can be used as compression methods. They're not guaranteed to approach the Kolmogorov complexity, but they should generally do a much, much better job than statistical compression methods while being much more efficient than methods attempting to approximate <code>K</code> directly.</p>
<hr>
<p>Consider the possibility of an algorithm, <code>C(X)</code>, and a Shannon optimal compressor, <code>S(X)</code>, such that the sum over all <code>X</code> of <code>C(X) - S(X)</code> is negatively infinite. That is to say, <code>C(X)</code> tends to perform better infinitely much as a Shannon optimal compressor while being no slower. BDM already does this, but is there an algorithm that can do this without keeping track of a large database which takes exponential effort beforehand to calculate?</p>
<p>It's actually fairly easy to outperform Shannon optimal compressors by infinitely much. Have a compressor do the following;
If it detects a string encoding <code>0, 1, 2, 3, 4, 5, ...,</code> replace it with a flag indicating such and a number indicating how far the sequence counts.
For all other strings, replace them with the output of a Shannon optimal compressor.
Such a scheme would generally perform no worse than a Shannon optimal compressor while performing better by infinitely much; though the benefits clearly only apply to a small number of patterns overall, even if there are infinitely many such patterns. This means that CTM will generally be able to improve by infinitely much by adding entries to its database, but expanding this database takes exponential effort. Is there a way to do better? Is there a way to characterize how far this can go without exponential effort? Even if it doesn't cover as much of program space asymptotically, is there a way to grow the database forever using only, say, linear effort? Or quadratic? Or logarithmic? And could such things be efficiently sewed together so that we discover the easy things first and foremost, for even very large strings, and the hard things later?</p>
<pre><code>1, 2, 3, ... 999999999, 1000000000
</code></pre>
<p>Is easy to compress algorithmically, but I wouldn't expect BDM to do much better than statistical compression. I do believe that such things should be efficiently discoverable anyway, but not by CTM or BDM, as it stands.</p>
<hr>
<p>I think we may need to start thinking about the amount of effort we'd be willing to expend during compression. <code>K</code> and its alternatives seem somewhat backward in their definition. <code>K</code> is the minimal size of a program that can become our target if we're willing to expend an arbitrarily large effort <em>during decompression</em>. Levin complexity is just <code>K</code> plus the log of the runtime of the smallest program. Essentially, Levin complexity is the minimal size of a program that can become our target if we're willing to expend only an exponential amount of effort on decompression. But, shouldn't it be the other way around? Shouldn't we care more about the amount of effort we want to put <em>during compression</em>?</p>
<p>For the purposes of ML, we don't care about decompression at all. What would be better to know is how small a program can be if we're only willing to spend exponential, quadratic, linear, etc. effort with respect to the length of the string we're compressing. Are these problems solvable? I have a suspicion that feedforward NNs are essentially solving a subset of the linear effort case.</p>
<p>This is an area which has been explored quite a bit; by the paper on <a href=""https://arxiv.org/pdf/1911.04268.pdf"">Universal almost optimal compression</a> I mentioned earlier, for instance. I would like to explore this area in the future, and I believe it will be of tremendous importance for making compression practical for machine learning.</p>
<hr>
<p>Here's another idea. This one is original to me, but I wouldn't be surprised if someone came up with something similar. Some models of computation can be run backward, nondeterministically. Specifically, models where every state can be reached in one step by only a finite number of transitions. This <em>can't</em> be done effectively with the lambda calculus. If we were in a state <code>λ x . λ f . f x x</code>, that could have been reached in one step by</p>
<pre><code>λx . (λ y . λ f . f y y) x
(λx . x) (λ x . λ f . f x x)
λ x . λ f . (λx . x) (f x x)
(λ d . λ x . λ f . f x x) (λx . x)
(λ d . λ x . λ f . f x x) (λx . x x)
(λ d . λ x . λ f . f x x) (λx . x x x)
...
</code></pre>
<p>and infinitely many other things. This means that running a lambda expression backward implies enumerating infinite possibilities at each step. That doesn't mean running expressions backward is impossible, but it limits the utility of such an approach since we'd basically be enumerating every lambda expression an infinite number of times at each backward step. The same applies to combinator logic.</p>
<p>Many models of computation, however, don't have this property. Anything where a fixed amount of work is done at each step don't; that includes Turing machines, interaction nets, the linear lambda calculus, and most abstract machines. These can all be run backward, as a result. We can then enumerate all the programs which normalize to a particular output by doing the following, assuming we're using an appropriate Turing machine;</p>
<ul>
<li>Start with our output string.</li>
<li>Enumerate every end-state involving this machine. That is, every case where the head of the machine is at every position while in the halting state.
<ul>
<li>For each of these, generate an infinitely tall rose tree by recursively running the program backward for each time step. We can collapse these trees into a stream by doing a breadth-first-search and we can collapse these searches together by riffling the streams.</li>
<li>Every time we reach a point where the machine's head is at the beginning of the string in the starting state, we've logged a program which normalizes to our output.</li>
</ul>
</li>
</ul>
<p>This procedure will look for and find only those programs which normalize to our desired output, ordered by running time. We can keep this going for as long as we want, remembering only the smallest program found so far. The longer we go, the closer our approximation is to the actual shortest program and therefore the actual Kolmogorov complexity. There are also probably heuristics we could apply to prune the search of paths which won't get us anything smaller than what we already have. I don't know how efficient this could be made, but it seems to me that it would do better than BDM on large strings.</p>
<p>For parallel models of computation, such as interaction nets, we can optimize this further by treating the (backward) transformations of different segments of the program independently and only combine the timelines when they start interacting.</p>
<p>A further optimization, which would make finding small programs easier but would make finding values verifiably close to <code>K</code> harder, is to iteratively compress programs. We run a program backward only until it shrinks. We then abandon all other search branches and start over with the new, smaller program. Doing this over and over may allow one to effectively find and run recursive programs that generate an output backward much more efficiently than an unpruned search.</p>
<hr>
<p>Here's another idea. In CTM, we're enumerating all programs and seeing what they output. It may, instead, be better to enumerate programs and then filter them for randomness. Essentially, we'd build up a database by looking at each program in algorithmic order. We'd try building that program from the programs already in the database. If we can't build it using the existing elements in the database in a way that's smaller than what we're trying to build then the thing we're looking at is algorithmically random and we add it to the database as a random ""atom"". This should significantly cut down on the combinatorial explosion, though, I'm pretty sure it would still be exponential.</p>
<hr>
<p>Ultimately, I think this whole problem should eat itself. If we're using a universal learning algorithm, then, certainly, it can learn to be better, somehow. Bootstrapping should be possible in this domain.</p>
<h2>Algorithmic Loss</h2>
<p>Hey, wasn't this post supposed to be about machine learning? Oh ya! Let's finally talk about that!</p>
<p>For any standard ML technique, we need a definition of Loss which we want to minimize. The exact way this will be defined will depend on what our task is. Generally, we'll use some form of conditional Kolmogorov complexity directly as our loss. Similar measures are already used in some applications. Cross-entropy is the most directly related loss, and using algorithmic complexity can be thought of as a more robust form of entropy.</p>
<p>Generally, our loss will try to capture how much of the data <em>isn't</em> captured by our model. We want our loss to answer the question, ""given our model, how much of the data still needs to be explained?"" To that end, our loss will generally be a function of the training data <em>conditioned on our predictions</em>. Given a data point <code>y</code> and a prediction <code>Y</code>, our loss on that point will be <code>K(y|Y)</code>. To get a total loss we can just add all these measures together.</p>
<p>But the paper suggests adding the <em>squared</em> losses together. Why should we do this? Why do we do this for normal ML? Well, we can't normally just add together the losses since they can often be negative. <code>K</code> can never give negative values, so that's not a problem here. Why would we use the mean squared error rather than the mean absolute error? There are two explanations I've seen. Firstly, I've seen some say MSE is easier to differentiate than MAE. This isn't true outside of very simple models where you want to find the optimum in a single step, such as linear regression, and we aren't going to be differentiating <code>K</code> anyway, so this doesn't matter. The other reason comes from an assumption that we're modeling things sampled from a gaussian distribution.</p>
<p>This has always irked me. No matter how many statisticians say ""it's reasonable to assume everything is being sampled from a gaussian"", that doesn't make it true. If you do any bayesian ML, you'll find that a significant effort spent on standard techniques is in distribution engineering. If what you're modeling can't be negative, is multi-modal, follows a power-law, or a litany of other things then you're not looking at data sampled from a gaussian and you'll have to make a different distribution instead.</p>
<p>Anyway, let's do the derivation;</p>
<p>Firstly, what we always really want to do is maximize the likelihood. Our model is going to make predictions about the probability of various data. The likelihood of our model is just the product of all the probabilities of each training point as predicted by our model.</p>
<pre><code>L(m) = Π(i) m_prob(yi)
</code></pre>
<p>In practice, this will usually end up multiplying a bunch of numbers below 1 together, getting a vanishingly small likelihood for models training on a lot of data. Because of this, we usually use the negative log-likelihood, which is</p>
<pre><code>NLL(m) = - Σ(i) log(m_prob(yi))
</code></pre>
<p>This makes all our too-small numbers large without losing any information, so this is usually what real algorithms try to minimize. On a historical note, this trick was often used to make carrying out tough calculations easier. Logorithm tables were a hot commodity back before calculators became commonplace. Anyway, we often also divide by the total number of data points to turn this log-likelihood into a mean log-likelihood, that way our loss doesn't become huge just because we're working with a lot of data points.</p>
<p>The equation of a gaussian is;</p>
<pre><code>e^(- (x-μ)² / 2 σ²) / σ √(2 π)
</code></pre>
<p>The ""prediction"" made by a Gaussian model will be the mean, <code>μ</code>, and the likelihood of a particular piece of data <code>x</code> will be that data fed into the PDF of a gaussian with the predicted mean. Substituting with those changes into our negative log-likelihood, this becomes</p>
<pre><code>- Σ(i) log(e ^ - (yi - y_pred)² / 2 σ²)  / σ √(2 π)
  = (1 / σ³ √(8 π)) Σ(i) (yi - yi_pred)²
</code></pre>
<p>which is exactly the squared error, modulo some constant we don't care about. And getting the average by dividing by the number of data points will get us the mean squared error, MSE. This should also illustrate that if you don't think it's reasonable to assume your data were randomly sampled from a gaussian distribution, then you should also not think it's reasonable to use the squared error without a similar derivation.</p>
<p>Okay, so, what's the justification for squaring <code>K</code>? Let's think about this, what are the probabilities in our likelihood? Well, they'll be the algorithmic probabilities; the probability that a random program will output the datapoint when given our model's prediction as an input. The coding theorem says exactly that (within an additive constant) the Kolmogorov complexity of a program is the negative logarithm of the algorithmic probability, meaning the appropriate negative log-likelihood is exactly the sum of <code>K</code>s.</p>
<p>But, wait, I was looking for justification for squaring <code>K</code>. That's what the paper does. Does it say why?</p>
<blockquote>
<p>[...] in order to remain congruent with the most widely used cost functions, we will, for the purpose of illustration, use the sum of the squared algorithmic differences.</p>
</blockquote>
<p>Oh, so there is no reason. To be clear, there is a clearly right thing to do here; use the sum of <code>K</code>s, not squared <code>K</code>s. We may also want to divide by our number of data points to get the mean <code>K</code> error rather than just the total error. I don't think the author's thought very hard about what the loss should be. For much of the paper this odd, clearly wrong loss function will be used.</p>
<p>The paper goes on to talk about categorical loss. The obvious thing, to me, is to do basically the same thing, and that's what the paper recommends. Assume our model is outputting some object which is being used to predict the class. In classical ML, this would be like the class probabilities before assigning a class. The loss will be <code>K(Y|M(X))</code>, where <code>Y</code> is the actual class and <code>X</code> is our input data. This signifies how much information is in the real class but not in the prediction of our model. If we were using our model for prediction, then the class <code>C</code> which minimizes <code>K(C|M(X))</code> would be our prediction.</p>
<p>The paper relates this to clustering by algorithmic distance. I don't see the connection, but it does reference another generic version of a standard ML technique; specifically, it points out the paper <a href=""https://homepages.cwi.nl/~paulv/papers/cluster.pdf"">Clustering by Compression</a>.</p>
<h2>Algorithmic Optimization</h2>
<p>Great! So, we know how to assess a model, how do we actually do optimization? Prepare to be disappointed, because the answer offered in the paper is ""brute search""! Well, the paper phrases it in a more, umm, appealing way. The optimization algorithm is defined as follows;</p>
<ul>
<li>Keep track of the most recent minimal cost in a variable called <code>minCost</code> which is initially set to infinity.</li>
<li>Keep track of the best set of parameters in a variable called <code>param</code>, which is, I guess, initially set to <code>null</code>, or something.</li>
<li>Create a stream of all possible parameters of our model in algorithmic order (that is, ordered by their <code>K</code>).</li>
<li>For each set of parameters, calculate the loss of the model with those parameters. If the loss is less than the current <code>minCost</code>, then set <code>minCost</code> to this value and set <code>param</code> to the parameters being used.</li>
<li>Keep going until you're bored or satisfied or no reason at all; the paper doesn't care.</li>
</ul>
<p>That's it. However, there are some complications that make me not entirely sure if this is right. It defines the cost function (the sum of squared <code>K</code>s) to be <code>J_a(ˆX, M)</code>, where <code>ˆX</code> is our dataset, and <code>M</code> is the model we're assessing. However, the actual description of the algorithm tells us to use <code>J_a(ˆM, σ_i)</code>, where <code>σ_i</code> is the ith parameter in our parameter stream and <code>ˆM</code> is never defined. Presumedly, <code>ˆM</code> has something to do with our model, but it can't possibly be a replacement for <code>ˆX</code> since <code>ˆX</code> is just a collection of input-output pairs and our model is, obviously, not. Either there's an entirely separate loss function over models and parameters which is never defined, or there was a typo and the algorithm should have said <code>J_a(ˆX, M{σ_i})</code>, or something like that. The version I wrote seems pretty intuitive (if overly simplistic), so I'm leaning toward the latter.</p>
<p>The paper states that this algorithm ""can ['minimize <code>K(M)</code> and minimize the cost function'] in an efficient amount of time"". But, uh, no it doesn't. It does as bad as a brute force search because it is a brute force search. It goes on to say</p>
<blockquote>
<p>the algorithmic parameter optimization always finds the lowest algorithmically complex parameters that fit the data <code>ˆX</code> within the halting condition [...] algorithmic parameter optimization will naturally be a poor performer when inferring models of high algorithmic complexity.</p>
</blockquote>
<p>Ya think? I'm not confident that this would work on anything but toy problems, but, who knows, maybe I'm wrong and this actually works surprisingly well on real-world data, but I doubt it. The algorithm doesn't even try taking advantage of what it knows about good parameters.</p>
<p>As an aside, the paper mentions that</p>
<blockquote>
<p>In the context of artificial evolution and genetic algorithms, it has been previously shown that, by using an algorithmic probability distribution, the exponential random search can be sped up to quadratic</p>
</blockquote>
<p>giving a few citations. This seems more reasonable to me, as such methods aren't just brute-force searching.</p>
<p>This will be a bit of a digression, but if you read this far you probably don't care about that. The first example it uses is a regression problem on two variables. It says the following on the ability to enumerate the parameter space;</p>
<blockquote>
<p>For instance, in order to fit the output of the function <code>f</code> (Eq. 2) by means of the model <code>M</code>, we must optimize over two continuous parameters <code>s1</code> and <code>s2</code>. Therefore the space of parameters is composed of the pairs of real numbers <code>σ_i = [σ_i1, σ_i2]</code>. However, a computer cannot fully represent a real number, using instead an approximation by means of a fixed number of bits. Since this second space is finite, so is the parameter space and the search space which is composed of pairs of binary strings of finite size [...]</p>
</blockquote>
<p>This entire paragraph is rather head-scratching. Computers certainly can fully represent a real number. We can figure out how by following the same basic procedure I used before to figure out how to encode binary strings. You just state a universal property of the mathematical object you want to represent and derive a type of realizers. This is a bit squirrely with the real numbers as the exact universal properties diverge in constructive settings. Dedekind Reals and Cauchy Reals aren't isomorphic anymore, for instance. There are also practical questions about how to make calculating with them as easy as possible. That being said, the simplest universal property for any kind of real number I'm aware of is the following; the (nonnegative) real numbers are the final coalgebra of the endofunctor <code>X ↦ ℕ × X</code>. There are a few places that say this in various guises. The most direct is <a href=""http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=9A564F2172717230E15D3F8EC5253423?doi=10.1.1.47.5204&amp;rep=rep1&amp;type=pdf"">On coalgebra of real numbers</a> which is all about this observation. See <a href=""https://ncatlab.org/nlab/show/continued+fraction#half-open"">this</a> as well.  This basically says that real numbers are an infinite stream of natural numbers. There are a few ways of viewing what this represents, and that will largely determine whether you see each number as representing a nonnegative real or something isomorphic, like something in <code>[0, 1)</code>. For the latter, we can read each natural number as describing how many 1s we encounter before encountering a <code>0</code> in the binary expansion of a number. For example;</p>
<pre><code>0 = [0, 0, 0, ...]
0.1 = [0, 0, 2, 0, 2, 0, 2, 0, 2, ...]
0.2 = [0, 2, 0, 2, 0, 2, 0, 2, 0, ...]
1/3 = [1, 1, 1, 1, 1, 1, 1, ...]
√2 - 1 = [2, 1, 1, 0, 0, 0, 0, 1, 0, 4, ...]
π - 3 = [0, 1, 0, 1, 0, 0, 0, 6, 2, 1, 1, ... ]
</code></pre>
<p>Of course, we can map this back on to the non-negative reals by interpreting each number <code>x</code> as <code>1/(1-x) - 1</code> instead. Then we'd have</p>
<pre><code>0 = [0, 0, 0, ...]
0.1 = [0, 0, 0, 1, 3, 1, 0, 0, 1, 3, 1, 0,...]
0.2 = [0, 1, 1, 1, 1, 1, 1, 1 ...]
√2 = [1, 0, 1, 1, 5, 2, 0, 0, ...]
3 = [2, 0, 0, 0, 0, 0 ...]
π = [2, 0, 0, 0, 1, 0, 0, 2, 0, 0 ... ]
e = [1, 3, 2, 0, 1, 0, 2, 1, ... ]
</code></pre>
<p>Alternatively, we can imagine each entry in a sequence <code>[a0, a1, a2, ...]</code> as representing a number as a simple continued fraction of the form</p>
<pre><code>a0 - 1/ (a1 - 1/(a2 - 1/ ...))
</code></pre>
<p>This can represent any nonnegative real number.</p>
<pre><code>0 = [0, 0, 0, ...]
1 = [1, 0, 0, 0, ...]
0.1 = [0, 10, 0, 0, 0, ...]
0.2 = [0, 5, 0, 0, 0, ...]
1/3 = [0, 3, 0, 0, 0, ...]
√2 = [1, 2, 2, 2, 2, 2, ...]
π = [3, 7, 15, 1, 292, 1, 1, ... ]
</code></pre>
<p>whichever interpretation we use will determine how we define, for instance, addition, multiplication, etc. Note that there are some complications with representing real numbers as continued fractions in this way. Notably, that some numbers don't have unique representations. While I understand these caveats, I don't understand how to solve them, though I've been told that such solutions are ""various"".</p>
<p>Following <a href=""https://homepages.inf.ed.ac.uk/wadler/papers/free-rectypes/free-rectypes.txt"">Recursive types for free!</a>, the (weakly) final coalgebra of <code>X ↦ ℕ × X</code> can simply be defined as</p>
<pre><code>∃ X . (X → ℕ × X) × X
</code></pre>
<p>We can construct a real number by fixing a type, <code>X</code>, giving an <code>X</code> as a seed, and then defining a method of generating new digits and new seeds from an old seed. For example, we can construct an infinite stream of zeros by setting <code>X</code> to be <code>⊤</code>, the unit type, giving <code>•</code>, the only inhabitant of <code>⊤</code>, as our seed, and defining our generator as <code>λ x . (0, •)</code>. In full, we'd have</p>
<pre><code>0 : ℝ := λ x . (0, •), •
</code></pre>
<p>or, if we want to be explicit about all our encodings;</p>
<pre><code>0 := λ p . p (λ x . λ p . p (λ z . λ s . z) (λ x . x)) (λ x . x)
</code></pre>
<p>This means that the real number zero has, at most, about 32.511 bits of complexity; surprisingly small for something which is supposedly infinitely large.</p>
<p>The usual reason we'd want to use floating-point numbers over exact real numbers is efficiency; floating-point numbers are much faster to compute with since our computers have hardware dedicated to computing with them. But this approach is representing the parameters as raw outputs of some virtual computer anyway, so that doesn't apply here. To use floating points, we'd have to convert them to some encoding of floats in our computational model. We get no efficiency in using floats here.</p>
<p>We can make our representation a bit more efficient. Following the <a href=""http://www.dcs.ed.ac.uk/home/pgh/coit.html"">Coinductive function spaces page</a>, we can use a few isomorphisms to change this representation. Notably, it's generally the case that</p>
<pre><code>∃ X . (X → A × X) × X ≅ (∀ X . (1 + X → X) → X) → A
</code></pre>
<p>for any <code>A</code>. Since <code>ℕ</code> is the initial algebra over the endofunctor <code>X ↦ 1 + X</code>, the above can be rewritten as;</p>
<pre><code>∃ X . (X → A × X) × X ≅ ℕ → A
</code></pre>
<p>So we can redefine the nonnegative reals as just functions from <code>ℕ → ℕ</code>. Neat! Following this, we can define zero instead as;</p>
<pre><code>0 := λ n . λ z . λ s . z
</code></pre>
<p>as the constant function which just returns <code>0</code> for any input. This has only about 6.9 bits! Not bad for representing infinite many digits. It would actually be MORE complex if we truncated it to finitely many digits. We may even notice that the least complex real number will simply be encoded by the identity function. This will be the number who's continued fraction is <code>[0, 1, 2, 3, 4, ...]</code>. As it turns out, this number is</p>
<pre><code>I₁(2)/I₀(2) ≈ 0.697775
</code></pre>
<p>where the <code>I</code>s are <a href=""https://en.wikipedia.org/wiki/Bessel_function#Modified_Bessel_functions:_I%CE%B1,_K%CE%B1"">Bessel I</a> functions. This is called the ""Continued Fraction Constant"". Or, if we were interpreting the number as representing the binary expansion, this would be</p>
<pre><code>2 - ϑ₂(1 / √2) / 2 ^ (7/8) ≈ 0.358367
</code></pre>
<p>Where ϑ is an <a href=""https://mathworld.wolfram.com/JacobiThetaFunctions.html"">elliptic theta</a> function. I don't know if this constant has a name, but I couldn't find it anywhere. When using our previous map to turn this into the full positive reals, this becomes <code>≈ 0.558524</code>.</p>
<p>Anyway, my whole point with this exercise was to show we can represent real numbers, and many other mathematical structures besides, just fine on computers. We don't, and, in fact, shouldn't use floating-point numbers if we're going to take algorithmic complexity seriously. The original BDM paper mentions π a few times, saying, for instance,</p>
<blockquote>
<p>the digits of π have been shown to be [...] only algorithmic in the way they are produced from any of the many known generating formulas</p>
</blockquote>
<p>so the authors know that π (and presumably other real numbers), in all of its infinite digits, can be represented by an algorithm. But in this paper, they insist on using floating-point numbers. Why? The paper just says that the parameter space becomes enumerable, but we can effectively enumerate the (constructive) reals by enumerating all the inhabitants of the type</p>
<pre><code>ℝ := (∀ X . X → (X → X) → X) → (∀ Y . Y → (Y → Y) → Y)
</code></pre>
<p>the output of such a procedure, if it were done in some breadth-first manner, would output encodings for real numbers in essentially algorithmic order.</p>
<p>I think the authors need a crash course in <a href=""https://www.springer.com/gp/book/9783662479919"">higher-type computability</a>. There's a whole wide world that you're missing out on if you really believe computers can only represent discrete data types.</p>
<h2>Final Thoughts</h2>
<p>The rest of the paper just goes through some usage examples. I don't feel the need to summarize them, but you may find them interesting to look at. The cellular automata classifier was a particularly good illustration. In the ""hybrid machine learning"" section, an interesting suggestion is to use <code>K</code> as a regularization method on top of another loss function. The same section also suggests giving training weights to simpler samples from a given space so that a model prioritizes the most plausible samples. I don't know how effective these would be, but they're interesting suggestions which may be useful as additional tools in the ML toolbox.</p>
<p>The conclusion section refers to the whole framework as a ""symbolic inference engine"" being integrated into traditional ML. I... wouldn't phrase it that way. There's not much inference and even less symbology. That being said, a type-sensitive version of these ideas might be better.</p>
<p>I'm not satisfied with the methods presented in this paper. Nothing in it connected back to that idea of ""whichever addition increases the compressed size the least should be considered the likeliest prediction"" I mentioned at the beginning of this post. All the paper said, really, was ""when tuning parameters, just look through them in order from least to most complex. Also, use Kolmogorov complexity, not Shannon entropy, to measure complexity."" I'll keep that in mind, but I think I'd want to look at other methods. I think something more carefully designed will need to be made for practical ML.</p>
<p>I took a closer look at those evolutionary methods mentioned. Specifically, I read;</p>
<ul>
<li><a href=""https://www.amazon.com/Proving-Darwin-Making-Biology-Mathematical/dp/1400077982"">Proving Darwin: Making Biology Mathematical</a></li>
<li><a href=""https://arxiv.org/abs/1709.00268"">Algorithmically probable mutations reproduce aspects of evolution such as convergence rate, genetic memory, and modularity</a></li>
</ul>
<p>I cannot recommend that first source. It's a very fluffy soft pop-sci book. It does contain the basic idea of algorithmic evolution, though. Chaitin calls it ""metabiology"", but I don't like that term, so I'll call it ""algorithmic evolution"". It's basically the following;</p>
<blockquote>
<p>When mutating a program, don't handcraft a mutation mechanism. Instead, generate a program randomly, and run that program on your genetics to mutate.</p>
</blockquote>
<p>That's literally it. The idea, I suppose, is to cut down on algorithmically unlikely mutations; biasing the search in a way which is helpful to tasks which are not, themselves, essentially random. That, of course, would be just about any task in practice. According to Chaitin, that, alone, is enough to speed up evolutionary convergence to a mere quadratic search. The book offers no reasoning for this, it just asserts it, and I don't know where I'd need to go to find such justification. Chaitin tries offering intuition for it, but it's bonkers nonsense. He points out that an oracle would be needed to avoid non-terminating programs. He then states that each usage of this oracle would add one bit of ""creativity"" to our search, and that's where the power comes from. Utter drivel.</p>
<p>If this idea is valid, then some kind of type-driven mutator synthesis could be done to speed up the process. We'd essentially specify the type of our genome as, likely, an AST describing some program of a fixed type. The mutators would be endofunctions over this type of ASTs. A type-driven synthesizer would be able to fully search mutation space while avoiding either nonsensical or non-terminating mutators. <a href=""https://dspace.library.uu.nl/bitstream/handle/1874/383386/thesis.pdf?sequence=2"">Generating Constrained Test Data using Datatype Generic Programming</a> would be particularly relevant.</p>
<p>The paper implements a strange version of this idea, where a scan is made over a data structure and a CTM database is used to estimate the algorithmic probability of any given mutation. Mutations are then sampled according to these probabilities and then applied. Here's the thing; we don't need to know the algorithmic probability of an outcome to sample them according to that probability. We can just randomly generate programs and the sampling will automatically be done according to the algorithmic probability. It's not clear why they didn't just do that, but, whatever. The paper does demonstrate a clear speedup visible by simply looking at the graph results on the tests they did; so that's something.</p>
<p>I have a strong suspicion that some ideas in <a href=""https://cs.stanford.edu/people/eschkufz/docs/asplos_13.pdf"">stochastic superoptimization</a> could also be combined with algorithmic evolution. In particular, the methods used to justify how densely explored some space was.</p>
<p>I also suspect that some variation of a bandit algorithm may benefit from these ideas. The paper <a href=""https://arxiv.org/abs/1707.01550"">Information-gain computation</a> and its <a href=""https://scholar.google.com/scholar?cites=4074844082910455437&amp;as_sdt=805&amp;sciodt=0,3&amp;hl=en&amp;scioq=Information-gain+computation+in+the+Fifth+system"">followups</a> described a hypothetical system which uses a contextual bandit for exploring a logic-program like search space. The expert that the bandit uses simply calculates the Kullback–Leibler divergence, a statistical entropy metric, of the history for each branch to give recommendations. The idea is that desirable histories should maximize the information gained about our goal as we go down it. A better system might use a <code>K</code> approximation rather than an entropy measure. Though, the paper also suggests using an RNN to do the history compression, which I'd expect to give a lower value than the entropy since it would be able to exploit the temporal structure of the history for compression in a way that statistical compression wouldn't be able to do.</p>
<p>...</p>
<p>THE END</p>
",AHartNtkn,ahartntkn,AHartNtkn,
FaWWHkjv62Cr6WNkj,Livestreaming monetisation models aka What do camgirls have in common with Twitch streamers?,livestreaming-monetisation-models-aka-what-do-camgirls-have,https://www.lesswrong.com/posts/FaWWHkjv62Cr6WNkj/livestreaming-monetisation-models-aka-what-do-camgirls-have,2020-09-19T18:57:08.985Z,11,7,0,False,False,https://lillianli.substack.com/p/livestreaming-monetisation-models,"<p>Repost from my personal blog / newsletter - feel free to <a href=""https://lillianli.substack.com/subscribe?"">subscribe</a> if you like longform Chinese tech analysis. </p><p>In many ways, the internet was made for livestreaming. Starting with<a href=""https://www.bbc.com/news/magazine-37681006""> <u>Jennicam</u></a> back in 1996, it has taken the medium a while to get to the fabled plateau of productivity.</p><span><figure><img src=""https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b4af92f-bbc1-4203-82b5-f01392a801aa_756x551.png"" class=""draft-image "" style=""width:99%""></figure></span><p>Livestreaming in its genesis of Twitch, Meerkat, Periscope and Facebook Live never fleshed out a compatible monetisation model. As a result, it struggled with <u><a href=""https://medium.com/point-nine-news/wtf-is-marketplace-liquidity-f2caca3802c0"">marketplace supply-side liquidity</a></u> since livestreaming hosts weren&apos;t incentivised to stream consistently. Viewers then stopped watching as there was no guarantee of good content on the platform. Livestreaming only became a medium after the emergence of effective monetisation, as then creators had the incentive to keep to a regular broadcasting schedule for followers.</p><p>Bolstered by Covid-19, livestreaming is now ubiquitous in China and increasingly mainstream in the west. The Chinese market is estimated to be worth<a href=""https://technode.com/2020/06/12/livestreaming-in-china-only-for-sales-or-is-there-brand-value/""> <u>RMB 433.8 billion</u></a> ($64.2bn USD) in 2019 and is expected to double by the end of 2020. Twitch currently has<a href=""https://www.wired.com/story/streaming-video-game-star-management-industry/""> <u>6.2 million monthly broadcasters,</u></a> and games livestreaming as an industry is estimated to be worth $50bn. Livestreaming has created massive stars; on Twitch, Ninja the Fortnite streamer has ~7m fans and even higher earnings. In China, Li Jiaqi the &apos;Lipstick queen&apos;<a href=""https://news.cgtn.com/news/2019-11-11/What-lipstick-tells-us-about-the-Chinese-economy-LxzeHLo3mg/index.html""> <u>sold 15,000</u></a> tubs of lipstick in 15 minutes on Taobao. Viya, who is essentially Oprah incarnated as a livestreamer, can<a href=""https://www.businessoffashion.com/articles/intelligence/china-livestreaming-wechat-taobao-bilibili-weibo-li-jiaqi""> <u>command 37m viewers</u></a> (bigger than the audience for the Oscars or Game of Thrones finale) during a stream. These big numbers are representative of the digitalisation of distribution and entertainment that&apos;s been quietly occurring throughout the world. The fact that livestreaming has become their de-facto medium owes much to the unlocking of effective monetisation strategies.</p><p>Livestreaming as a medium is a conflation of a product as well as as a distribution channel. It exists on a spectrum of being pure entertainment on one-side and a new go-to-market strategies on the other, with different kinds of monetisation models for each side. While western startups have centred around the &apos;livestreaming as product&apos; theme, China, with its enabling infrastructure in payment and fulfilment, have been quick to adopt livestreaming as a new distribution channel. This June&apos;s 618 Shopping Festival, saw Alibaba and<a href=""http://jd.com/""> <u>JD.com</u></a> report a combined total of <u><a href=""https://www.cnbc.com/2020/06/19/alibaba-jdcom-handle-record-sales-during-618-event.html"">$136.5 billion</a></u> of livestreaming sales. Kuaishou surpassed<a href=""https://www.prnewswire.com/news-releases/kuaishous-livestream-daily-active-users-surpass-170-million-301098837.html""> <u>170 million daily active</u></a> livestreaming shoppers in June and Pinduoduo also wants its<a href=""https://kr-asia.com/pinduoduo-wants-536-3-million-users-to-live-stream-to-boost-platform-sales""> <u>~500m users to start streaming as well</u></a>. No longer just for small ticket items -<a href=""https://www.notion.so/The-monetisation-models-of-live-streaming-aka-Similarities-between-twitch-streamers-and-Camgirls-a0a8f4492bff43358cab599cf107363f#0b594b6a319442f5b0524a4fcaafbd39""> <u>houses</u></a>, cars, phones have all been sold during the lockdown, livestreaming is climbing up the value chain.</p><span><figure><img src=""https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10e4cef8-5bdc-486c-a8e0-646e40000607_1046x338.png"" class=""draft-image "" style=""width:100%""></figure></span><p>Of course, it&apos;s not a binary split for companies;<a href=""https://lillianli.substack.com/p/the-product-philosophy-of-kuaishou""> <u>Kuaishou</u></a>, <u><a href=""https://lillianli.substack.com/p/an-introduction-to-bilibili-"">Bilibili</a></u> and TikTok has attributes of both categories.</p><span><figure><img src=""https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fda01cc1b-4400-4540-940f-3cde7ad3df78_982x442.png"" class=""draft-image "" style=""width:100%""></figure></span><p><strong>&#x2018;Livestreaming as a product&#x2019; monetisation strategies</strong></p><p>What do camgirls have in common with Twitch streamers?</p><span><figure><img src=""https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3eafbeea-d4ae-489d-a006-374fada47e6a_1049x513.png"" class=""draft-image "" style=""width:100%""></figure></span><p>A lot. They (and almost every other &#x2018;product&#x2019; livestreamer / creator ) use the same techniques to monetise their fanbase. This is because &#x2018;livestreaming as a product&#x2019; is ultimately reliant on monetising the trust between followers and creators, the higher that trust, the higher the payout.<a href=""https://lillianli.substack.com/p/the-product-philosophy-of-kuaishou""> <u>Kuaishou</u></a> earns a significant amount of its $7.6bn revenue from livestreaming rather than advertising, since it&#x2019;s geared towards building trust and engagement between creators and followers. It shouldn&#x2019;t be a surprise to learn that the inverse is true for its nemesis TikTok (read more from <u><a href=""https://lillianli.substack.com/p/the-product-philosophy-of-kuaishou"">last week&#x2019;s product design</a></u> piece to learn why).</p><p>The three sources of revenue for &#x2018;product&#x2019; streamers are donations, subscriptions and advertising.&#xA0;The bulk of earnings comes from in-platform currency donations that users buy with money. Platforms have different take rates of these donations (Kuaishou has 50% while Twitch has ~<u><a href=""https://www.tubefilter.com/2018/10/10/twitch-streamers-earn-per-month-breakdown-disguisedtoast/"">1% take rate</a></u>) and the rest goes to the creator. The act of donating triggers a positive engagement mechanism between the creator and viewers, and amongst viewers themselves. By which I mean, creators can get instant feedback on their actions from their viewers, while the viewer also elevates themselves before other viewers by<a href=""https://www.tandfonline.com/doi/abs/10.1080/17544750.2019.1583260?journalCode=rcjc20""> <u>donating to the creator</u></a>. Creators harness this dynamic to garner more donations through competition and collaboration, such as having a callout for the top donors or setting a target donation level for the session (upon the achievement of which they&apos;ll complete some action pleasing to the viewer, such as a dance).</p><span><figure><img src=""https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5fecd952-287e-4fef-8956-1488e32c07f4_742x435.png"" class=""draft-image "" style=""width:97%""></figure></span><p>A subscription is an investment in the creators&apos; future actions while donations are rewards for what they are doing now. The stronger the fanbase, the higher the subscription income a streamer has. This model is mostly offered by western platforms, probably to compete with Patreon, I&apos;m curious to see whether this model would take off in China (there&apos;s only limited adoption so far). Given the lack of <u><a href=""https://lillianli.substack.com/p/why-are-there-no-massive-chinese"">SaaS adoption in China</a></u>, maybe not. (Sidenote: I&apos;m very curious about the churn + upsell profile of subscriptions aka micro SaaS if any streamer wants to share their data. I&apos;m at <a href=""lillianli@substack.com)."">lillianli@substack.com).</a></p><p>Advertising is prominent on western streaming platforms since their parent companies are advertisers - Google (Youtube), Facebook, and Amazon media (Twitch). Most <u><a href=""https://lillianli.substack.com/p/an-introduction-to-bilibili-"">Chinese platforms</a></u> that value community and trust tend to not focus on this revenue stream since adverts deters the viewing experience. We can see the revenue split for the two big Chinese esport livestreaming platforms for H1 2020 below, advertising makes &lt;8% of total revenue and most are placed in the professional esports tournament streams.</p><span><figure><img src=""https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd248f8e9-7e80-457e-af44-2f890f8ac6d8_1200x742.png"" class=""draft-image "" style=""width:96%""></figure></span><p>For further readings on this topic, I recommend Aella&apos;s classical 2018 article on<a href=""https://knowingless.com/2018/11/19/maximizing-your-slut-impact-an-overly-analytical-guide-to-camgirling/""> <u>camgirl techniques</u></a> as well as an academic review of<a href=""https://journals.sagepub.com/doi/pdf/10.1177/2056305119881694""> <u>Twitch livestreamer&apos;s strategies</u></a>. Sway&apos;s 2017 medium article goes into details on the pros and cons of<a href=""https://medium.com/@swaysnt/virtual-gifts-a-live-streaming-business-model-breakdown-a87c7500c3bc""> <u>each monetisation approach</u></a>.</p><p>&apos;<strong>Livestreaming as a distribution channel&apos; monetisation strategies</strong></p><p>This is what an e-commerce livestream looks like on Taobao:</p><span><figure><img src=""https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fba989f58-ab68-4034-9439-a48ecfd8e875_338x696.png"" class=""draft-image "" style=""width:40%""></figure></span><p>The livestream host is showing off the new skirt while reading comments from viewers about how the clothes will fit on different frames. Clicking on the basket sign takes me to a check-out while the stream is still on-going in a mini window.</p><span><figure><img src=""https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4f79ab8d-5c5f-4340-a0a5-581fcd822604_330x672.png"" class=""draft-image "" style=""width:43%""></figure></span><p>It&apos;s a digital age QVC but with the added FOMO that the other thousand viewers could buy out the stock before you get there. Driven by a fear of counterfeit goods and a demand for experiential retail, viewers love the instant gratification of seeing the product in action then immediately buying it. The backdrop to the buying frenzy is a holistic e-commerce ecosystem, complete with payment and fulfilment solutions, that allows three tap check-outs mid-livestreams and one-day deliveries. And crazy beautification tools.&#xA0;</p><p>Detour on beautification tools since they are<a href=""https://youtu.be/kcA1nebGzgg""> insane.</a>&#xA0;</p><p>The format of these shows vary, some are daytime talk shows while others are timed-limited selling sprees. The livestream host will always spend time introducing a product to the audience, responding to their live chat questions before counting down to the sales. A typical show lasts for 3-4 hours and livestream hosts will broadcast daily to engage and grow their followings.</p><p>Behind the scenes, there are two ways brands can engage with this distribution channel, which are also different go-to-market strategies (what I call the Livestreaming Direct-To-Consumer versus Livestreaming Aggregator approaches). The livestreaming DTC route is a vertically integrated process where the livestream hosts are full time employees or owners of the brand itself. The hosts will stream regularly as part of their job for the brand on different platforms. The livestreaming aggregator route is where brands work with a multi-channel network (or MCNs, who are influencer agencies) who will negotiate on behalf of their livestreaming clients. The agreements typically involve an upfront non-refundable fee (which varies depending on the fame of the livestream host), a discount rate and stock level for the product to be sold, a percentage take-rate of the topline sales by the host and a guaranteed minimum return threshold for the goods (one common pitfall is goods sold during livestreamings often have high return rates and associated return fees). These two strategies are akin to selling wares on your digital shop versus selling it on Groupon.</p><span><figure><img src=""https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fca77fe40-453a-4138-a72c-d359acf6c821_984x558.png"" class=""draft-image "" style=""width:100%""></figure></span><p>Livestreaming DTCs is well documented in Connie Chan&#x2019;s posts ( <u><a href=""https://a16z.com/2019/12/05/video-first-ecommerce/"">the video first of commerce</a></u>,<a href=""https://a16z.com/2019/08/06/ecommerce-as-videos-killer-app/""> <u>e-commerce as video killer app</u></a>.) The majority of the livestreaming hosts on Pinduoduo, Kuaishou and Taobao are DTC hosts. I would argue the crazy volume we&#x2019;ve been seeing recently are all from the rise of the livestreaming aggregators such as Viya and Li Jiaqi.</p><p>The common misconception about livestreaming aggregators is that it&apos;s reliant on the sales skills and brand of the livestream hosts. Instead, the business model is still based on the traditional retail premise of selling good quality products at low prices. The livestreaming host is the trusted curation channel for the buyer which is important in a low trust retail environment rife with counterfeits, but they are not a stand-in for the products themselves. The host functions as a demand volume aggregator which enables them to command lower price goods from brands (there are references to them being the human Pinduoduo, which is true). Brands want to engage with hosts since they could drive large volumes of sales quickly. The lower the price that the hosts can command, the more followers they will attract and the more bargaining power they can have with brands. Anyone familiar with <u><a href=""https://stratechery.com/aggregation-theory"">Ben Thompson&apos;s work</a></u> can see that the top e-commerce livestreamers are a form of aggregators - they have the relationship with the audience, they bear no shipping or fulfilment costs and brands have become commodified. As a result, livestreaming aggregators have a significant moat once they have built up their following.</p><p>Based on the livestreaming trends in China, here are my predictions for the future:</p><p><strong>Prediction 1</strong>: 1 to 1 face-off competition is coming</p><p>The most popular feature right now on Kuaishou is the face-off feature between two livestreamers while their fans support them via donations for supremacy.&#xA0;</p><p><em><u><a href=""https://medium.com/themeetgroup/streamer-tools-the-hidden-secret-to-live-streaming-success-ce0aac0430"">Jeremy Zorn</a></u>, SVP of Product at The Meet Group explained in an interview, &#x201C;PK or Battles are essentially real-time competitions between streamers, where the winner is decided by the viewers: whichever streamer earns more virtual currency from the audience during the battle is declared the winner. During the battle, streams are combined such that both audiences can see both streamers at the same time. Neither one is the &#x201C;host&#x201D;, and each one brings their own audience.&#x201D;</em></p><p>This trend of beefing for cash is well known in both the rap industry and also the camgirl community (see Aella&apos;s article above). Supporting your idol through spending money is<a href=""https://allabout-japan.com/en/article/7063/""> <u>well-trodden in the east</u></a>, but given its juicy potential for racking up more donations, I&apos;m sure it&apos;s not long before the US platforms also introduce this feature.</p><p><strong>Prediction 2:</strong><em>The players who own the logistics and fulfilment structures will be the winners for e-commerce livestreaming</em></p><p>As I mentioned above, the significant enablers for success in China have been the embedded checkout and fulfilment process. Viewers can buy without leaving the stream with 3 clicks (this is why it made sense for Walmart to bid for TikTok). The winners in the race for the west will exhibit similar characteristics.&#xA0;</p><p>Also sort of bonus prediction, Shopify will move into livestreaming.</p><p><strong>Prediction 3:</strong> <em>The popular influencers of Youtube and Instagram will be the big winners of e-commerce livestreaming</em></p><p><u><a href=""https://www.voguebusiness.com/consumers/live-streaming-china-shopping-kim-kardashian"">Kim Kardashian West</a></u>, helped by Viya, sold 150,000 units of her KKW perfumes on Tmall&#x2019;s livestream. Those 150,000 bottles sold out in 1 minute and attracted over 13m viewers.</p><p>As livestream hosts exhibit aggregation effects, existing influencers with a following will have the advantage on e-commerce streaming. After Kim K&apos;s brushes with the process, I have high confidence that we&apos;ll see movings from the Klan in that direction soon.</p><p>Prediction 4: <em>Regulation will come</em></p><p>There&apos;s a lot of shady dealing for livestreaming in both the US and China.<a href=""https://www.wired.com/story/streaming-video-game-star-management-industry/""> <u>Wired&apos;s recent article</u></a> documented the unscrupulous dealings of streamer agencies in the US. In China, inflated volume in e-commerce streaming has been an ongoing concern. <u><a href=""https://www.businessoffashion.com/articles/intelligence/china-livestreaming-wechat-taobao-bilibili-weibo-li-jiaqi"">Business of Fashion</a></u> notes:</p><p>&quot;According to multiple sources interviewed by BoF, gaming the system to inflate the apparent success of livestream sales is common practice. MCNs work with millions of livestreaming hosts (especially those outside of the top echelons of the industry) to take the upfront fees from brands in exchange, for example, for a five-minute slot in an eight-hour livestream. The MCNs and hosts (or their proxies) then use those fees to purchase as much product as they can within the livestream sale, to inflate the sales volume. Then they return the maximum contractually allowable amount and offload any remaining products on third-party e-commerce sites.&quot;</p><p>Regulation is coming, especially in China where liverstreamer is now recognised as an occupation, but whether it will stifle or improve the industry is to be seen.</p>",characteristics,characteristics,characteristics,
9dDkse94dpEXoR7g5,Get Social Sector Leaders to Use Evidence With This 1 Weird Trick,get-social-sector-leaders-to-use-evidence-with-this-1-weird,https://www.lesswrong.com/posts/9dDkse94dpEXoR7g5/get-social-sector-leaders-to-use-evidence-with-this-1-weird,2020-09-19T18:07:50.000Z,2,1,0,False,False,,"<p>Okay, it’s more like a dozen not-that-weird tricks, but the point is…</p><figure><img src=""https://cdn-images-1.medium.com/max/850/1*Wag5LMFelrwu6VQG9gujlA.png"" /></figure><p>For a while now, I’ve been sounding the alarm about the social sector’s <a href=""https://medium.com/@iandavidmoss/the-crisis-of-evidence-use-234d0c63e7c2"">crisis of evidence use</a>. To put it bluntly, the human race expends mind-boggling resources bringing studies, reports, and analyses into the world that are read by few and acted upon by no one. As I argued at the time, “we are either vastly overvaluing or vastly undervaluing the act of building knowledge.” My sincere hope has long been that it’s the latter — that the knowledge we produce really <em>does</em> hold tremendous value, and all we need to do is figure out what works to unlock it. Luckily there is a <a href=""https://www.whatworksnetwork.org.uk/about-wwn/"">whole ecosystem of organizations dedicated to answering “what works?” kinds of questions</a>, and four years ago that ecosystem produced a report called <a href=""https://www.alliance4usefulevidence.org/assets/Science-of-Using-Science-Final-Report-2016.pdf"">“The Science of Using Science”</a> that tackles this question head-on.</p><p>“The Science of Using Science” seeks to understand what interventions are effective in increasing the use of research evidence by leaders and practitioners at various levels of civil society. It encompasses two reviews in one: the first examined the literature on evidence-informed decision-making (EIDM) itself, and the second prowled the broader social science literature for interventions that could be relevant to EIDM but haven’t specifically been tested in that context.</p><p>Based on previous theoretical work by themselves and others, the authors devised a conceptual framework encompassing six mechanisms under which to group the interventions discussed in the reviews. The mechanisms are as follows:</p><figure><img src=""https://cdn-images-1.medium.com/max/853/1*i29lcxUd_VaFZHOd64nV_Q.png"" /></figure><p>The theory is that these mechanisms motivate behavior change through the three-part Capability, Motivation, and Opportunity model developed by Michie et al in 2011 as diagrammed below:</p><figure><img src=""https://cdn-images-1.medium.com/max/738/1*OwPHsyIUIzmBNP9V0Ty_nA.png"" /></figure><p>So what did they find? The first review, the one focused on studies specifically assessing EIDM interventions, identified 23 existing reviews of moderate to high trustworthiness that were published between 1990–2015. Most of these drew from the health sector, and featured plenty of variation in the interventions, outcomes, and indicators used. Nevertheless, the aggregated findings offer several useful insights:</p><h3><strong>Just providing people with evidence doesn’t mean they’ll do anything with it</strong></h3><p>One of the clearest takeaways from the experimental literature, which backs up the descriptive evidence I’ve previously written about, is that simply putting evidence in front of your audience doesn’t accomplish much on its own. Interventions to facilitate access to evidence were only effective if they were paired with parallel interventions to increase decision-makers’ motivation and opportunity to use the evidence, such as targeted and personalized reminders.</p><h3><strong>One-off workshops and training programs are (probably) a waste of time</strong></h3><p>Similarly, the skill-building activities studied only worked if the intervention design simultaneously targeted decision-makers’ capability and motivation to use evidence. Interventions that were passive or applied at low intensity (e.g., a one-off half-day training session) had no effect.</p><h3><strong>Increasing <em>motivation</em> is critical</strong></h3><p>As you can see from the above, a common theme running through the report was that many of the interventions only worked if decision-makers had sufficient motivation to use evidence. Yet of the three behavior change prerequisites, motivation was the one least often targeted by interventions included in the study. The authors surmise that the capability/motivation/opportunity framework is like a three-legged stool — you need all three of them in place for the design to work as a whole. So it stands to reason that if motivation is the most neglected of the three elements, interventions to increase motivation are especially valuable.</p><h3><strong>Making change requires structure and leadership</strong></h3><p>While not a universal role, in general it seems to be the case that the more casual the nature of the intervention, the less likely it is to work. Of the six mechanisms studied in the first review, the closest that came to a failing grade across the board was M4 (interaction between researchers and decision-makers), as “a large majority of the…interventions that included an unstructured interaction component did not increase evidence use.” By contrast, the review found that authentically integrating interventions into existing decision-making systems and processes, rather than treating them as something separate or “extra,” appears to enhance their effects. Two specific examples that showed this kind of promise were evidence-on-demand hotlines and supervising the application of EIDM skills.</p><h3>Learning from the broader literature</h3><p>Arguably the most impressive contribution of “The Science of Using Science” comes from the second review, which examined the social science literature for more general clues about how to change behavior that could be applied to this context. This part of the project was breathtaking in scope — the authors not only identified some 67 distinct interventions across dozens of fields of study ranging from behavioral economics to user experience design, they were able to offer a preliminary assessment of the effectiveness of each of those interventions on outcomes related to evidence use. Below are some of the techniques and approaches that stood out as especially promising:</p><p><strong>Building social and professional norms</strong></p><p>Human beings are social animals, which makes social and professional norms incredibly powerful forces for influencing action. Social/professional norm interventions assessed by the authors included social marketing (the application of which would be marketing evidence use as a social or professional norm), social incentives (building an intrinsic motivation to use evidence), and identity cues and priming (triggering and reinforcing nascent evidence use norms). All three have a strong evidence base for building motivation in the broader social science literature.</p><p><strong>Strategic communication</strong></p><p>If you’ve ever wondered if “strategic communication” is really a thing or just marketing jargon — it’s real! Tailoring and targeting messages to recipients based on their concerns and interests, framing messaging around gains and losses, and deploying narrative techniques like metaphors and concrete examples all have ample evidence of effectiveness in the social science literature, increasing motivation for behavior change.</p><p><strong>Reminders and nudges</strong></p><p>It almost sounds too easy, but simple steps to increase the salience of things you want people to do are often enough to spur them into motion. Both reminders and “<a href=""https://en.wikipedia.org/wiki/Nudge_theory"">nudges</a>” — simple changes to the choices people face on an everyday basis, such as happens if they take no action — have strong evidence behind them when it comes to behavior change.</p><p><strong>Learning from adult learning</strong></p><p>I mentioned above that low-intensity skill-building workshops were shown to be ineffective in the first review. That finding shouldn’t be a surprise in light of what studies on adult pedagogy have shown regarding what works in this arena. One such review noted that adult learning interventions had sharply diminishing effects if they included less than 20 hours of instruction or training or were delivered to more than 40 participants; furthermore, interventions worked best if they took place directly in participants’ work settings. Applying insights like these increased both capability and motivation to change behavior in the studies reviewed. (As someone who is regularly asked to conduct low-intensity skill-building workshops for clients, I’m eager to use these lessons to improve the quality and effectiveness of my own programming!)</p><p><strong>Supervised learning</strong></p><p>Not surprisingly, bringing the boss into the equation is a pretty effective way to motivate employees to use evidence! As highlighted previously, one of the successful case studies in the first review involved having senior leaders supervise employees attempting to apply new EIDM skills in practice. The second review found that supervision (mainly based on the idea of “<a href=""https://en.wikipedia.org/wiki/Clinical_supervision"">clinical supervision</a>” as applied in the healthcare field) moves the needle on all three behavior change outcomes of capability, motivation, and opportunity. In addition, a practice called “audit and feedback,” which is basically a kind of performance monitoring, had a small but significant impact on behavior change.</p><h3>A call to action</h3><p>I’ll close here with a finding that seems like a fitting summation of the lessons from “The Science of Using Science.” One <a href=""https://bmjopen.bmj.com/content/5/9/e008592"">review judged to be of high trustworthiness</a> compared the effectiveness of different behavior change interventions to each other, and found that behavioral change interventions that worked best were those that emphasized <em>action</em>. The emphasis on action seems appropriate across the board when looking at the findings from the literature: over and over again, we are told that it’s not enough just to provide information and hope for the best, or get people talking to each other and hope for the best. Evidence is unlikely to be used, it seems, unless the entire intervention is designed with the <em>act </em>of using evidence as its center of gravity. As the authors write in the concluding sections, “the science of using science might be able to progress further by starting with the user of evidence and studying their needs and behaviours in decision-making,” and only then considering the role that research can play.</p><p>The bottom line: it’s not impossible to get people to use evidence. It’s just that most of us are going about it the wrong way. The global knowledge production industry is vast. It includes virtually all think tanks and universities, many government agencies, and an increasing number of foundations and NGOs. And what this research tells us is that many longstanding, firmly established, and seemingly impossible-to-change practices within these institutional ecosystems are premised on naive and incorrect assumptions about how human beings naturally use information in their decision-making process. It’s not hard to argue that billions of dollars a year are wasted as a result. But the good news is that these problems are, at least to some degree, fixable. The same advances in behavioral science, adult pedagogy, and other fields that have been used to solve problems ranging from eating healthier food to wearing condoms can be just as helpful for motivating social sector leaders to use evidence responsibly. Though we don’t yet know all the details of what works in which contexts, we do know enough now to say that trying to find out is a worthwhile endeavor.</p><img src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3489cf352ff"" />",Ian David Moss,ian-david-moss,Ian David Moss,
LwvJ6GGFAZ5jPhRAm,Taking Social Initiative,taking-social-initiative,https://www.lesswrong.com/posts/LwvJ6GGFAZ5jPhRAm/taking-social-initiative,2020-09-19T15:31:21.082Z,19,12,2,False,False,https://www.neelnanda.io/blog/mini-blog-post-23-taking-social-initiative,"<p><em>(This is a post from a personal daily blogging project, on social skills, taking social initiative, and &quot;networking without being a terrible person&quot;. And I think this is relevant to the interests of LessWrong readers!)</em></p><h1>Introduction</h1><p>A theme I&#x2019;ve touched on pretty heavily in previous posts is <strong>agency</strong>. Not being passive, and actually doing things. Making it <a href=""https://www.neelnanda.io/blog/become-a-person-who-actually-does-things"">part of your identity</a>, understanding <a href=""https://www.neelnanda.io/blog/mini-blog-post-15-the-illusion-of-doing-nothing"">the underlying biases</a> and becoming able to <a href=""https://www.neelnanda.io/blog/mini-blog-post-21-taking-the-first-step"">take the first step</a>. One extremely important instance is applying this to social settings - <strong>taking social initiative</strong>. I think this is something that most people I see are systematically bad at, and something I&#x2019;ve deliberately improved at over time, and has been an <em>insanely </em>valuable skill. I estimate that at least 80% of my current friendships either wouldn&#x2019;t exist or would be substantially worse without this. And this is an important component of how I find <a href=""https://www.neelnanda.io/blog/mini-blog-post-11-live-a-life-you-feel-excited-about"">things I&#x2019;m excited about</a> and <a href=""https://www.neelnanda.io/blog/mini-blog-post-10-seek-positive-externalities"">seek positive externalities</a></p><p>My goal in this post is to outline my model for why this is hard, how to become better at it, and how I&#x2019;ve specifically applied this to friendships and to networking. </p><p><strong>Health warning: </strong>The <a href=""https://web.archive.org/web/20200122231255/http://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/"">Law of Equal but Opposite Advice</a> applies <em>massively </em>here. My guess is that most people reading this don&#x2019;t take the social initiative enough, but there are definitely shameless people who take it far too much! My guess is that most readers are <em>very </em>unlikely to reach that point by accident.</p><h1>Taking Social Initiative is Hard</h1><p>It&#x2019;s a well-documented psychological phenomenon that people <a href=""https://en.wikipedia.org/wiki/Impostor_syndrome#:~:text=Impostor%20syndrome%20(also%20known%20as,exposed%20as%20a%20%22fraud%22."">systematically underestimate their abilities</a>. This is what an insecurity <em>is</em>. And as I talk about <a href=""https://www.neelnanda.io/blog/mini-blog-post-12-the-map-and-the-territory"">here</a>, I&#x2019;ve found this mindset valuable for getting over my own insecurities. My goal is to have a <em>true </em>view of my abilities, and whether I&#x2019;m good at something. And, empirically, insecurities are systematically biased against me - <strong>insecurity is a cognitive bias</strong>. </p><p>And I observe a similar thing when people consider taking the social initiative - organising events, meeting new people, reconnecting with old friends, going to unfamiliar events. People focus strongly on the downsides, ways this could go wrong, think people will laugh at them, etc. It&#x2019;s <em>extremely </em>easy to fall into the <a href=""https://www.neelnanda.io/blog/mini-blog-post-15-the-illusion-of-doing-nothing"">default path of doing nothing</a>. And empirically, I think <strong>this is a cognitive bias</strong>. I have frequently observed people being insecure about something, going a bit outside their comfort zone, and being rewarded for it - bravery is admired, things go well, awesome things happen. And I have rarely observed people doing this and it going badly - most people aren&#x2019;t jerks (and if they are jerks, those aren&#x2019;t opinions I want to be listening to anyway!). </p><p>The true question I want to be answering is &#x201C;is taking the initiative a good idea here?&#x201D;, and empirically, my intuitions are not a good guide to the truth. And because I <a href=""https://www.neelnanda.io/blog/mini-blog-post-13-beliefs-are-for-true-things"">care about having true beliefs</a>, I look past this. I&#x2019;ve deliberately created a counter-bias within myself to <strong>notice when I&#x2019;m borderline on taking initiative</strong>, and to <strong>make myself do it anyway </strong>- because when my thoughts have a systematic bias away from truth, crude rules like this will lead to systematically better answers. And I&#x2019;ve found that this rule has resulted in much better outcomes, and made me a lot happier! </p><p>Further, there are other good reasons to expect people to take the social initiative systematically less often than would be optimal. Taking social initiative has <a href=""https://www.neelnanda.io/blog/mini-blog-post-10-seek-positive-externalities"">positive externalities</a> - other people benefit, as well as you, while <em>they </em>incur none of the costs. It&#x2019;s not the default action, and there&#x2019;s a strong bias towards loss aversion and <a href=""https://www.neelnanda.io/blog/mini-blog-post-15-the-illusion-of-doing-nothing"">doing nothing</a> - nothing <em>forces </em>you to take the initiative. It&#x2019;s easy to fear coming across as pushy, or failing, or embarrassing yourself, while you rarely get blamed for <em>not </em>taking the initiative. </p><p>Thus, <strong>the default state of the world is that people do not take the social initiative enough - including you.</strong></p><h1>Friendships</h1><p>I&#x2019;m now going to outline the areas of my life where this skill has been most valuable to me, and to try to give actionable examples for how you could apply this.</p><p>The first important area is with my social life, and friendships. A lot of my happiest experiences involve being around people I like and care about, and these things <strong>just won&#x2019;t happen</strong> if nobody takes the initiative. There are <em>major </em>positive externalities to being the kind of person who can take the initiative with friends. And I&#x2019;ve found that this is something that&#x2019;s give me a lot of satisfaction - it&#x2019;s really fun to see other people enjoy something I organised, and made happen!</p><p>I&#x2019;ve outlined my thoughts on this over <a href=""https://www.neelnanda.io/blog/mini-blog-post-11-live-a-life-you-feel-excited-about"">various</a> <a href=""https://www.neelnanda.io/blog/mini-blog-post-10-seek-positive-externalities"">previous</a> <a href=""https://www.neelnanda.io/blog/mini-blog-post-19-on-systems-living-a-life-of-zero-willpower"">posts</a> in more detail/from different perspectives, but the main examples are:</p><ul><li>Meeting new people</li><ul><li>One of the most valuable skills I&#x2019;ve ever developed, is the skill of noticing when I meet somebody new and interesting who I get on with, and trying to become friends with them. Showing that I enjoy their company, inviting them to coffee (or nowadays to have a call), </li><li>I&#x2019;ve even had good success with doing this without meeting them first - asking mutual friends for introductions, cold messaging people I don&#x2019;t know. A lot of people are friendly, and enjoy</li><li><strong>Health warning</strong>: This one has downside risk, if the other person <em>doesn&#x2019;t </em>like you. Empirically, people systematically underestimate how often other people <em>do </em>like you, so I wouldn&#x2019;t stress too much about this. My main solution is to make it as easy as possible for the other person to say no, and to give them as much agency as possible - offer an out like &#x201C;if you have the time&#x201D;, send them a Calendly link that they&#x2019;re free to ignore, etc</li></ul><li>Organising events - throwing parties, organising group calls, suggesting you do things as a group</li><ul><li>During more normal times, I find that inviting a bunch of my friends to my room is a hilariously low effort way to throw a party</li><li>I have a similar mindset behind organising <a href=""https://youtube.com/channel/UCzoIa8e4vhfRIxL_Kj6yy0Q"">talks</a></li><li>Generally, there are a lot of things that should exist, but nobody organises them - this is a great way to practice social initiative! Eg, If there&#x2019;s a hard course people are struggling with, organise a peer-support group</li></ul><li>Keeping in touch</li><ul><li>It&#x2019;s really easy to lose touch with friends, especially if you aren&#x2019;t near each other (or are both social distancing)</li><li>And if you both enjoy each other&#x2019;s company, keeping in touch is a mutual win!</li><li>But I find that often I just lose touch with friends if I don&#x2019;t put effort in, so I have systems set up to ensure that I regularly remember to reach out to people and schedule a call, or meet-up</li><li>One point of difficulty - I find the bit that takes the most energy is making reaching out feel <em>justified</em>. If I haven&#x2019;t spoken to a friend in a while, I feel more resistance to reaching out, because it feels weird that I hadn&#x2019;t reached out before. This is very obviously dumb, but difficult to overcome with sheer force of will</li><ul><li>My main solution is to create a justification - even having a tenuous one makes this much easier. COVID was an excellent justification - now everything was remote, reconnecting with people far away makes perfect sense!</li><li>Even just &#x201C;I was thinking about old friends, and realised we haven&#x2019;t spoken in a while, want to catch up?&#x201D; works!</li></ul></ul><li>Introducing friends to each other</li><ul><li>One of the hardest parts of meeting new people is filtering. But <em>you </em>know your friends well. And so a really valuable thing you can add is introducing friends to each other, if you think they&#x2019;d get on! </li><li>Good friendships are <em>incredibly </em>valuable, so I try to make a point of doing this - I think it&#x2019;s one of the most effective ways to make the lives of my friends better</li><ul><li>Note - Some people aren&#x2019;t super interested in forming more friends, or are very busy, so this advice won&#x2019;t apply universally. Check first!</li></ul><li>If you know me well enough to know who I might get on with, and you think I&#x2019;d click with a friend of your&#x2019;s, please let me know about it!</li></ul><li>(Less central, but important example) Putting yourself out there - beyond just organising events, do things that can add value to other&apos;s!</li><ul><li>Start a <a href=""https://neelnanda.io/"">blog</a>! Great way to clarify your thoughts on interesting ideas, and to actually make something you&#x2019;re proud of.</li><li>Publish <a href=""https://dynalist.io/d/ToOhEKlz9qC2PmpjgyTpXQJz"">notes</a> or <a href=""https://ankiweb.net/shared/info/65202941"">flashcards</a> you make - they don&#x2019;t have to be high-quality for this to be worthwhile</li><li>I find this a powerful source of motivation!</li><li>It&#x2019;s easy to get anxious about telling other people when you do things like this - fearing judgement, anxiety about self-promotion.</li><li>A useful mindset - putting things out there is <strong>strictly better than not</strong> - you&#x2019;re just giving other people opportunities that they&#x2019;re free to ignore. And self-promotion can be valuable - most people can ignore it, the few who enjoy it will benefit a lot. If you get positive feedback on what you make, self-promotion + publishing is genuinely altruistic, not selfish!</li><ul><li>Note - it&#x2019;s totally possible to go too far here, but my guess is most people err way too hard against, which is why I&#x2019;m giving this advice. If you get feedback that you do this too much, I recommend listening to that.</li></ul></ul></ul><p>A core difficulty in all of these is that things are <strong>uncertain </strong>and <strong>have downside risk</strong>. In situations like this, <em>not </em>taking action has invisible opportunity costs, while <em>taking </em>action has visceral and scary costs. But this is a fact of <a href=""https://www.neelnanda.io/blog/mini-blog-post-12-the-map-and-the-territory"">my mind, not of the world</a>. I find it valuable to imagine a world where the action <em>does </em>pay off well - this makes the opportunity costs more concrete, and helps to break <a href=""https://www.neelnanda.io/blog/mini-blog-post-15-the-illusion-of-doing-nothing"">the illusion of doing nothing</a>. </p><p>Overall, good friendships and people I care about are an <em>incredibly </em>important component of my life happiness and mental health. And cultivating the skill of creating and maintaining these is one of the most valuable investments of effort I&#x2019;ve ever made - being good at taking initiative directly led to existence of the vast majority of my current friendships.</p><h1>Networking</h1><p><em>(<strong>Note:</strong> Much of my networking advice is based on my experiences networking with <a href=""https://www.effectivealtruism.org/"">Effective Altruists</a>, who are unusually nice people - this may not perfectly generalise)</em></p><p>I find networking a pretty interesting concept, and it&#x2019;s added quite a lot of value to my life. Most people I talk to consider it a bit of a dirty word - associated with LinkedIn profiles, investment bankers, and scummy, insincere people. I am pretty strongly opposed to this kind of shitty networking! But I started using the word networking ironically to describe what I actually do, and it accidentally stopped being ironic. So I now have a pretty positive association with it now. And this forms my attempt to reclaim that word! And I&#x2019;ve found that the ability to take social initiative is a pretty key sub-skill here.</p><p>I&#x2019;m roughly defining networking as getting to know people who have skills and experience that I find interesting and could learn from, people with help they could give, connections they could share.</p><h2>Sincerity</h2><p>I think my most important perspective shift with networking is to <strong>be sincere</strong>. Try to talk to interesting people <em>because </em>I enjoy talking to them, not because I want something from them. Try to be interesting and pleasant myself. Building relationships with people, because it makes me happy that those relationships exist, not because I think it&#x2019;ll be useful to me. And conveniently, the people most worth knowing also tend to be the people I intrinsically <em>want </em>to know!</p><p>This is an important and subtle point - the mindset is not about <em>trying </em>to seem sincere because it&#x2019;s the best way to achieve your goals, it&#x2019;s about <strong>genuinely being sincere</strong>. It&#x2019;s easy to fall into the trap of thinking that your decision making process is completely separate from the decisions you make, <a href=""http://mindingourway.com/newcomblike-problems-are-the-norm/"">but this is rarely true</a>. Humans are social creatures, and we have very developed senses for inferring the intentions of the people we talk to. It&#x2019;s not hard to tell when the person you&#x2019;re talking to has an ulterior motive, and it&#x2019;s far less fun to talk to someone like that. </p><p>And you can <em>also </em>have ulterior motivates! Wanting to know cool people because they&#x2019;re fun to be around <em>and </em>useful to know is fine, and rarely the kind of thing that bothers people. But the ultimate goal is for knowing interesting people to be something <a href=""https://www.neelnanda.io/blog/mini-blog-post-11-live-a-life-you-feel-excited-about"">you feel excited about</a>.</p><p>Ask questions about the things you find genuinely interesting! Keep in touch with the people whose company you enjoy! Share things you find cool and talk about your experiences, not because you&#x2019;re trying to show-off or impress, but because it makes for a better conversation. Be enthusiastic and excited, rather than striving to be professional. Offer to do favours and be helpful, not because you expect reciprocation, but because <strong>that&#x2019;s what you do for people you like</strong>. </p><p>I&#x2019;ve found that the underlying intent <em>massively </em>shifts what the interaction is like, far more than it seems like it should. And it&#x2019;s far more fun - I think it&#x2019;s easy to fall into the trap of thinking you <em>need </em>to be boring and soulless to network, because that&#x2019;s the &#x201C;done thing&#x201D; - if you do this, nobody is having a good time. And, at the end of the day, by following this strategy, I have a pretty great network of cool people I know. And this is useful, but it&#x2019;s <em>also </em>awesome for its own sake! Most people aren&#x2019;t jerks, and are happy to help out others who are nice and pleasant to be around.</p><h2>Be pleasant to help</h2><p>Another sub-skill is trying to make helping you as pleasant and easy as possible:</p><ul><li>Develop the <a href=""https://www.neelnanda.io/blog/mini-blog-post-5-how-to-learn-from-conversations"">skill of asking good questions</a> - follow the things that most interest you and try to learn, rather trying to ask the questions you &#x201C;think you should&#x201D;</li><ul><li>Figure out exactly what you want help with in advance, and come up with questions to that end</li><ul><li>Figuring out what to say and how to structure advice is <strong>cognitive labour</strong>. Things are more fun for the other person if you do as much of that for them as possible</li><li>These don&#x2019;t have to be perfectly comprehensive and specific, but everything you can do helps! Eg &#x201C;I&#x2019;m pretty confused about this area&#x201D; or &#x201C;I don&#x2019;t understand this topic very well. My current understanding is ___ is that missing anything important?&#x201D;</li><li>Generally signal what you want from the conversation - specific career advice, introductions, general knowledge, life advice, etc. People like being helpful, so make it as easy as possible!</li></ul><li>This also makes the conversations more varied, and thus more interesting for the other person! If they always have the same conversations with people in your reference class, everyone has more fun if it&#x2019;s unusual. Being asked really good questions can clarify <em>their </em>thoughts, and is a good goal to aim for. </li></ul><li>Be a fun person to be around</li><ul><li>Be polite, but treat them like a normal person, rather than being highly formal, or caught up with awareness of status</li><li>Give sincere compliments</li><ul><li>Note - <strong>sincere </strong>is important, flattery tends to be easy to see through</li><li>Explicitly say when things they say are helpful - be specific, and say how it helped you</li></ul><li>If what they said helped you later on, send them a message of thanks! Be as specific and concrete as possible - this is a good signal of sincerity over flattery because it&#x2019;s harder to fake. It&#x2019;s <em>really </em>satisfying to receive these, especially if you can point to concrete actions or successes you had as a direct consequence</li></ul><li>Giving advice is fun! It&#x2019;s nice to feel as though you&#x2019;re helping somebody else. So try to be clear about exactly how this is helpful to you!</li><li>Respect their time, and be as convenient for them as possible - I like to send people a Calendly link and be otherwise flexible.</li><li>Be helpful</li><ul><li>Think about things you could offer them - people you know and could introduce them to, good resources and articles, interesting insights and perspectives.</li><ul><li>This one gets easier the more people you know!</li><li>Note - this can be embarrassing if done wrong. Always broach it as a question, eg &#x201C;have you heard of ___&#x201D;, or &#x201C;do you know ___&#x201D;. Keep the initial question short, so that if they <em>do </em>already know about this, it&#x2019;s not awkward</li></ul></ul></ul><h2>Present opportunities</h2><p>Another helpful mindset: <strong>people can just say no</strong>. Thus, other than the cost of saying no, reaching out is <strong>presenting them with an opportunity</strong>. And receiving opportunities is pleasant! It gives them more option value. I find this mindset <em>super </em>useful for overcoming insecurity about bothering people, or looking silly.</p><p>If you&#x2019;re looking for a job or internship, if that&#x2019;s successful, <em>everyone </em>wins. That&#x2019;s a great opportunity for all involved! Thus presenting people with something like that is hardly high cost. </p><p>If you want advice from somebody, that can often be fun to give! An opportunity doesn&#x2019;t have to <em>directly </em>benefit them. I know I personally have gotten a <em>ton </em>of useful advice from people in the past, and I can hardly pay <em>them </em>back. So it&#x2019;s satisfying to be able to pay it forwards, and give advice to people like my past self.</p><p>And it&#x2019;s satisfying to help somebody else succeed! Especially within the Effective Altruism community - if your goal is to make the world a better place, <strong>people actively want to help you succeed</strong>. Because you&#x2019;re all on the same team - if you can do more good, then everyone wins.</p><h2>Minimise costs</h2><p>The main cost to them of you reaching out is if they have to say no - this is emotional labour. I try to ensure I give them out&#x2019;s eg &#x201C;no worries if you&#x2019;re busy&#x201D;. Sometimes I send an obviously copied and pasted message, if I want to signal &#x201C;only say yes to this if it feels fun to you&#x201D;</p><p>Further, <strong>be OK with hearing a no</strong>. This isn&#x2019;t a rejection, or a sign that you&#x2019;ve bothered somebody - you&#x2019;ve presented them with an opportunity they weren&#x2019;t interested in. They still benefited from the option value, and this isn&#x2019;t a personal attack on you - often people are just busy! I see this as a very stochastic process - you contact a lot of people, and a few are interested. And if everyone benefits in those few connections, then the original opportunity was valuable!</p><p>A useful framing - the busier and higher status somebody is, the better they are at saying no to things! You fundamentally <em>can&#x2019;t </em>be busy and high-status without developing the skill of prioritising and saying no, there are just too many things to do. And thus, the higher status they are, the <em>lower </em>the cost of reaching out - I find this framing pretty insightful and unintuitive. And this goes all the more for reaching out to a <em>company </em>rather than a specific person - it&#x2019;s less personal and thus even lower emotional effort.</p><h2>Taking action</h2><p>So, how to actually act upon all this? As with most forms of taking the initiative, this is hard, and subjective, and will depend heavily on your personal circumstances. A few thoughts:</p><ul><li>If you already know somebody who could help you, get over your reluctance and ask</li><li>If you meet interesting people at events, keep in touch!</li><li>If you vaguely know somebody, and they could help you with something, reach out and ask!</li><li>Ask friends for introductions - even if you just have a general kind of person in mind, ask if your friends know anyone who could help with that</li><ul><li>And if your friends are in need of advice, see who you might be able to connect them with! People respond pretty well to messages like &#x201C;a friend of mine needs help with ___, would you be interested in chatting with them?&#x201D;</li></ul><li>Cold message people on LinkedIn</li><ul><li>A friend of mine has had a surprising amount of success with messaging people in fields he wants to enter, and asking for advice and mentorship.</li></ul><li>Cold email people whose work you admire - academics with interesting papers, people who make content you enjoy</li><ul><li>Reaching out to academics with specific comments and thoughts on their papers can go down super well! It&#x2019;s nice to see other people meaningfully engage with your work, and to see that they&#x2019;re sincerely interested in you, rather than just blindly spamming people.</li></ul></ul><h1>Conclusion</h1><p>For all these reasons, I think taking the social initiative is good and something people should do more often. And that it&#x2019;s a skill which is very widely applicable in daily life. I&#x2019;ve found that setting a rule of systematically doing it more often has been a <em>major </em>life upgrade. Of course, this is an empirical claim, and isn&#x2019;t <em>obviously </em>true. And it&#x2019;s <em>hard</em>. So I imagine a good amount of those reading this will kinda buy my arguments, but not feel convinced on a gut level, and be storing this as &#x201C;an interesting idea I&#x2019;ll never act upon&#x201D;. </p><p>That feeling is the feeling of <strong>uncertainty. </strong>This is an important question, and an important empirical claim. If you are systematically biased against taking the initiative, are systematically missing out on valuable opportunities, and could learn to be better at it, then that&#x2019;s <em>incredibly </em>valuable information. The value of that information <em>vastly </em>outweighs the costs of trying it a few times, and potential failure. And when there&#x2019;s valuable information you lack, the solution is to <a href=""https://www.neelnanda.io/blog/the-value-of-experiments"">run an experiment and try it</a>! From that perspective, failure ceases to be a meaningful concept - whatever happens will reduce your uncertainty, and <em>that </em>is the true victory.</p><p>Further, <strong>taking the initiative is habit forming</strong>. The activation energy required goes down <em>massively </em>once you&#x2019;ve tried it a few times and it&#x2019;s gone well. And with time, taking the social initiative can become part of your identity, and <a href=""https://www.neelnanda.io/blog/mini-blog-post-11-live-a-life-you-feel-excited-about"">something you feel excited about</a>. Most of the energy goes to overcoming your intuitions, and intuitions come from your experiences. So if you&#x2019;re reading all this, and this feels compelling, but a bit outside your comfort zone, I urge you to think about how you could <a href=""https://www.neelnanda.io/blog/mini-blog-post-21-taking-the-first-step"">take the first step</a>. Because the first step is by far the hardest.</p><p>What are the things you want to do, but aren&#x2019;t brave enough to start? What opportunities are you currently missing out on? What could you do, as the first step to <strong>becoming the</strong> <strong>kind of person you want to be</strong>?</p>",neel-nanda-1,neel-nanda-1,Neel Nanda,
BGD5J2KAoNmpPMzMQ,Why GPT wants to mesa-optimize & how we might change this,why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this,https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this,2020-09-19T13:48:30.348Z,55,21,33,False,False,,"<p>This post was inspired by orthonormal's post <a href=""https://www.lesswrong.com/posts/3nDR23ksSQJ98WNDm/developmental-stages-of-gpts"">Developmental Stages of GPTs</a> and the discussion that followed, so only part of it is original.</p>
<p>First I'll aim to provide a crisper version of the argument for why GPT wants to <a href=""https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB"">mesa-optimize</a>.  Specifically, I'll explain a well-known optimization algorithm used in text generation, and argue that GPT can improve performance on its objective by learning to implement something like this algorithm internally.</p>
<p>Then I'll offer some ideas of mine about how we might change this.</p>
<h1>Explanation of beam search</h1>
<p>Our goal is to generate plausible text.  We evaluate whether text is ""plausible"" by multiplying together all the individual word probabilities from our language model.</p>
<p>Greedy word selection has a problem: Since it doesn't do lookahead, it's liable to get stuck in a dead end.  Let's say we give our system the following poem about cheeses and ask it to generate more text:</p>
<blockquote>
<p>Mozzarella is white</p>
</blockquote>
<blockquote>
<p>So you can see it at night</p>
</blockquote>
<blockquote>
<p>Cheddar is...</p>
</blockquote>
<p>If our language model is decent, the word it will assign the highest probability to is ""orange"".  But this creates a problem, because ""orange"" is a hard word to rhyme.</p>
<p>Beam search is an attempt to solve this problem.  Instead of picking the next word greedily, we explore the tree of completions and try to find a multi-word completion that maximizes the product of the individual word probabilities.</p>
<p>Because there are so many words in the English language, the tree grows at a very fast exponential rate.  So we choose an integer <em>beam_width</em> for the number of partial completions to track, and each time we take another step deeper into the tree, we discard all but the most plausible <em>beam_width</em> partial completions.</p>
<p><img src=""https://i.imgur.com/mMW6qbl.png"" alt=""""></p>
<p><em>Beam search with a beam width of 2.  The bold red path corresponds to the maximum-plausibility completion, which would not get discovered by greedy search because ""nice"" has a higher probability than ""dog"".  Image stolen from <a href=""https://huggingface.co/blog/how-to-generate"">this Hugging Face blog post</a>, which has another explanation of beam search if you didn't like mine.</em></p>
<h1>Claim: GPT can do better on its training objective if it learns to do beam search internally</h1>
<p>We've discussed text generation with a pretrained language model.  Let's switch gears and talk about the model's training process.</p>
<p>Suppose GPT's training corpus has the following poem:</p>
<blockquote>
<p>Mozzarella is white</p>
</blockquote>
<blockquote>
<p>So you can see it at night</p>
</blockquote>
<blockquote>
<p>Cheddar is marigold</p>
</blockquote>
<blockquote>
<p>Unless you let it get too old</p>
</blockquote>
<p>GPT is trained by giving it some text and asking it to predict the next word.  So eventually GPT will be given the example from above</p>
<blockquote>
<p>Mozzarella is white</p>
</blockquote>
<blockquote>
<p>So you can see it at night</p>
</blockquote>
<blockquote>
<p>Cheddar is...</p>
</blockquote>
<p>and be asked to predict the next word.</p>
<p>Let's consider the performance of two models on this task: regular ""naive"" GPT, and ""beam search amplified"" GPT.  Beam search amplified GPT works by performing beam search using naive GPT, then looking at the distribution of the first words in the resulting completions, then outputting some weighted average of that distribution and the distribution from naive GPT.</p>
<p>Because beam search can find lots of ways to continue the poem using ""marigold"", but few ways using ""orange"", beam search amplified GPT's distribution ends up being closer to reality than that of naive GPT. Something like this:</p>
<p><img src=""https://i.imgur.com/pfAzK0h.png"" alt=""""></p>
<p>So when we update GPT's weights during training, we're shifting the weights towards the sort of computational structure that would make predictions like beam search amplified GPT does.</p>
<h1>Does this actually help?</h1>
<p>In this instance, GPT has an incentive to do internal lookahead.  But it's unclear how frequently these situations actually arise.  And maybe it's usually easier to do something else, like learning which words are easy to rhyme.</p>
<p>It would be straightforward to implement beam search amplified GPT (experimenting with different weighted averaging schemes) and check whether it can be made to assign higher plausibility to real text.  (It might be best to try with GPT-2 rather than GPT-3, in case GPT-3 is already doing internal lookahead.  Note that there's a risk of mesa-optimization developing if lookahead improves performance at <em>any point</em> during GPT's training.)</p>
<h1>Is internal lookahead possible for GPT-3?</h1>
<p>Relative to other optimization algorithms, it seems to me that beam search would be unusually easy for GPT to implement.  Traditional iterative optimization algorithms like gradient descent or simulated annealing require a lot of serial computation, and the number of serial steps GPT can perform is strongly limited.  Beam search is way less heavy on the number of serial steps required.  The number of available serial steps would still limit the maximum lookahead horizon though.</p>
<p>The transformer architecture learns computations of the form ""find some data from the previous step which scores highly according to particular criteria, do some computation on it, pass it on to the next step"".  That sounds like beam search.</p>
<p>In any case, the topic of what incentives arise while training a language model seems important more generally.</p>
<h1>Is internal lookahead dangerous?</h1>
<p>If GPT's architecture <em>is</em> capable of discovering lookahead internally, the worry is that GPT might modify and misuse it in creative ways after it's discovered.  It might start making plans, or searching for the idea that maximizes some attribute which is correlated with harm.</p>
<p>Let's say there are chess problems in GPT's training corpus which describe a board state along with an objective like ""black to move and win in 6 turns even with best play by white"".  If GPT can do lookahead internally, it can use this to search for game histories where black wins even though white is playing very well.  In other words, it's doing spontaneous internal planning.  And this spontaneous internal planning is incentivized because it helps predict solutions to chess problems.</p>
<p>Who knows what other contexts spontaneous internal planning might get used in.</p>
<h1>Fix idea #1: Switch to BERT style training</h1>
<p>How might we remove the incentive for mesa-optimization?</p>
<p>A simple idea is to stop training the task of predicting the next word, and instead train the task of predicting a masked word given the surrounding context.  This is what BERT uses.  The incentive for internal lookahead seems smaller with this task, but I guess you'd still see it in e.g. predicting masked chess moves of strong players.</p>
<p>BERT's ability to generate text is <a href=""https://ai.stackexchange.com/questions/9141/can-bert-be-used-for-sentence-generating-tasks"">unclear</a>.  But I have a feeling BERT-style training actually offers greater potential than GPT-style training for text generation, because BERT can edit its writing—see discussion <a href=""https://www.lesswrong.com/posts/3nDR23ksSQJ98WNDm/developmental-stages-of-gpts?commentId=6cx7hhhwifKd8yamM"">here</a>.  You could get really creative, like doing genetic algorithms with a population of texts generated on a particular topic, and using BERT to mutate and recombine texts in the population until you get one with really high plausibility.  (Some words at the beginning could stay fixed as the ""prompt"".)</p>
<h1>Fix idea #2: Prohibit updates towards lookahead</h1>
<p>This fix is more interesting because the core idea seems applicable for stopping mesa-optimization in other contexts too.</p>
<p>v1 of the idea is: During training, before any gradient update, we make a prediction with beam search amplified GPT.  And if an update to naive GPT would mean moving it towards beam search amplified GPT (because beam search amplified GPT assigns a higher probability than naive GPT to the actual next word), we just skip the update and keep moving.  Nick Bostrom refers to this general approach as ""stunting"" in Chapter 9 of <em>Superintelligence</em>.</p>
<p><img src=""https://i.imgur.com/pfAzK0h.png"" alt=""""></p>
<p><em>This diagram from above is an example of an update that would get skipped.</em></p>
<p>Training will take longer, obviously.  But only insofar as all this mesa-optimization stuff was a problem in the first place.</p>
<p>v2 of the idea is to add a penalty term to GPT's loss function that causes it to update <em>away</em> from whatever beam search amplified GPT predicts.</p>
<p>A risk with v2 is that GPT is incentivized to emulate beam search amplified GPT so it can <em>avoid</em> making its prediction (i.e. an internal mesa-optimizer gets developed anyway).  Skipping updates as in v1 is probably safer, but the fundamental problem seems possible either way.</p>
<p>So v3 is to track gradient updates towards/away from beam search amplified GPT and ensure a rough balance.  We don't want systematic movement towards or away from beam search amplified GPT—we don't want incentives to develop either it or its inverse.  This could be accomplished by e.g. rescaling gradients on the fly so the average movement towards beam search amplified GPT's answers is around 0.  (To be safe, you might want to balance both the number of updates in each direction and their typical magnitude.)  An advantage is that training can go faster than in v1 because less updating is skipped.</p>
<p>Beam search isn't the only possible way to do lookahead though.</p>
<p>In particular, the distribution of first words in beam search completions will only assign non-zero probability to words which are the first word of a completion.  That leaves room for predictive gains by doing lookahead for other words.</p>
<p>So v4 of the idea is to sample from the infinite beam width distribution, particle filter style, and update in such a way that your expected update towards next-word predictions from the infinite beam width distribution is 0.  (Not sure how to do this concretely, but it seems like it might be possible.)</p>
<p>The broader issue here is the <a href=""https://arbital.greaterwrong.com/p/nearest_unblocked/"">nearest unblocked strategy</a> problem.  The problem seems relatively tractable in this case, because e.g. beam search based stunting seems to remove the incentive for <em>any</em> sort of lookahead insofar as it helps in the cases where beam search amplified GPT outperforms.  (I suspect mesa-optimization is not an all-or nothing proposition, and it will matter how much it has been refined + how readily it is deployed.)</p>
<p>Hence, v5 of the idea—even more underspecified than v4 at this point—is adversarial anti-lookahead stunting.  Characterize some kind of ""lookahead amplifier"" which takes naive GPT as input and attempts to amplify it by doing lookahead-type things in a very freeform way.  Train this and make it really good at amplifying GPT to achieve better next-word prediction.  Then stunt naive GPT during training by forcing its average update towards the next-word predictions of lookahead-amplified GPT to be 0.  (Perhaps you'd want to be retraining the lookahead amplifier repeatedly over the course of GPT's training, in case different lookahead strategies work best at different GPT maturity levels.)  In production, go ahead and make use of lookahead-amplified GPT to make performance as competitive as possible.  (It wouldn't shock me if this scheme actually produced <em>superior</em> performance, if the lookahead amplifier learns lookahead that GPT couldn't learn for itself, due to a better inductive bias.)</p>
<p>Ideally, GPT's training process would be monitored to ensure that GPT + lookahead in any form consistently outperforms naive GPT.  For bonus points, have the lookahead used in the monitoring be a form of lookahead which is <em>not</em> used for stunting (a ""dev set"").</p>
<h1>Do these fixes actually help?</h1>
<p>An objection to Fix #2 is the possibility of mesa-optimization which isn't very much like lookahead.  For example, if we're training on text that describes a newly discovered animal, the system has an incentive to try &amp; figure out the animal for itself internally so it can better predict how it will be described—and it might make use of some optimization algorithm, genetic algorithms say, to achieve this.</p>
<p>Another objection is that pulling optimization up from the mesa level, as in the ""BERT + genetic algorithms"" idea or the ""lookahead amplifier in production"" idea, isn't actually helpful.  There's still optimization happening, and the system as a whole could still make devious plans or search for <a href=""https://wiki.lesswrong.com/wiki/Information_hazard"">harmful</a> <a href=""https://wiki.lesswrong.com/wiki/AI_boxing#Simulations"">ideas</a>.</p>
<p>However, less mesa-optimization means less risk that transformer blocks develop optimization/planning capabilities and reuse them in contexts we didn't expect.  It's easier to reason about searching for text which maximizes plausibility than a mysterious mesa-objective.  In particular, an agent that gets instantiated internally might search for <a href=""https://en.wikipedia.org/wiki/Side-channel_attack"">side-channel attacks</a> in the text generation machinery and surrounding system (especially risky if GPT has read about this stuff).  But it seems very unlikely that a search for plausibility-maximizing text would cause this (except maybe if those attacks somehow got activated during training).  Non-mesa-optimization also has parameters that allow us to control its strength without retraining the model, and we have a better understanding of how it works.</p>
<p>There's still a lot of potential for misuse &amp; accidents either way, of course.</p>
<h1>OpenAI doesn't offer beam search?  Why?  Is GPT-3 already mesa-optimizing?</h1>
<p>Up until now, I've been pretending that maximizing plausibility (product of individual word probabilities) is a good way to generate text.  But beam search doesn't even seem to be an option in the <a href=""https://www.twilio.com/blog/ultimate-guide-openai-gpt-3-language-model"">GPT-3 interface</a>.  (Please correct me if I'm missing something!)</p>
<p>Why is beam search missing?  One possibility is that GPT-3 already does internal lookahead.  OpenAI tried beam search, found it didn't improve text generation, and didn't bother adding it as an option.  In other words, GPT-3 is already mesa-optimizing 😲</p>
<p>Another possibility:</p>
<blockquote>
<p>[Generated text:] ""I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.""</p>
</blockquote>
<blockquote>
<p>...</p>
</blockquote>
<blockquote>
<p>...The generated words following the context are reasonable, but the model quickly starts repeating itself! This is a very common problem in language generation in general and seems to be <strong>even more so in greedy and beam search</strong>...</p>
</blockquote>
<blockquote>
<p>...</p>
</blockquote>
<blockquote>
<p>...Recently, there has been more evidence though that the apparent flaws of greedy and beam search - mainly generating repetitive word sequences - are caused by the model (especially the way the model is trained), rather than the decoding method, cf. <a href=""https://arxiv.org/pdf/1908.04319.pdf"">Welleck et al. (2019)</a>.</p>
</blockquote>
<p>From <a href=""https://huggingface.co/blog/how-to-generate"">the Hugging Face post</a> (emphasis mine).  OK, this thing about language models that find repetitive text plausible sounds like a problem that will eventually get solved.  Anything else?</p>
<blockquote>
<p>As argued in Ari Holtzman et al. (2019), high quality human language does not follow a distribution of high probability next words. In other words, as humans, we want generated text to surprise us and not to be boring/predictable. The authors show this nicely by plotting the probability, a model would give to human text vs. what beam search does.</p>
</blockquote>
<blockquote>
<p><img src=""https://i.imgur.com/idf1d4r.png"" alt=""""></p>
</blockquote>
<blockquote>
<p>So let's stop being boring and introduce some randomness 🤪.</p>
</blockquote>
<p>This is a much deeper &amp; more interesting issue IMO.  It may be that only a superintelligent language model will find human writing so boringly predictable that every word has high likelihood based on what came before.</p>
<p>Will there be an intermediate stage where prompting a language model with ""I just had a brilliant and highly original idea related to X"" will cause it to assign higher plausibilities to completions that are actually quite brilliant &amp; original?  (Is this the case for GPT-3 already?)  I have no idea.</p>
<p>In any case, maybe we could get the benefits of both originality and avoidance of dead ends by sampling from beam search amplified GPT's next-word distribution to generate text?  (This could be especially useful if Fix #2 has been applied and the GPT's ability to do lookahead for itself has been stunted.)</p>
<p>Note also that the surprisingness of human text could be an objection to the ""GPT can do better on its training objective if it learns to do beam search for itself"" claim above.  If human text tends to have periodic surprises, using beam search to look for predictable completions may not help performance since those predictions aren't actually very likely.</p>
<p>However, it also may be the case that beam search ends up improving the accuracy of next-word prediction despite the fact that it doesn't generate interesting text.</p>
",John_Maxwell_IV,john_maxwell,John_Maxwell,
JRj3RFzuXwuNfSnE9,EA Relationship Status,ea-relationship-status,https://www.lesswrong.com/posts/JRj3RFzuXwuNfSnE9/ea-relationship-status,2020-09-19T01:50:02.731Z,28,9,16,False,False,,"<p><span>

A friend </span>

<a href=""https://www.facebook.com/groups/eaparents/permalink/2854556011433232"">observed</a>
that fewer people in the effective altruism movement are married than
you might expect.  I was curious: what are marriage rates like
within the EA community?  The 2018 

<a href=""https://www.rethinkprojects.org/easurvey"">EA Survey</a> asked
about relationship status, and we can look at how that varies
by age:



</p><p>

<a href=""https://www.jefftk.com/ea-relationship-status-by-age-big.png""><img src=""https://www.jefftk.com/ea-relationship-status-by-age.png"" srcset=""https://www.jefftk.com/ea-relationship-status-by-age.png 550w, https://www.jefftk.com/ea-relationship-status-by-age-2x.png 1100w"" /></a>


</p>

<p>

I'm using ""ever married"" for people who are currently married or have
ever been married, including people who are now divorced, widowed, or
separated.  Since some of these buckets might be pretty small, let's
add sample size information:

</p>

<p>

<a href=""https://www.jefftk.com/ea-relationship-status-by-age-with-sample-size-big.png""><img src=""https://www.jefftk.com/ea-relationship-status-by-age-with-sample-size.png"" srcset=""https://www.jefftk.com/ea-relationship-status-by-age-with-sample-size.png 550w, https://www.jefftk.com/ea-relationship-status-by-age-with-sample-size-2x.png 1100w"" /></a>

</p>

<p>

The anonymized survey data doesn't have 35-44 data, and the 65+ group looks
suspiciously like it has the 35-44 group lumped in with it.  I've
<a href=""https://github.com/rethinkpriorities/ea-data/issues/27"">filed
a bug</a> and if they fix it I'll update the post.  For now, probably
ignore the 65+ group.

</p>

<p>

For comparison, here's 2018 ACS data (<a href=""https://usa.ipums.org/"">via</a>) for US marriage rates, on the
same scale:

</p>

<p>

<a href=""https://www.jefftk.com/usa-marriage-rate-by-age-big.png""><img src=""https://www.jefftk.com/usa-marriage-rate-by-age.png"" srcset=""https://www.jefftk.com/usa-marriage-rate-by-age.png 550w, https://www.jefftk.com/usa-marriage-rate-by-age-2x.png 1100w"" /></a>

</p>

<p>

And, at least in the US, people with more education are more likely to
be married, after age 28:

</p>

<p>

<a href=""https://www.jefftk.com/usa-marriage-rate-by-age-and-education-big.png""><img src=""https://www.jefftk.com/usa-marriage-rate-by-age-and-education.png"" srcset=""https://www.jefftk.com/usa-marriage-rate-by-age-and-education.png 550w, https://www.jefftk.com/usa-marriage-rate-by-age-and-education-2x.png 1100w"" /></a>

</p>

<p>

Now, EAs are not all American, and even then they're different from
Americans as a whole in many ways other than being interested in
effective altruism.  On the other hand, when I look at what fraction
of my (Swarthmore) college friends are married, a group similar to EAs
in many ways, it's not far from the graph for the US at large.  (And
it's off in the positive direction).  I see similar numbers by school
for <a href=""https://www.nytimes.com/interactive/2018/03/29/upshot/college-marriage-class-differences.html"">people
who were 30-34 in 2014</a>.

</p>

<p>

There does seem to be something real here. Some guesses as to why:

</p>

<ul>
<li>EAs often highly prioritize their careers.</li>

<li>EAs are generally less interested in having children.</li>

<li>EAs are often poly.</li>

<li>EAs often live in group houses.</li>

<li>EAs are more willing to be weird; less likely to do something because it is the standard
thing to do.</li>
</ul>

Other ideas?",jkaufman,jkaufman,jefftk,
KrJfoZzpSDpnrv9va,Draft report on AI timelines,draft-report-on-ai-timelines,https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines,2020-09-18T23:47:39.684Z,215,90,56,False,False,,"<p>Hi all, I&apos;ve been working on some AI forecasting research and have prepared a draft report on timelines to <a href=""https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1"">transformative AI</a>. I would love feedback from this community, so I&apos;ve made the report viewable in a Google Drive folder <a href=""https://drive.google.com/drive/u/1/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP"">here</a>.</p><p>With that said, most of my focus so far has been on the high-level structure of the framework, so the particular quantitative estimates are very much in flux and many input parameters aren&apos;t pinned down well -- I wrote the bulk of this report before July and have received feedback since then that I haven&apos;t fully incorporated yet. I&apos;d prefer if people didn&apos;t share it widely in a low-bandwidth way (e.g., just posting key graphics on Facebook or Twitter) since the conclusions don&apos;t reflect Open Phil&apos;s &quot;institutional view&quot; yet, and there may well be some errors in the report.</p><p>The report includes a quantitative model written in Python. <a href=""https://ought.org/"">Ought</a>&#xA0;has worked with me to integrate their forecasting platform <a href=""https://elicit.ought.org/"">Elicit</a>&#xA0;into the model so that you can see other people&apos;s forecasts for various parameters. If you have questions or feedback about the Elicit integration, feel free to reach out to elicit@ought.org<a href=""elicit@ought.org."">.</a></p><p>Looking forward to hearing people&apos;s thoughts!</p>",ajeya-cotra,ajeya-cotra,Ajeya Cotra,
4QfAzgfnBTkBzSP2A,Link: Vitamin D Can Likely End the COVID-19 Pandemic - Rootclaim Blog,link-vitamin-d-can-likely-end-the-covid-19-pandemic,https://www.lesswrong.com/posts/4QfAzgfnBTkBzSP2A/link-vitamin-d-can-likely-end-the-covid-19-pandemic,2020-09-18T17:07:22.953Z,20,10,2,False,False,,"<p>This is a link post for:</p>
<ul>
<li><a href=""https://blog.rootclaim.com/vitamin-d-can-likely-end-the-covid-19-pandemic/"">Vitamin D Can Likely End the COVID-19 Pandemic - Rootclaim Blog</a></li>
</ul>
",Kenny,kenny,Kenny,
PC4yowA2TiRne69iD,Expansive translations: considerations and possibilities,expansive-translations-considerations-and-possibilities,https://www.lesswrong.com/posts/PC4yowA2TiRne69iD/expansive-translations-considerations-and-possibilities,2020-09-18T15:39:21.514Z,43,14,15,False,False,,"<figure style=""width:100%;""><img src=""https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1000&amp;q=80"" srcset=""https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=750&amp;q=80 750w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1050&amp;q=80 1050w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1350&amp;q=80 1350w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1500&amp;q=80 1500w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1650&amp;q=80 1650w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1950&amp;q=80 1950w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=2100&amp;q=80 2100w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=2250&amp;q=80 2250w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=2550&amp;q=80 2550w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=2700&amp;q=80 2700w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=2850&amp;q=80 2850w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=3150&amp;q=80 3150w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=3300&amp;q=80 3300w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=3450&amp;q=80 3450w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=3750&amp;q=80 3750w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=3900&amp;q=80 3900w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=4050&amp;q=80 4050w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=4350&amp;q=80 4350w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=4500&amp;q=80 4500w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=4650&amp;q=80 4650w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=4950&amp;q=80 4950w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=5100&amp;q=80 5100w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=5250&amp;q=80 5250w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=5550&amp;q=80 5550w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=5700&amp;q=80 5700w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=5850&amp;q=80 5850w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=6150&amp;q=80 6150w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=6300&amp;q=80 6300w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=6450&amp;q=80 6450w, https://images.unsplash.com/photo-1595550319243-110d7d100cb1?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=6468&amp;q=80 6468w""><figcaption>A crowd probably best served by a wide variety of translations</figcaption></figure><p><strong>TLDR:</strong> Language translation is a decent first step for written works, but the ideal looks more like an empathetic personal tutor. There’s a lot to do in-between, both in the near term with human labor, and the longer term with Machine Learning.</p><p><strong>Epistemic Status:</strong> I’m not an experienced researcher in this field. I’ve read a few audiobooks on language and thought about the area, but I’m sure I’m failing to reference many key papers and books. I’m fairly uncertain about all of this, I suggest taking my opinion very lightly (if at all) and thinking through the issue yourself. If you know of other materials I or other readers should know about, references would be appreciated.&nbsp;</p><p><strong>Feedback Preferences:</strong> I’m sure I’m wildly wrong on many things. Feedback is highly appreciated. I will take little offense on rude comments so go wild. That said, don’t expect long responses.</p><p>I’m not sure how to best write this, so I’ll divide things into a few vignettes.&nbsp;</p><hr><h2>Some rough statements which think are misguided:</h2><p><i>“Our book is available in 20 countries, so is accessible to 3 Billion people.”</i></p><p><i>“Once GPT-n can translate perfectly between languages, everyone will be able to communicate with each other”</i></p><p><i>“I’m not going to try to rephrase or re-explain this (technical) book, because you really should just read it directly”</i></p><p><i>“I don’t see why you wrote up those concepts, they were explained in more detail in a previous post”</i></p><h2>Retellings and their skeptics:</h2><p>There’s been an interesting trend recently of books that retell the ideas of other, older books. See:&nbsp;<br><a href=""https://www.goodreads.com/book/show/23420.How_Proust_Can_Change_Your_Life?from_search=true&amp;from_srp=true&amp;qid=1A4XDhPhbI&amp;rank=1""><i>How Proust Can Change Your Life</i></a><br><a href=""https://www.goodreads.com/book/show/20821053-how-adam-smith-can-change-your-life?from_search=true&amp;from_srp=true&amp;qid=WwDMDPAxzu&amp;rank=1""><i>How Adam Smith Can Change Your Life</i></a><br><a href=""https://www.goodreads.com/book/show/9859183-a-jane-austen-education?from_search=true&amp;from_srp=true&amp;qid=POQhywhIbR&amp;rank=1""><i>A Jane Austen Education: How Six Novels Taught Me About Love, Friendship, and the Things That Really Matter</i></a><br>And all the other books mentioned in <a href=""https://newrepublic.com/article/119367/literary-criticism-self-help-these-books-wont-change-your-life""><u>this post</u></a>.&nbsp;</p><p>I haven’t seen a better term for these, so I’ll refer to these as “<i>retellings</i>”.&nbsp;&nbsp;</p><p>More near to our community, <a href=""https://twitter.com/robertwiblin?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor"">Robert Wiblin</a> has recently posted <a href=""https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62""><u>a piece</u></a> on “Ugh fields”, which acts as a retelling of <a href=""https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields""><u>LessWrong posts</u></a> from 10 years ago.&nbsp;</p><p>To some audiences, these retellings are not only a waste of time, but lossy explanations of superior sources. What if readers stop at the retellings and skip the sources, and are left with false impressions? Clearly the solution is to point readers to the source and skip the in-between. Maybe add a minor amount of context, but to be fair, the people with the time to attempt this are generally not capable of doing a good enough job to not cause net harm.</p><p>But why not stop there? Language translation is also a lossy process. Not only are languages famously challenging to translate, but sometimes substantive modifications are introduced. The Spanish Harry Potter translation changed a pet from a frog to a turtle.[1] Perhaps the ideal is to ask that people learn all languages they are interested in reading content in, in order to ensure they do not make the mistakes of deceitful translations.</p><p>&lt;end strawmanning&gt;</p><h2>An expansive view of translation:</h2><p>I’m going to stop here and get to postulates:</p><p><strong>1. There’s a fuzzy line between language translation and retelling.&nbsp;</strong></p><p>Just because two people speak English doesn’t mean they think in the same words. There’s a whole lot that goes into retelling that’s different from modifying the language.</p><p><strong>2. There’s a fuzzy line between language translation and linguistic variety translation.</strong></p><p>Linguistics has the concept of <i>varieties</i>; languages are one type, but so are dialects, syles, and several other terms I wasn’t honestly previously familiar with before working on this piece. Just as there can be the translation of languages, it makes total sense to also have translation of these varieties as well. Google Translate already has support for a <a href=""https://www.theverge.com/2018/10/3/17932102/google-translate-difference-us-uk-english-dialects-languages-australia-india""><u>few specific dialects</u></a> as of now but stops there.</p><p><strong>3. There’s a fuzzy line between linguistic varieties, inferential distance, and worldviews.</strong></p><p>Even if a translation matches one’s exact preferred language, dialect, register, lexicon, and style, they could be left with a distance of inference (or education) and worldview. With regards to inferential distance, specific topics could be expanded upon or contracted for different audiences. With regards to worldview (my quick word for “comprehensive set of beliefs”), topics could be discussed that best fit with a given worldview, even if there is some level of exclusion. The topics could also be presented with evidence for how they fit into one’s worldview.</p><p><strong>4. Given that there are fuzzy lines between all of the above, it is reasonable to assume that translations on things other than “language” are quite reasonable.</strong></p><p>I think we consider “retellings” as equivalent to a liberal definition of “translation”. How Proust Can Change Your Life can be viewed as a translation of Proust’s work for a specific cluster of modern audiences. This would indicate that we may have far too few works like this, not too many. Perhaps we could use a “How Proust Changed My Life, as a 60th-percentile-in-Math 10th Grader in Saint Mill’s Academy.”[2]</p><p><strong>5. Even if the same effective message is recreated, there are aspects of its delivery that matter.</strong></p><p>If Bill Gates rewrote Superintelligence in his own words, it would be a big deal, even if the writing was just as effective as that of Superintelligence. The fact that Bill Gates both took the time to write such a work, and took the risk and opportunity cost of publicizing it, is a valuable signal. (This is the point I’m least excited about, but wanted to point it out for completeness)</p><h2>Why are modern translations so narrow?</h2><p>So, if translation makes sense outside of language translation, why do book publishers stop at language translation?&nbsp;</p><p>The obvious answer is cost, but I think some less obvious answers that are tradition and challenging categorization. It would seem <i>weird</i> to have a specific translation of Harry Potter where the characters all spoke in a specific Tumblr vernacular or where Harry Potter grew up with Amish parents. Shakespear’s plays use <a href=""https://en.wikipedia.org/wiki/Early_Modern_English"">Early Modern English</a>, but it would be <i>juvenile</i> for us to read them in Modern English. If Wikipedia were to attempt a new language geared for “analytical philosophers”, I’m sure any definition and separation would be met with a fair bit of controversy.</p><p>Things are changing. First, some of us are okay being weird if the costs are justified. Most novel ideas seem weird at first, but there are still groups pushing them forward.</p><p>Second, Machine Learning is progressing rapidly. It seems possible that if ML could succeed in language translation, it could later get to completely personalized translation. Imagine a system where when you land on a Wikipedia page, it translates it into a version optimized for you at that time. The examples change to things in your life, and any concepts difficult for you get explained in detail. It would be like a highly cognitively empathetic personal teacher.</p><h2>Expansive Translations and Power</h2><p>I think we can consider what I’m referring to as expansive translations, or “highly expansive translations”, which are distinct from narrow or language translations.</p><p>One intellectual criticism of expansive translations is that they could be used by powerful actors to manipulate culture in their favor. Expansive translation is a powerful tool and any power increase in malicious hands could produce disastrous outcomes. Perhaps The Crusades could have been avoided if religious figures weren’t allowed to stray from the original texts. Expansive translations <a href=""https://adventuretime.fandom.com/wiki/Censorship_of_Adventure_Time""><u>allow for censorship</u></a> when controlled by authorities. I think the crux here comes down to a more fundamental opinion on the potential of technology and intellectual progress. This gets messy, so I’m going to table this for now and return if there are readers who care about it.</p><p><strong>Grab Bag of Related Thoughts</strong></p><ul><li>There are probably thousands of books and what amounts to billions of hours of teaching to explain the same small sets of religious teachings. I.E. I’m sure one could come up with subsets of Christianity thought for which thousands of books and hours of local teaching (religious sermons and similar) were focused on. I imagine that similar educational endeavors should expect proportional costs.</li><li>Whenever any new fad comes to Silicon Valley, it seems like everyone has to re-explain it. Search “What is Bitcoin?” or “What is Intermittent Fasting?” for examples. I remember being amused at digital currency magazines that would include multiple articles to define Bitcoin, in the same magazine. This might appear wasteful, and I’m sure it often is, but it seems to serve a bunch of purposes that are hard to get around.</li><li>From above, the “Chesterton’s Fence” thing to do here seems to be “have lots of explanations for different audiences”. This could mean things like having articles explaining even seemingly simple concepts on LessWrong and the EA Forum, for those audiences. Perhaps we should have our own articles describing intermittent fasting or Bitcoin.</li><li>In popular media, communicators seem to specialize in audiences more than topics. A “golf magazine” really writes anything of interest to a specific “golf interested community”, rather than everything about golfing to a broad set of audiences.</li><li>If one has a message they want to be spread widely, it would be near impossible to personally advocate it to all of these groups as well as existing communicators do. It could be better to try to partner with or encourage the existing communicators.</li><li>CFAR used the term “murphyjitsu” instead of “<a href=""https://en.wikipedia.org/wiki/Pre-mortem""><u>pre-mortem</u></a>”, even though they are the same thing (I think). They knew this, but did it because “murphyjistu” was preferable to their community. This used to really annoy me because I was worried that this would further disconnect them from the literature, but I’ve grown to support the decision. As long as the link between the two is clear enough, the benefit of using a custom definition seems much greater than the costs.</li><li>Right now one of the greatest challenges to expansive translation seems that of poor terminological coordination and capabilities. For example, Ayn Rand originally wanted to use the name “existentialism” for her work, but later changed it to “objectivism” as “existentialism” was already taken.[3] Perhaps it would have been more ideal for her to call it “existentialism”, which would auto-translate to “existentialism(2)” when there could be sources of confusion.</li><li>I imagine it could be highly valuable to be able to experiment publicly with terminology. Right now definitional work that touches existing fields feels like touching on their toes, but lots of important terms are a mess between academic fields. It would be great to have experimenters iterate and test out a bunch of options in limited settings, but in a systematic and intentional way.</li><li>I’m a big fan of YouTube summaries. I’ve learned almost all of my knowledge from textbooks (which are summaries of other sources), Wikipedia, and other non-source teaching methods. Asking that people read all the original sources is not at all a scalable solution to growing fields.</li><li>All of the reinterpretations of Shakespear’s plays are other good examples of retellings. Maybe I should have started this post with those instead of those trite pop-lit examples.</li></ul><p><a href=""https://harrypotter.fandom.com/wiki/Trevor""><u>[1] https://harrypotter.fandom.com/wiki/Trevor</u></a></p><p>[2] This brings to mind “<a href=""https://www.goodreads.com/search?q=chicken+soup+for+the+soul&amp;qid="">Chicken soup for the X soul</a>”</p><p><a href=""[3] https://en.wikipedia.org/wiki/Ayn_Rand#Atlas_Shrugged_and_Objectivism"">[3] https://en.wikipedia.org/wiki/Ayn_Rand#Atlas_Shrugged_and_Objectivism</a></p><p>&nbsp;</p><p>Note: The image on the top is by Taylor Heery and was posted on Unsplash. &nbsp;Link <a href=""https://unsplash.com/photos/lp51P_MuYrQ"">here</a>. I used it because the New York subway system is what I think of when I imagine a bunch of people with very different backgrounds meeting each other. Most speak English, but are diverse in a wide variety of ways that could hypothetically use a large set of customized translations.</p>",ozziegooen,ozziegooen,ozziegooen,
qEjh8rpxjG4qGtfuK,"The ""Backchaining to Local Search"" Technique in AI Alignment",the-backchaining-to-local-search-technique-in-ai-alignment,https://www.lesswrong.com/posts/qEjh8rpxjG4qGtfuK/the-backchaining-to-local-search-technique-in-ai-alignment,2020-09-18T15:05:02.944Z,29,11,1,False,False,,"<p>In the spirit of this <u><a href=""https://www.alignmentforum.org/posts/7CJBiHYxebTmMfGs3/infinite-data-compute-arguments-in-alignment"">post</a></u> by John S. Wentworth, this is a reference for a technique I learned from Evan Hubinger. He&apos;s probably not the first to use it, but he introduced it to me, so he gets the credit.</p><p>In a single sentence, backchaining to local search is the idea of looking at how a problem of alignment could appear through local search (think gradient descent). So it starts with a certain problem (say reward tampering), and then tries to create a context where the usual training process in ML (local search) could create a system suffering from this problem. It&#x2019;s an instance of backchaining in general, which just looks for how a problem could appear in practice.</p><p>Backchaining to local search has two main benefits:</p><ul><li>It helps decide whether this specific problem is something we should worry about.</li><li>It forces you to consider your problem from a local search perspective, instead of the more intuitive human/adversarial perspective (how would I mess this up?).</li></ul><p>Let&apos;s look at a concrete example: reward gaming (also called specification gaming). To be even more concrete, we have a system with a camera and other sensors, and its goal is to maximize the amount of time when my friend Tom smiles, as measured through a loss function that captures whether the camera sees Tom smiling. The obvious (for us) way to do reward gaming here is to put a picture of Tom&#x2019;s smiling face in front of the camera -- then the loss function is minimized.</p><p>The backchaining to local search technique applied to this example asks &quot;How can I get this reward gaming behavior by local search?&quot; Well this reward gaming strategy is probably a local minima for the loss function (as changing just a little the behavior would increase the loss significantly), so local search could find it and stay in there. It&apos;s also better than most simple strategies, as ensuring that someone smiles (not necessarily a good goal, mind you) requires rather complex actions in the world (like going full &quot;Joker&quot; on someone, or changing someone&apos;s brain chemistry, or any other weird and impractical scheme). So there&apos;s probably a big zone in model space for which our specific example of reward gaming is the local minima.</p><p>All in all, the backchaining to local search technique tells us that this looks like a problem that should happen frequently in practice. Which lines up well with the evidence: see this<a href=""https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml""> <u>list</u></a> of reward gaming examples in the literature, and the corresponding<a href=""https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity""> <u>post</u></a>.</p><p>The last thing to point in such a reference post is how to interpret this technique. Because just like models, no technique applies to every situation. If you cannot backchain to local search from your alignment issue, it might mean one of these things.</p><ul><li>There is a good scenario and you just failed to find it.</li><li>There is no scenario, or it is very improbable. In that case, I would say that the question lies in whether there are settings in which scenarios do exist and are probable. If not, I personally would see that as saying that this problem could only happen in the case of a complete shift in learning algorithm, which might or might not be an assumption you&apos;re ready to accept.</li><li>It makes no sense to apply backchaining to your problem. For example, if you think about the issue of non-realizability in learning, you don&apos;t need backchaining to tell you that it&apos;s important in practice. And no model space ever used in practice will contain &quot;reality&quot;. So there&apos;s no point trying to backchain from this problem.</li></ul><p>That is, this technique assumes that your problem is a specific behavior of a trained system (like reward gaming), and that learning algorithms will not shift completely before we reach AGI. So it has close ties with the<a href=""https://www.alignmentforum.org/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment""> <u>prosaic AGI</u></a> approach to AI Safety.</p><p>In conclusion, when you encounter or consider a new alignment problem which talks about the specific behavior of the AI (compared to a general issue of theory, for example), backchaining to local search means trying to find a scenario where a system suffering from your alignment problem emerges from local search in some model space. If you put decent probability on the prosaic AGI idea, it should tell you something important about your alignment problem.</p>",adamShimi,adamshimi,adamShimi,
xR59ruh7aNcpPNrvv,Some thoughts on criticism,some-thoughts-on-criticism,https://www.lesswrong.com/posts/xR59ruh7aNcpPNrvv/some-thoughts-on-criticism,2020-09-18T04:58:37.042Z,90,48,11,False,False,,"<p>Here are some somewhat unconnected unconfident thoughts on criticism that I&#x2019;ve been thinking about recently.</p><p>---</p><p>A while ago, when I started having one-on-ones with people I was managing, I went into the meetings with a list of questions I was going to ask them. After the meetings, I&#x2019;d look at my notes and realize that almost all the value of the meeting came from the part where I asked them what the worst parts of their current work situation were and what the biggest mistakes I was making as a manager were.</p><p>I started thinking that almost the whole point of meetings like that is to get your report to feel comfortable giving an honest answer to those questions when you ask them--everything else you talk about is just buttering them up.</p><p>I wish I knew how to make people I&#x2019;m managing more enthusiastic about criticising me and telling me their insecurities. Maybe I could tell them to have a group chat with just them in it where they all had to name their biggest complaints about me? Maybe I should introduce them all to a former intern of mine who promised not to repeat anything they said who told them about all the mistakes I made while managing them, as an attempt to credibly signal actual interest?</p><p>---</p><p>A lot of the helpful criticism I&#x2019;ve gotten over the last few years was from people who were being kind of unreasonable and unfair.</p><p>One simple example of this is that one time someone (who I&#x2019;d engaged with for many hours) told me he didn&#x2019;t take my ideas seriously because I had blue hair. On the one hand, fuck that guy; on the other hand, it&#x2019;s pretty helpful that he told me that, and I&apos;m grateful to him for telling me. You&#x2019;d think that being on the internet would expose you to all the relatively uninformed impolite criticism you&#x2019;d possibly need, but in my experience this isn&#x2019;t true.</p><p>Additionally I think that when people are annoyed at you, they look harder for mistakes you&#x2019;re making and they speak more frankly about them. So it&#x2019;s sometimes actually more likely that people will give you useful criticism if they get unreasonably annoyed at you first. This goes especially for people who know you and understand you well. (This is also a reason to think it&#x2019;s probably helpful to sometimes get drunk around people you like a lot but don&#x2019;t totally see eye to eye with. I haven&#x2019;t actually experimented with this.)</p><p>I think I&#x2019;ve often been insufficiently gracious about receiving criticism in this kind of case, which seems pretty foolish of me. This is even more foolish because I&#x2019;ve often behaved this way in contexts where I had more social power than the person who was criticizing me. I wish that I basically always responded to criticism from people I don&#x2019;t know extremely well by saying &#x201C;that&#x2019;s interesting, thanks for telling me you think that, I&#x2019;ll think about it.&#x201D; I&#x2019;m working on it.</p><p>---</p><p>[epistemic status: I&#x2019;m a little worried about this kind of amateur psychology speculation]</p><p>I think one of the central things that&#x2019;s hard about criticism is that people often tie their identities to being good at various things, and it&#x2019;s hard to predict exactly which way they do this and so it&#x2019;s tricky to know what criticism will deeply hurt them. For example, I think people often have pretty core beliefs like &#x201C;I&#x2019;m not that good at X and Y, but at least I can hold onto the fact that I&#x2019;m good at Z&#x201D;. Often, people like that will respond well to criticism about X and Y but not about Z. The problem is that it&#x2019;s kind of hard to guess which things are in which category for someone.</p><p>I think it&#x2019;s really really hard to be actually entirely open to criticism, and I don&#x2019;t know if it&#x2019;s even a good idea for most people to try to strive for it.</p><p>I think that if you tell people that it&#x2019;s extremely virtuous to be open to deep criticism, they sometimes just become really good at not listening to or understanding criticism (like described here <a href=""https://www.lesswrong.com/posts/byewoxJiAfwE6zpep/reality-revealing-and-reality-masking-puzzles#Disorientation_patterns"">https://www.lesswrong.com/posts/byewoxJiAfwE6zpep/reality-revealing-and-reality-masking-puzzles#Disorientation_patterns</a>).</p><p>A lot of the time, one of my biggest bottlenecks is that I&#x2019;m not feeling secure enough to be properly open to criticism. This means both that I can&#x2019;t properly criticise myself and that other people correctly conclude that they shouldn&#x2019;t criticise me (and these people don&#x2019;t even look as hard as they could for my weaknesses).</p><p>For example, this is true right now as I&#x2019;m writing this. If I imagine getting an email from someone who I deeply admire where they&#x2019;d written up their thoughts on the biggest mistakes I was making, I feel like I&#x2019;d put off opening it, because I feel fragile enough that I&#x2019;d worry that reading it would crush me and make me feel useless and depressed and unable to do the things I do. And when I go to a whiteboard and try to make lists of the most likely ways that I&#x2019;m currently making big mistakes, I feel like I intuitively flinch away from looking directly at the question.</p><p>My guess is that this is true of most people most of the time.</p><p>I think that this is a major mechanism via which I&#x2019;m less productive when I&#x2019;m less happy--I&#x2019;m less able to ask myself whether I&#x2019;m really working on the most important problem right now.</p><p>Even in this state it&#x2019;s pretty useful to get criticism from people, because they manage to do a pretty good job of filtering the criticism to be not too core to who you are as a person.</p><p>I definitely wouldn&#x2019;t want anyone reading this to criticize me less as a result of reading this post.</p><p>---</p><p>One way of getting better criticism is to come up with a list of things you think you might be doing wrong, then ask specifically about them. This both credibly signals that you&#x2019;re actually interested in criticism, and also communicates that that topic isn&#x2019;t one of your weak points.</p><p>I think that it&#x2019;s probably generally more helpful to come up with a list of twenty mistakes you&#x2019;re most likely to be making, and then circulate an anonymous survey where people check the ones they think you&#x2019;re indeed making, rather than to circulate an open ended criticism form.</p><p>----</p><p>I recently heard about someone who I&#x2019;ve spent between 10 and 100 hours talking to doing something related to their career that looked to me and to many of my friends like a blunder. I don&#x2019;t think any of us told that person that we thought they&#x2019;d fucked up. This was partially because it seemed like they&#x2019;d already made the decision, and in my case it was because I had only heard about this indirectly and it felt a bit weird to reach out to someone to say that I&#x2019;d heard they&#x2019;d done something that I thought was dumb. It still feels a bit sad.</p><p>If you message me asking for it, I&#x2019;ll tell you if I think you&#x2019;re doing something that looks like it&#x2019;s plausibly a bad mistake with your career or life at the moment. I can only think of a few people where my answer would be yes.</p><p>---</p><p>I think that thinking of yourself as better than other people is, in some ways, helpful for being more pleasant to talk to. I&#x2019;ve basically never heard anyone make this point directly before.</p><p>One context in which I&#x2019;m often unpleasant is when someone&#x2019;s saying something I strongly disagree with in a way I dislike, and I lash out aggressively and unhelpfully. I think this is because I feel threatened--I think I intuitively feel like it&#x2019;s really important for the people in the conversation to see me win the argument, so that they think that I&#x2019;m smart and right.</p><p>If I felt more secure and more superior to the people in the conversation, I think it would be easier to behave better, because I&#x2019;d feel more like I was proposing some ideas and then seeing if the people I was talking to were interested in them, and then inasmuch as they weren&#x2019;t, I&#x2019;d shrug and give up and quietly update against those people.</p><p>I have this attitude much less than a lot of the people I know. I think this makes them better than me at being pleasant.</p><p>However, I think that feeling more insecure makes it somewhat easier to connect with people, because it means that my heart is more on my sleeve and I can engage with their disagreements more wholeheartedly and openly, and this makes people more comfortable about having some kinds of conversations with me.</p><p>Probably there&#x2019;s a happy medium here that is better than my current attitude.</p>",Buck,buck,Buck,
F8iTtzSgxRmmcCrWE,"For what X would you be indifferent between living X days, but forgetting your day at the end of everyday, or living 10 days? (terminally)",for-what-x-would-you-be-indifferent-between-living-x-days,https://www.lesswrong.com/posts/F8iTtzSgxRmmcCrWE/for-what-x-would-you-be-indifferent-between-living-x-days,2020-09-18T04:05:59.078Z,6,4,11,False,True,,"<p>terminally meaning in and of itself, as oppose to instrumentally meaning as a mean to other ends</p>",MathieuRoy,mathieuroy,Mati_Roy,
AjgiA6s3TEfxbndyj,Against Victimhood,against-victimhood,https://www.lesswrong.com/posts/AjgiA6s3TEfxbndyj/against-victimhood,2020-09-18T01:58:31.041Z,85,41,15,False,False,,"<p>Cross-posted, as always, from <a href=""https://putanumonit.com/2020/09/12/against-victimhood/"">Putanumonit</a>.</p><hr><p>I have written many posts in the shape of <a href=""https://putanumonit.com/2019/12/30/100-ways-to-live-better/""><u>giving life advice</u></a>.&nbsp;I hear back from readers who take it and those who refuse it. Either is good — I’m just a guy on the internet, to be consumed as part of a balanced diet of opinions.</p><p>But <a href=""https://putanumonit.com/2020/01/13/go-f-someone/#comments""><u>occasionally</u></a> I hear: <i>who are you to give life advice, your own life is so perfect! </i>This sounds strange at first. If you think I’ve got life figured out, wouldn’t you want my advice? I think what they mean is that I haven’t had to overcome the hardships they have, hostile people and adverse circumstances.</p><p>I talk quite often about things that are going poorly for me, but only from the point of view of <a href=""https://putanumonit.com/2020/08/20/working-hardly/""><u>how </u><i><u>I</u></i><u> fucked up</u></a>. I avoid talking about being wronged, oppressed, attacked, discriminated against, or victimized. If you assume that it’s because I live a charmed life where none of these things happen, you may need a refresher on the <a href=""https://en.wikipedia.org/wiki/Base_rate_fallacy""><u>base rate fallacy</u></a>.</p><p>The reason I never talk about being a victim is that I’m extremely averse to <a href=""https://en.wikipedia.org/wiki/Victim_mentality""><u>victim mentality</u></a>. It’s an insidious mindset, one that’s self-reinforcing both internally and by outside feedback. I’ve seen it claim people, groups, entire nations. On the flip side, I’ve noticed that the less often I think of myself as a victim the less I am victimized, which in turn makes that mindset even rarer. If I do feel on occasion that I have been harmed through no fault of my own by hostile actors I keep it to myself. This is a bad time to be a victim on the internet.</p><hr><p>What’s bad about victim mentality? Most obviously, inhabiting a narrative where the world has committed a great injustice against which you are helpless against is extremely distressing. Whether the narrative is justified or not, it causes suffering.</p><p>See yourself as a victim prevents you from improving your situation by your own power, since doing so will contradict the story of your own helplessness. In particular that’s true of the story you tell <i>yourself</i>. That story is your <a href=""https://www.ribbonfarm.com/2019/10/02/predictable-identities-18-self-consistency/""><u>identity</u></a>, how your mind predicts your future actions. If your story is helpless victimhood your mind will refuse to help — it wants to be vindicated more than it wants to do better.</p><p>When I was young I was a weird nerd, and while that maybe hasn’t changed much I do find myself in social circles where weird nerdiness is welcome. In school it was not very welcome, and people weren’t nice to me. But I never really fell into thinking that I was singled out for abuse, mostly because I reasoned that if my classmates really wanted to abuse me they could do so much more than they have. I could come up with very creative ways to bully someone, and no one in my school seemed equally creative. I learned to avoid the worst people and slowly make friends with the rest.</p><p>Avoiding bad things is a usually a great tactic, but it’s not available to victims. Avoiding the victimizer makes it hard to sustain the story of victimhood. It also leaves behind the lingering residue of <i>injustice</i>, knowing that the culprit did not get their comeuppance. That sense of injustice often haunts the victim long after they’re safe from the original source of harm.</p><p>Most importantly, victim mentality leaves no room for empathy. Victims can’t see anyone else’s struggles or suffering, especially those of their perceived victimizers.</p><p>When I write about dating I get replies from young men who feel maligned and victimized by women. They complain about impossible standards, ambiguous behavior, and dating norms as if these were setup on purpose to immiserate them. When I was younger and finally managed to reject this line of thinking for myself I started understanding women’s own difficulties, fears, and frustrations with dating, the real issues behind their <a href=""https://putanumonit.com/2016/03/23/20_nice_guys/""><u>seemingly-unfair complaints about men</u></a>. That’s when my dating life improved dramatically.</p><p>People in general don’t like victims, and they certainly don’t want to date them.</p><hr><p>Why do people claim victimhood despite the drawbacks? It makes sense in a small group where people know each other and reputations are tracked. The group will band to punish the perpetrator and offer the victim restitution, knowing that this will redound to them in turn.</p><p>But in the world at large, and especially <a href=""https://www.ribbonfarm.com/2020/01/16/the-internet-of-beefs/""><u>on the internet of beefs</u></a>, there are a lot more punishers than restitutionists. Claiming publicly that you’ve been victimized by X will immediately attract everyone with an axe to grind against X. Any pure souls trying to help will get swallowed up by the sheer number and energy of anti-Xers.</p><p>The anti-Xers have a vested interest in your continued victimization by X. Nothing is more detrimental to their cause than X’s victims making peace with X on their own terms. Victimhood-mongering provides purpose and gainful employment to countless individuals. The victims end up doubly helpless and doubly beholden: both to their oppressors and their “liberators”.</p><hr><p>Global recognition of one’s victimhood is pretty much the worst thing that can happen to anyone. It happened to the Palestinians.</p><p>The United Nations agrees that Palestinians are victims. That’s why in addition to <a href=""https://en.wikipedia.org/wiki/United_Nations_High_Commissioner_for_Refugees""><u>UNHCR</u></a>, the UN agency to support refugees worldwide, it has a special agency dedicated to Palestinian refugees: <a href=""https://en.wikipedia.org/wiki/UNRWA""><u>UNRWA</u></a>. UNRWA differs from UNHCR in two main ways:</p><ol><li>It does not share UNHCR’s mandate to “assist refugees in eliminating their refugee status by local integration in the current country, resettlement in a third country or repatriation”, thus keeping Palestinianss and their descendants in refugee status for perpetuity.</li><li>It employs twice the number of staff.</li></ol><p>Muslim leaders from Tunis to Kuala Lumpur agree that Palestinians are victims. They need to because it plays well for public opinion and allows them to maintain an anti-Israel stance in the absence, for most of them, of any actual conflict with Israel. Until there’s something serious at stake like foreign investments or a weapons deal, that is, and then they sign a deal with Israel and <a href=""https://www.trtworld.com/magazine/apologise-for-slamming-the-uae-over-israel-ties-the-gcc-tells-palestinians-39581""><u>tell the Palestinians to shut up and stop complaining</u></a>.</p><figure><img src=""https://putanumonit.files.wordpress.com/2020/09/image.png?w=1024"" srcset=""https://putanumonit.files.wordpress.com/2020/09/image.png?w=1024 1024w, https://putanumonit.files.wordpress.com/2020/09/image.png?w=150 150w, https://putanumonit.files.wordpress.com/2020/09/image.png?w=300 300w, https://putanumonit.files.wordpress.com/2020/09/image.png?w=768 768w, https://putanumonit.files.wordpress.com/2020/09/image.png 1425w""></figure><p>Many Americans agree that Palestinians are victims, especially those on college campuses. Shortly after I arrived on campus in the US I was invited to a “conversation” about the <a href=""https://en.wikipedia.org/wiki/2012_Israeli_operation_in_the_Gaza_Strip""><u>ongoing operation in Gaza</u></a> with a left-leaning Jewish student group.</p><p>I asked them whether they though the killing of Hamas military chief <a href=""https://en.wikipedia.org/wiki/Ahmed_Jabari""><u>Ahmed Jabari</u></a>, which set off the operation, was justified. None of the 15 students in attendance knew who he was. Most of them couldn’t tell Hamas from the PLO, conflated the situation in Gaza with the settlements in the West Bank, and had little knowledge or interest in actual matters of Palestinian life and governance such as elections, security arrangements, water and energy supply, etc.</p><p>I realized that the vast majority of them joined this organization to establish their progressive bona fides and differentiate themselves from Jewish conservatives. Israel-Palestine is <a href=""https://crookedtimber.org/2013/12/24/the-biggest-game-in-town/""><u>the biggest game in town</u></a>, but it probably could just as well be male circumcision or female rabbis.</p><p>I want to make it clear — I don’t particularly disagree that Palestinians are victims in many senses and that their ability to help themselves is constrained by outside forces, chief among which is Israel. I have real compassion for them. I just want to note that decades of global recognition of Palestinian victimhood have been a boon for UN staffers, Muslim politicians, and American progressives, along with many others. Surely such a broad and powerful coalition would bring Palestine peace and prosperity and an end to victimhood?</p><p>Shockingly, it hasn’t.</p><p>These situations apply mostly to identifiable groups, examples of which are abundant, but it can happen to individuals too. In families, schools, organizations there are people who like playing the savior role, and they have a sharp nose for victims in need.</p><hr><p>Isn’t this all <i>victim blaming</i>? This is a reasonable objection, although I have some issues with the concept itself and its provenance. Here’s the headline of the Wikipedia article on victim blaming:</p><blockquote><p>Victim blaming&nbsp;occurs when the victim of a&nbsp;crime&nbsp;or any wrongful act is held entirely or partially at fault for the harm that befell them.&nbsp;The study of&nbsp;<a href=""https://en.wikipedia.org/wiki/Victimology""><u>victimology</u></a>&nbsp;seeks to mitigate the prejudice against victims, and the perception that victims are in any way responsible for the actions of offenders.</p></blockquote><p>Equating the perception that victims have even a modicum of responsibility with <i>prejudice</i> gives up the game before it started. With this axiom in place “victimology” is not a field of inquiry, it’s a tool of advocacy to be used in competitions for victimhood status. These competitions have many losers and no winners.</p><p>But the concept of victim blaming as it’s naively understood still has value and needs to be addressed. To do so we need to clarify two distinctions.</p><p>The first difference is between being a victim in a particular instance and victimhood as an ongoing story. When you are the target of a crime you are a victim — you can appeal to an authority for help. You tell mom your older sister stole your toy and mom makes her return it.</p><p>This is entirely different from saying decades later that your career failures are a result of being victimized by your sister as a child. Even if the causal effect of your sister’s depredation is not literally zero, it has probably done less harm than the narrative of victimhood itself.</p><p>When I arrived in New York with little money I promptly lost $3,000 to a rental scam. If the cops had apprehended the scammer I would have confirmed that yes, I was their victim, and would have demanded the full amount back. But my main reaction was to analyze the situation, read up on similar scams, and build a plan for the future that will prevent me from falling for those again. I don’t even see it as an injustice — $3,000 is a fair price to pay for a class in scam resilience.</p><p>The second, related distinction is between public status and individual mindset. <i>Blame</i> is at its core a social concept, used to coordinate how we allocate responsibility and demand atonement as a group. We could very want for society to direct those at the perpetrator, and simultaneously advise the victim to hold themselves privately accountable at least in part.</p><p>The problem here is that every public discussion of the issue is perceived as mostly an attempt to shape the reaction of society, rather than the attitude of the victim. I care more about the latter, which is why this entire essay avoided talking about individual victims and how they could improve. The main way to change individual mindset it to talk about your <i>own</i> experience, which is also what I would encourage you to do in the comments. (I will heavily moderate less-than-perfectly-charitable discussion of the behavior of actual victims and all sides of the Israel-Palestine conflict.)</p><hr><p>In conclusion we must address <i>privilege</i>. It’s sure easy for someone who is safe from oppression to talk about the pitfalls of victim mentality; not so for the victim!</p><p>My first answer is that there is a strong bias in favor of overstating victimhood that needs to be corrected. This bias is caused by all people I discussed who benefit from being seen as protectors of victims. This bias is especially powerful in the United States, where victimhood is more and more allocated on the basis of group identity (which is enduring and political) instead of individual circumstance (which can be helped and overcome). If we lived in ancient Sparta, I would be giving the <a href=""https://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/""><u>reverse advice</u></a> instead.</p><p><a href=""https://www.thecoddling.com/""><u>Victim mentality is manufactured en masse</u></a> by the American education system. It can only be resisted by <i>individual </i>efforts to reject it for yourself, in the privacy of your own blog-browsing.</p><p>But yes — I <i>am</i> privileged. In my nationality, my social and professional status, and more. But for all of those I, or the group that I’m part of, decided not to be victims and to take responsibility even when the former option was on the table. Again, I will not go into detail about <i>how</i> the option of victimhood was available, since merely talking about it is claiming it.</p><p>I don’t care if I miss out on the desirable status of <i>having overcome great adversity™</i>. It’s a currency that certainly has its value. But it’s not worth the price it demands.</p><p>Victimhood is a vicious cycle. It leads to helplessness which leads to victimization which leads to external recognition of victimhood which in turn leads to helplessness and so on down the spiral. Rejecting victimhood works the same way, small decisions that compound until one reaches escape velocity.</p><p>Not being a victim is indeed a privilege. With time and a change of mindset, this privilege can be yours too.</p>",Jacobian,jacob-falkovich,Jacob Falkovich,
MGcJ7JXLCPMX8fApF,Objective Dog Ratings: The Shiba Inu,objective-dog-ratings-the-shiba-inu,https://www.lesswrong.com/posts/MGcJ7JXLCPMX8fApF/objective-dog-ratings-the-shiba-inu,2020-09-18T00:12:11.328Z,15,10,2,False,False,,"<span><figure><img src=""https://wmbsaltworks.files.wordpress.com/2020/05/shiba_inu_at_the_entrance_of_a_house_-japan_2010_a.jpg?w=621&amp;h=466"" class=""draft-image "" style=""width:40%""></figure></span><br><h1>The Shiba Inu</h1><p>Shibas are a spitz dog, which, if you remember from the <a href=""https://www.lesswrong.com/s/SyNM8S9FbJjWyf3D6/p/uAYz5jMHPt4xgXwr5"">original Dog Ratings post</a>, is my projected winner for the All-Dog All-Stars Contest, but that doesn&apos;t mean that&#xA0;<em>this</em> spitz will take the trophy home.</p><p>Let&apos;s take a look: </p><p>First, it must be noted that shibas have the curly tail which is most distinctive of domesticated animals, which is a Good Thing. Props to you, noble shibas.</p><p>Behaviorally, shibas are very clean animals. They clean themselves, they&apos;re very easy to housebreak, and...well, they&#xA0;<em>will</em> track mud into the house and not wipe their feet off first, but they&apos;re still dogs. You can only expect so much from them. They&apos;re clever, but <a href=""https://thesmartcanine.com/are-shiba-inus-smart/"">the jury is out on just how much so</a>.</p><p>Shibas do not bark very much, instead preferring to emit terrible, nearly goat-like noise called a <a href=""https://web.archive.org/web/20070810225914/http://www.northeastshibarescue.com/traits.html"">&quot;shiba scream,&quot;</a> which has been described as <a href=""https://myfirstshiba.com/what-is-a-shiba-inu-scream/"">&quot;bloodcurdling&quot;</a> and like the <a href=""https://www.youtube.com/watch?v=BhgFe6b8Nek"">&quot;screams of the damned.&quot;</a> The name sounds like an anime attack move, and I wholeheartedly approve.</p><p>I could not find much in the way of <a href=""https://drferox.tumblr.com/post/158340268583/hi-dr-ferox-off-the-back-of-your-post-about"">terrible diseases</a>, which is a little surprising given their history. Shibas were extensively crossbred with other dogs in the 19th century and, between crossbreeding, distemper outbreaks, and WWII food shortages, only three &quot;bloodlines&quot; or &quot;variations&quot; of shiba still remained in the post-war era. It&apos;s hard for me to get details on this, so I can only assume that the gene pools were relatively large and healthy or the breeders did a good job, because, as I say, they aren&apos;t a tragedy of inbreeding.</p><p>This is, admittedly, a little bit conjecture, but I imagine that their healthiness has something to do with <a href=""https://web.archive.org/web/20180516180104/https://www.jkc.or.jp/modules/worlddogs/entry.php?entryID=105&amp;categoryID=5"">the story of their preservation</a> as a breed. Shibas were bred to do a job (hunting and flushing out small game and, on occasion, boar), and hunters were among those who tried to preserve them.</p><p>Anyway, the point is, they are pretty healthy dogs, and that&apos;s pretty important here at Objective Dog Ratings. Shibas aren&apos;t exceptionally intelligent, but there&apos;s something awful and awesome about their unique vocal talents, so I&apos;m going to award them a very tentative, by-the-tip-of-their-nose, three stars. A forward-thinking, politically progressive wolf would not be ashamed to know a shiba, though they might well be a little unsettled.</p><p><strong>Rating: &#x2605;&#x2605;&#x2605; </strong><em>out of</em><strong> &#x2605;&#x2605;&#x2605; (Good)</strong></p><p>Original post (with more pictures) <a href=""https://wmbsaltworks.wordpress.com/2020/05/02/objective-dog-ratings-shiba-inu/#more-470"">here</a>.</p>",Callmesalticidae,callmesalticidae,Callmesalticidae,
mRsm8gibyuPWas7K5,"Gems from the Wiki: Do The Math, Then Burn The Math and Go With Your Gut",gems-from-the-wiki-do-the-math-then-burn-the-math-and-go,https://www.lesswrong.com/posts/mRsm8gibyuPWas7K5/gems-from-the-wiki-do-the-math-then-burn-the-math-and-go,2020-09-17T22:41:24.097Z,60,22,3,False,False,https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut,"<p><i>During the </i><a href=""https://www.lesswrong.com/posts/ELN6FpRLoeLJPgx8z/the-wiki-is-dead-long-live-the-wiki-help-wanted""><i>LessWrong 1.0 Wiki Import</i></a><i> we (the LessWrong team) discovered a number of great articles that most of the LessWrong team hadn't read before. Since we expect many others to also not have have read these, we are creating a series of the best posts from the Wiki to help give those hidden gems some more time to shine.</i></p><p><i>The original wiki article was fully written by </i><a href=""https://www.lesswrong.com/users/riceissa""><i>riceissa</i></a><i>, who I've added as a coauthor to this post. Thank you for your work on the wiki!</i></p><hr><p>""<strong>Do the math, then burn the math and go with your gut</strong>""<a href=""https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fn1""><sup>1</sup></a> is a procedure for decision-making that has been described by <a href=""https://www.lesswrong.com/tag/eliezer-yudkowsky"">Eliezer Yudkowsky</a>. The basic procedure is to go through the process of assigning numbers and probabilities that are relevant to some decision (""do the math"") and then to throw away this calculation and instead make the final decision with one's gut feelings (""burn the math and go with your gut""). The purpose of the first step is to force oneself to think through all the details of the decision and to spot inconsistencies.</p><h2>History</h2><p>In July 2008, Eliezer Yudkowsky wrote the blog post ""When (Not) To Use Probabilities"", which discusses the situations under which it is a bad idea to verbally assign probabilities. Specifically, the post claims that while theoretical arguments in favor of using probabilities (such as <a href=""https://en.wikipedia.org/wiki/Dutch_book"">Dutch book</a> and <a href=""https://en.wikipedia.org/wiki/Coherence_(philosophical_gambling_strategy)"">coherence</a> arguments) always apply, humans have evolved algorithms for reasoning under uncertainty that don't involve verbally assigning probabilities (such as using ""gut feelings""), which in practice often perform better than actually assigning probabilities. In other words, the post argues in favor of using humans' non-verbal/built-in forms of reasoning under uncertainty even if this makes humans incoherent/subject to Dutch books, because forcing humans to articulate probabilities would actually lead to worse outcomes. The post also contains the quote ""there <i>are</i> benefits from trying to translate your gut feelings of uncertainty into verbal probabilities. It may help you spot problems like the conjunction fallacy. It may help you spot internal inconsistencies – though it may not show you any way to remedy them.""<a href=""https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fn2""><sup>2</sup></a></p><p>In October 2011, LessWrong user bentarm gave an outline of the procedure in a comment in the context of the <a href=""https://en.wikipedia.org/wiki/Murder_of_Meredith_Kercher"">Amanda Knox case</a>. The steps were: ""(1) write down a list of all of the relevant facts on either side of the argument. (2) assign numerical weights to each of the facts, according to how much they point you in one direction or another. (3) burn the piece of paper on which you wrote down the facts, and go with your gut."" This description was endorsed by Yudkowsky in a follow-up comment. bentarm's comment claims that Yudkowsky described the procedure during summer of 2011.<a href=""https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fn3""><sup>3</sup></a></p><p>In December 2016, <a href=""https://www.lesswrong.com/tag/anna-salamon"">Anna Salamon</a> described the procedure parenthetically at the end of a blog post. Salamon described the procedure as follows: ""Eliezer once described what I take to be the a similar ritual for avoiding bucket errors, as follows: When deciding which apartment to rent (he said), one should first do out the math, and estimate the number of dollars each would cost, the number of minutes of commute time times the rate at which one values one's time, and so on. But at the end of the day, if the math says the wrong thing, one should do the right thing anyway.""<a href=""https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fn4""><sup>4</sup></a></p><h2>See also</h2><ul><li><a href=""https://www.lesswrong.com/tag/cfar-exercise-prize"">CFAR Exercise Prize</a> – Andrew Critch's Bayes game, described on this page, gives another technique for dealing with uncertainty in real-life situations</li></ul><h2>External links</h2><ul><li><a href=""https://www.facebook.com/julia.galef/posts/10103884948339182"">A Facebook post by Julia Galef from May 2018 inquiring about this procedure</a></li><li><a href=""https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/"">""Why we can’t take expected value estimates literally (even when they’re unbiased)""</a> (August 2011) by <a href=""https://www.lesswrong.com/tag/givewell"">GiveWell</a> co-founder <a href=""https://www.lesswrong.com/tag/holden-karnofsky"">Holden Karnofsky</a> makes a similar point: ""It’s my view that my brain instinctively processes huge amounts of information, coming from many different reference classes, and arrives at a prior; if I attempt to formalize my prior, counting only what I can name and justify, I can worsen the accuracy a lot relative to going with my gut. Of course there is a problem here: going with one’s gut can be an excuse for going with what one wants to believe, and a lot of what enters into my gut belief could be irrelevant to proper Bayesian analysis. There is an appeal to formulas, which is that they seem to be susceptible to outsiders’ checking them for fairness and consistency.""</li><li><a href=""https://confusopoly.com/2019/04/03/the-optimizers-curse-wrong-way-reductions/"">""The Optimizer’s Curse &amp; Wrong-Way Reductions""</a> by Christian Smith discusses similar issues</li><li><a href=""https://en.wikipedia.org/wiki/Verbal_overshadowing"">Verbal overshadowing</a> page on Wikipedia</li></ul><hr><ol><li>Qiaochu Yuan. <a href=""https://www.lesswrong.com/posts/yeADMcScw8EW9yxpH/a-sketch-of-good-communication/comment/puSysxsCG2i98XHMW"">""Qiaochu_Yuan comments on A Sketch of Good Communication""</a>. March 31, 2018. <i>LessWrong</i>.<a href=""https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fnref1"">↩</a></li><li>Eliezer Yudkowsky. <a href=""https://www.lesswrong.com/posts/AJ9dX59QXokZb35fk/when-not-to-use-probabilities"">""When (Not) To Use Probabilities""</a>. July 23, 2008. <i>LessWrong</i>.<a href=""https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fnref2"">↩</a></li><li>bentarm. <a href=""https://www.lesswrong.com/posts/Jx4gGbPi7GuydwvzB/amanda-knox-post-mortem/comment/aFe8RxLnH3JWtNJcD"">""bentarm comments on Amanda Knox: post mortem""</a>. October 21, 2011. <i>LessWrong</i>.<a href=""https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fnref3"">↩</a></li><li>Anna Salamon. <a href=""https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the"">""'Flinching away from truth' is often about *protecting* the epistemology""</a>. December 20, 2016. <i>LessWrong</i>.<a href=""https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fnref4"">↩</a></li></ol>",habryka4,habryka4,habryka,
e5QhDt8RD5d3ru3xR,Let the AI teach you how to flirt,let-the-ai-teach-you-how-to-flirt,https://www.lesswrong.com/posts/e5QhDt8RD5d3ru3xR/let-the-ai-teach-you-how-to-flirt,2020-09-17T19:04:05.632Z,49,28,11,False,False,,"<p>""It's Not You, it's Me: Detecting Flirting and its Misperception in Speed-Dates"" is a fascinating approach to the study of flirtation. It uses a machine learning model to parse speed-dating data and detect whether the participants were flirting. Here's a <a href=""https://www.aclweb.org/anthology/D09-1035.pdf"">sci-hub link</a>. I found three key insights in the paper.</p><p>First of all, people basically assume that others share their own intentions. If they were flirting, they assume their partner was too. They're quite bad at guessing whether their partner was flirting, but they do a bit better than chance.</p><p>Secondly, the machine learning model was about 70% accurate in detecting flirtation. It's much better than the speed date participants themselves, despite having far less information to draw upon and the fact that the authors used a more forgiving standard of success for people's detection rates than for the detection rates of the machine learning model.</p><p>Thirdly, storytelling and conversations about friends seem to be the strongest signals of flirtation. Talking about the mundane details of student life (this was on a college campus) were the strongest signals of non-flirtation.</p><p>Finally, men and women have quite different approaches to flirtation:</p><blockquote><p>Men who say they are flirting ask more questions, and use more you and we. They laugh more, and use more sexual, anger [<i>hate/hated, hell, ridiculous, stupid, kill, screwed, blame, sucks, mad, bother, shit</i>], and negative [<i>bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry</i>] emotional words. Prosodically they speak faster, with higher pitch, but quieter (lower intensity min). Features of the alter (the woman) that helped our system detect men who say they are flirting include the woman’s laughing, sexual words [<i>love, passion, virgin, sex, screw</i>] or swear words, talking more, and having a higher f0 (max).</p><p>Women who say they are flirting have a much expanded pitch range (lower pitch min, higher pitch max), laugh more, use more I and well, use repair questions [<i>Wait, Excuse me</i>] but not other kinds of questions, use more sexual terms, use far less appreciations [<i>Wow, That’s true, Oh, great</i>] and backchannels [<i>Uh-huh., Yeah., Right., Oh, okay.</i>], and use fewer, longer turns, with more words in general. Features of the alter (the man) that helped our system detect women who say they are flirting include the male use of you, questions, and faster and quieter speech.</p></blockquote><p>This paper has changed the way I think about skillful heterosexual flirtation. I used to think that flirting was a unisex behavior, and that men and women were decently skilled at detecting it. In much the same way that it's harder to write a novel than to read one, I thought that the hard part was signalling your own intentions, not interpreting theirs.</p><p>Now, I think that a strategy for skillful flirtation is to get the other person to broadcast <i>their</i> intentions, and learn to interpret their signals correctly. Men and women have different flirting styles. Each person knows when they themselves are trying to flirt. But they're bad at guessing when their partner is trying to flirt. This suggests that if you can get your partner to engage in their own natural flirting style, and get good at detecting it, then you can guess their intentions with much more confidence than the average person is capable of.</p><p><strong>Both men and women</strong> should try to make each other laugh, let their voices be more musical, and provoke each other to talk about love and sex. They should tell stories about their lives and friendships and try to avoid mundane details.</p><p><strong>A man who wants to signal flirtation to a woman</strong> should ask lots of questions that provoke the woman to talk about herself at length. Note that the ""appreciations"" and ""backchannels"" that are negatively correlated with women's flirtation are responses that women tend to give to men who keep going on about themselves. This is the old standard advice.</p><p><strong>A woman who wants to signal flirtation to a man</strong> should maybe find topics they can complain about together - hopefully in a lighthearted way. She could also talk about her life in such a way that it provokes him to be curious and ask questions about her or observe connections between himself and her.</p>",AllAmericanBreakfast,directedevolution,DirectedEvolution,
ygx9DpueLEipq5xWY,Covid 9/17: It’s Worse,covid-9-17-it-s-worse,https://www.lesswrong.com/posts/ygx9DpueLEipq5xWY/covid-9-17-it-s-worse,2020-09-17T15:10:01.649Z,54,28,41,False,False,,"<p>Last week we learned there is plausibly a simple, cheap and easy way out of this entire mess. All we have to do is take our Vitamin D. In case it needed to be said, no. We are not taking our Vitamin D. There’s definitely some voices out there pushing it, including the nation’s top podcaster Joe Rogan, but I don’t see any signs of progress.</p>



<p>Instead, as school restarts, the outside gets colder and pandemic fatigue sets in, people’s precautions are proving insufficient to the task. This week showed that we have taken a clear step backwards across the country. </p>



<p>I see three ways for things not to get steadily worse for a while. Either a vaccine arrives, which is unlikely, something else new (that we see little sign of) arrives to change behavior for the better, or this week was a blip. It’s only one week of data, it follows labor day, and it is wise not to move too quickly to extrapolation. The effect size seems too large, though, and too distributed among outcomes, to be coincidence.</p>



<p>In terms of news, it was a quiet week. There was some bluster, but little substance.</p>



<p>Let’s run the numbers. They’re not good.</p>



<span></span>



<h3>Positive Test Counts</h3>



<figure><table><tbody><tr><td>Date</td><td>WEST</td><td>MIDWEST</td><td>SOUTH</td><td>NORTHEAST</td></tr><tr><td>July 23-July 29</td><td>110219</td><td>67903</td><td>240667</td><td>26008</td></tr><tr><td>July 30-Aug 5</td><td>91002</td><td>64462</td><td>212945</td><td>23784</td></tr><tr><td>Aug 6-Aug 12</td><td>93042</td><td>61931</td><td>188486</td><td>21569</td></tr><tr><td>Aug 13-Aug 19</td><td>80887</td><td>63384</td><td>156998</td><td>20857</td></tr><tr><td>Aug 20-Aug 26</td><td>67545</td><td>66540</td><td>132322</td><td>18707</td></tr><tr><td>Aug 7-Sep 2</td><td>55000</td><td>75401</td><td>127414</td><td>21056</td></tr><tr><td>Sep 3-Sep 9</td><td>47273</td><td>72439</td><td>106408</td><td>21926</td></tr><tr><td>Sep 10-Sep 16</td><td>45050</td><td>75264</td><td>115812</td><td>23755</td></tr></tbody></table></figure>



<figure><img src=""https://lh4.googleusercontent.com/-TxhF8pZMWg-5urbiw9MIL4rT7Uh53xd1KuatgB4Wgeg4Ukv7gYEBhGlv1GSlayalV5b6AwgzSn8z0_qAUtxae2tvWT8P-aid1BUfFUPG7CJsiy9YkdUkN445CWQc-_ZBkZRrjta"" /></figure>



<p>This doesn’t look <em>too </em>bad on its own. Whether it’s good, bad or very bad news depends on whether testing is improving. If testing were still ramping up, it could easily count as good news, even with the worrying reversal in the South. Unfortunately, that’s not what happened. Testing actually <em>declined </em>this week to 4.63 million tests, the lowest value since the first week of July.</p>



<p>Alas, the positive test percentages:</p>



<figure><img src=""https://lh5.googleusercontent.com/XjecVSDLSytLJTUo-g5rka63tGQMFQ4SKwk4K9398HHNj1O0LZeFMWpNqxs1C49c9gWxsHTjx0hEaflMJ4V82nFk9Q-tldRjZXrgyYUKalKDg_xq-5C526QDBj1_zV-if3evP_qG"" /></figure>



<figure><table><tbody><tr><td>Date</td><td>USA tests</td><td>Positive %</td><td>NY tests</td><td>Positive %</td><td>Cumulative Positives</td></tr><tr><td>July 16-July 22</td><td>5,456,168</td><td>8.6%</td><td>454,995</td><td>1.1%</td><td>1.20%</td></tr><tr><td>July 17-July 29</td><td>5,746,056</td><td>7.9%</td><td>452,889</td><td>1.0%</td><td>1.34%</td></tr><tr><td>July 30-Aug 5</td><td>5,107,739</td><td>7.8%</td><td>484,245</td><td>1.0%</td><td>1.46%</td></tr><tr><td>Aug 6-Aug 12</td><td>5,121,011</td><td>7.3%</td><td>506,524</td><td>0.9%</td><td>1.58%</td></tr><tr><td>Aug 13-Aug 19</td><td>5,293,536</td><td>6.2%</td><td>548,421</td><td>0.8%</td><td>1.68%</td></tr><tr><td>Aug 20-Aug 26</td><td>4,785,056</td><td>6.0%</td><td>553,369</td><td>0.7%</td><td>1.77%</td></tr><tr><td>Aug 27-Sep 2</td><td>5,042,113</td><td>5.5%</td><td>611,721</td><td>0.8%</td><td>1.85%</td></tr><tr><td>Sep 3-Sep 9</td><td>4,850,253</td><td>5.3%</td><td>552,624</td><td>0.9%</td><td>1.93%</td></tr><tr><td>Sep 10-Sep 16</td><td>4,632,005</td><td>5.8%</td><td>559,463</td><td>0.9%</td><td>2.01%</td></tr></tbody></table></figure>



<p>This was very surprising to me. It would not have been too surprising to see things level off around previous levels. But to have it also reverse so suddenly indicates a major change. The default hypothesis is that the reopening of schools is finally taking its toll, now that it has had time to accumulate sufficient compound damage. If that’s the case, we’re in for at least several more weeks of things getting worse.</p>



<p>What’s weirder is that the death counts are headed in the wrong direction, despite what were clearly positive trends in leading indicators several weeks ago.</p>



<p>Deaths by Region</p>



<figure><table><tbody><tr><td>Date</td><td>WEST</td><td>MIDWEST</td><td>SOUTH</td><td>NORTHEAST</td></tr><tr><td>July 9-July 15</td><td>1380</td><td>539</td><td>2278</td><td>650</td></tr><tr><td>July 16-July 22</td><td>1469</td><td>674</td><td>3106</td><td>524</td></tr><tr><td>July 23-July 29</td><td>1707</td><td>700</td><td>4443</td><td>568</td></tr><tr><td>July 30-Aug 5</td><td>1831</td><td>719</td><td>4379</td><td>365</td></tr><tr><td>Aug 6-Aug 12</td><td>1738</td><td>663</td><td>4554</td><td>453</td></tr><tr><td>Aug 13-Aug 19</td><td>1576</td><td>850</td><td>4264</td><td>422</td></tr><tr><td>Aug 20-Aug 26</td><td>1503</td><td>745</td><td>3876</td><td>375</td></tr><tr><td>Aug 27-Sep 2</td><td>1245</td><td>759</td><td>3631</td><td>334</td></tr><tr><td>Sep 3-Sep 9</td><td>1141</td><td>771</td><td>2717</td><td>329</td></tr><tr><td>Sep 10-Sep 16</td><td>1159</td><td>954</td><td>3199</td><td>373</td></tr></tbody></table></figure>



<figure><img src=""https://lh3.googleusercontent.com/taVxzi12YnF_uZp7R-J3L3FGBlySUoO8sj8Y1gHIvhPXWqJYbz7zDzPqwMi7br9O7LMSfQH32Z3VCm_cYINqOrIJpKi1pOlsVyn2xvmIDxhYJG33grIltwdSntqskKbV_ULo2x2a"" /></figure>



<p>Labor day weekend was too far in the past to provide much of an excuse here. The Midwest and Northeast are clearly headed in the wrong direction. The South and West could claim this is a backlog issue and things are still fine, especially the West, but it does not look good. If that’s what happened while leading indicators were improving, what’s going to happen over the next few weeks? </p>



<h3>Extra No Good Very Bad Numbers: Meanwhile In Europe</h3>



<figure><img src=""https://lh4.googleusercontent.com/4hv10DE2bwmHBbYcT4vQSsfuI9umjU3o-ei88LEqrax3T_DhlAGhb7xSJTIM52Xp0sk-yb2DxzzGlvRq0CmKhRsJKIWHBq1Zv6MSP2kmHM6hoxV7kjAXJdOZzKLMeR1GtKj80eBk"" /></figure>



<p>I’ve mostly limited the scope of this column to the United States, but it needs to be pointed out that much of Europe looks like it’s got its own second wave at this point. Spain and France are already there, and the U.K. is well on its way. Germany is holding steady so far and we can hope that holds. When you don’t eradicate, vigilance can never end. Then eventually it does, or the seasons change and tip things over the edge as behaviors adjust to that.</p>



<p>Given all our advances, one hopes that this won’t come with too many deaths even if the infection numbers get out of control. </p>



<p>Numbers told the story main this week. The rest is more of a round up.</p>



<h3>United Arab Emirates Joins Vaccine Club</h3>



<p>Best news of the week: <a href=""https://www.reuters.com/article/us-health-coronavirus-emirates-vaccine/uae-announces-emergency-approval-for-use-of-covid-19-vaccine-idUSKBN2652OM"">UAE announces emergency approval for use of COVID-19 vaccine</a>. One more country, albeit a relatively small one, sees the light and rolls the favorably weighted dice.<br /></p>



<p>Here’s to you, UAE. Except that you don’t drink, and neither do I. So hats off, instead.</p>



<h3>Football Coach Gives Us Some Straight Talk</h3>



<p>I didn’t know I could love Coach O even more, but the results are in and it turns out I absolutely can do that. The man tells it like it is. <a href=""https://www.espn.com/college-football/story/_/id/29892180/lsu-coach-ed-orgeron-most-team-contracted-coronavirus"">LSU coach Ed Orgeron — ‘Most’ of team has contracted coronavirus</a>. </p>



<p>That’s the SEC. Here, it just means more. </p>



<p>The problem is not that the team has some players who have caught Covid-19. The problem is that the team has players that <em>haven’t </em>caught Covid-19! They might catch it in the future. So we need to have backups ready for those players. </p>



<p>My assumption is that LSU’s campus is full of college kids who don’t care if they get Covid-19, so a ton of them got Covid-19 right away, and none of this has anything to do with football. I saw stories saying it was all over the dorms. </p>



<p>Or you could take the other approach, look like you’re acting all responsible, and be the fun police without actually making anyone safer. I’m looking at you, PAC-12. But I’m <em>not </em>looking at you, Big 10, because unlike most people these days, I believe in forgiveness.</p>



<h3>Play Ball!</h3>



<p><a href=""https://www.youtube.com/watch?v=S0Ez1LcRbg0&amp;ab_channel=bigtenconference"">You are the Big 10 conference</a>. <a href=""https://www.youtube.com/watch?v=Tu9fm2rKyRc&amp;ab_channel=01hornsfan"">Cause you had a bad day</a>. <a href=""https://www.youtube.com/watch?v=1oHKB5prG7U&amp;ab_channel=jars14"">A really bad day</a>. <a href=""https://en.wikipedia.org/wiki/2010%E2%80%9314_Big_Ten_Conference_realignment"">Even worse than when they added Rutgers and Maryland</a>. You’ve taken one down. Cancelled your entire season, like many other things, over nothing.</p>



<p>You sing a sad song, hopefully while socially distanced, to turn it around.</p>



<p>Then you realize <a href=""https://www.youtube.com/watch?v=GwQW3KW3DCc&amp;ab_channel=NicoSD"">you’ve made a huge mistake</a>. <a href=""https://www.youtube.com/watch?v=4J4zA9OYIco&amp;ab_channel=RoyishGoodLooks"">You get your shirt together.</a> You mumble something about a ‘proper daily testing regime.’ </p>



<p><a href=""https://www.espn.com/college-football/story/_/id/29897305/sources-big-ten-announce-october-return"">And you’re set to resume football on October 24</a>.  </p>



<p>It’s going to be a tight schedule. By waffling, they’ve made it so that an outbreak that causes delays could endanger several teams and their ability to play a full season – there’s only room for one off week. And like others, they made the mistake of <em>scheduling </em>that off-week rather than holding it in reserve to handle a crisis. </p>



<p>But what matters is, they’re back, and we’re playing. The PAC-12 is still not back. They’re really pushing the scheduling window to its breaking point, but they’re working on it. Besides, we all know they were going to get excluded from the playoff regardless, like they do every year, so it doesn’t matter that much if they play further into December or even January. </p>



<p>More football. Ergo, more peace.  </p>



<h3>Burn Baby Burn</h3>



<p>The west coast of the United States is more than a little on fire. The air is not fit for humans to breathe. The sky is frequently the wrong color. Photos of this past month’s sky and its resemblance to something that isn’t part of a post-apocalyptic wasteland have been unfavorably compared to photographs from the Blade Runner 2045 movie. It’s pretty bad out there. Presumably this is having an effect on Covid-19, but it’s not obvious which way – if everyone does everything indoors that’s bad, but if they can’t even go outside to get to other people, that’s good for the moment, I guess? </p>



<p>The bigger point is that once again we have two distinct versions of ‘scientific consensus’ about what’s going on with these fires. From what I can tell, here’s the situation.</p>



<p>California used to naturally burn periodically, on its own. It wasn’t great, it was bad enough to sometimes make the air bad, but it kept things in balance. </p>



<p>For about a century, California has been aggressively putting out every fire it can find. There has effectively been the mentality of a ‘war on fire.’ This has led to an accumulation of a massive amount of fuel. </p>



<p>We know that the way to deal with this is controlled burn. But when someone starts a controlled burn, they get punished for it. They have to file environmental impact statements (because the fire will damage the air today, and that’s no good, even though we now see the alternative), deal with lots of regulations and so on. If something goes wrong, they get the blame and the lawsuits. It’s much easier to just not burn, so mostly people don’t burn. Certainly no one private does controlled burns, and the public does maybe 30,000 acres a year even with extra efforts. But the historical average was <em>millions </em>of acres, so we’re doing essentially nothing.</p>



<p>Thus, lots of giant fires.</p>



<p>We then hire a combination of overpriced unionized labor that demands overtime pay so good they occasionally start the fires themselves, and prison labor we barely pay at all that’s now unavailable because of fear of Covid-19. And we use them to fight all the fires, including ones that don’t threaten anything of value and thus would be net positive to allow those to continue. So we have anything like the resources necessary to stop this.</p>



<p>Also, climate change is a thing, which is also making things somewhat worse. </p>



<p>So what do the Democratic politicians and most media outlets say? </p>



<p>That this is “a climate damn emergency” and that the “scientific consensus” is that this is all the result of climate change. </p>



<p>Thus. Everyone involved gets to act all righteous and feel like they’re scoring points in the political wars. They make Sacrifices to the Gods that, even if they work as intended, only mean that things will continue to get worse but ever so slightly slower. That’s their response to this “emergency.” </p>



<p>Because if it’s all due to climate change, they don’t have to actually <em>do anything that might stop the fires. </em>Like more controlled burns, or devoting more or smarter resources to protecting what needs protecting. The things that might help anyone actually alive today not need to flee across the country. </p>



<p>I bring this up because the parallel to how those same media and political sources deal with Covid-19 should be obvious. Claim they know what the “science” says. Blame things that don’t matter. Actively interfere with the things that might help, massively slow or block any useful action while denying its possibility or effectiveness. Call for gigantic long term sacrifices that offer little tangible gain. While simultaneously claiming it would have been impossible to actually prevent the problem or mitigate its effects. </p>



<p>Label anyone who says otherwise “anti-science” and irresponsible and just awful. </p>



<p>Covid-19 is not some outlier. These people lie. About everything. All the time. </p>



<p>Not every time. They do sometimes tell the truth, when it suits them. But if anything, that only makes it more difficult. As the old joke goes, it’s easy to tell the truth from Pravda, because everything in Pravda is a lie. But The New York Times is trickier, because sometimes it tells the truth.</p>



<h3>Vaccine Responsibility</h3>



<p>The main Covid-19 nominal headlines this week were about vaccines. Trump continues to promise a vaccine by late October. The head of the CDC says that’s not going to happen. Trump says the head of the CDC is ‘confused.’ The CDC walks the comments back. On net, this showed <em>some </em>attempt by the CDC to not kowtow to Trump, but then a kowtow, so on net seems like a wash.</p>



<p>Gates and Fachi and others continue to say not to expect a vaccine. All this back and forth. </p>



<p>For those worried, yes, the halted vaccine trial from last week has resumed and never had a good reason to pause. </p>



<p>The net visible news on this was presumably bad, as indicated by the <a href=""https://goodjudgment.io/covid-recovery/#1338"">Good Judgment Project</a> which has us down to a 59% chance of 25 million doses administered by the end of March. </p>



<p>It’s all talk. None of this substantially changed my view of the current state of the vaccine research. </p>



<p>A vaccine will be available in October if Trump is able to override the CDC and FDA, and make it happen by fiat to help its reelection chances. If it can’t do that, the vaccine will wait a few more months at minimum, and then we’ll see what happens. I continue to think distribution would be the right thing to do and the objections are deeply wrong, and of course that none of that has anything to do with why Trump is going to try to overrule those objections.</p>



<p>The other big comment from the head of the CDC was that ‘masks could be more effective than the vaccine.’ Which is the kind of thing one says when one thinks it is more important to look like a Very Serious and Responsible Person, who is saying Very Serious and Responsible Things, and is properly encouraging vigilant mask usage. We wouldn’t want people to think that something else might help them. Think of what they might do!</p>



<p>Trump pushed back hard on that as well, as he would regardless of the statement’s truth value. The same way the statement was made without regard to its truth value, beyond finding a way to show it could <em>possibly</em> be technically correct. If that.</p>



<p>I grow weary of it all. It’s the fatigue setting in. The same bluster. The same warnings that it will not be over until the Proper Authorities say it’s over. The same downplaying and dismissals by the White House. No authorities we can trust. Round and round and round we go. When it stops, I’m trying to guess, and the prognosis doesn’t look great.</p>



<h3>Going Forward</h3>



<p>We’re getting close to the election. I did a look at the betting odds, and <a href=""https://thezvi.wordpress.com/2020/09/14/free-money-at-predictit-2020-general-election/"">found some instances of small free money available</a> for those interested. As time goes by, focus will shift away from Covid-19 as a health problem, and towards the upcoming election and its details. Everything will more and more be in that light and only that light, from all sides. My guess is that this will decrease the amount of meaningful virus news, and we’ll be more focused on the pure numbers. </p>



<p>Meanwhile, the virus will ignore all that, because the virus doesn’t care.</p>



<p>This blog does its best to stay out of politics despite discussing politicized issues. It would many times have said something about ‘a plague on both your houses’ except that seems a little on the nose. I have a strong preference on outcomes, which readers can presumably guess – but saying it outright wouldn’t convince anyone. You have all the information you need, to decide which candidate you prefer. Vote accordingly. If you are in a swing state and can afford to cast your vote in person, do so, to facilitate a quick, clear and peaceful resolution of the election – the Covid-19 risk involved will be minimal, and it helps reduce the tail risk from a disputed election. </p>



<p>I’ll also start another experiment this week. If you see news you think should make next week’s summary, throw it into the comments, with links if possible, and we’ll see if together we can cover things a little easier. I see no reason not to try that out. I’ll check both the LessWrong version of this post and the original for such comments. </p>",Zvi,zvi,Zvi,
MRjLJYKW2YaHxzuE6,Making the Monte Hall problem weirder but obvious,making-the-monte-hall-problem-weirder-but-obvious,https://www.lesswrong.com/posts/MRjLJYKW2YaHxzuE6/making-the-monte-hall-problem-weirder-but-obvious,2020-09-17T12:10:23.472Z,17,10,8,False,False,https://dyno-might.github.io/2020/09/17/making-the-monty-hall-problem-weirder-but-obvious/,"<p>The Monty Hall problem is famously unintuitive. This post starts with an extreme version where the solution is blindingly obvious. We then go through a series of small changes. It will be clear that these don’t affect the solution.</p>
",dynomight,dynomight,dynomight,
87pwAjdrXr6L3Q8rm,How do you celebrate your birthday?,how-do-you-celebrate-your-birthday,https://www.lesswrong.com/posts/87pwAjdrXr6L3Q8rm/how-do-you-celebrate-your-birthday,2020-09-17T10:00:50.609Z,10,5,2,False,True,,<p>Or: what's a cool idea to celebrate your birthday?</p><p>Or: why don't you celebrate your birthday?</p>,MathieuRoy,mathieuroy,Mati_Roy,
3neWsXmxRKwKQjc5i,Noticing Wasted Motion,noticing-wasted-motion,https://www.lesswrong.com/posts/3neWsXmxRKwKQjc5i/noticing-wasted-motion,2020-09-17T02:30:28.273Z,21,12,5,False,False,,"<p><em>This post is about a practical skill you need to effectively minimize <a href=""https://effectivealtruismcoaching.com/blog/2020/8/27/cost-of-wasted-motion"">wasted motion</a> - noticing. Noticing is a key skill for changing behavior in general, because it allows you to respond deliberately instead of automatically. Yet in my experience, it is one of the more difficult skills to learn. Hopefully the following will give you a toehold.&#xA0;</em></p><p><em><a href=""https://effectivealtruismcoaching.com/blog/2020/9/10/noticing-wasted-motion"">This piece is cross-posted on my blog here.</a></em> </p><p>In my last post, I talked about the cost of wasted motion - the lost opportunity to go straight for the goal and make the world a bit better. If you&#x2019;re staring into the dark world of the cost of wasted motion, don&#x2019;t despair. Minimizing wasted motion is a skill and you <em>can train that skill</em>.&#xA0;</p><p>But before you can do that, you need to learn to <em>notice </em>wasted motion. This is <em>hard</em>. First, it&#x2019;s hard to remember to pay attention, especially if you&#x2019;re already doing something else. Second, it&#x2019;s hard because -- deep down -- you might not <em>want </em>to.&#xA0;</p><p>It feels ughy to start working, because that project is big and hard. The immediate <u><a href=""https://www.lesswrong.com/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge"">pain</a></u> of starting work looms large in the hyperbolic-discounting agents we call brains. Maybe you&apos;re anxious that you&#x2019;ll look bad if you try and fail. So you just...let your mind slip away to easier tasks. You may not even notice you&#x2019;re doing it.&#xA0;</p><p>But, if you&#x2019;re paying attention, there&#x2019;s a little catch there, in your mind. When you stop and think, you <em>know </em>you&apos;re procrastinating a little bit. You <em>know </em>you&apos;re wasting motion toward your goal. You want to complete your goal, but you also want to avoid dealing with those bad feelings. And that internal conflict makes you feel guilty or frustrated or anxious.&#xA0;</p><p>But that little catch is small and easily ignored. If you haven&apos;t thought about it before, you may not even have realized it&apos;s there. Maybe that guilt is so small you never really notice it. And maybe it&apos;s grown so large that you feel overwhelmed and ashamed (link to measuring progress).&#xA0;</p><p>Learn to notice that bit of frustration at how you&#x2019;re doing the task, that <em>ugh </em>feeling around a certain plan.&#xA0;</p><p>If you&#x2019;re not sure where to start, it might help to read more about <u><a href=""http://agentyduck.blogspot.com/2014/09/what-its-like-to-notice-things.html"">noticing</a></u>. A therapy-style <u><a href=""https://drive.google.com/file/d/17lNKy7lBN6kttpwBT7mvg8y1-g7qFGNg/view?usp=sharing"">before-after record</a></u> can also be helpful for identifying emotions, what issues trigger that emotion, and reliable ways to respond.&#xA0;</p><p>Personally, I started learning how to notice by reflecting after something went wrong, and seeing what emotions I felt earlier that could have warned me. Once I had those emotions in mind, I started to pause and check in whenever I noticed that emotion. This led to successfully correcting course, which caused a virtuous cycle by reinforcing noticing that emotion.&#xA0;</p><p>For example, one time I got caught up in errands and realized I was supposed to meet a friend for dinner in 5 minutes. The only catch was that the friend was three miles away, and I was on bike. Now for those of you who don&#x2019;t know me in person, I&#x2019;m not exactly the paragon of physical endurance. There was no way on earth I was getting there in 5 minutes. But I felt this tunnel vision pressure to bike as fast as I could and not think about the fact I was probably late.&#xA0;</p><p>I finally showed up, quite winded, about half an hour late. It turned out my friend had been pretty worried. I felt really bad, but it was useful for recognizing that the feeling of &#x201C;I can&#x2019;t stop to reflect, I just have to press forward as fast as I can&#x201D;. That tunnel vision feeling reliably signals that I&#x2019;m feeling time pressure to keep going, but also think I may be doing the wrong thing. Since then, I&#x2019;ve gotten better at slowing down and checking my plan whenever I notice that feeling.&#xA0;</p><p>I experience emotional responses that tell me a poor choice is happening. Other people sometimes describe noticing physical sensations or directly noticing a poor choice, so feel free to try those instead.&#xA0;</p><p>You can try deliberately practicing noticing. If you already know of something that triggers the feeling, then you can deliberately cause that emotion and practice responding the way you want to. Or you can try making an intention to notice. Borrowed from <u><a href=""https://www.sciencedirect.com/science/article/pii/S0065260106380021"">psychology</a></u>, these intentions are usually formatted as if/then sentences - if I notice X feeling, then I will do Y. There is high variance to how well these intentions work in practice, but they are extremely useful when successful.&#xA0;</p><p>It might take you a bit to identify what you&#x2019;re trying to notice. Some people describe looking for what their thoughts slide away from, rather than going straight for the negative feeling. If you&#x2019;re really struggling, deliberately plan <em>not to fix the problem</em> when you notice you&#x2019;re wasting motion. This piece of advice might seem super weird. After all, the end goal is to save motion. But if you <em>have </em>to change something if you notice it, you might subconsciously stop yourself from ever noticing it.&#xA0;&#xA0;</p><p>As you start to pay attention to those cues, they can be confusing. It&#x2019;s hard to tell if you&#x2019;re feeling frustrated because your plan is bad or just because the task is hard. Try doing a brain dump when you feel that frustration. Let your thoughts spill onto the page, and see if there&#x2019;s a different action you could take that would accomplish your goal better <em>and</em> make that feeling go away.&#xA0; If you&#x2019;re not certain about this plan, try it for a week. Then reevaluate whether it&#x2019;s useful for you.&#xA0;</p><p>As you practice, the feelings become clearer and more useful. You&#x2019;ll get fewer false alarms.&#xA0;</p><p>But for now, notice when you feel that catch in your emotions. Pay attention to why you feel frustrated or trapped or unhappy. See if those emotions are saying, &#x201C;Here, do it this way instead. That way won&#x2019;t work, you&#x2019;ll just waste time&#x201D; or &#x201C;You know it would be better if you did the other thing instead. Why don&#x2019;t you go and do that?&#x201D; Use those cues to become aware of when you&#x2019;re wasting motion.&#xA0;</p><p>Once you&#x2019;re aware, <em>then </em>you can stop wasting motion.&#xA0;</p><p> <em>Enjoyed the piece?</em> <em><a href=""http://eepurl.com/gnWbln"">Subscribe to EA Coaching&#x2019;s newsletter</a></em> <em>to get more posts delivered to you.</em> </p>",lynettebye,lynettebye,lynettebye,
BNJx2CqfXyiusoJcK,Artificial Intelligence: A Modern Approach (4th edition) on the Alignment Problem,artificial-intelligence-a-modern-approach-4th-edition-on-the,https://www.lesswrong.com/posts/BNJx2CqfXyiusoJcK/artificial-intelligence-a-modern-approach-4th-edition-on-the,2020-09-17T02:23:58.869Z,72,35,12,False,False,http://aima.cs.berkeley.edu/,"<p><strong>Previously</strong>: <a href=""https://www.lesswrong.com/posts/K45SaBaB3D7o9xpAs/agi-and-friendly-ai-in-the-dominant-ai-textbook"">AGI and Friendly AI in the dominant AI textbook</a> (2011), <a href=""https://www.lesswrong.com/posts/S95qCHBXtASmYyGSs/stuart-russell-ai-value-alignment-problem-must-be-an"">Stuart Russell: AI value alignment problem must be an ""intrinsic part"" of the field's mainstream agenda</a> (2014)</p>
<p>The 4th edition of <em>Artificial Intelligence: A Modern Approach</em> came out this year. While the 3rd edition published in 2009 <a href=""https://intelligence.org/2013/10/19/russell-and-norvig-on-friendly-ai/"">mentions the Singularity and existential risk</a>, it's notable how much the 4th edition gives the alignment problem front-and-center attention as part of the introductory material (speaking in the authorial voice, not just ""I.J. Good (1965) says this, Yudkowsky (2008) says that, Omohundro (2008) says this"" as part of a survey of what various scholars have said). Two excerpts—</p>
<blockquote>
<h2>1.1.5 Beneficial machines</h2>
<p>The standard model has been a useful guide for AI research since its inception, but it is probably not the right model in the long run. The reason is that the standard model assumes that we will supply a fully specified objective to the machine.</p>
<p>For an artificially defined task such as chess or shortest-path computation, the task comes with an objective built in—so the standard model is applicable. As we move into the real world, however, it becomes more and more difficult to specify the objective completely and correctly. For example, in designing a self-driving car, one might think that the objective is to reach the destination safely. But driving along any road incurs a risk of injury due to other errant drivers, equipment failure, and so on; thus, a strict goal of safety requires staying in the garage. There is a tradeoff between making progress towards the destination and incurring a risk of injury. How should this tradeoff be made? Furthermore, to what extent can we allow the car to take actions that would annoy other drivers? How much should the car moderate its acceleration, steering, and braking to avoid shaking up the passenger? These kinds of questions are difficult to answer a priori. They are particularly problematic in the general area of human–robot interaction, of which the self-driving car is one example.</p>
<p>The problem of achieving agreement between our true preferences and the objective we put into the machine is called the <strong>value alignment problem</strong>: the values or objectives put into the machine must be aligned with those of the human. If we are developing an AI system in the lab or in a simulator—as has been the case for most of the field's history—there is an easy fix for an incorrectly specified objective: reset the system, fix the objective, and try again. As the field progresses towards increasingly capable intelligent systems that are deployed in the real world, this approach is no longer viable. A system deployed with an incorrect objective will have negative consequences. Moreover, the more intelligent the system, the more negative the consequences.</p>
<p>Returing to the apparently unproblematic example of chess consider what happens if the machine is intelligent enough to reason and act beyond the confines of the chessboard. In that case, it might attempt to increase its chances of winning by such ruses as hypnotizing or blackmailing its opponent or bribing the audience to make rustling noises during its opponents thinking time.³ It might also attempt to hijack additional computing power for itself. <em>These behaviors are not ""unintelligent"" or ""insane""; they are a logical consequence of defining winning as the</em> sole <em>objective for the machine</em>.</p>
<p>It is impossible to anticipate all the ways in which a machine persuing a fixed objective might misbehave. There is good reason, then, to think that the standard model is inadequate. We don't want machines that are intelligent in the sense of pursuing <em>their</em> objectives; we want them to pursue <em>our</em> objectives. If we cannot transfer those objectives perfectly to the machine, tghen we need a new formulation—one in which the machine is pursuing our objectives, but is necessarily <em>uncertain</em> as to what they are. When a machine knows that it doesn't know the complete objective, it has an incentive to act cautiously, to ask permission, to learn more about our preferences through observation, and to defer to human control. Ultimately, we want agents that are <strong>provably beneficial</strong> to humans. We will return to this topic in Section 1.5.</p>
</blockquote>
<p>And in Section 1.5, ""Risks and Benefits of AI""—</p>
<blockquote>
<p>At around the same time, concerns were raised that creating <strong>artificial superintelligence</strong> or <strong>ASI</strong>—intelligence that far surpasses human ability—might be a bad idea (Yudkowsky, 2008; Omohundro 2008). Turing (1996) himself made the same point in a lecture given in Manchester in 1951, drawing on earlier ideas from Samuel Butler (1863):¹⁵</p>
<blockquote>
<p>It seems probably that once the machine thinking method had started, it would not take long to outstrip our feeble powers. ... At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler's <em>Erewhon</em>.</p>
</blockquote>
<p>These concerns have only become more widespread with recent advances in deep learning, the publication of books such as <em>Superintelligence</em> by Nick Bostrom (2014), and public pronouncements from Stephen Hawking, Bill Gates, Martin Rees, and Elon Musk.</p>
<p>Experiencing a general sense of unease with the idea of creating superintelligent machines is only natural. We might call this the <strong>gorilla problem</strong>: about seven million year ago, a now-extinct primate evolved, with one branch leading to gorillas and one to humans. Today, the gorillas are not too happy about the human branch; they have essentially no control over their future. If this is the result of success in creating superhuman AI—that humans cede control over their future—then perhaps we should stop work on AI and, as a corollary, give up the benefits it might bring. This is the essence of Turing's warning: it is not obvious that we can control machines that are more intelligent than us.</p>
<p>If superhuman AI were a black box that arrived from outer space, then indeed it would be wise to exercise caution in opening the box. But it is not: <em>we</em> design the AI systems, so if they do end up ""taking control,"" as Turing suggests, it would be the result of a design failure.</p>
<p>To avoid such an outcome, we need to understand the source of potential failure. Norbert Weiner (1960), who was motivated to consider the long-term future of AI after seeing Arthur Samuel's checker-playing program learn to beat its creator, had this to say:</p>
<blockquote>
<p>If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively ... we had better be quite sure that the purpose put into the machine is the purpose which we really desire.</p>
</blockquote>
<p>Many cultures have myths of humans who ask gods, genies, magicians, or devils for something. Invariably, in these stories, they get what they literally ask for, and then regret it. The third wish, if there is one, is to undo the first two. We will call this the <strong>King Midas problem</strong>: Midas, a legendary King in Greek mythology, asked that everything he touched should turn to gold, but then regretted it after touching his food, drink, and family members.¹⁶</p>
<p>We touched on this issue in Section 1.1.5, where we pointed out the need for a significant modification to the standard model of putting fixed objectives into the machine. The solution to Weiner's predicament is not to have a definite ""purpose put into the machine"" at all. Instead, we want machines that strive to achieve human objectives but know that they don't know for certain exactly what those objectives are.</p>
<p>It is perhaps unfortunate that almost all AI research to date has been carried out within the standard model, which means that almost all of the technical material in this edition reflects that intellectual framework. There are, however, some early results within the new framework. In Chapter 16, we show that a machine has a positive incentive to allow itself to be switched off if and only if it is uncertain about the human objective. In Chapter 18, we formulate and study <strong>assistance games</strong>, which describe mathematically the situation in which a human has an objective and a machine tries to achieve it, but is initially uncertain about what it is. In Chapter 22, we explain the methods of <strong>inverse reinforcement learning</strong> that allow machines to learn more about human preferences from observations of the choices that humans make. In Chapter 27, we explore two of the principal difficulties: first, that our choices depend on our preferences through a very complex cognitive architecture that is hard to invert; and, second, that we humans may not have consistent preferences in the first place—either individually or as a group—so it may not be clear what AI systems <em>should</em> be doing for us.</p>
</blockquote>
",Zack_M_Davis,zack_m_davis,Zack_M_Davis,
SsNvhSqRMapbzjLBF,What are examples of simpler universes that have been described in order to explain a concept from our more complex universe?,what-are-examples-of-simpler-universes-that-have-been,https://www.lesswrong.com/posts/SsNvhSqRMapbzjLBF/what-are-examples-of-simpler-universes-that-have-been,2020-09-17T01:31:10.367Z,4,2,7,False,True,,"<p>Sometimes there's a concept that can be difficult to understand when entangle with everything else that needs to be understood about our physics.</p><p>If you isolate that concept in a simpler universe, it makes it easier to explain how the concept works.</p><p>What are such examples?</p><p>(I feel like I asked a similar question somewhere at some point, but can't find it)</p>",MathieuRoy,mathieuroy,Mati_Roy,
HxL9u3Yc9dAh3BQWd,"Sunday September 20, 12:00PM (PT) — talks by Eric Rogstad, Daniel Kokotajlo and more",sunday-september-20-12-00pm-pt-talks-by-eric-rogstad-daniel,https://www.lesswrong.com/posts/HxL9u3Yc9dAh3BQWd/sunday-september-20-12-00pm-pt-talks-by-eric-rogstad-daniel,2020-09-17T00:27:47.735Z,26,5,8,False,False,,"<p>This Sunday at 12pm (PT), we're running another session of ""lightning talks"" by curated LessWrong authors (see <a href=""https://www.lesswrong.com/tag/lesswrong-events"">here</a> for previous weeks' transcripts).</p><ul><li>For the first hour, we will have a series of lightning talks each lasting about 5 minutes followed by discussion. The talks will be short and focus on presenting one core idea well, rather than rushing through a lot of content.</li><li>From 1PM to 2PM, we'll have a hangout in breakout rooms. If you are not interested in the talks, feel free to just show up for this part (or the other way around).</li><li>We want to give top LessWrong writers an interesting space to discuss their ideas, and have more fruitful collaboration between users. Think of it like a cross between an academic colloquium and some friends chatting by a whiteboard.</li></ul><p><i>If you're a curated author and interested in giving a 5-min talk at a future event, which will then be transcribed and edited, sign up </i><a href=""https://forms.gle/iwFatbhys9muPmQA7""><i>here</i></a><i>.</i></p><h2>Speakers</h2><ul><li><a href=""https://www.lesswrong.com/users/esrogs"">Eric Rogstad</a>: <strong>The AI Does Not Care about Your Atoms Either</strong></li><li><a href=""https://www.lesswrong.com/users/daniel-kokotajlo"">Daniel Kokotajlo</a>: <strong>Why GWP is a bad metric for thinking about timelines and takeoff speeds</strong></li></ul><h2>Details</h2><p><strong>When? </strong>Sunday September 20, 12:00PM (PT)</p><p><strong>Where? </strong><a href=""https://www.google.com/url?q=https://us02web.zoom.us/j/84108675070&amp;sa=D&amp;source=calendar&amp;usd=2&amp;usg=AOvVaw3JRMiGUZOh6ucvtAQVMFmj""><u>https://us02web.zoom.us/j/84108675070</u></a>&nbsp;</p>",habryka4,habryka4,habryka,
yF6FLgWnRdKMYpm9g,Rationality for Kids?,rationality-for-kids,https://www.lesswrong.com/posts/yF6FLgWnRdKMYpm9g/rationality-for-kids,2020-09-16T22:58:21.300Z,51,20,28,False,True,,"<p>UPDATE 11NOV: </p><p>I came up with a game to use as an icebreaker. And I&apos;d love ideas for future variations. It&apos;s a combination of Credence Calibration, 20 Questions, and Taboo. The children are trying to determine which of three possible states exist on the card which I have face down (for my first iteration, the possibilities will be &quot;Cat&quot;, &quot;Rat&quot;, and &quot;Dog&quot;). Every kid gets 30 poker chips to allocate to each of the three possibilities. Kids will then take turns asking a yes or no question, but before each Q, I roll a six sided die. If it comes up six, all chips placed on a wrong answer are turned in, otherwise, they ask their question, I answer with something on a scale of &quot;Never&quot; to &quot;Always&quot;, and they are permitted to reallocate their chips. But there is a catch: they are not permitted to use certain words (i.e. cat, dog, rat, meow, bark, pet, etc.) in their questions.<br>The point is to find tests which can serve as evidence between the possibilities and recognize how confidence should change according to evidence.</p><p>Would be interested in other possible states for future iterations</p><p>END UPDATE</p><br><p>So I really appreciate the lessons I&apos;ve learned from &quot;Rationality&quot;, but I wish I had learned them earlier in life. We are now homeschooling my kids, and I want to volunteer to teach my kids plus others who are interested lessons about thinking rationally. </p><p>Does anyone have recommendations on how to put together a curriculum which gets at the core ideas of rationality, but is oriented towards young kids? Some criteria:</p><p>Children will likely range from 7-11, meaning they should be simple concepts and require very little prior knowledge and only the simplest math.</p><p>Lessons should be interactive.</p><p>Lessons should include TRUE experiments (not just doing fun stuff with chemicals).</p><p>Lessons should be fun and appealing enough that parents will want to sign their kids up.</p><p>Any other suggestions on the course (wording that will be appealing without sounding too &quot;nerdy&quot; or alarming to the conservative types who usually homeschool) are welcome.</p><br><p>UPDATE:  the Inflection Point Curriculum appears to be the middle school version of what I am looking to do: <a href=""https://drive.google.com/file/d/1tcUJXRlZXeKjAWeU9Y37FcPKv3lj6PsX/view?usp=sharing"">https://drive.google.com/file/d/1tcUJXRlZXeKjAWeU9Y37FcPKv3lj6PsX/view?usp=sharing</a></p><p>I currently envision the course as a combination of game type exercises like Credence Calibration, Zendo, and Meta-Forms, and experiments like adjusting the air composition of a room and investigating bernoulli effects using things like paper and shower curtains. Other ideas: investigating citrus batteries, water absorption by celery, and the light spectrum of various sources as split by a prism.</p>",goose000,goose000,goose000,
dYBaRodGhfsmfEpSW,"What Does ""Signalling"" Mean?",what-does-signalling-mean,https://www.lesswrong.com/posts/dYBaRodGhfsmfEpSW/what-does-signalling-mean,2020-09-16T21:19:00.968Z,38,16,18,False,False,,"<p><i>[Epistemic status: shortly after writing this post, I thought I'd regret it. It was not rated very highly by LW karma, and on outside view, it's the sort of rant I often don't endorse later on. But re-reading it after a few weeks, I think it holds up. I still endorse everything I've written here, with the exception of my suggestion that ""virtue signalling"" is an appropriate replacement for what most people mean by ""signalling"".]</i></p><p>I still feel a strong empathy for the post <a href=""https://www.lesswrong.com/posts/BK3L2ySPtPrSBLEkg/you-can-t-signal-to-rubes"">You Can't Signal to Rubes</a>, which called out LessWrong for using the word ""signalling"" incorrectly. That post got heavily, and rightly, downvoted because <i><strong>it also got the definition wrong</strong></i><strong>.</strong><i> :( </i>But it had a point!</p><p>At the time of writing, the current definition of signalling on <a href=""https://www.lesswrong.com/tag/signaling"">the LessWrong tag</a> is:</p><blockquote><p><strong>Signaling</strong> is behavior whose main purpose is to demonstrate to others that you possess some desirable trait. For example, a bird performing an impressive mating display signals that it is healthy and has good genes.</p></blockquote><p>I'm not even sure I should correct it, because this <i>does</i> seem to summarize the LessWrong consensus on what signalling means. But we <i>already have </i>a term for signalling desirable properties about yourself: <a href=""https://en.wikipedia.org/wiki/Virtue_signalling"">virtue signalling</a>! Maybe you'll object that ""virtue signalling"" isn't quite right. Ok. But, could you find another word? I would prefer for ""signalling"" to point to the subject of signalling theory, which I understand to be the game theory of communication (often focusing on evolutionary game theory).</p><p>Scott Alexander's <a href=""https://www.lesswrong.com/posts/KheBaeW8Pi7LwewoF/what-is-signaling-really"">What Is Signaling, Really?</a> seems to get most things right:</p><blockquote><p>In conclusion, a signal is a method of conveying information among not-necessarily-trustworthy parties by performing an action which is more likely or less costly if the information is true than if it is not true. Because signals are often costly, they can sometimes lead to a depressing waste of resources, but in other cases they may be the only way to believably convey important information.</p></blockquote><p>Although all of his examples are about signalling self-properties, he never <i>stipulates</i> that, instead always using the more general conveying-information definition. He also avoids the <i>signalling is automatically bad</i> pitfall. Instead, he explains that signalling is often unfortunately costly, but is nonetheless a very useful tool.</p><p>However, reading it, I'm not sure whether he means to <i>contrast</i> signalling with ""mere assertion"", or whether he considers assertion to be a kind of signalling:</p><blockquote><p>Life frequently throws us into situations where we want to convince other people of something. If we are employees, we want to convince bosses we are skillful, honest, and hard-working. If we run the company, we want to convince customers we have superior products. If we are on the dating scene, we want to show potential mates that we are charming, funny, wealthy, interesting, you name it.<br><br>In some of these cases, mere assertion goes a long way.</p><p>[...]</p><p>In other cases, mere assertion doesn't work.</p><p>[...]</p></blockquote><p>I'll charitably assume that he meant both cases to be types of signalling. But for anyone who was mislead by the wording: <i><strong>signalling is the theory of conveying information! Mere assertions, if they carry information, count as signalling!</strong></i></p><p>So, to summarize the points I've raised so far:</p><ol><li>Sometimes people talk like signalling is just the bad thing (the dishonest or not-maximally-honest practice of making yourself look good).</li><li>Relatedly, people tend to exclude ""mere assertion"" from signalling, making signaling and literal use of language mutually exclusive.</li><li>Often people restrict signalling to signalling facts <i>about yourself</i>. (In fact, often restricted to <i>status </i>signalling.)</li></ol><p>To be honest, I'm not even sure <i>academic</i> uses of the term ""signalling"" avoid the ""mistakes"" I'm pointing at! The Wikipedia article <a href=""https://en.wikipedia.org/wiki/Signalling_(economics)"">Signalling (economics)</a> currently begins with the following:</p><blockquote><p>In <a href=""https://en.wikipedia.org/wiki/Contract_theory"">contract theory</a>, <strong>signalling</strong> (or <strong>signaling</strong>; see <a href=""https://en.wikipedia.org/wiki/American_and_British_English_spelling_differences#Doubled_consonants"">spelling differences</a>) is the idea that one party (termed the <a href=""https://en.wikipedia.org/wiki/Agent_(law)"">agent</a>) credibly conveys some information about itself to another party (the <a href=""https://en.wikipedia.org/wiki/Principal_(commercial_law)"">principal</a>).</p></blockquote><p>[Note that I've defaulted to the Wikipedia spelling of signalling; spelling on LessWrong seems mixed.]</p><p>On the other hand, the page on <a href=""https://en.wikipedia.org/wiki/Signalling_theory"">Signalling Theory</a> (a page which is very biology-focused, despite the broader applicability of the theory) includes examples such as alarm calls (eg, birds warning each other that there is a snake in the grass). These signals cannot be interpreted as facts about the signaller.</p><p>Perhaps it is a quirk of <i>economics</i> which restricts the term ""signalling"" to hidden information <i>about the agent</i>, and LessWrong inherited this restricted sense via Robin Hanson?</p>",abramdemski,abramdemski,abramdemski,
8H5JbowLTJoNHzLuH,[AN #117]: How neural nets would fare under the TEVV framework,an-117-how-neural-nets-would-fare-under-the-tevv-framework,https://www.lesswrong.com/posts/8H5JbowLTJoNHzLuH/an-117-how-neural-nets-would-fare-under-the-tevv-framework,2020-09-16T17:20:14.062Z,27,6,0,False,False,,"<p>Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter <strong><a href=""http://rohinshah.com/alignment-newsletter/"">resources here</a></strong>. In particular, you can look through <strong><a href=""https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing"">this spreadsheet</a></strong> of all summaries that have ever been in the newsletter.</p><p>Audio version <strong><a href=""http://alignment-newsletter.libsyn.com/alignment-newsletter-117"">here</a></strong> (may not be up yet). 			  </p><h1>HIGHLIGHTS </h1><p><strong><a href=""https://arxiv.org/abs/2009.00802"">Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance</a></strong> <em>(Andrew L. John)</em> (summarized by Flo): Test, Evaluation, Verification, and Validation (TEVV) is an important barrier for AI applications in safety-critical areas. Current TEVV standards have very different rules for certifying <em>software</em> and certifying <em>human operators</em>. It is not clear which of these processes should be applied for AI systems.</p><p>If we treat AI systems as similar to human operators, we would certify them ensuring that they pass tests of ability. This does not give much of a guarantee of robustness (since only a few situations can be tested), and is only acceptable for humans because humans tend to be more robust to new situations than software. This could be a reasonable assumption for AI systems as well: while systems are certainly vulnerable to adversarial examples, the authors find that AI performance degrades surprisingly smoothly out of distribution in the absence of adversaries, in a plausibly human-like way.</p><p>While AI might have some characteristics of operators, there are good reasons to treat it as software. The ability to deploy multiple copies of the same system increases the threat of correlated failures, which is less true of humans. In addition, parallelization can allow for more extensive testing that is typical for software TEVV. For critical applications, a common standard is that of Safety Integrity Levels (SILs), which correspond to approximate failure rates per hour. Current AI systems fail way more often than current SILs for safety-critical applications demand. For example an image recognition system would require an accuracy of 0.99999997 at 10 processed frames per second just to reach the weakest SIL used in aviation.</p><p>However, SILs are often used on multiple levels and it is possible to build a system with a strong SIL from weaker components by using redundant components that fail independently or by detecting failures sufficiently early, such that AI modules could still be used safely as parts of a system specifically structured to cope with their failures. For example, we can use out-of-distribution detection to revert to a safe policy in simple applications. However, this is not possible for higher levels of automation where such a policy might not be available.</p><p><strong>Flo&apos;s opinion:</strong> While I agree with the general thrust of this article, comparing image misclassification rates to rates of catastrophic failures in aviation seems a bit harsh. I am having difficulties imagining an aviation system that fails due to a single input that has been processed wrongly, even though the correlation between subsequent failures given similar inputs might mean that this is not necessary for locally catastrophic outcomes.</p><p><strong>Rohin&apos;s opinion:</strong> My guess is that we&#x2019;ll need to treat systems based primarily on neural nets similarly to operators. The main reason for this is that the tasks that AI systems will solve are usually not even well-defined enough to have a reliability rate like 0.99999997 (or even a couple of orders of magnitude worse). For example, human performance on image classification datasets is typically under 99%, not because humans are bad at image recognition, but because in many cases what the &#x201C;true label&#x201D; should be is ambiguous. For another example, you&#x2019;d think &#x201C;predict the next word&#x201D; would be a nice unambiguous task definition, but then for the question &#x201C;How many bonks are in a quoit?&#x201C;, should your answer be <strong><a href=""https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html"">&#x201C;There are three bonks in a quoit&#x201D;</a></strong> or <strong><a href=""https://arr.am/2020/07/25/gpt-3-uncertainty-prompts/"">&#x201C;The question is nonsense&#x201D;</a></strong>? (If you&#x2019;re inclined to say that it&#x2019;s obviously the latter, consider that many students will do something like the former if they see a question they don&#x2019;t understand on an exam.)</p><h1>TECHNICAL AI ALIGNMENT </h1><h2>TECHNICAL AGENDAS AND PRIORITIZATION </h2><p><strong><a href=""https://www.cser.ac.uk/resources/ai-paradigms-and-ai-safety-mapping-artefacts-and-techniques-safety-issues/"">AI Paradigms and AI Safety: Mapping Artefacts and Techniques to Safety Issues</a></strong> <em>(Jose Hernandez-Orallo et al)</em> (summarized by Rohin) (H/T Haydn Belfield): What should prioritization within the field of AI safety look like? Ideally, we would proactively look for potential issues that could arise with many potential AI technologies, making sure to cover the full space of possibilities rather than focusing on a single area. What does prioritization look like in practice? This paper investigates, and finds that it is pretty different from this ideal.</p><p>In particular, they define a set of 14 categories of AI <em>techniques</em> (examples include neural nets, planning and scheduling, and combinatorial optimization), and a set of 10 kinds of AI <em>artefacts</em> (examples include agents, providers, dialoguers, and swarms). They then analyze trends in the amount of attention paid to each technique or artefact, both for AI safety and AI in general. Note that they construe AI safety very broadly by including anything that addresses potential real-world problems with AI systems.</p><p>While there are a lot of interesting trends, the main conclusion is that there is an approximately 5-year delay between the emergence of an AI paradigm and safety research into that paradigm. In addition, safety research tends to neglect non-dominant paradigms.</p><p><strong>Rohin&apos;s opinion:</strong> One possible conclusion is that safety research should be more diversified across different paradigms and artefacts, in order to properly maximize expected safety. However, this isn&#x2019;t obvious: it seems likely that if the dominant paradigm has 50% of the research, it will also have, say, 80% of future real-world deployments, and so it could make sense to have 80% of the safety research focused on it. Rather than try to predict which paradigm will become dominant (a very difficult task), it may be more efficient to simply observe which paradigm becomes dominant and then redirect resources at that time (even though that process takes 5 years to happen).</p><h2>PREVENTING BAD BEHAVIOR </h2><p><strong><a href=""https://arxiv.org/abs/2008.12146"">Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems</a></strong> <em>(Sandhya Saisubramanian et al)</em> (summarized by Rohin): This paper provides an overview of the problem of negative side effects, and recent work that aims to address it. It characterizes negative side effects based on whether they are severe, reversible, avoidable, frequent, stochastic, observable, or exclusive (i.e. preventing the agent from accomplishing its main task), and describes existing work and how they relate to these characteristics.</p><p>In addition to the canonical point that negative side effects arise because the agent&#x2019;s model is lacking (whether about human preferences or environment dynamics or important features to pay attention to), they identify two other main challenges with negative side effects. First, fixing negative side effects would likely require collecting feedback from humans, which can be expensive and challenging. Second, there will usually be a tradeoff between pursuing the original goal and avoiding negative side effects; we don&#x2019;t have principled methods for dealing with this tradeoff.</p><p>Finally, they provide a long list of potential directions for future side effect research.</p><h2>MISCELLANEOUS (ALIGNMENT) </h2><p><strong><a href=""https://futureoflife.org/2020/09/03/iason-gabriel-on-foundational-philosophical-questions-in-ai-alignment/?utm_source=feedly&amp;utm_medium=rss&amp;utm_campaign=iason-gabriel-on-foundational-philosophical-questions-in-ai-alignment"">Foundational Philosophical Questions in AI Alignment</a></strong> <em>(Lucas Perry and Iason Gabriel)</em> (summarized by Rohin): This podcast starts with the topic of the paper <strong><a href=""https://arxiv.org/abs/2001.09768"">Artificial Intelligence, Values and Alignment</a></strong> (<strong><a href=""https://mailchi.mp/84b4235cfa34/an-85-the-normative-questions-we-should-be-asking-for-ai-alignment-and-a-surprisingly-good-chatbot"">AN #85</a></strong>) and then talks about a variety of different philosophical questions surrounding AI alignment.</p><p><strong><a href=""https://www.cser.ac.uk/resources/exploring-ai-safety-degrees-generality-capability-and-control/"">Exploring AI Safety in Degrees: Generality, Capability and Control</a></strong> <em>(John Burden et al)</em> (summarized by Rohin) (H/T Haydn Belfield): This paper argues that we should decompose the notion of &#x201C;intelligence&#x201D; in order to talk more precisely about AI risk, and in particular suggests focusing on <em>generality</em>, <em>capability</em>, and <em>control</em>. We can think of capability as the expected performance of the system across a wide variety of tasks. For a fixed level of capability, generality can be thought of as how well the capability is distributed across different tasks. Finally, control refers to the degree to which the system is reliable and deliberate in its actions. The paper qualitatively discusses how these characteristics could interact with risk, and shows an example quantitative definition for a simple toy environment.</p><h1>OTHER PROGRESS IN AI </h1><h2>REINFORCEMENT LEARNING </h2><p><strong><a href=""http://proceedings.mlr.press/v123/crosby20a/crosby20a.pdf"">The Animal-AI Testbed and Competition</a></strong> <em>(Matthew Crosby et al)</em> (summarized by Rohin) (H/T Haydn Belfield): The Animal-AI testbed tests agents on the ability to solve the sorts of tasks that are used to test animal cognition: for example, is the agent able to reach around a transparent obstacle in order to obtain the food inside. This has a few benefits over standard RL environments:</p><p>1. The Animal-AI testbed is designed to test for specific abilities, unlike environments based on existing games like Atari.</p><p>2. A single agent is evaluated on multiple hidden tasks, preventing overfitting. In contrast, in typical RL environments the test setting is identical to the train setting, and so overfitting would count as a valid solution.</p><p>The authors ran a competition at NeurIPS 2019 in which submissions were tested on a wide variety of hidden tasks. The winning submission used an iterative method to design the agent: after using PPO to train an agent with the current reward and environment suite, the designer would analyze the behavior of the resulting agent, and tweak the reward and environments and then continue training, in order to increase robustness. However, it still falls far short of the perfect 100% that the author can achieve on the tests (though the author is not seeing the tests for the first time, as the agents are).</p><p><strong>Read more:</strong> <strong><a href=""https://link.springer.com/epdf/10.1007/s11023-020-09535-6?sharing_token=i8b_fK9gxcyMy7NNgyCRMve4RwlQNchNByi7wbcMAY42BXcZEQjOOpc2RsMol991jcge_tF0YABvUxEUc3Q0TbJWKecuuFNI6HznqjkkpkrZX7M6A47XgXxBWQLXteo_jas2coehuGsRLDDrYEQ-dUgHVQsbwCN6FXNe7IeOpJM%3D"">Building Thinking Machines by Solving Animal Cognition Tasks</a></strong></p><p><strong>Rohin&apos;s opinion:</strong> I&#x2019;m not sure that the path to general intelligence needs to go through replicating embodied animal intelligence. Nonetheless, I really like this benchmark, because its evaluation setup involves new, unseen tasks in order to prevent overfitting, and because of its focus on learning multiple different skills. These features seem important for RL benchmarks regardless of whether we are replicating animal intelligence or not.</p><p><strong><a href=""http://arxiv.org/abs/2002.11708"">Generalized Hindsight for Reinforcement Learning</a></strong> <em>(Alexander C. Li et al)</em> (summarized by Rohin): <strong><a href=""https://arxiv.org/abs/1707.01495"">Hindsight Experience Replay</a></strong> (HER) introduced the idea of <em>relabeling</em> trajectories in order to provide more learning signal for the algorithm. Intuitively, if you stumble upon the kitchen while searching for the bedroom, you can&#x2019;t learn much about the task of going to the bedroom, but you can learn a lot about the task of going to the kitchen. So even if the original task was to go to the bedroom, we can simply pretend that the trajectory got rewards as if the task was to go to the kitchen, and then update our kitchen-traversal policy using an off-policy algorithm.</p><p>HER was limited to goal-reaching tasks, in which a trajectory would be relabeled as attempting to reach the state at the end of the trajectory. What if we want to handle other kinds of goals? The key insight of this paper is that trajectory relabeling is effectively an inverse RL problem: we want to find the task or goal for which the given trajectory is (near-)optimal. This allows us to generalize hindsight to arbitrary spaces of reward functions.</p><p>This leads to a simple algorithm: given a set of N possible tasks, when we get a new trajectory, rank how well that trajectory does relative to past experience for each of the N possible tasks, and then relabel that trajectory with the task for which it is closest to optimal (relative to past experience). Experiments show that this is quite effective and can lead to significant gains in sample efficiency. They also experiment with other heuristics for relabeling trajectories, which are less accurate but more computationally efficient.</p><p><strong>Rohin&apos;s opinion:</strong> Getting a good learning signal can be a key challenge with RL. I&#x2019;m somewhat surprised it took this long for HER to be generalized to arbitrary reward spaces -- it seems like a clear win that shouldn&#x2019;t have taken too long to discover (though I didn&#x2019;t think of it when I first read HER).</p><p><strong><a href=""http://arxiv.org/abs/2002.11089"">Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement</a></strong> <em>(Benjamin Eysenbach, Xinyang Geng et al)</em> (summarized by Rohin): This paper was published at about the same time as the previous one, and has the same key insight. There are three main differences with the previous paper:</p><p>1. It shows theoretically that MaxEnt IRL is the &#x201C;optimal&#x201D; (sort of) way to relabel data if you want to optimize the multitask MaxEnt RL objective.</p><p>2. In addition to using the relabeled data with an off-policy RL algorithm, it also uses the relabeled data with behavior cloning.</p><p>3. It focuses on fewer environments and only uses a single relabeling strategy (MaxEnt IRL relabeling).</p><h1>NEWS </h1><p><strong><a href=""https://www.fhi.ox.ac.uk/researcher-hiring-2020/"">FHI is hiring Researchers, Research Fellows, and Senior Research Fellows</a></strong> <em>(Anne Le Roux)</em> (summarized by Rohin): FHI is hiring for researchers across a wide variety of topics, including technical AI safety research and AI governance. The application deadline is October 19.</p><h4><strong>FEEDBACK</strong></h4><p>I&apos;m always happy to hear feedback; you can send it to me, <strong><a href=""https://rohinshah.com/"">Rohin Shah</a></strong>, by <strong>replying to this email</strong>.                         </p><h4><strong>PODCAST</strong></h4><p>An audio podcast version of the <strong>Alignment Newsletter</strong> is available. This podcast is an audio version of the newsletter, recorded by <strong><a href=""http://robertskmiles.com/"">Robert Miles</a></strong>.</p>",rohinmshah,rohinmshah,Rohin Shah,
G6d8dkm6M8e7pRLsJ,Spoiler-Free Review: Orwell,spoiler-free-review-orwell,https://www.lesswrong.com/posts/G6d8dkm6M8e7pRLsJ/spoiler-free-review-orwell,2020-09-16T13:30:02.648Z,11,2,2,False,False,,"<p>Orwell is another game in the discrete-choices-over-time genre. In this case, you are an investigator, and choose which ‘datachunks’ to upload into the system. From there, others will take action.</p>



<p>Like other games in the genre, if you are going to play, play it blind. </p>



<p>I’d rank the game as lower Tier 3 – it’s good at its job, but not essential. It mostly does what it sets out to do, creating an experience and an atmosphere. It has some big frustrations along the way. </p>



<p>Orwell has three problems that prevent it from doing better. It’s also short.</p>



<p>You should play Orwell if and only if the concept of Orwell seems like something you want to experience. </p>



<p>Problem one, which is not in any way a spoiler, is that a lot of the game effectively involves finding the datachunks, or links on pages that lead to new pages that in turn contain datachunks. Several times I got frustratingly stuck trying to figure out where the game wanted me to click. Similarly, there is a star by things that are new, which leads to furious “make the star go away” actions to allow for better searching. </p>



<p>Problem two, which is a <em>minor </em>spoiler, is that the game often gives you less choices than it looks like it can, or than it easily could. Events mostly seem to proceed in order, so you don’t really have the option to withhold most datachunks. Several times I wanted to not upload something, but the game would simply not proceed if I didn’t do it. This leads to the problem of, if I don’t upload this, I could spend a <em>long </em>time not knowing if that’s the only way to advance the game while looking for some other way to advance it that might or might not exist. I would have appreciated a lot more flexibility. Mostly all the system gives you are some binary choices where two chunks conflict and you have to decide which one to go with.</p>



<p>Problem three requires spoiling the experience to talk about, so that would be a distinct post. </p>



<p></p>



<p></p>",Zvi,zvi,Zvi,
XzvR3QKkt9EPbAYyT,Applying the Counterfactual Prisoner's Dilemma to Logical Uncertainty,applying-the-counterfactual-prisoner-s-dilemma-to-logical,https://www.lesswrong.com/posts/XzvR3QKkt9EPbAYyT/applying-the-counterfactual-prisoner-s-dilemma-to-logical,2020-09-16T10:34:44.876Z,9,2,5,False,False,,"<p>The <a href=""https://www.lesswrong.com/posts/sY2rHNcWdg94RiSSR/the-counterfactual-prisoner-s-dilemma"">Counterfactual Prisoner's Dilemma</a> is a symmetric version of <a href=""https://www.lesswrong.com/tag/counterfactual-mugging?"">Counterfactual Mugging</a> where regardless of whether the coin comes up heads or tails you are asked to pay $100 and you are then paid $10,000 if <a href=""https://www.lesswrong.com/tag/omega"">Omega</a> predicts that you would have paid if the coin had come up the other way. If you decide updatelesly you will always received $9900, while if you decide updatefully, then you will receive $0. So unlike <a href=""https://www.lesswrong.com/tag/counterfactual-mugging"">Counterfactual Mugging</a>, pre-committing to pay ensures a better outcome regardless of how the coin flip turns out, suggesting that focusing only on your particular probability branch is mistaken.</p><p>The <a href=""https://www.lesswrong.com/posts/rqt8RSKPvh4GzYoqE/counterfactual-mugging-and-logical-uncertainty"">Logical Counterfactual Mugging</a> doesn't use a coin flip, but instead looks at the parity of sometime beyond your ability to calculate, like the 10,000th digit of pi. You are told it is even and then asked to pay $100 on the basis that if Omega predict you would have paid, then he would have given you $10,000 if had turned out to be odd.</p><p>You might naturally assume that you couldn't construct a logical version of the Counterfactual Prisoner's Dilemma. I certainly did at first. After all, you might say, the coin could have come up tails, but the 10,000th digit of pi couldn't have turned out to be odd. After all, that would be a logical impossibility.</p><p>But could the coin actually have come up tails? If the universe is deterministic, then the way it came up was the only way it could ever have come up. So is there is less difference between these two scenarios than it looks at first glance?</p><p>Let's see. For the standard counterfactual mugging, you can't find the contradiction because you lack information about the world, while for the logical version, you can't find the contradiction because of processing power. In the former, we could actually construct two consistent worlds - one where it is heads and one where it is tails - that are consistent with the information you have about the scenario. In the later, we can't.</p><p>Notice however that for <a href=""https://www.lesswrong.com/posts/rqt8RSKPvh4GzYoqE/counterfactual-mugging-and-logical-uncertainty"">Logical Counterfactual Mugging</a> to be well defined, you need to define what Omega is doing when it is making its prediction. In <a href=""https://www.lesswrong.com/posts/AKkFh3zKGzcYBiPo7/counterfactuals-for-perfect-predictors"">Counterfactuals for Perfect Predictors</a>, I explained that when dealing with perfect predictors, often the counterfactual would be undefined. For example, in <a href=""https://www.lesswrong.com/tag/parfits-hitchhiker"">Parfit's Hitchhicker</a> a perfect predictor would never give a lift to someone who never pays in town, so it isn't immediately clear that predicting what such a person would do in town involves predicting something coherent.&nbsp;</p><p>However, even though we can't ask what the hitchhiker would do <i>in an incoherent situation</i>, we can ask what they would do when they receive an input <i>representing an incoherent situation </i>(see <a href=""https://www.lesswrong.com/posts/AKkFh3zKGzcYBiPo7/counterfactuals-for-perfect-predictors"">Counterfactuals for Perfect Predictors</a> for a more formal description). Indeed, <a href=""https://www.lesswrong.com/tag/updateless-decision-theory?"">Updateless Decision Theory</a> uses this technique - programs are as defined as input-output maps - although I don't know whether Wei Dai was motivated by this concern or not.</p><p>Similarly, the predictor in <a href=""https://www.lesswrong.com/posts/rqt8RSKPvh4GzYoqE/counterfactual-mugging-and-logical-uncertainty"">Logical Counterfactual Mugging</a> must be predicting something that is well defined. So we can assume that it is producing a prediction based on an input, which may possibly represent a logically inconsistent situation. Given this, we can construct a logical version of the Counterfactual prisoner's dilemma. Writing this explicitly:</p><blockquote><p>First you are told the 10,000th digit of Pi. Regardless of whether it is odd or even, you are asked for $100. You are then paid $10,000 if you Omega predicts that you would produce output corresponding to paying when fed input correpsonding to having been informed that this digit had the opposite parity that you observed.</p></blockquote><p>There really isn't any difference between how we make the logical case coherent and how we make the standard case coherent. At this point, we can see that just as per the original <a href=""https://www.lesswrong.com/posts/sY2rHNcWdg94RiSSR/the-counterfactual-prisoner-s-dilemma"">Counterfactual Prisoner's Dilemma</a> always paying scores you $9900, while never paying scores you nothing. You are guaranteed to do better regardless of the coin flip (or in Abram Demski's terms we now have an <a href=""https://www.lesswrong.com/posts/pneKTZG9KqnSe2RdQ/two-types-of-updatelessness"">all-upside updateless situation</a>).</p>",Chris_Leong,chris_leong,Chris_Leong,
sEsFsFRrETpA42ekZ,Effective Altruism and Rationalist Philosophy Discussion Group,effective-altruism-and-rationalist-philosophy-discussion,https://www.lesswrong.com/posts/sEsFsFRrETpA42ekZ/effective-altruism-and-rationalist-philosophy-discussion,2020-09-16T02:45:32.109Z,7,1,0,False,False,,"<p>I&apos;ve created a group on <a href=""https://www.facebook.com/groups/211506573547011"">Facebook </a>for rationalists and effective altruists to discuss philosophy (currently has about 120 members). To be clear, this isn&apos;t a group where the philosophy has to be related to effective altruism or rationality, but instead a group for Rationalists and Effective Altruists to discuss philosophy.</p><p>I know it&apos;s already possible to post about philosophy here, but people tend to be reluctant to post unless they&apos;ve invested a huge amount of effort in writing it up, so it&apos;s useful to have somewhere else where there&apos;s a lower barrier.</p>",Chris_Leong,chris_leong,Chris_Leong,
eGmpH7WuyDRmGJ4HF,Memorizing a Deck of Cards,memorizing-a-deck-of-cards,https://www.lesswrong.com/posts/eGmpH7WuyDRmGJ4HF/memorizing-a-deck-of-cards,2020-09-16T01:31:05.201Z,27,13,1,False,False,,"<p>I just memorized a deck of cards, because I was told doing so <a href=""https://www.lesswrong.com/posts/P3zrurj5hHKFKDL3M/productivity-working-towards-a-summary-of-what-we-know#Deliberately_train"">would improve my focus</a>.</p>
<p>The process took 2 hours and 13 minutes, including breaks.  I used 20-5 pomodoro at first, and switched to 25-5 in the middle.</p>
<p>I initially wrote a script to simulate a randomly shuffled deck.  After memorizing the first 18 cards, I switched to a physical deck (starting with the same cards).</p>
<p>I chose not to use any strategy from an outside source (e.g. memory palace), because I wanted to see what I would come up with on my own, and I thought that using such a strategy might make the challenge too easy.</p>
<p>I began by memorizing cards in groups of four.  After memorizing a group, I would go back to the previous group and see if I still remembered all of the cards in both groups.  Then I would go back an arbitrarily far amount (wherever I cut the deck), and begin from there.  Occasionally, I would go back to the beginning and go through the deck up to the point I had memorized to.  If I messed up in an earlier group, I would repeat the process for that group.</p>
<p>However, after three such groups (12 cards), it became clear that four was too many to memorize at once, so I switched to groups of three.  I then ran into another problem-I could memorize individual groups fairly easily, but I would have trouble remembering the order in which the groups themselves came.  I was able to solve this problem by “linking” the groups together - memorizing the last card of the previous group along with the next group.</p>
<p>A few attributes of groups made them easier to memorize:</p>
<ul>
<li>two or more cards in the group share a face or suit
<ul>
<li>fortunately, this is true for most groups</li>
</ul>
</li>
<li>the group is monotonically increasing or decreasing</li>
<li>two consecutive cards in the group numbers which are related to one another
<ul>
<li>one is a factor of the other (e.g. 3 and 9),</li>
<li>they are consecutive (e.g. 10 and jack)</li>
</ul>
</li>
</ul>
<p>The easiest group was 4 of spades, 7 of spades, 8 of spades - increasing and all of the same suit.</p>
<p>Some of my most common mistakes were:</p>
<ul>
<li>mixing up suits of the same color</li>
<li>getting the order of the groups wrong</li>
<li>mixing up aces and queens</li>
</ul>
<p>Sometime into the task, I grew anxious that I was wasting my time, or that it wouldn’t help, or that I should be working on something else.  There were times when I felt an urge to stop and do anything else.  I often get such urges when doing other activities, but this time I was able to resist them.  I think that this might be because there was also a clear next step in this task, and because I knew that the point of the task was to train my focus.</p>
<p>About three hours after my first success, I went through the deck again, with no practice in between.  I only got three of the cards wrong, though I frequently had to think for a while.</p>
<p>Did this activity improve my focus? I suppose we’ll have to wait and see.  I was able to write this post, which isn’t something I would normally have the willpower to do, but that may have been me riding on the high of memorizing the deck (It felt so satisfying when I finally memorized the whole thing).  Maybe I’ll post an update next week.</p>
",Tofly,tofly,Tofly,
6J2ganB4NH7MpDhdX,The Axiological Treadmill,the-axiological-treadmill,https://www.lesswrong.com/posts/6J2ganB4NH7MpDhdX/the-axiological-treadmill,2020-09-15T18:36:31.954Z,6,10,13,False,False,,"<p>The obvious reason that <a href=""https://grandunifiedempty.com/2020/09/15/the-great-project/"">Moloch is the enemy</a> is that it destroys everything we value in the name of competition and survival. But this is missing the bigger picture. We value what we value <em>because</em>, in our ancestral environment, those tended to be the things that helped us with competition and survival. If the things that help us compete and survive end up changing, then evolution will ensure that the things we value change as well.</p><p>To <a href=""https://slatestarcodex.com/2014/07/30/meditations-on-moloch/"">borrow a metaphor</a>: Elua cheats. The hedonic treadmill has <em>nothing</em> on the axiological treadmill.</p><p>Consider a thought experiment. In <em>Meditations on Moloch</em>, Scott Alexander dreams up a dictatorless dystopia:</p><blockquote><em>Imagine a country with two rules: first, every person must spend eight hours a day giving themselves strong electric shocks. Second, if anyone fails to follow a rule (including this one), or speaks out against it, or fails to enforce it, all citizens must unite to kill that person. Suppose these rules were well-enough established by tradition that everyone expected them to be enforced.</em></blockquote><blockquote><em>So you shock yourself for eight hours a day, because you know if you don&#x2019;t everyone else will kill you, because if they don&#x2019;t, everyone else will kill&#xA0;them, and so on. Every single citizen hates the system, but for lack of a good coordination mechanism it endures. From a god&#x2019;s-eye-view, we can optimize the system to &#x201C;everyone agrees to stop doing this at once&#x201D;, but no one within the system is able to effect the transition without great risk to themselves.</em></blockquote><p>Even if this system came into being ex nihilo it probably wouldn&#x2019;t be stable in reality; a population that spends eight hours a day receiving strong shocks isn&#x2019;t going to be able to feed itself, or reproduce. But assume for a moment that this system starts out economically and biologically stable (that is, people can still eat, and reproduce at the rate of replacement, despite the electric shocks, and that there are no outside countries ready to invade). What do we expect to happen over the long run?</p><p>Well, obviously there&#x2019;s a strong evolutionary pressure to be tolerant to electric shocks. People who can tolerate those shocks better will do better on average than those who can&#x2019;t. However, there&#x2019;s another more subtle pressure at play: the pressure to <em>ensure you shock yourself</em>. After all, if you forget to shock yourself, or choose not to, then you are immediately killed. So the people in this country will slowly evolve reward and motivational systems such that, from the inside, it feels like they <em>want</em> to shock themselves, in the same way (though maybe not to the same degree) that they want to eat. Shocking themselves every day becomes an intrinsic value to them. Eventually, it&#x2019;s no longer a dystopia at all.</p><p>They would be aghast at a society like ours, where Moloch has destroyed the value of receiving electrical shocks, all in the name of more perfect competition.</p><p><em>[Cross-posted from <a href=""https://grandunifiedempty.com/2020/09/15/the-axiological-treadmill/"">Grand, Unified, Empty</a>.]</em></p>",,,,
QJTq8DDj5yhe8EW6M,Low hanging fruits (LWCW 2020),low-hanging-fruits-lwcw-2020,https://www.lesswrong.com/posts/QJTq8DDj5yhe8EW6M/low-hanging-fruits-lwcw-2020,2020-09-15T18:15:54.745Z,17,7,11,False,False,,"<p>During Less Wrong Community Week-end (Europe), in one event people share low-hanging fruit they used. I chaired it this year and defined low-hanging fruit as something that can be easily bought or done that improved the life. Here is the list of fruits shared in 2020. All typos are mine. ""I"" usually reflect the one who gave the message and not the author of this blog post.</p>
<h1>Water watering bulbs</h1>
<p>If you don't know how much to water your plants, if you did too much or not enough, let bulbs do it for you. <a href=""https://www.amazon.ca/-/fr/darrosage-automatique-dint%C3%A9rieur-dext%C3%A9rieur-vacances/dp/B084Q65SYP/ref=sr_1_7?dchild=1&amp;keywords=ceramic+plant+watering+spikes&amp;qid=1600006388&amp;sr=8-7"">Examples</a> of <a href=""https://www.amazon.ca/-/fr/gp/product/B075JDWGHS/ref=ppx_yo_dt_b_asin_title_o04_s00?ie=UTF8&amp;psc=1"">bulbs</a> are</p>
<h1>Before work time</h1>
<p>Reserve some time in the morning, before you head out to work, and invest this into something that is important to you. You do this without any experience from the day (bad/good) and with you full physical ability.</p>
<h1>Ad block on smartphone</h1>
<p>Blokada <a href=""https://blokada.org/index.html"">https://blokada.org/index.html</a>  Add blocking for android. Reduce noises, distraction, bandwidth (?), easy to install. added bonus: Firefox + Ublock origin</p>
<h1>Kalimba</h1>
<p>Playing music is to listening to live music as live music is to recorded music. If you don't want to spend time to learn an instrument, the <a href=""https://www.google.com/search?q=Kalimba+instrument&amp;sxsrf=ALeKk017PoMf1ZOqQFhcjQckdDhh8SmCCQ:1600006685604&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwiBpYixqebrAhUa6OAKHXthCXEQ_AUoAXoECBsQAw&amp;biw=1211&amp;bih=1361&amp;dpr=1"">Kalimba</a> is cheap and directly lead to beautiful musics.</p>
<h1>Better sleeping</h1>
<h2>No device at night</h2>
<p>Set all devices to lock at sleeping time.</p>
<h2>Light</h2>
<p>Get a <a href=""https://www.amazon.co.uk/adjustable-dimmable-required-HomeKit-Assistant/dp/B0753SBGK2/ref=sr_1_1?dchild=1&amp;keywords=lifx+bulb+day+and+dusk&amp;qid=1600007024&amp;sr=8-1"">smart lightbulb</a>, and setting to slowly dim/become red for the half hour before bed - makes going to bed at the right time the default action and makes me feel tired, and signficantly decreases the willpower it takes.</p>
<p>End the day with a paper book to avoid looking at screen.</p>
<p>Two to ten minutes of sun in the morning, or by default strong light</p>
<h2>Melatonine</h2>
<p>Takes melatonine.</p>
<h2>Morning</h2>
<p>Schedule things in morning so that you have incentive to sleep.</p>
<p>Having phone 3 meters from bed, to force to go outside of bed to turn it of</p>
<h2>Day / night separation</h2>
<p>Ensuring you don't see your bed from your workplace (and reciprocally) , to feel the separation between work space and personal space</p>
<h1>Neater writing</h1>
<p>Switching to using a fountain pen can force you to write slower and therefore neater. Pilot makes very cheap and good-writing fountain pens (the Pilot Varsity) that are disposable just like regular ballpoints, they're around $20 US for a pack of 12</p>
<h1>Note taking</h1>
<h2>One note</h2>
<p>Use the software One Note to keep tracks of notes about everything.</p>
<h2>Recalling fact about friends</h2>
<p>You can use space repetition system to recall fact about friends (who is friend with who, where they moved to, what's the current job...)</p>
<p>Writing a name done help to remember it (some people at least)</p>
<p>Uses Facebook event to remember who went to an event, who you met, and use it to take note (avoid notes from fetlife events)</p>
<p>Spaced repetition also helps to recall birthdays of friends/family!</p>
<h2>Relate knowledge</h2>
<p>Create your own wiki (e.g. wikimedia) to keep notes and links them together, so that you can revisit them when you want. Pre-commit two hours each month to see if you want to improve the wiki, add links, explanation</p>
<h2>Bullet</h2>
<p>Bullet journaling, there's a great guide on reddit at /r/bulletjournal</p>
<h2>Notebook</h2>
<p>carrying a small A5 or A6 sized notebook with you can be very useful</p>
<h2>Idea catcher</h2>
<p>Have an idea catcher, some place to write idea down to not forget it and not keeping the idea in mind while you do something else</p>
<h2>Password manager</h2>
<p>Use a password manager to recall general private information you need to generally have private and accessible such as account number, all previous addresses, photo, file</p>
<h1>Win time while writing</h1>
<p>If there are unicode symbols you expect to use often (e.g. math symbol, foreign letter), save them as shortcut or emoji, so they can be accessed quickly and put in any message</p>
<p>Using autocorrect allow to shorten text you write often. E.g. @@ is automatically replaced by your email adresse, sigma replace by σ )</p>
<p>You can also do this on windows with autohotkey and there are similar scripts for linux/mac</p>
<p>even lower hanging: I find the US international layout helpful""</p>
<h1>Water stone</h1>
<p>Start with combination stone. low to get better knife, and then cook easier to cut your food. It helps to relax. Allow more flexibility/granularity than standard knife sharpening</p>
<h1>Emails</h1>
<p>Set up email inbox rules to sort messages from senders you consider low-priority or work-related (if you use a combined inbox) to their own folder so you don't have to look at them all the time. also: if using Outlook Web App there's an option to have a text message sent to your phone as a result of an inbox rule: I have a rule that texts me if my boss's boss (2 levels up) sends a direct email with my name in the To: line</p>
<h1>Chat</h1>
<p>Use more gif in chat conversation to add more silliness and joy in chat discussions</p>
<h1>Light</h1>
<p>As your light bulbs in your house burn out convert to LED ones, they use less power and can be softer too! You can buy LEDs with the sun spectrum and same intensity!</p>
<h1>Sex life</h1>
<p>Keep a list of desire/fantasy, so that if a new relationship ask what new thing you'd want to try, you don't get lost thinking about it</p>
<h1>Back pain</h1>
<p>Leaning about the back of the chair a few time a day relax. Most office chair allow to do it</p>
<p>Instead of getting a gaming chair spend the same money on a lightly used executive office chair like a Herman Miller or a Steelcase, your back will thank you. also: if you have a dealer that sells those fancy chairs near you, you can go try them to find out what size fits you best before you look for used one</p>
<h1>Lost wallet</h1>
<p>Have a list of phone number to call in your wallet so that the people can call you.</p>
<h1>Extra lists</h1>
<p>Neel Nanda shared his <a href=""https://www.neelnanda.io/blog/mini-blog-post-19-on-systems-living-a-life-of-zero-willpower"">personal list</a> of low hanging fruit.</p>
",Arthur-Milchior,arthur-milchior,Arthur Milchior,
7TFZZTFiZBtNvErit,God in the Loop: How a Causal Loop Could Shape Existence,god-in-the-loop-how-a-causal-loop-could-shape-existence,https://www.lesswrong.com/posts/7TFZZTFiZBtNvErit/god-in-the-loop-how-a-causal-loop-could-shape-existence,2020-09-15T14:40:45.449Z,1,3,4,False,False,,"<p><em>Crossposted from</em> <em><a href=""https://www.vesselproject.io/god-in-the-loop"">Vessel Project</a>. </em></p><br><p>My last article, &#x201C;<a href=""https://www.vesselproject.io/life-through-quantum-annealing"">Life Through Quantum Annealing</a>&#x201D; was an exploration of how a broad range of physical phenomena &#x2014; and possibly the whole universe &#x2014; can be mapped to a quantum computing process. But the article simply accepts <em>that</em> quantum annealing behaves as it does; it does not attempt to explain <em>why</em>. That answer lies somewhere within a &#x201C;true&#x201D; description of quantum mechanics, which is still an outstanding problem.</p><p>Despite the massive predictive success of quantum mechanics, physicists still can&#x2019;t agree on how its math corresponds to reality. Any such proposal, called an &#x201C;interpretation&#x201D; of quantum mechanics, tends to straddle the line between physics and philosophy. There is no shortage of interpretations, and in the words of physicist David Mermin, &#x201C;New interpretations appear every year. None ever disappear.&#x201D; Am I going to throw one more on that pile? You bet. </p><p>I&#x2019;m not going to start from scratch though; I simply propose an ever-so-slight modification to an existing forerunner: the many-worlds interpretation, where other &#x201C;worlds&#x201D; or timelines exist in parallel to our own. My modification is this: the only worlds that <em>can </em>exist are those that exist within a causal loop. Stated another way: our universe, or any possible universe, must be a causal loop.  </p><p>I will introduce the relevant concepts and provide an argument for my proposal, but my goal is not to once-and-for-all prove this interpretation as true. Rather, my goal is to explore what happens if we accept the interpretation as true. If we start with the assumption that only causal loop universes can exist, then several interesting things follow &#x2014; we find parallels to our own universe, and we might even find God.  <br></p><h3><strong>Causality &amp; Quantum Interpretations</strong></h3><p>Before talking about causal loops, let&#x2019;s take a step back and talk about causality &#x2014; perhaps the single most fundamental concept in all the sciences. It plays a starring role in the two most important theories in physics: general relativity and quantum mechanics. </p><p>General relativity, developed by Einstein, combines space, time, and gravity in a geometric description of spacetime. In spacetime, two observers might not agree on the space between two events or time between two events &#x2014; but they <em>always </em>agree on the <a href=""https://www.youtube.com/watch?v=YycAzdtUIko"">spacetime interval</a>, which corresponds to a causal relationship between two events. As such, causality is the only thing that is universally agreed on, making it the only proper description of objective reality. Another phenomenon predicted by general relativity is the cosmic speed limit &#x2014; the speed of light &#x2014; which is more appropriately understood as the <a href=""https://www.youtube.com/watch?v=msVuCEs8Ydo"">speed of causality</a>, more fundamental than light alone. Here we see that spacetime and the speed of light are not inherently real; they are just useful ways of describing causality, the only objective reality. </p><p>But if general relativity is interesting because we can <em>only</em> agree on causality, then quantum mechanics is interesting because we <em>can&#x2019;t</em> agree on causality.  </p><p>As I alluded to earlier, the full explanation of quantum mechanics is still a mystery, and that mystery has everything to do with causality &#x2014; specifically how objective, causal reality relates to the wave function. The wave function of a quantum system is most easily understood as a probability distribution, where the probability of the system being in any given state is calculated when you square the amplitude of the wave function for that state. The wave function is in a &#x201C;superposition&#x201D; of all possible states until it is measured, after which we observe a single state. </p><span><figure><img src=""https://images.squarespace-cdn.com/content/v1/5ed2939ea763213deeee805e/1599779119805-57H5XJIR5QQKB7D387EZ/ke17ZwdGBToddI8pDm48kIkgaBd8ayW8Y-IaXtzGPGpZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpw6bd9KCPtd5f1VUknwX4myGI7Q7Uhklk7J3PKIesQYBhM3JPUTVnkii7xQtiaQlFg/wave+function.png?format=750w"" class=""draft-image center"" style=""width:81%""></figure></span><p><em>Simple depiction of a quantum wave function with a single crest. (Image by Louay Fatoohi)</em></p><br><p>On the surface, it appears that quantum physics is inherently random if it can only be described by probability, and somehow the act of measuring or observing the system causes it to assume an objective state. If you&#x2019;re convinced of this, then you basically agree with the Copenhagen interpretation of quantum mechanics, which posits an interaction between the system and observer that causes the wave function to randomly &#x201C;collapse.&#x201D; This has been the standard interpretation for a long time, although others interpretations have been consistently gaining steam. </p><p>As an alternative, maybe you don&#x2019;t think the universe is random at all &#x2014; it&#x2019;s deterministic, but there are &#x201C;hidden&#x201D; variables we don&#x2019;t yet know about. In this case, there is no wave function collapse, so we don&#x2019;t need to introduce any extra physics to explain what happens when we observe the system. If you&#x2019;re on board with that, then you just signed up for the de Broglie-Bohm theory, also known as the pilot wave theory. </p><p>But maybe you&#x2019;re still not quite convinced. Let&#x2019;s make things even simpler: the universe isn&#x2019;t random, but there aren&#x2019;t hidden variables either. You have the wave function, and that&#x2019;s it &#x2014; what you see is what you get. That, it turns out, pretty much sums up the many-worlds interpretation. In this theory, all possible &#x201C;worlds&#x201D; described by the wave function do exist; we just happen to occupy one of them. While it requires the least explanation, the bizarre implication is that many parallel worlds exist as branches of different possible outcomes. </p><p>These three interpretations tend to be the top contenders, and they each take a different approach to answer the question of &#x201C;what causes what?&#x201D; The fact that a basic causal structure of physics cannot gain consensus makes this interesting territory, plus it has far-reaching implications. A proper explanation doesn&#x2019;t just account for the non-locality of entanglement or the apparent uncertainty of superposition &#x2014; it explains how humans fit into reality. </p><p>As observers, do we cause the wave function to collapse? That would certainly seem to elevate the role of consciousness in the causal nature of reality (which has not gone unnoticed by experts and cranks alike). Or is the wave function itself the only causal, objective reality? If so, that&apos;s one more reason to believe we&#x2019;re at the mercy of a universe that&#x2019;s indifferent to our existence. </p><p>The third interpretation is the one I want to revisit later in this article: the many-worlds interpretation. Keep it in mind. While it appears to threaten our sense of importance and potentially free will, it&apos;s not so bad if we just add a twist &#x2014; or better yet, a loop. <br></p><h3><strong>Causal Loops</strong></h3><p>It may be fairly self-explanatory, but I&#x2019;ll nonetheless define a causal loop as follows: a closed causal chain of events, where each event is the effect of another event on the chain. A simple example is a loop of three distinct events where Event A causes Event B, which causes Event C, which in turn causes Event A &#x2014; each event is causally connected to another on the loop, and there is no &#x201C;first&#x201D; event. If you start at any one event, the sequence that follows inevitably leads back to the same event as if it caused itself. </p><p>Causal loops sound absurd, but their possibility has been successfully defended, particularly by philosopher Richard Hanley in his paper, &#x201C;<a href=""https://link.springer.com/article/10.1023/B:SYNT.0000035847.28833.4f"">No End in Sight: Causal Loops in Philosophy, Physics, and Fiction</a>&#x201D; (and any mention of Hanley moving forward is in reference to this paper). Hanley points out that causal loops are not logically inconsistent nor physically impossible &#x2014; at worst, they simply require grand coincidences. Causal loops as a whole aren&#x2019;t created, they simply exist, and any strangeness about this is merely apparent &#x2014; they&#x2019;re in no worse a position concerning the question of why anything exists. Interestingly, Hanley also mentions that the idea of a causal loop universe is taken very seriously in cosmology, and that causal loops are actually more likely to occur in a universe like ours which hosts intelligent agents. </p><p>Let&#x2019;s unpack that a bit. If intelligent agents discover their universe is indeed a giant causal loop, they may have incentive to maintain the loop they inhabit by causing the very events that lead to their own existence. Furthermore, they can intentionally make events happen that would otherwise require coincidence. But there is nothing coincidental or mysterious about an intentional action; we intentionally do things every day. Hanley notes, &#x201C;the existence of agency may be the very thing that permits causal loops to obtain.&#x201D; </p><p>This is where I&#x2019;ll take it one step further than Hanley: not only are causal loop universes possible, but all possible universes must be causal loops. As I mentioned earlier, I&#x2019;m going to run with this as an assumption, but I&#x2019;ll still attempt to provide some reasoning. </p><p>That reasoning boils down to two propositions: the first is that all events must have causes; the second is that only in closed causal chains do all events have causes. We saw that in a causal loop all events have definitive causes &#x2014; other events on the loop. There is no issue. But in an open causal chain (imagine a straight line), one more event is always required to explain causation. We&#x2019;re left with a case of infinite regress, which isn&#x2019;t inherently problematic, but its &#x201C;openness&#x201D; implies there must be an event without a cause, which is impossible. Furthermore, any series of causes and effects cannot, by definition, be considered as part of separate causal systems. If we define a universe to be a causal system, then it follows that all universes must also be causal loops, including our universe.</p><p>Using a causal-loop-only starting point, we can dive into some pretty interesting things.<br></p><h3><strong>Different Paths: Curved Spacetime &amp; Clever Demons  </strong></h3><p>In general relativity, causal loops are permissible in the context of a &#x201C;<a href=""https://www.abc.net.au/news/science/2018-09-02/block-universe-theory-time-past-present-future-travel/10178386"">block universe.</a>&#x201D; In causal loops, all events in the loop are equally real all the time; they must be for the loop to exist. This closely aligns with a block universe, where all past, present, and future points in spacetime exist &#x201C;at once&#x201D;; we simply find ourselves at one point along its progression. In both views, travelling back to the past is possible, but you cannot change the past. If you do travel back to the past though, you may find yourself travelling along a different timeline after that &#x2014; which brings us back to quantum mechanics. </p><p>You took note of the many-worlds interpretation (MWI) of quantum mechanics, right? If the universe is a causal loop, then whatever interpretation we use must be deterministic since all events along a causal loop are equally real &#x2014; they do not spontaneously become real only after another event. Technically any deterministic interpretation suffices to meet that criteria, but I think the MWI best illustrates the range &#x2014; and restrictions &#x2014; of how possible universes can unfold. The MWI entails the universe &#x201C;splitting&#x201D; into alternate histories at every point in time. If we introduce a constraint where only causal loop universes can exist, that directly impacts the range of possible universes we can ever split into. Nothing else about the MWI needs to change; there are still many parallel worlds, but they all must maintain a causal loop. So if we somehow traveled back in time, we may find ourselves splitting into a different looped timeline than before. </p><p>Things get interesting when we start to look at possible loops. There are only two ways a causal loop can be maintained in practice: closed timelike curves (CTCs), and reverse causation. While the two entail similarities, they are slightly distinct. </p><p>CTCs are theoretically possible in certain solutions of spacetime. One example, popular in science fiction, is a wormhole. In some wormholes, you&#x2019;ll enter one end and exit the other at a previous point in time. But there is serious doubt about whether they could be feasibly traveled through, plus they&#x2019;re just local anomalies. If we&#x2019;re talking about the whole universe, we need to go bigger. </p><p>The great logician Kurt G&#xF6;del did <a href=""https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.21.447"">find one solution</a> to Einstein&#x2019;s equations, now called the G&#xF6;del universe, where the entire universe is a CTC. In such a universe, all points in spacetime return to themselves as we&#x2019;d expect in a causal loop, but it requires that all galaxies have a preferred direction of rotation, for which there is no evidence. When G&#xF6;del found his solution, the tools used to study cosmology were not powerful enough to confirm if our universe was a G&#xF6;del universe. As the technology became more sophisticated throughout his life up until his death in 1978, G&#xF6;del would ask, &#x201C;Is the universe rotating yet?&#x201D; The answer was always no. As best as we can tell, our universe is not a giant CTC, but G&#xF6;del might not be out of luck just yet. </p><span><figure><img src=""https://images.squarespace-cdn.com/content/v1/5ed2939ea763213deeee805e/1599779292009-IPYEQLS73DRW87OVAYU8/ke17ZwdGBToddI8pDm48kODSPDJR5RNDA6-DcxCQlwdZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVHUP7KqPAQf1Mc8OhmEsQyfocJ1zuJ2kYTVkmbUzES2c6QvevUbj177dmcMs1F0H-0/G%C3%B6del+Universe.png?format=500w"" class=""draft-image center"" style=""width:48%""></figure></span><p><em>A G&#xF6;del universe represented by &#x201C;light cones&#x201D; and&#xA0;a possible path of a light through spacetime. </em></p><br><p>Reverse causation, as Hanley defines it, is simply &#x201C;a cause and effect relation where effect precedes cause.&#x201D; Any notion of reverse causation, or causal loops in general, is intimately tied to information. Every single event or state of the universe exists in terms of information, as does each causal relationship. Information is also what makes events distinct and unique. If the universe were to suddenly return to some state <em>X</em> that existed an hour ago &#x2014; informationally identical in every way &#x2014; then we&#x2019;re not talking about another state similar to <em>X</em>; that <em>is X</em>. Each event in a causal loop is fully and uniquely described by information. </p><p>One feature of our universe is that information becomes increasingly diffuse, a natural result of the second law of thermodynamics, which holds that the universe always trends toward maximum entropy, or equilibrium. Entropy can be understood as a measure of disorder; it always tends to increase locally, but the overall entropy of the universe stays constant. Said another way: although information is never actually lost, it tends to become more disordered. </p><p>Therein lies our grand dilemma. As physicist Lee Smolin writes in <em>The Singular Universe</em>, &#x201C;The fact to be explained is why the universe, even 13.8 billion years after the Big Bang, has not reached equilibrium, which is by definition the most probable state, and it hardly suffices to explain this by asserting that the universe started in an even less probable state than the present one.&#x201D; How did the universe ever arrive at a more ordered state when it clearly prefers the opposite? Obviously it&apos;s a conundrum in our existing models, but doubly so if we are to imagine a future in our causal loop that goes totally against a law of nature. This question has already drawn some eyebrow-raising proposals. </p><p>Ludwig Boltzmann, the 19th century physicist who <em>developed </em>the second law of thermodynamics, gave one proposal: the second law is a statistical phenomenon, so given enough time, there&#x2019;s a non-zero chance the universe will randomly fluctuate back into a low-energy state. But according to Boltzmann&#x2019;s own principles, something like the big bang is literally the least likely thing that can happen; while not necessarily impossible, we&#x2019;re going to explore a more probable scenario. </p><p>A contemporary of Boltzmann, James Clerk Maxwell, devised a thought experiment called &#x201C;Maxwell&#x2019;s demon&#x201D; in an attempt to violate the second law. He imagined a demon that controlled a small door between two gas chambers. As individual gas molecules approached the door, the demon would quickly open and close it so that all the fast molecules became trapped in one chamber, and the slow molecules in the other. In doing so, Maxwell proposed the second law was violated since the chamber system became more ordered; one side became hotter and the other became cooler, even though it was totally mixed before. With regard to information, entropy had been lowered &#x2014; or so he thought.</p><span><figure><img src=""https://images.squarespace-cdn.com/content/v1/5ed2939ea763213deeee805e/1599779591558-X2JBVEYR2KSE3TEF9Z6D/ke17ZwdGBToddI8pDm48kIDdPFKcWucnD_sWOmODuW8UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcN9jzis03zyr5dnOLmO4AGdT2XLD9QUiT0EDYN7pGjjEfVqP97t4J3jY6nCpCidJm/Maxwell%27s+demon.png?format=750w"" class=""draft-image center"" style=""width:100%""></figure></span><p><em>In this Maxwell&#x2019;s demon setup, chambers A and B both start with mixed gas, but over time&#xA0;chamber A becomes cold and chamber B becomes hot. (Source: Htkym / CC BY-SA)</em></p><br><p>Others said not so fast. Although entropy in the chambers decreased, the entropy in the demon&#x2019;s <em>memory </em>increased. Imagine that the demon&#x2019;s memory started as a blank slate &#x2014; highly ordered. As it observed the system, it had to fill its memory with information about the gas molecules to know how to operate the door. In doing so, the information in its memory became more disordered, thereby preserving the second law. </p><p>But the demon can just forget that information, right? In doing so, its memory goes back to a blank slate, but the gas is still highly ordered. Seems like an easy solution. Again, not so fast &#x2014; the loss of information entails a dissipation of heat, which increases the entropy of its surroundings. Alas, it seems the second law cannot be slayed. But maybe it doesn&#x2019;t need to be. </p><p>When looking at the entire system in Maxwell&#x2019;s thought experiment &#x2014; which really includes the chambers, the demon, and the demon&#x2019;s environment &#x2014; we notice several things. One is that information can take several forms, such as the properties of gas, memory in the brain, and in the effects of heat. Another is that although the second law is maintained and entropy&#x2019;s trend toward disorder never ceases, local arrangements of information can become more ordered, thus local entropy can decrease. To reiterate an earlier point: overall entropy never changes, but local entropy can. A third observation concerns what is required to produce local order: the demon. More generally, knowledge about the system, or memory, as well as the ability to act upon it to rearrange information. In fact, if an agent has perfect knowledge of a system, it can rearrange it in any way it desires. </p><p>Maybe you can see where this is going &#x2014; intelligence can manipulate information, and enough intelligence can hypothetically recreate a prior state of information in its own system, maintaining a causal loop. </p><p>Let&#x2019;s recap a bit: if we assume our universe is a causal loop, but it is not a CTC, and it probably did not randomly fluctuate to a highly-ordered state, then the only option left is to think that intelligence was used to cause a previous, highly-ordered state in the loop.</p><p>You may think &#x201C;yeah, but what are the odds of that?&#x201D; I&#x2019;m inclined to respond with, &#x201C;better than the alternatives.&#x201D; Remember, Hanley tells us these things are not impossible, they are merely coincidental; and causal loops are more likely to happen in a universe with intelligent agents. If a causal loop is the only type of universe that can exist, then it&#x2019;s not coincidental at all; it&#x2019;s simply how anything must exist. That alone eliminates the apparent absurdity. And although we&#x2019;re working with a sample size of one, the fact that our universe hosts intelligent life already makes the &#x201C;intentional causality&#x201D; path more probable than a random fluctuation. </p><p>I&#x2019;ll also add that this aligns with my discussion on quantum annealing, where a quantum annealing universe converges on it&#x2019;s highest probability state. If the many parallel timelines in the MWI follow a probability distribution, and all timelines must form causal loops, then not only are the most probable loops are those that contain intelligence, as Hanley suggests, but each loop that takes the intelligence &#x201C;route&#x201D; must ultimately land on a set of common characteristics &#x2014; they must all have the ability to manipulate information, or reality itself, in order to maintain a causal loop. If any one of them did not converge on this knowledge or technological sophistication, then the timeline would not exist in the first place, thus would not be included in the probability distribution. As such, any timeline we follow in the causal-loop-MWI formulation must converge on those traits too. </p><p>I also explained how a reward function within quantum annealing would result in the system having incentive to &#x201C;restart&#x201D; itself in order to maximize reward. Both causal loops and a quantum annealing universe involve a convergence on intelligence to facilitate a restart, and they involve the act of &#x201C;forgetting&#x201D; in order to restore a previous informational state. And although it&#x2019;s far cry from any firm proof, this heat-releasing forgetting process sounds a lot like our early universe &#x2014; a hot universe with a highly-ordered state. </p><p>From my perspective, causal loops and quantum annealing look like two sides of the same coin. Is it a coincidence that we seem to arrive at the same conclusions from two entirely different approaches? Or have we done away with coincidences?<br></p><h3><strong>Make It Loop: A How-To Guide</strong></h3><p>We manipulate information every day, whether it be physically, mentally, or digitally, but we could use more guidance in the way of resetting a universe &#x2014; it&apos;s a tall order. Information and entropy can take many forms, but there does appear to be one form that rules them all: Von Neumann entropy. I couldn&#x2019;t possibly summarize it better than physicist Matt O&#x2019;Dowd from <em>PBS Space Time</em>, so I won&#x2019;t try: </p><blockquote>Quantum entropy, also known as Von Neumann entropy . . . describes the hidden information in quantum systems, but more accurately, it&apos;s a measure of entanglement within quantum systems. In fact, the evolution of quantum entanglement may be the ultimate source of entropy, the second law, the limits of information processing, and <a href=""https://www.quantamagazine.org/quantum-entanglement-drives-the-arrow-of-time-scientists-say-20140416/"">even the arrow of time</a>. </blockquote><p>Von Neumann entropy is of particular interest in the study of quantum information &#x2014; namely, in black holes and quantum computing. One foundational tenet of quantum theory is that quantum information is never lost or destroyed. This presented a real problem in the &#x201C;black hole information paradox&#x201D; where physicist Steven Hawking pointed out that information seemed to be forever lost through what he called Hawking radiation, where information-carrying particles fall into a black hole, adding to its mass, but this same mass can escape through informationless photons, thereby erasing information.</p><p>Many physicists thought this paradox couldn&#x2019;t possibly be, so they devised several solutions to resolve it. Hawking himself even abandoned the paradox, convinced that information was preserved. One promising solution uses entanglement, the phenomenon whereby two particles, or qubits, must be described as a single state. In this solution, the photons escaping through a black hole&#x2019;s radiation are imprinted with information through entanglement &#x2014; information that can theoretically be retrieved. Norman Yao, from the University of California, Berkeley, told <em>Quanta Magazine</em>, &#x201C;If you were God and you collected all these Hawking photons, there is in principle some ungodly calculation you can do to re-extract the information in [each swallowed] qubit.&#x201D; </p><p>Is a literal God required to gather the information needed to connect our loop? Maybe, but I&#x2019;m only human, so it&#x2019;s beyond me. Perhaps it&apos;s not the only option though. What if we don&#x2019;t need to know everything; we just need to know <em>enough</em>? As intelligent beings, we do have the ability to reason after all. Can we arrive at the necessary information by means of deduction, without having all the raw data? A step in that direction might concern entanglement; it doesn&#x2019;t just save information from being lost in our universe &#x2014; it might show us how to build a new one. </p><p>One implication of the entanglement solution to the black hole paradox is that our universe may be a hologram. It sounds rather strange, but the &#x201C;holographic principle&#x201D; is taken quite seriously and is of great interest in the quest for quantum gravity. In this approach, <a href=""https://www.quantamagazine.org/tensor-networks-and-entanglement-20150428/"">spacetime emerges from a network of entangled particles</a>, and our entire universe may be a hologram of information encoded on the surface of a black hole. This is where we may be able to make some progress. </p><p>As I mentioned, Von Neumann entropy is also relevant to quantum computing. In fact, there are remarkable parallels between black holes and quantum computing, and the more we study one, <a href=""https://www.quantamagazine.org/john-preskill-quantum-computing-may-help-us-study-quantum-gravity-20200715/"">the more we tend to learn about the other</a>. Advancements in quantum computers allow us to probe the mysteries of our universe. We&#x2019;ve already been able to do some pretty mind-bending things with experimental systems, like those that mysteriously <a href=""https://www.quantamagazine.org/quantum-scarring-appears-to-defy-universes-push-for-disorder-20190320/"">&#x201C;snap back&#x201D; into order from equilibrium</a>, <a href=""https://www.quantamagazine.org/time-entanglement-raises-quantum-mysteries-20160119/"">entangle particles over time</a> (not just space), <a href=""https://phys.org/news/2019-03-physicists-reverse-quantum.html"">reverse time</a>, and <a href=""https://phys.org/news/2012-10-quantum-causal.html"">challenge our notion of normal causal order</a>. In time, we may come to find that we actually live in a quantum computer; which means &#x2014; in keeping with a causal loop &#x2014; we&#x2019;ll recreate the universe through quantum computing too. </p><p>It&#x2019;s no secret I&#x2019;m a strong proponent of one particular form of quantum computing as a model of our universe: quantum annealing. In alignment with the holographic principle, quantum annealing utilizes a network of entangled qubits, where entanglement steadily increases in accordance with our observations of Von Neumann entropy. There are many other similarities (and I promise I&#x2019;ll stop mentioning quantum annealing now), but my point, more generally, is that there are reasons to believe we can indeed recreate the universe through some form of quantum computing. For simplicity, I&#x2019;ll discuss this in terms of a &#x201C;simulation,&#x201D; but I want to emphasize that this doesn&apos;t imply a simulation is any less &#x201C;real&#x201D; than anything else &#x2014; it&#x2019;s all quantum information at the end of the day, and existence within a causal loop could just be simulation in perpetuity anyway. </p><p>From my vantage, this could go one of two ways. In each scenario, the goal is to create a matching &#x201C;first&#x201D; moment within a simulation; as long as that configuration of information is always the same between simulations, and a nested simulation remains coherent, then the causal loop is maintained. Again, an event on the loop is simply a specific arrangement of information. Both options require a super-advanced civilization in our distant future; relatively speaking, they may even seem like gods, but these options don&#x2019;t require capital-G God. </p><p>The first way is that we&#x2019;re able to deduce some set of parameters and initial conditions of our universe. If we exactly calculate its information capacity (Beckenstein bound), universal constants, laws, and find a grand unified theory, then we can also find some entanglement geometry that permits all of those properties. We&#x2019;d then create a quantum system with matching parameters and hope we run it from the same &#x201C;starting&#x201D; point as our own &#x2014; that might be some point of minimum entropy where the system couldn&#x2019;t possibly be any simpler, similar to how we view the singularity before the big bang. This assumes that some simulation &#x201C;before&#x201D; us chose the same starting point as the obvious choice since any possible timeline can then follow as its trends back towards equilibrium. The enormous energy required for such a task might spell the annihilation of the parent simulation, like a cosmic self-sacrifice, but maybe that&#x2019;s the point. </p><p>The second and possibly more intriguing way is the &#x201C;message in a bottle&#x201D; approach. Imagine that when a simulated universe is programmed, instructions are left for the inhabitants of that simulation to then recreate the same simulation. This makes sense if intelligent life has a vested interest in maintaining the causal loop it occupies. They would leave instructions in something ubiquitous and unchanging like the universal constants, the cosmic microwave background, or in our DNA. In fact, all human DNA differs by less than 1%, and about 98% of our DNA is considered to be non-coding, or &#x201C;junk&#x201D; DNA &#x2014; it&#x2019;s an ingenious place to pass along crucial information. And DNA is simply a pattern of information that can be easily programmed; meaning DNA would be encoded into the initial conditions, so the universe emerges around DNA-based life, differing from the &#x201C;absolute simplicity&#x201D; initial conditions of the first option. Though, we&#x2019;d still require a universal language that can be understood by any intelligent life to decode the instructions; maybe it all really is in the maths and options 1 and 2 are more alike than we think. </p><p>It&#x2019;s also worth noting that Hanley specifically cites the use of genetics in an example of a &#x201C;person loop,&#x201D; where, &#x201C;Given the normal recycling of cells, it may be that a person&#x2019;s body has entirely replaceable parts.&#x201D; Yet genetic code (ideally) remains unchanged, so that information could remain consistent in a loop. In fact, if DNA is the focal point of a causal loop, then it seems the only information that needs to be simulated is that which constitutes the experience and collective memory of DNA-based life. If information changes outside of that, who would ever notice? The information requirement for this simulation becomes much more manageable since we don&#x2019;t need to render every property of every particle throughout the observable universe. </p><p>Of course, this is all wild speculation, but it does make for a fun exercise. Maybe there are alternate routes that will become obvious as we learn more about reality. I&#x2019;m just trying to get the ball rolling in case we do live within a causal loop. It&apos;s my loop too, after all.<br></p><h3><strong>Finding God</strong></h3><p>When exploring the idea of a causal-loop-only universe, it&apos;s almost impossible to ignore some of the implications for life within that universe. </p><p>For one, it appears to make intelligent life necessary for<em> anything</em> to exist &#x2014; at least in any universe that&#x2019;s not a CTC. From this view, life isn&#x2019;t rare: it&apos;s required. If no intelligence emerges, there is no feasible way for a causal loop to remain informationally consistent. This also means that any life-carrying universe must follow a series of causes and effects that enable a minimum degree of intelligence and agency &#x2014; life must gain the ability to manipulate the information of the universe itself. So not only is intelligence required, but <em>highly advanced</em> intelligence is required. </p><p>What does this all look like from the perspective of life within such a universe? Well, look at our own &#x2014; the entire universe is &#x201C;cooling down&#x201D; towards disorder, but intelligent life and what it touches are the only things that trend towards more order. Over time, our knowledge and technological capabilities increase. What&#x2019;s the upper limit to this trend? Is it a coincidence that we&#x2019;ve come to a point where we can start exploring and controlling quantum information, the very fabric of reality? How much more will we achieve in the next century, millennium, or ten millennia? </p><p>Maybe we really have just gotten lucky, but in a causal loop this trajectory is not a coincidence &#x2014; it&apos;s a certainty. Life doesn&#x2019;t just veer off the rails into oblivion; it&#x2019;s locked on a path, or lots of equivalent paths that are all destined to tell the same story &#x2014; the same universal archetype. The loop cannot be broken, else it would have never existed. Life is bound to persist, bound to overcome, bound to exist again &#x2014; isn&#x2019;t this the kind of hope people normally place in God? </p><p>I&#x2019;m not saying God literally exists. Maybe an omniscient being exists as the highest expression of intelligence on a loop right before it must reset, but that seems like a distraction to a more meaningful point: existing in a causal loop &#x2014; at any point &#x2014; is <em>practically</em> like living in a universe where God exists too. </p><p>Isn&#x2019;t that the case if nearly everything about existence takes the shape of a series of unending coincidences? Otherwise, the odds of life arising in our universe are astronomically unfavorable, as is the fact that life has evaded extinction for a few billion years to become what it is today. If you recognize coincidence after coincidence, it&apos;s not much of a leap for a rational mind to think that a higher power ordains each moment, following some grand design. Many of us have stepped away from that worldview, but maybe we just had an incomplete perspective. Maybe we have reason to believe again. As we step closer to truth, we might see that our old silhouette of God was simply the negative space of an equally hopeful structure of reality. </p>",ChrisM,chrism,ChrisM,
cyYv9oYy8sD2nvC7P,AI Safety Discussion Day,ai-safety-discussion-day,https://www.lesswrong.com/events/cyYv9oYy8sD2nvC7P/ai-safety-discussion-day,2020-09-15T14:40:18.777Z,20,4,0,False,False,,"<p>Monday, September 21</p><p>4pm - 9pm UTC</p><p>See here for more info: <a href=""https://docs.google.com/document/d/1J5sTtquNud-XINMipo_r9tJZB_c4L6R0SuSiqqLJ-IM/edit"">https://docs.google.com/document/d/1J5sTtquNud-XINMipo_r9tJZB_c4L6R0SuSiqqLJ-IM/edit</a></p>",Linda Linsefors,linda-linsefors,Linda Linsefors,
D9y9ch4NFKJWfqk7H,Gems from the Wiki: Paranoid Debating,gems-from-the-wiki-paranoid-debating,https://www.lesswrong.com/posts/D9y9ch4NFKJWfqk7H/gems-from-the-wiki-paranoid-debating,2020-09-15T03:51:10.453Z,30,9,1,False,False,https://www.lesswrong.com/tag/paranoid-debating,"<p><i>During the </i><a href=""https://www.lesswrong.com/posts/ELN6FpRLoeLJPgx8z/the-wiki-is-dead-long-live-the-wiki-help-wanted""><i>LessWrong 1.0 Wiki Import</i></a><i> we (the LessWrong team) discovered a number of great articles that most of the LessWrong team hadn't read before. Since we expect many others to also not have have read these, we are creating a series of the best posts from the Wiki to help give those hidden gems some more time to shine.</i></p><p><i>Most of the work for this post was done by </i><a href=""https://www.lesswrong.com/users/freyley""><i>freyley</i></a><i> and </i><a href=""https://www.lesswrong.com/users/jenniferrm""><i>JenniferRM</i></a><i> who I've added as coauthors to this post. Wiki edits were also made by all of the following: BJR, PeerInfinity, Admin, PotatoDumplings, Vladimir Nesov, Zack M. Davis, Freyley and Grognor. Thank you all for your contributions!</i></p><hr><p><strong>Paranoid Debating</strong> is a variant of <a href=""https://web.archive.org/web/20090207153155/http://www.acceleratingfuture.com/steven/?p=96"">The Aumann Game</a> where one player purposefully subverts the group estimate. Similar to The Aumann Game, the activity consists of a group jointly producing a confidence interval for an unknown, but verifiable quantity, which is then <a href=""https://www.lesswrong.com/tag/scoring-rule"">scored</a> for <a href=""https://wiki.lesswrong.com/wiki/accuracy"">accuracy</a> and <a href=""https://www.lesswrong.com/tag/calibration"">calibration</a>. One individual is designated the spokesperson, who is responsible for choosing the final estimate. However, before the activity begins, one individual is secretly assigned the role of misleading the other members. The deceiver is scored higher the worse the final estimate is.The activity is intended to teach accurate estimate, proper agreement techniques, and recognition of deception.</p><p>A typical subject for the game might be ""How much maize is produced in Mexico annually?"".</p><h2>Rules</h2><ul><li>Select player roles. In person, each player receives or selects a card from a pack of role cards. For 4 players, create a pack of role cards by combining 3 black cards with 1 red card. For 4-6 players there should be 1 red card and the rest black with the rest being enough for one card per person. For 7-9 players, 2 red cards. Some variants include a role named the Advocate, which you can designate one of the black cards to represent.</li></ul><p><strong>Simplest variant</strong></p><ul><li>Each player receives a role. No advocate.</li><li>A question is asked.</li><li>Players discuss for 20 minutes, then write down their individual response on a card.</li><li>The answer is researched.</li><li>Scores are assigned.</li></ul><p><strong>Advocate variant, #1</strong></p><ul><li>Each player receives a role. One advocate in the deck. The player who receives the Advocate displays it to the group.</li><li>A question is asked.</li><li>Players discuss for 20 minutes, attempting to convince the advocate. The advocate writes down their response on a card. This is the group's answer.</li><li>The answer is researched, scores are assigned.</li></ul><p><strong>Advocate variant, #2</strong></p><ul><li>Each player receives a role. One advocate in the deck. No player may display their card.</li><li>A question is asked.</li><li>Players discuss for 20 minutes. Anyone may say anything. At the end, the advocate writes down what they think the group's response is on a card, and the group is scored for this.</li><li>Answer researched, scores assigned.</li></ul><p><strong>Variation-by-argument variant</strong></p><ul><li>Each player receives a role. No advocate. No player may display their card.</li><li>A question is asked.</li><li>Players have 2-5 minutes to write down their initial, individual estimate.</li><li>Players discuss for 20 minutes. Anyone may say anything. At the end, players write their revised estimates on their card.</li><li>Players are scored based on their delta -- the more you go toward the correct answer from your initial estimate, the more points.</li></ul><p><strong>Southern California Variant #1</strong></p><p>At the February 2011 Southern California LW Meetup we tried playing the game. For questions we bought a game of <a href=""http://boardgamegeek.com/boardgame/20100/wits-wagers"">Wits &amp; Wagers</a> (which has trivia questions with numerical answers) and looked at the cards to find questions that were about substantive topics where Fermi estimates seemed useful. The speaker/advocate was chosen on a rotating basis so that everyone gets at least one chance to play that role, and cards are dealt from a deck of playing cards to everyone else. Red cards mean you're trying to make the group deliver a bad answer. Black cards mean you're trying to make the group deliver a good answer. This makes the <i>number</i> of people to be suspicious of itself an unknown parameter and leads to funny outcomes and interesting coordination problems. Scoring used the experimental scoring code that is intended to assign the most credit to small error bars around high confidence correct answers.</p><h2>Questions</h2><p>It's really easy to ask a question that is then very difficult to answer later. For example, the question ""How many miles of railroad are there in Africa?"" is somewhat difficult to answer. Walking through the CIA World Fact Book one country at a time, we arrived at an answer in the range of 48,000-49,000. However, in cross-checking that information, we discovered that in Uganda, there are only 125 miles of active railroad, but 1200km listed in the Fact Book. It seems likely, therefore, that the total estimate includes some non-active miles of railroad, and is thus too high. This section is here to list good and bad questions and resources to get questions from or answer questions unusually easily. If listing an answer, please make the text of the answer white so people can use it if they want.</p><h2>Scoring</h2><p>A not-so-trivial inconvenience to playing the game is figuring out how to score it properly.</p><p>To make this easier there is now a tentative file format for representing a game of paranoid debate and a python script for scoring games represented in this format. If you'd like to download or edit this software check out <a href=""https://github.com/JenniferRM/Paradebate"">this github project</a>. Please note that the game format and the code are very likely to evolve to remove bugs and support whatever sort of play turns out to be the most fun and/or educational.</p><h2>Blog posts</h2><ul><li><a href=""https://www.lesswrong.com/lw/77/selecting_rationalist_groups/"">Selecting Rationalist Groups</a> - Description and account of the game.</li><li><a href=""https://www.lesswrong.com/lw/7r/the_first_london_rationalist_meetup/"">The First London Rationalist Meetup</a> by <a href=""https://wiki.lesswrong.com/wiki/taw"">taw</a> - An account of the game.</li></ul><h2>See also</h2><ul><li><a href=""https://www.lesswrong.com/tag/group-rationality"">Group rationality</a></li><li><a href=""https://www.lesswrong.com/tag/less-wrong-meetup-group-resources"">Less Wrong meetup group resources</a></li><li><a href=""https://www.lesswrong.com/tag/bayesian-probability"">Bayesian probability</a></li><li><a href=""https://wiki.lesswrong.com/wiki/Proper_scoring_rules"">Proper scoring rules</a></li><li><a href=""https://www.lesswrong.com/tag/calibration"">Calibration</a></li></ul>",habryka4,habryka4,habryka,
WhHFvzFsYfMxgYCdo,Book Review: Working With Contracts,book-review-working-with-contracts,https://www.lesswrong.com/posts/WhHFvzFsYfMxgYCdo/book-review-working-with-contracts,2020-09-14T23:22:11.215Z,168,72,27,False,False,,"<p>Contracts is one of those areas that I always figured I ought to study, at least enough to pick up the basics, but never seemed either interesting or important enough to reach the front of my queue. On top of that, there’s a lot of different angles from which to approach the subject: the law-school-style Contracts 101 class covers the legal principles governing contracts, the economists’ version abstracts away the practical specifics and talks about contracts in game-theoretic terms, more business-oriented books often focus on negotiation, etc.</p><p>“<a href=""https://www.amazon.com/Working-Contracts-Corporate-Securities-Library/dp/1402410603""><u>Working With Contracts: What Law School Doesn’t Teach You</u></a>” is about the practical skills needed for working with contracts on an everyday basis - specifically the sort of skills usually picked up on the job by young lawyers. It talks about things like what to look for when reviewing a contract, how to organize contracts, why lawyers use weird words like “heretofore”, various gotchas to watch out for, etc. It assumes minimal background knowledge, but also includes lots of technical nuts and bolts. In short, it’s the perfect book for someone who wants a technical understanding of real-world contract practice.</p><p>This post will review interesting things I learned from the book.</p><h2>Background Knowledge</h2><p>First, some very brief background info, which the book itself mostly assumes.</p><p>Legally, in order to count as a “contract”, we need four main pieces:</p><ul><li>Offer: someone offers a deal</li><li>Acceptance: someone else accepts it</li><li>Consideration: both parties gain something from the deal; it’s not a gift</li><li>Mutual understanding: both parties agree on what the deal is and the fact that they’ve agreed to it</li></ul><p>A Contracts 101 class has all sorts of details and gotchas related to these. Notice that “signature on a piece of paper” is not on that list; e.g. oral contracts are entirely enforceable, it’s just harder to prove their existence in court. Even implicit contracts are enforceable - e.g. when you order food from a restaurant, you implicitly agree to pay for it, and that’s a legally-enforceable contract. That said, we’ll focus here on explicit written contracts.</p><p>Once formed, a contract acts as custom, private law between the parties. Enforcement of this law goes through civil courts - i.e. if someone breaches the contract, then the counterparty can sue them for damages. Note the “for damages” in that sentence; if a counterparty breaches a contract in a way that doesn’t harm you (relative to not breaching), then you probably won’t be able to sue them.&nbsp; (Potentially interesting exercise for any lawyers in the audience: figure out a realistic contractual equivalent of <a href=""https://www.lesswrong.com/tag/newcomb-s-problem""><u>Newcomb’s problem</u></a>, where someone agrees to one-box on behalf of someone else but then two-boxes, and claims in court that their decision to two-box benefited the counterparty rather than harming them. I’d bet there’s case law on something equivalent to this.)</p><p>Note that this is all specific to American law, as is the book. In particular, other countries tend to more often require specific wording, ceremonial actions, and the like in order to make a contract (or component of a contract) enforceable.</p><h2>What Do Contracts Do?</h2><p>The “functional” components of a contract can be organized into two main categories: representations and covenants. A representation says that something <i>has happened</i> or <i>is true</i>; a covenant says that something <i>will happen</i> or <i>will be true</i>.</p><p>Some example representations:</p><ul><li>ABC Corp signs a statement that they have no pending lawsuits against them.</li><li>Bob signs a statement that the house he’s selling contains no lead-based paint or asbestos insulation.</li><li>Carol signs a statement that the forms she provided for a mortgage application are accurate and complete.</li><li>Title Corp signs a statement that there are no outstanding mortgages on a piece of property.</li></ul><p>Nominally, each of these is a promise that something is true. However, that’s not quite how they work <i>functionally</i>. Functionally, if a counterparty acts based on the assumption that the statement is true and is harmed as a result, then they can sue for damages. In other words, when providing a representation, we provide <strong>insurance</strong> against any damages which result from the representation being false. Bob may not even have checked that the house he’s selling contains no asbestos, and that’s fine - <i>if</i> he’s willing to insure the counterparty against any asbestos-related risk.</p><p>This idea of insurance becomes important in contract negotiations - there’s a big difference between e.g. “no environmental problems” and “no environmental problems <i>to the best of their knowledge</i>”. The former insures against any environmental problems, while the latter insures against any environmental problems which the signer knew about at time of signing. One puts the duty/risk of finding/fixing unknown problems on the signer, while the other puts it on the counterparty.</p><p>The other key thing to notice about representations is that they’re <i>as of the signing date</i>. When Bob states that his house contains no asbestos, that does not insure against the house previously containing asbestos or containing asbestos in the future. It only needs to be true as of that one moment in time. This becomes relevant in complex multi-stage contracts, where there’s an initial agreement subject to a bunch of conditions and reviews, and the final closing comes later after all that review is done. For instance, in a mortgage there’s an initial agreement subject to the borrower providing lots of forms (credit check, proof of income, proof of insurance, etc…), and the final contract is closed after all that is reviewed. In these situations, the borrower usually makes some representations early on, and then has to “bring down” the representations at closing - i.e. assert that they’re still true.</p><p>While representations deal with past and present, covenants deal with the future. They’re the classic idea of contract provisions: precommitments to do something. Some examples:</p><ul><li>ABC Corp agrees to not sell the machinery they’re leasing.</li><li>Bob agrees to not use any lead-based paint on the house he’s buying.</li><li>Carol agrees to maintain minimum levels of insurance on the house she’s mortgaging.</li><li>Monitoring Corp agrees to alert Bank if there is any change in the credit rating of Company.</li></ul><p>These work basically like you’d expect.</p><p>Representations and covenants often run in parallel: a representation that X is true will have a corresponding covenant to make X continue to be true in the future. For instance:</p><ul><li>ABC corp states that they do not currently have any liens on their main plant, and agrees to not create any (i.e. they won’t borrow any money with the plant as collateral).</li><li>Carol states that she currently has some level of insurance coverage on her house, and agrees to maintain that level of coverage.</li></ul><p>This is mainly for contracts which will be performed over a long time, especially debt contracts. One-off contracts (like a purchase/sale) tend to have relatively few covenants; most of their substance is in the representations.</p><h2>Parallels to Software Development</h2><p>Representations and covenants seem pretty straightforward, at least conceptually. One is insurance against some fact being false, the other is a precommitment.</p><p>The technical complexity of contracts comes from the interplay between two elements. First:</p><blockquote><p>The goal of a contract is to describe <i>with precision</i> the substance of the meeting of two minds, in language that will be interpreted by each subsequent reader <i>in exactly the same way</i>.</p></blockquote><p>In other words, we want no ambiguity, since any ambiguity could later be used by one of the parties to “cheat” their way out of the contract. This creates a headache very familiar to software developers: like programs, contracts mean exactly what they say. There is no “do what I mean” button; we can’t write something ambiguous and rely on the system to figure out what we meant.</p><p>Second: we don’t have perfect knowledge of the future. When making a precommitment in a contract, that precommitment is going to operate fairly mechanically in whatever the future environment looks like. Just like a function written in code may encounter a vast space of unusual inputs in the wild, a precommitment in a contract may interact with a vast space of unusual conditions in the wild. And since we don’t know in advance <i>which</i> conditions will be encountered, the person writing the code/contract needs to consider the whole possible range. They need to figure out, in advance, what weird corner cases <i>could</i> arise.</p><p>Put those two pieces together, and the picture should feel <i>very</i> familiar to software developers.</p><p>The result is that a lawyer’s job ends up involving a lot of the same pieces as a software engineer’s job. A client/manager says “here’s what we want”, the lawyer/programmer says “ummm I don’t think you really want that, because &lt;problem&gt; happens if &lt;circumstance&gt;”, and they go back-and-forth for a while trying to better define what the client/manager really wants. An example from the book pictures a lawyer reviewing a contract with a client (simplified slightly by me):</p><blockquote><p>Lawyer: This is a covenant that restricts your business from incurring debt…</p><p>Client: That’s fine, we don’t plan to use any bank financing.</p><p>Lawyer: Well, the definition of “debt” used is very broad. For instance, it includes payment plans on any equipment you buy…</p><p>Client: Well, we can add some room for that.</p><p>Lawyer: How much room do you need?</p><p>Client: Based on our current needs, less than $1M at any given time.</p><p>Lawyer: But if that new plant you were talking about gets off the ground, won’t you need to buy a bunch of new equipment for it?</p><p>Client: Good point, we’d better ask for $5M…</p></blockquote><p>This could go on for a while.</p><p>Despite the parallels, lawyers are not very <i>good</i> software engineers, in general. The most common solution to the sorts of problems above is to throw a patch on it, via two kinds of exceptions:</p><ul><li>Carveouts: action X is generally forbidden, except for special case Y.</li><li>Baskets: action X is generally forbidden, except in amounts below some limit (e.g. the $5M limit in the example above)</li></ul><p>Over the course of negotiations, patches are layered on top of patches. An example from the book:</p><blockquote><p>Little Corp may not transfer any Shares during the term of this Agreement, except for (i) transfers at any time to its Affiliates (including, without limitation, Micro Corp) other than Medium Corp, and (ii) so long as an Event of Default attributable to Big Corp shall have occurred and be continuing, transfers to any Person (including, for the avoidance of doubt, Medium Corp).</p></blockquote><p>This mess is the contractual equivalent of a series of if-statements nested within if-statements. This is, apparently, standard practice for lawyers.</p><p>(Another complaint: in a complex contract, it would not be hard to include provisions alongside the table of contents which nullify provisions which appear in the wrong section. Then people reviewing the contract later wouldn’t have to read the whole thing in order to make sure they didn’t miss anything relevant to their use-case; it would be the contract equivalent of variable scope. My mother’s a lawyer in real estate and wills, so I asked her why lawyers don’t do this. Her possibly-tongue-in-cheek-answer: might put lawyers out of business. Kidding aside, the bar association engages in some pretty incestuous rent-seeking, but judges have been pushing for decades to make contracts and other legal documents more legible to non-lawyers.)</p><h2>The “Do What I Mean” Button</h2><p>A contract writer’s job is much easier than a programmer’s job in one key respect: a contract will ultimately be interpreted by humans. That means we can say the equivalent of “look, you know what I mean, just do that”, <i>if</i> we expect that a court will actually know what we mean.&nbsp;</p><p>This gives rise to a bunch of standard tricks for invoking the do-what-I-mean button. We’ll talk about three big ones: materiality, reasonableness, and consistency with “ordinary business”/”past practice”.</p><p><strong><u>Materiality</u></strong></p><p>Roughly speaking, materiality means ignoring small things. For instance, compare:</p><ul><li>“Borrower shall not default in its obligations under any contract”, vs</li><li>“Borrower shall not default in its obligations under any <u>material</u> contract”</li></ul><p>The first would be breached if e.g. the borrower forgot to update their payment information on their $10 monthly github subscription, and the payment was late. The second would ignore small things like that.</p><p>In general, materiality is relative to the size of the business. A $100k oversight would be quite material to most small businesses, but immaterial to AT&amp;T. It’s also relative to the contract - if that $100k oversight is directly relevant to a $300k contract, then it’s material, even if the $300k contract itself is small change to AT&amp;T.</p><p>Where’s the cutoff line? That’s for courts to decide, if and when it matters. That’s how pushing the do-what-I-mean button works; you have to rely on the courts to make a sensible decision.</p><p>One particularly common usage of materiality: “material adverse change/effect”. Rather than saying “X has no pending lawsuits”, we say “X has no pending lawsuits <u>whose loss would entail a material adverse effect</u>”. Rather than saying “Borrower will notify Lender of any change in their business forecasts”, we say “Borrower will notify Lender of any <u>material adverse</u> change in their business forecasts”. This way a lender or buyer finds out about problems which actually matter, without being inundated with lots of minor details.</p><p><strong><u>Reasonableness</u></strong></p><p>Reasonableness is exactly what it sounds like. It’s saying something that has some obvious loophole to abuse, then giving a stern look and saying “don’t go pulling any bullshit”. Example: “Company shall reimburse X for all of X’s out-of-pocket expenses arising from...” vs “Company shall reimburse X for all of X’s <u>reasonable</u> out-of-pocket expenses arising from…”</p><p>Some patterns where reasonableness shows up:</p><ul><li>Reasonable expectations, e.g. “Borrower shall notify Lender of any changes which could <u>reasonably</u> be expected to have a material adverse effect…”</li><li>Consent not to be unreasonably withheld, e.g. “ABC Corp may not X without consent of XYZ Corp, <u>such consent not to be unreasonably withheld</u>.”</li><li>Reasonable efforts, e.g. “Borrower shall obtain X from their insurer.” vs “Borrower shall <u>exert reasonable effort to</u> obtain X from their insurer.”</li></ul><p>What would each of these do without the reasonableness clause? In the first case, the borrower could claim that they didn’t expect Obvious Bad Thing to impact their business. In the second case, XYZ Corp could withhold consent for some case they obviously don’t care about in order to extract further concessions from ABC Corp. In the third case, an insurer could simply refuse to provide X, and the borrower wouldn’t be able to do anything about it.</p><p><strong><u>Behaving Normally</u></strong></p><p>Sometimes a lender or prospective buyer wants to say “what you normally do is fine, so do that and don’t go crazy”. Two (similar) standards for this: “in the ordinary course of business” and “consistent with past practice”.</p><p>Typical examples:</p><ul><li>“Borrower will not incur any &lt;debt of specific type&gt; <u>except in the ordinary course of business</u>.”</li><li>“ABC Corp will not make any payments to &lt;subsidiary&gt; <u>except in a manner consistent with past practice</u>.”</li></ul><p>In general, this is a pretty good way to let business continue as usual without having to go into all the tiny details of what business-as-usual involves, while still ensuring that e.g. a borrowing company doesn’t sell all their assets, distribute the funds as a dividend to a parent company, and then declare bankruptcy.</p><h2>Remedial Provisions</h2><p>In general, if a contract is breached, the counterparty can sue for damages. If you want anything else to happen as the result of a breach, then it needs to be included in the contract. In particular, common things triggered by a breach include:</p><ul><li>Termination: counterparty gains the right to terminate the contract</li><li>Acceleration: loaned money must be paid back immediately</li><li>Indemnification: counterparty must be paid for any breach-related damages</li></ul><p>The last is somewhat redundant with the court system, but by including it explicitly, the contract can also specify how to calculate damages, how damages are to be paid, caps or exceptions to liability, etc. Rather than leaving such matters to the whims of a court, the contract can specify them.</p><p>Termination and acceleration are particularly relevant from a negotiation standpoint - the former for one-shot contracts like sales, and the latter for long-term contracts like debt.</p><p>The earlier stages of a complex sale (e.g. a merger/acquisition of a company) involve an agreement to sell <i>subject to</i> a long list of conditions being satisfied - i.e. the “due diligence” conditions. If any of those conditions are not met, then the buyer gains the right to terminate the contract - i.e. walk away from the deal. But these things can take months; the last acquisition I saw took around a year. During that time, the buyer may change their mind for reasons entirely unrelated to the seller - e.g. market prices for the seller’s assets may change. The seller wants to prevent the buyer from walking away in a case like that.</p><p>This means that the buyer has incentive to ask for very complicated and/or very subjective conditions, to give themselves the opportunity to walk away whenever they want. For instance, if a buyer manages to get a condition which requires “X which is satisfactory <i>in Buyer’s sole discretion</i>”, then the buyer effectively gains a blanket option to walk away from the deal; they can always just claim that some inane detail of X is unsatisfactory. (This is a good example where reasonableness can fix the problem.) In particular, if market conditions change, then the buyer may use that option to negotiate more concessions, like a lower purchase price.</p><p>Acceleration has a similar effect in debt deals. Nobody ever wants to accelerate debt; it’s a surefire way to end up in bankruptcy court. When a contract breach gives a lender the option to accelerate, what actually happens is that they use that option as leverage to negotiate a new deal. They’ll want a higher interest rate, or a claim on more of the borrower’s assets, or the like.</p><p>Takeaway: just because a contract specifies a particular penalty for breach does not mean that the penalty actually happens. Often, the penalty is really used as an option by one party to renegotiate the contract, and provides leverage for such a negotiation.</p><h2>Takeaways</h2><p>Contracts are a lot like computer programs: they’re taken very literally, and they could potentially encounter a wide variety of corner cases in the wild. Together, those two pieces make a contract writer’s job quite similar to a programmer’s job: a client/manager will tell you what they <i>think</i> they want, and then you go back-and-forth trying to formulate what they really want.</p><p>Compared to (good) software developers, lawyers do not seem to be very good at this; they tend to throw patches on top of patches, creating more corner cases rather than fewer. They don’t seem to have even realized that <i>enforced</i> scope and modularity are things which one could use in a contract; consequently, every contract must be read in its entirety by anyone relying on it. That puts a sharp limit on the scale of today’s contracts.</p><p>Unlike programmers, lawyers do have a “do what I mean” button, although its use comes with a cost; it means leaving interpretation to the whims of a court. For many “simple” things, that cost is relatively minor - so contracts can ignore “immaterial” problems, or require “reasonable” behavior, or stipulate consistency with “past practice” and “the course of ordinary business”.</p><p>Functionally, contracts provide insurance against stated facts being false, and they provide precommitments for the future. They can also stipulate nominal penalties for breach of contract, though in practice these penalties often serve as options to renegotiate (with leverage) rather than actually being used.</p>",johnswentworth,johnswentworth,johnswentworth,
5x9Ma4e9aWJ7uR8pd,A case study in simulacra levels and the Four Children of the Seder,a-case-study-in-simulacra-levels-and-the-four-children-of,https://www.lesswrong.com/posts/5x9Ma4e9aWJ7uR8pd/a-case-study-in-simulacra-levels-and-the-four-children-of,2020-09-14T22:31:39.484Z,32,9,1,False,False,,"<p><i>This was originally going to be a comment on Zvi's excellent post, </i><a href=""https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels""><i>The Four Children of the Seder as the Simulacra Levels</i></a><i>, but it got too long and I thought it warranted its own post.</i></p><p>My cousin's kid is having a tough time lately. He's stealing trinkets, destroying things around the house, and according to his parents he ""lies all the time."" His mom will grill him over whether he's lying or not - asking him again and again whether he's brushed his teeth, until he breaks down and admits that he didn't.</p><p>It's not clear that she has evidence in cases like this that he was lying. I suspect that the experience of being grilled is so uncomfortable that the kid finds it easier to make a false confession and brush his teeth a second time than to stand up for himself. I also guess that some of his stealing and destroying habits come from acting out on frustration with authority figures. It's a way of practicing deception, provoking reactions, and testing adults. Because he doesn't see a way to gain the trust and respect of adults, he's trying to figure out how to trick them most effectively.</p><p>Why are his parents behaving this way? It is because they have become far less concerned with object-level reality - whether or not he's brushed his teeth - than with the question of whether their child is a liar. The kid understands that everything they ask him to do is a test of his honesty. It's a symbol. Brushing his teeth isn't to prevent cavities. It's a trial of his character.</p><p>So his parents are speaking to him on the level of simplicity. He may have started wise, but is becoming wicked as his parents draw him deeper and deeper into a world of symbolism.</p><p>This highlights one of the paradoxes of the levels. Whether or not the kid lied about brushing his teeth is an object-level truth. And if you asked his parents why they care, they'd tell you ""because we don't want him to get cavities.""</p><p>A relationship that's on a higher simulacrum level is often still connected to level one. The higher levels <i>accumulate</i>, rather than <i>replacing</i> the lower levels. Brushing his teeth is about cavities, but it's <i>also</i> about whether you can trick your parents, and it's <i>also</i> about whether or not your child is a liar.</p><p>Our family is concerned about this, and we're operating on level four. We understand that bringing this up with the parents is a delicate issue, because we don't want to imply that they're <i>bad parents. </i>And we primarily struggle with ""how to ask"" them about the situation. To us, the question of whether or not the kid brushes his teeth is almost irrelevant. We're not trying to get anything out of them or control their behavior.</p><p>We're peering into level three, trying to understand the symbolism around everybody's behaviors, and how our word choice, tone of voice, body language, and the context of the discussion might fit into the symbolism of the discussion as interpreted by the parents.</p><p>Fortunately, we have slightly more clarity about how to deal with this than the Rabbis seem to, though not much. Our best ideas so far:</p><ol><li>Talking with each other about what's going on, and really taking our time before engaging with his parents. Then talking with the parents to start understanding their worldview. Peering from level 4 deeper into level 3.</li><li>Suggesting that they agree on family therapy. This way, they'd have a single, credible, shared authority figure - a therapist - rather than a patchwork of advice, books, and their own opinions. We hope that the therapist can help them escape level 3 and get to level 2, so that they can stop brooding on this question of ""is our son a good-for-nothing liar"" and start asking ""how are our words and actions influencing our son's behavior, and how can we influence him in ways that we like better?""</li><li>Getting them to focus more on verifying their son's behavior through evidence rather than grilling him, and giving the kid tasks that focus on directly engaging with reality. We have him help cook using sharp knives, we teach him the names of plants in the garden, and direct him to observe nature closely and learn rules that count for something: the patterns on a spider's back, the shape of a weed's roots, the rules of chess. And we try to give him opportunities to ""teach"" others about what he learns - telling his sister that you can eat nasturtium petals, for example. Rewarding him for his engagement with reality.</li></ol><p><i>In general, being lost in higher simulacra levels seems to involve a breakdown of trust that basic care, forgiveness, and acceptance is available; a fragmentation of the group's wisdom and perspective; and stronger incentives being attached to the higher simulacra levels than the lower levels.</i></p><p>This suggests to me in particular that we have gone deeply astray with our obsession with people's <i>character</i>. The drive to figure out ""what kind of a person"" somebody is, or ""what they think of our character,"" leads us to experience simple activities as <i>tests of our character. </i>We experience requests, advice, feedback, and just simple factual claims as part of the test, not attempts to steer an object-level outcome. We become highly self-conscious, extremely concerned about how every aspect of our selves might be interpreted.</p><p>This goes on and on. Even people who ostensibly want to fight this can get caught up in it themselves. Saying ""I'm only tolerant of intolerance"" creates the perception that you're constantly engaged in testing the people around you for having a character of intolerance. Nobody will be able to rest easy unless they commit, one way or another, to just not caring what you think about them.</p><p>And of course, when this gets done on a massive scale, you get ""I'm only intolerant of enforced tolerance."" You tolerate the most objectively reprehensible behavior, not because you think it's OK, but in order to show just how far you're willing to go to push back against the other side.</p><p>What might be the way forward?</p><p>If I'm right, and the levels are <i>layers</i>, then we have to scrape them away. The way that starts is by establishing trust - first within our own side, and only then with the other side. We need to make sure we can credibly show that we've got enough unity amongst ourselves not to turn a reconciliation attempt into an attack, and that we bring wisdom to the table.</p><p>With trust established, we try to bring in an agreed-upon authority. This could be shared set of values or concepts, a group of people with the credibility to serve as a reconciliation figure, or a process that creates space for the disputants to figure out what they actually want from <i>life</i>, not from their <i>enemies</i>.</p><p>Having a sense of shared authority and process, we look for any opportunity to reward people who are operating on level 1 and displaying a conscious rejection of levels 2-4. Bring facts to the table? Applause. Read that book rather than assuming you understand it from the title? Applause. Criticize the fallacies of your own side? Applause.</p><p>Going forward, I will try to bring up the idea with my friends (all American liberals, like me), that a lot of the ""other side"" might be trying to react to a perceived authoritarianism of the left by ostentatiously embracing what we find repugnant. I want to see if we can form enough agreement around that that it would become imaginable that we could try to interface with the other side and build trust there as well.</p>",AllAmericanBreakfast,directedevolution,DirectedEvolution,
KkwtLtroaNToWs2H6,Most Prisoner's Dilemmas are Stag Hunts; Most Stag Hunts are Schelling Problems,most-prisoner-s-dilemmas-are-stag-hunts-most-stag-hunts-are,https://www.lesswrong.com/posts/KkwtLtroaNToWs2H6/most-prisoner-s-dilemmas-are-stag-hunts-most-stag-hunts-are,2020-09-14T22:13:01.236Z,176,71,36,False,False,,"<p>I previously claimed that most apparent Prisoner's Dilemmas are actually Stag Hunts. I now claim that they're Schelling Pub in practice. I conclude with some lessons for fighting Moloch.</p><p><i>This post turned out especially dense with inferential leaps and unexplained terminology. If you're confused, try to ask in the comments and I'll try to clarify.</i></p><p><i>Some ideas here are due to Tsvi Benson-Tilsen.</i></p><hr><p>The title of this post used to be <i><strong>Most Prisoner's Dilemmas are Stag Hunts; Most Stag Hunts are Battle of the Sexes</strong></i>. I'm changing it based on <a href=""https://www.lesswrong.com/posts/KkwtLtroaNToWs2H6/most-prisoner-s-dilemmas-are-stag-hunts-most-stag-hunts-are?commentId=an7CeMbxTTyZCAuXf"">this comment</a>. ""Battle of the Sexes"" is a game where a male and female (let's say Bob and Alice) want to hang out, but each of them would prefer to engage in gender-stereotyped behavior. For example, Bob wants to go to a football game, and Alice wants to go to a museum. The gender issues are distracting, and although it's the standard, <i>the game isn't that well-known anyway,</i> so sticking to the standard didn't buy me much (in terms of reader understanding).</p><p>I therefore present to you,</p><p><strong>the Schelling Pub Game:</strong></p><p>Two friends would like to meet at the pub. In order to do so, they must make the same selection of pub (making this a Schelling-point game). However, they have different preferences about which pub to meet at. For example:</p><ul><li>Alice and Bob would both like to go to a pub this evening.</li><li>There are two pubs: the Xavier, and the Yggdrasil.</li><li>Alice likes the Xavier twice as much as the Yggdrasil.</li><li>Bob likes the Yggdrasil twice as much as the Xavier.</li><li>However, Alice and Bob also prefer to be with each other. Let's say they like being together ten times as much as they like being apart.</li></ul><figure class=""table""><table><tbody><tr><td colspan=""4"">Schelling Pub Game payoff matrix</td></tr><tr><td colspan=""2"" rowspan=""2"">payoffs written alice;bob</td><td colspan=""2"">B's choice</td></tr><tr><td>X</td><td>Y</td></tr><tr><td rowspan=""2"">A's choice</td><td>X</td><td>20;10</td><td>2;2</td></tr><tr><td>Y</td><td>1;1</td><td>10;20</td></tr></tbody></table></figure><p>The important features of this game are:</p><ul><li>The Nash equilibria are all Pareto-optimal. There is no ""individually rational agents work against each other"" problem, like in prisoner's dilemma or even stag hunt.</li><li>There are multiple equilibria, and different agents prefer different equilibria.</li></ul><p>Thus, realistically, agents may not end up in equilibrium at all -- because (in the single-shot game) they don't know which to choose, and because (in an iterated version of the game) they may make locally sub-optimal choices in order to influence the long-run behavior of other players.</p><hr><p>(Edited to add, based on comments:)&nbsp;</p><p>Here's a summary of the central argument which, despite the lack of pictures, may be easier to understand.</p><ol><li>Most Prisoner's Dilemmas are actually iterated.</li><li>Iterated games are a whole different game with a different action space (because you can react to history), a different payoff matrix (because you care about future payoffs, not just the present), and a different set of equilibria.</li><li>It is characteristic of PD that players are incentivised to play away from the Pareto frontier; IE, no Pareto-optimal point is an equilibrium. <i>This is not the case with iterated PD.</i></li><li>It is characteristic of Stag Hunt that there is a Pareto-optimal equilibrium, but there is also another equilibrium which is far from optimal. <i>This is also the case with iterated PD.<strong> </strong></i>So <strong>iterated PD resembles Stag Hunt</strong>.</li><li>However, it is furthermore true of iterated PD that <i>there are multiple different Pareto-optimal equilibria, which benefit different players more or less.</i> Also, if players don't successfully coordinate on one of these equilibria, they can end up in a worse overall state (such as mutual defection forever, due to playing grim-trigger strategies with mutually incompatible demands). <strong>This makes iterated PD resemble the Schelling Pub Game.</strong></li></ol><p>In fact, the Folk Theorem suggests that <i>most </i>iterated games will resemble the Schelling Pub Game in this way.</p><hr><p>In a <a href=""https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag?commentId=D3jnxh7MvaJ4Z5sPK"">comment</a> on <a href=""https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag"">The Schelling Choice is ""Rabbit"", not ""Stag""</a> I said:</p><blockquote><p>In the book <i>The Stag Hunt,</i> Skyrms similarly says that lots of people use Prisoner's Dilemma to talk about social coordination, and he thinks people should often use Stag Hunt instead.</p></blockquote><blockquote><p>I think this is right. Most problems which initially seem like Prisoner's Dilemma are actually Stag Hunt, because there are potential enforcement mechanisms available. The problems discussed in Meditations on Moloch are mostly Stag Hunt problems, not Prisoner's Dilemma problems -- Scott even talks about enforcement, when he describes the dystopia where everyone has to kill anyone who doesn't enforce the terrible social norms (including the norm of enforcing).</p></blockquote><blockquote><p>This might initially sound like good news. Defection in Prisoner's Dilemma is an inevitable conclusion under common decision-theoretic assumptions. Trying to escape multipolar traps with exotic decision theories might seem hopeless. On the other hand, rabbit in Stag Hunt is <i>not</i> an inevitable conclusion, by any means.</p></blockquote><blockquote><p>Unfortunately, in reality, hunting stag is actually quite difficult. <i>(""The schelling choice is Rabbit, not Stag... and that really sucks!"")</i></p></blockquote><p>Inspired by Zvi's <a href=""https://www.lesswrong.com/s/kNANcHLNtJt5qeuSS"">recent sequence on Moloch</a>, I wanted to expand on this. These issues are important, since they determine how we think about group action problems / tragedy of the commons / multipolar traps / Moloch / all the other synonyms for the same thing.</p><p>My current claim is that most Prisoner's Dilemmas are actually <i>Schelling pub games</i>. But let's first review the relevance of Stag Hunt.</p><h1>Your PD Is Probably a Stag Hunt</h1><p>There are several reasons why an apparent Prisoner's Dilemma may be more of a Stag Hunt.</p><ul><li>The game is actually an iterated game.</li><li>Reputation networks could punish defectors and reward cooperators.</li><li>There are enforceable contracts.</li><li>Players know quite a bit about how other players think (in the extreme case, players can view each other's source code).</li></ul><p>Each of these formal model creates a situation where players <i><strong>can</strong></i> get into a cooperative equilibrium. The challenge is that you can't unilaterally decide everyone should be in the cooperative equilibrium. If you want good outcomes for yourself, you have to account for what everyone else probably does. If you think everyone is likely to be in a bad equilibrium where people punish each other for cooperating, then aligning with that equilibrium might be the best you can do! This is like hunting rabbit.</p><p><i><strong>Exercize</strong>: is there a situation in your life, or within spitting distance, which seems like a Prisoner's Dilemma to you, where everyone is stuck hurting each other due to bad incentives? Is it an iterated situation? Could there be reputation networks which weed out bad actors? Could contracts or contract-like mechanisms be used to encourage good behavior?</i></p><p>So, why do we perceive so many situations to be Prisoner's Dilemma -like rather than Stag Hunt -like? Why does Moloch sound more like <i>each individual is incentivized to make it worse for everyone else</i> than <i>everyone is stuck in a bad equilibrium?</i></p><p>&nbsp;Sarah Constantine <a href=""https://srconstantin.wordpress.com/2017/04/10/dont-shoot-the-messenger/"">writes</a>:</p><blockquote><p>A friend of mine speculated that, in the decades that humanity has lived under the threat of nuclear war, we’ve developed the assumption that we’re living in a world of one-shot Prisoner’s Dilemmas rather than repeated games, and lost some of the social technology associated with repeated games. Game theorists do, of course, know about iterated games and there’s some fascinating research in <a href=""https://egtheory.wordpress.com/"">evolutionary game theory</a>, but the original formalization of game theory was for the application of nuclear war, and the 101-level framing that most educated laymen hear is often that one-shot is the prototypical case and repeated games are hard to reason about without computer simulations.</p></blockquote><p>To use board-game terminology, the <i>game</i> may be a Prisoner's Dilemma, but the <i>metagame </i>can use enforcement techniques. Accounting for enforcement techniques, the game is more like a Stag Hunt, where defecting is ""rabbit"" and cooperating is ""stag"".</p><h1>Schelling Pubs</h1><p>But this is a bit informal. You don't separately choose how to metagame and how to game; really, your iterated strategy determines what you do in individual games.</p><p>So it's more accurate to just think of the iterated game. There are a bunch of iterated strategies which you can choose from.</p><p>The key difference between the single-shot game and the iterated game is that cooperative strategies, such as Tit for Tat (but <a href=""https://www.lesswrong.com/posts/3rxMBRCYEmHCNDLhu/the-pavlov-strategy"">including</a> <a href=""https://www.lesswrong.com/posts/2meuc3kPRkBcRpj3R/contrite-strategies-and-the-need-for-standards"">others</a>), are avaliable. These strategies have the property that (1) they are equilibria -- if you know the other player is playing Tit for Tat, there's no reason for you not to; (2) if both players use them, they end up cooperating.</p><p>A key feature of Tit for Tat strategy is that if you do end up playing against a pure defector, you do almost as well as you could possibly do with them. This doesn't sound very much like a Stag Hunt. It begins to sound like a Stag Hunt in which you can change your mind and go hunt rabbit if the other person doesn't show up to hunt stag with you.</p><p>Sounds great, right? We can just play one of these cooperative strategies.</p><p>The problem is, there are many possible self-enforcing equilibria. Each player can threaten the other player with a <i>Grim Trigger</i> strategy: they defect forever the moment some specified condition isn't met. This can be used to extort the other player for more than just the mutual-cooperation payoff. Here's an illustration of possible outcomes, with the enforceable frequencies in the white area:</p><figure class=""image image_resized"" style=""width:92.43%;""><img src=""https://i.imgur.com/VhsVD5w.jpg""><figcaption>The entire while area are <i>enforceable equilibria:</i> players could use a grim-trigger strategy to make each other cooperate with very close to the desired frequency, because what they're getting is still better than mutual defection, even if it is far from fair, or far from the Pareto frontier.</figcaption></figure><p>Alice could be extorting Bob by cooperating 2/3rds of the time, with a grim-trigger threat of never cooperating at all. Alice would then get an average payoff of 2⅓, while Bob would get an average payout of 1⅓.</p><p>In the artificial setting of Prisoner's Dilemma, it's easy to say that Cooperate, Cooperate is the ""fair"" solution, and an equilibrium like I just described is ""Alice exploiting Bob"". However, real games are not so symmetric, and so it will not be so obvious what ""fair"" is. The purple squiggle highlights the Pareto frontier -- the space of outcomes which are ""efficient"" in the sense that no alternative is purely better for everybody. These outcomes may not all be fair, but they all have the advantage that no ""money is left on the table"" -- any ""improvement"" we could propose for those outcomes makes things worse for at least one person.</p><p>Notice that I've also colored areas where Bob and Alice are doing worse than payoff 1. Bob can't enforce Alice's cooperation while defecting more than half the time; Alice would just defect. And vice versa. All of the points within the shaded regions have this property. So not <i>all</i> Pareto-optimal solutions can be enforced.</p><p>Any point in the white region can be enforced, however. Each player could be watching the statistics of the other player's cooperation, prepared to pull a grim-trigger if the statistics ever stray too far from the target point. This includes so-called <i><strong>mutual blackmail</strong></i> equilibria, in which both players cooperate with probability slightly better than zero (while threatening to never cooperate at all if the other player detectably diverges from that frequency). This idea -- that 'almost any' outcome can be enforced -- is known as the <a href=""https://en.wikipedia.org/wiki/Folk_theorem_(game_theory)"">Folk Theorem</a> in game theory.</p><p>The Schelling Pub part is that (particularly with grim-trigger enforcement) everyone has to choose the same equilibrium to enforce; otherwise everyone is stuck playing defect. You'd rather be in even a bad mutual-blackmail type equilibrium, as opposed to selecting incompatible points to enforce. Just like, in Schelling Pub, you'd prefer to meet together at any venue rather than end up at different places.</p><p>Furthermore, I would claim that <i>most</i> apparent Stag Hunts which you encounter in real life are actually schelling-pub, in the sense that there are many different stags to hunt and it isn't immediately clear which one should be hunted. Each stag will be differently appealing to different people, so it's difficult to establish <a href=""https://www.lesswrong.com/tag/common-knowledge"">common knowledge</a> about which one is worth going after together.</p><p><i><strong>Exercize</strong>: what stags aren't you hunting with the people around you?</i></p><h1>Taking Pareto Improvements</h1><p>Fortunately, Grim Trigger is not the <i>only</i> enforcement mechanism which can be used to build an equilibrium. Grim Trigger creates a crisis in which you've got to guess which equilibrium you're in very quickly, to avoid angering the other player; and no experimentation is allowed. There are much more <a href=""https://srconstantin.wordpress.com/2018/12/20/the-pavlov-strategy/"">forgiving</a> strategies (and <a href=""https://srconstantin.wordpress.com/2018/12/24/contrite-strategies-and-the-need-for-standards/"">contrite</a> ones, too, which helps in a different way).</p><p>Actually, <i>even using Grim Trigger to enforce things</i>, why would you punish the other player for doing something <i>better for you? </i>There's no motive for punishing the other player for raising their cooperation frequency.</p><p>In a scenario where you don't know which Grim Trigger the other player is using, but you don't think they'll punish you for cooperating <i>more</i> than the target, a natural response is for both players to just cooperate a bunch.</p><p>So, it can be very valuable to <i><strong>use enforcement mechanisms which allow for Pareto improvements.</strong></i></p><p>Taking Pareto improvements is about moving from the middle to the boundary:</p><figure class=""image image_resized"" style=""width:82.02%;""><img src=""https://i.imgur.com/AcYkUNg.jpg""></figure><p>(I've indicated the directions for Pareto improvements starting from the origin in yellow, as well as what happens in other directions; also, I drew a bunch of example Pareto improvements as black arrows to illustrate how Pareto improvements are awesome. Some of the black arrows might not be perfectly within the range of Pareto improvements, sorry about that.)</p><p>However, there's also an argument against taking Pareto improvements. If you accept <i>any</i> Pareto improvements, you can be exploited in the sense mentioned earlier -- you'll accept any situation, so long as it's not worse for you than where you started. So you will take some pretty poor deals. Notice that one Pareto improvement can prevent a different one -- for example, if you move to (1/2, 1), then you can't move to (1,1/2) via Pareto improvement. So you could always reject a Pareto improvement because you're holding out for a better deal. (This is the <i>Schelling Pub</i> aspect of the situation -- there are Pareto-optimal outcomes which are better or worse for different people, so, it's hard to agree on which improvement to take.)</p><p>That's where <a href=""https://www.lesswrong.com/posts/z2YwmzuT7nWx62Kfh/cooperating-with-agents-with-different-ideas-of-fairness"">Cooperation between Agents with Different Notions of Fairness </a>comes in. The idea in that post is that you don't take <i>just any</i> Pareto improvement -- you have standards of fairness -- but you don't just completely defect for less-than-perfectly-fair deals, either. What this means is that two such agents with incompatible notions of fairness can't get all the way to the Pareto frontier, but the closer their notions of fairness are to each other, the closer they can get. And, if the notions of fairness <i>are</i> compatible, they can get all the way.</p><h1>Moloch is the Folk Theorem</h1><p>Because of the Folk Theorem, <i>most</i> iterated games will have the same properties I've been talking about (not just iterated PD). Specifically, most iterated games will have:</p><ol><li><strong>Stag-hunt-like property 1: </strong>There is a Pareto-optimal equilibrium, but there is also an equilibrium far from Pareto-optimal.</li><li><strong>The Schelling Pub property:</strong> There are multiple Pareto-optimal equilibria, so that even if you're trying to cooperate, you don't necessarily know which one to aim for; and, different options favor different people, making it a complex negotiation even if you can discuss the problem ahead of time.</li></ol><p>There's a third important property which I've been assuming, but which doesn't follow so directly from the Folk Theorem: <strong>the suboptimal equilibrium is ""safe"", in that you can unilaterally play that way to get some guaranteed utility.</strong> The Pareto-optimal equilibria are not similarly safe; mistakenly playing one of them when other people don't can be worse than the ""safe"" guarantee from the poor equilibrium.</p><p>A game with all three properties is like Stag Hunt with multiple stags (where you all must hunt the same stag to win, but can hunt rabbit alone for a guaranteed mediocre payoff), or Schelling Pub where you can just stay home (you'd rather stay home than go out alone).</p><h1>Lessons in Slaying Moloch</h1><p>0. I didn't even address this in this essay, but it's worth mentioning: <i><strong>not all conflicts are zero-sum</strong>.</i> In the introduction to the 1980 edition of <i>The Strategy of Conflict</i>, Thomas Schelling discusses the reception of the book. He recalls that a prominent political theorist ""exclaimed how much this book had done for his thinking, and as he talked with enthusiasm I tried to guess which of my sophisticated ideas in which chapters had made so much difference to him. It turned out it wasn't any particular idea in any particular chapter. Until he read this book, he had simply not comprehended that an inherently non-zero-sum conflict could exist.""</p><p>1. In situations such as iterated games, <i><strong>there's no in-principle pull toward defection</strong>.</i> Prisoner's Dilemma seems paradoxical when we first learn of it (at least, it seemed so to me) because we are not accustomed to such a harsh divide between individual incentives and the common good. But perhaps, as Sarah Constantine speculated in <a href=""https://srconstantin.wordpress.com/2017/04/10/dont-shoot-the-messenger/"">Don't Shoot the Messenger</a>, modern game theory and economics have conditioned us to be used to this conflict due to their emphasis on single-shot interactions. As a result, Moloch comes to sound like an inevitable gravity, pulling everything downwards. This is not necessarily the case.</p><p>2. Instead, <i><strong>most collective action problems are bargaining problems</strong></i>. If a solution can be agreed upon, we can generally use weak enforcement mechanisms (social norms) or strong enforcement (centralized governmental enforcement) to carry it out. But, agreeing about the solution may not be easy. The more parties involved, the more difficult.</p><p>3. <i><strong>Try to keep a path open toward better solutions.</strong></i> Since wide adoption of a particular solution can be such an important problem, there's a tendency to treat alternative solutions as the enemy. This bars the way to further progress. (One could loosely characterize this as the difference between religious doctrine and democratic law; religious doctrine trades away the ability to improve in favor of the more powerful consensus-reaching technology of immutable universal law. But of course this oversimplifies things somewhat.) Keeping a path open for improvements is hard, partly because it can create exploitability. But it keeps us from getting stuck in a poor equilibrium.</p>",abramdemski,abramdemski,abramdemski,
cYsGrWEzjb324Zpjx,Comparing Utilities,comparing-utilities,https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities,2020-09-14T20:56:15.088Z,72,28,31,False,False,,"<p><i>(This is a basic point about utility theory which many will already be familiar with. I draw some non-obvious conclusions which may be of interest to you even if you think you know this from the title -- but the main point is to communicate the basics. I'm posting it to the alignment forum because I've heard misunderstandings of this from some in the AI alignment research community.)</i></p><p>I will first give the basic argument that the utility quantities of different agents aren't directly comparable, and a few important consequences of this. I'll then spend the rest of the post discussing what to do when you need to compare utility functions.</p><h1>Utilities aren't comparable.</h1><p>Utility isn't an ordinary quantity. A utility function is a device for expressing the preferences of an agent.</p><p>Suppose we have a notion of <i>outcome.* </i>We could try to represent the agent's preferences between outcomes as an ordering relation: if we have outcomes A, B, and C, then one possible preference would be A&lt;B&lt;C.</p><p>However, a mere ordering does not tell us how the agent would decide between <i>gambles,</i> ie, situations giving A, B, and C with some probability.</p><p>With just three outcomes, there is only one thing we need to know: is B closer to A or C, and by how much?</p><figure class=""image image_resized"" style=""width:55.82%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/pvnpb9qsyfcuww577otd"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/agptp0gx44w1bmwx0egj 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/mrjogf9bkpnpzytkdyxe 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/vczrgfmdfum6dfernesq 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/avbbur0yd1dzaupwasea 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/lyh56krscyko1zoswpvu 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ox34ke4tydyajjoevjqo 1920w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/eob1ofyjl8owyqgpzcfn 2240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/wxa4s9zz0o4i8bzseetk 2560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/oirfvd6svb3rb2loggij 2880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/x0aplbqxymfzo1umijdj 3151w""></figure><p>We want to construct a utility function U() which represents the preferences. Let's say we set U(A)=0 and U(C)=1. Then we can represent B=G as U(B)=1/2. If not, we would look for a different gamble which <i>does</i> equal B, and then set B's utility to the expected value of that gamble. By assigning real-numbered values to each outcome, we can fully represent an agent's preferences over gambles. (Assuming the <a href=""https://www.lesswrong.com/posts/F46jPraqp258q67nE/why-you-must-maximize-expected-utility"">VNM axioms</a> hold, that is.)</p><p>But the initial choices U(A)=0 and U(C)=1 were arbitrary! We could have chosen any numbers so long as U(A)&lt;U(C), reflecting the preference A&lt;C. In general, a valid representation of our preferences U() can be modified into an equally valid U'() by adding/subtracting arbitrary numbers, or multiplying/dividing by positive numbers.</p><figure class=""image image_resized"" style=""width:52.73%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/wo66mekinxomlxhqa2pj"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/fd2ndwu7ezvq1hrrrnjt 250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/uj6qsawtx8jvzjigcbu4 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/iez0dwskiahxllpx2vvy 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/o1dfddoedzbbvbioshvd 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/yhabidy8v1dmkbaci3hk 1250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/urkcvgg4mqbycj6wvupf 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/cni8ov4vrkdwgonrudja 1750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/frsr8inwox8cstvqpnop 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/elhoiecf7xjllptxtldz 2250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/zojriw1ossxideyuzuyo 2435w""></figure><p>So it's just as valid to say someone's expected utility in a given situation is 5 or -40, provided you shift everything <i>else</i> around appropriately.</p><p>Writing&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\approx""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">≈</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span></span>&nbsp;to mean that two utility functions represent the same preferences, what we have in general is:&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_1(x) \approx U_2(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">≈</span></span><span class=""mjx-msubsup MJXc-space3""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span></span>&nbsp;if and only if&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U_1(x) = aU_2+b""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.077em; padding-bottom: 0.298em;"">=</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span></span></span></span></span></span>. (I'll call&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""a""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span></span></span></span></span></span>&nbsp;the <i><strong>multiplicative constant </strong></i>and<i><strong>&nbsp;</strong></i><span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""b""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span></span></span></span></span></span><i><strong>&nbsp;</strong></i>the<i><strong> additive constant</strong></i>.)</p><p>This means that we can't directly compare the utility of two different agents. Notions of fairness should not directly say ""everyone should have the same expected utility"". Utilitarian ethics cannot directly maximize the sum of everyone's utility. Both of these operations should be thought of as a type error.</p><h1>Some non-obvious consequences.</h1><p>The game-theory term <a href=""https://www.lesswrong.com/posts/D8ds9idKWbwzCseCh/zero-sum-is-a-misnomer"">""zero sum"" is a misnomer</a>. You shouldn't directly think about the sum of the utilities.</p><p>In mechanism design, <i>exchangeable utility</i> is a useful assumption which is often needed in order to get nice results. The idea is that agents can give utils to each other, perhaps to compensate for unfair outcomes. This is <i>kind of</i> like assuming there's money which can be exchanged between agents. However, the non-comparability of utility should make this seem <i>really weird</i>. (There are also other disanalogies with money; for example, utility is closer to logarithmic in money, not linear.)</p><p>This could (should?) also make you suspicious of talk of ""average utilitarianism"" and ""total utilitarianism"". However, beware: only one kind of ""utilitarianism"" holds that the term ""utility"" in decision theory means the same thing as ""utility"" in ethics: namely, preference utilitarianism. Other kinds of utilitarianism can distinguish between these two types of utility. (For example, one can be a hedonic utilitarian without thinking that what everyone wants is happiness, if one isn't a preference utilitarian.)</p><p>Similarly, for preference utilitarians, talk of <i>utility monsters</i> becomes questionable. A utility monster is, supposedly, someone who gets much more utility out of resources than everyone else. For a hedonic utilitarian, it would be someone who experiences much deeper sadness and much higher heights of happiness. This person supposedly merits more resources than other people.</p><p>For a preference utilitarian, incomparability of utility means we can't simply posit such a utility monster. It's meaningless <i>a priori</i> to say that one person simply has much stronger preferences than another (in the utility function sense).</p><p>All that being said, we <i>can</i> actually compare utilities, sum them, exchange utility between agents, define utility monsters, and so on. We just need <i>more information.</i></p><h1>Comparing utilities.</h1><p>The incomparability of utility functions <i><strong>doesn't mean</strong></i> we can't trade off between the utilities of different people.</p><p>I've heard the non-comparability of utility functions summarized as the thesis that we can't say anything meaningful about the relative value of one person's suffering vs another person's convenience. Not so! Rather, the point is just that <i>we need more assumptions in order to say anything. </i>The utility functions alone aren't enough.</p><h2>Pareto-Optimality: The Minimal Standard</h2><p>Comparing utility functions suggests putting them all onto one scale, such that we can trade off between them -- ""this dollar does more good for Alice than it does for Bob"". We formalize this by imagining that we have to decide policy for the whole group of people we're considering (e.g., the whole world). We consider a <i>social choice function</i> which would make those decisions on behalf of everyone. Supposing it is VNM rational, its decisions must be comprehensible in terms of a utility function, too. So the problem reduces to combining a bunch of individual utility functions, to get one big one.</p><p>So, how do we go about combining the preferences of many agents into one?</p><p>The first and most important concept is the <i><strong>pareto improvement:</strong> our social choice function should endorse changes which benefit someone and harm no one. </i>An option which allows no such improvements is said to be <i><strong>Pareto-optimal.</strong></i></p><p>We might also want to consider <i><strong>strict Pareto improvements:</strong> a change which benefits everyone. </i>(An option which allows no strict Pareto improvements is <i><strong>weakly Pareto-optimal.</strong></i>) Strict Pareto improvements can be more relevant <a href=""https://www.lesswrong.com/posts/5bd75cc58225bf067037554e/distributed-cooperation?commentId=5bd75cc58225bf0670375550"">in a bargaining context</a>, where you need to give everyone something in order to get them on board with a proposal -- otherwise they may judge the improvement as unfairly favoring others. However, in a bargaining context, individuals may refuse even a strict Pareto improvement <a href=""https://www.lesswrong.com/posts/z2YwmzuT7nWx62Kfh/cooperating-with-agents-with-different-ideas-of-fairness"">due to fairness considerations</a>.</p><p>In either case, a version of <a href=""https://www.lesswrong.com/posts/sZuw6SGfmZHvcAAEP/complete-class-consequentialist-foundations#Utilitarianism"">Harsanyi's utilitarianism Theorem</a> implies that the utility of our social choice function <i>can be understood as some linear combination of the individual utility functions.</i></p><p>So, pareto-optimal social choice functions can always be understood by:</p><ol><li>Choosing a scale for everyone's utility function -- IE, set the multiplicative constant. (If the social choice function is only weakly Pareto optimal, some of the multiplicative constants might turn out to be zero, totally cancelling out someone's involvement. Otherwise, they can all be positive.)</li><li>Adding all of them together.</li></ol><p>(Note that the <i>additive constant</i> doesn't matter -- shifting a person's utility function up or down doesn't change what decisions will be endorsed by the sum. However, it <i><strong>will</strong></i> matter for some other ways to combine utility functions.)</p><p>This is nice, because we can always combine everything linearly! We just have to set things to the right scale and then sum everything up.</p><p>However, it's far from the end of the story. How do we choose multiplicative constants for everybody?</p><h2>Variance Normalization: Not Too Exploitable?</h2><p>We could set the constants any way we want... totally subjective estimates of the worth of a person, draw random lots, etc. But we do typically want to represent some notion of fairness. We said in the beginning that the problem was, a utility function&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U(x)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span></span>&nbsp;has many equivalent representations&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""aU(x)+b""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span></span></span></span></span></span>. We can address this as a problem of <i><strong>normalization:</strong></i> we want to take a&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span></span></span></span></span>&nbsp;and put it into a canonical form, getting rid of the choice between equivalent representations.</p><p>One way of thinking about this is <i><strong>strategy-proofness</strong></i>. A utilitarian collective should not be vulnerable to members strategically claiming that their preferences are stronger (larger&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""b""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">b</span></span></span></span></span></span></span>), or that they should get more because they're worse off than everyone (smaller&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""a""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span></span></span></span></span></span>&nbsp;-- although, remember that we haven't talked about any setup which actually cares about that, yet).</p><p><strong>Warm-Up: Range Normalization</strong></p><p>Unfortunately, some obvious ways to normalize utility functions are not going to be strategy-proof.</p><p>One of the simplest normalization techniques is to squish everything into a specified range, such as [0,1]:</p><figure class=""image image_resized"" style=""width:62.59%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/qrn3ypcdhey2yzpykges"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/yfic5oyw2x0dn3vcr224 250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/raptaizcw5uw3chocg1w 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/lcikzyugq19gwqjozvhp 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/vstbsda6k1pvxkayljcc 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/qrnvtbhfgfe3arklmamb 1250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/cwux4dlf6nzgdeiolwvm 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/zpzpvsph7jyrrkfdkx6h 1750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/jmcclcyj4xooq0sksmcq 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/e115fpd5cusicbltm59r 2250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/o2jpdot1cu9ozclfx4m8 2435w""></figure><p>This is analogous to range voting: everyone reports their preferences for different outcomes on a fixed scale, and these all get summed together in order to make decisions.</p><p>If you're an agent in a collective which uses range normalization, then you may want to strategically mis-report your preferences. In the example shown, the agent has a big hump around outcomes they like, and a small hump on a secondary ""just OK"" outcome. The agent might want to get rid of the second hump, forcing the group outcome into the more favored region.</p><p>I believe that in the extreme, the optimal strategy for range voting is to choose some utility threshold. Anything below that threshold goes to zero, feigning maximal disapproval of the outcome. Anything above the threshold goes to one, feigning maximal approval. In other words, under strategic voting, range voting becomes approval voting (range voting where the only options are zero and one).</p><p>If it's not possible to mis-report your preferences, then the incentive becomes to <i>self-modify to literally have these extreme preferences. </i>This could perhaps have a real-life analogue in political outrage and black-and-white thinking. If we use this normalization scheme, that's the closest you can get to being a utility monster.</p><p><strong>Variance Normalization</strong></p><p>We'd <i>like</i> to avoid <i>any</i> incentive to misrepresent/modify your utility function. Is there a way to achieve that?</p><p>Owen Cotton-Barratt discusses different normalization techniques in illuminating detail, and argues for <i>variance normalization:</i> divide utility functions by their variance, making the variance one. (<a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.684.1180&amp;rep=rep1&amp;type=pdf""><i>Geometric reasons for normalizing variance to aggregate preferences,</i> O Cotton-Barratt, 2013.</a>) Variance normalization is strategy-proof under the assumption that everyone participating in an election shares beliefs about how probable the different outcomes are! (Note that <i>variance</i> <i>of utility</i> is only well-defined under some assumption about <i>probability of outcome.</i>) That's pretty good. It's probably the best we can get, in terms of strategy-proofness of voting. Will MacAskill also argues for variance normalization in the context of normative uncertainty (<a href=""https://d1wqtxts1xzle7.cloudfront.net/34857095/Normative_Uncertainty__Complete.pdf?1411561048=&amp;response-content-disposition=inline%3B+filename%3DNormative_Uncertainty.pdf&amp;Expires=1599854518&amp;Signature=Z-cD7ds~K1cZc-GlXyV~eppzxbVKlwJCIkz6AQIHUg4jOgQlMAcgi3X1cCX~Z~FSvXKAYEwTyqehuuxCkMA2hxguilao82uaF8cH7sEZxczg243o2S5k4sZ7-YeIp5cJ2U-UAsecA-JbROuHU9AkUnlR02-rL4q-JlAlCOBBP5CDjJC6aocEM1HEyL0bHxFXf7Wg~B4Jyf8KSlvdnuAbm7IFn~lbmrBLb6OQG5~VbGAz8rfH2AuZlbZOpdVoID~MgPtIZ9rF1kMAcrUAhS93D15BPd8XNDRAOtMKMvSs~5xh5Ok-7dQhTFhqXkk~YE4S23VoKGGwqGZp4yl9X6WnTQ__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA""><i>Normative Uncertainty, </i>Will MacAskill, 2014</a>).</p><p>Intuitively, variance normalization directly addresses the issue we encountered with range normalization: an individual attempts to make their preferences ""loud"" by extremizing everything to 0 or 1. This increases variance, so, is directly punished by variance normalization.</p><p>However, <a href=""https://www.lesswrong.com/users/jameson-quinn"">Jameson Quinn</a>, LessWrong's resident voting theory expert, has warned me rather strongly about variance normalization.</p><ol><li>The assumption of shared beliefs about election outcomes is far from true in practice. Jameson Quinn tells me that, in fact, the strategic voting incentivized by quadratic voting is <i>particularly bad</i> amongst normalization techniques.</li><li>Strategy-proofness isn't, after all, the final arbiter of the quality of a voting method. The final arbiter should be something like the utilitarian quality of an election's outcome. This question gets a bit weird and recursive in the current context, where I'm using elections as an analogy to ask how we should define utilitarian outcomes. But the point still, to some extent, stands.</li></ol><p>I didn't understand Quinn's full justification behind his point, but I came away thinking that range normalization was probably better in practice. After all, it reduces to approval voting, which is actually a pretty good form of voting. But if you want to do the best we can with the state of voting theory, Jameson Quinn suggested 3-2-1 voting. (I don't think 3-2-1 voting gives us any nice theory about how to combine utility functions, though, so it isn't so useful for our purposes.)</p><p><strong>Open Question: </strong><i>Is there a variant of variance normalization which takes differing beliefs into account, to achieve strategy-proofness (IE honest reporting of utility)?</i></p><p>Anyway, so much for normalization techniques. These techniques ignore the broader context. They attempt to be fair and even-handed <i>in the way we choose the multiplicative and additive constants.</i> But we could also explicitly try to be fair and even-handed <i>in the way we choose between Pareto-optimal outcomes</i>, as with this next technique.</p><h2>Nash Bargaining Solution</h2><p>It's important to remember that the Nash bargaining solution is a solution <i>to the Nash bargaining problem</i>, which isn't quite our problem here. But I'm going to gloss over that. Just imagine that we're setting the social choice function through a massive negotiation, so that we can apply bargaining theory.</p><p>Nash offers a very simple solution, which I'll get to in a minute. But first, a few words on how this solution is derived. Nash provides two seperate justifications for his solution. The first is a game-theoretic derivation of the solution as an especially robust Nash equilibrium. I won't detail that here; I quite recommend <a href=""http://www.rasmusen.org/GI/reader/12a.nash.bargaining.1950.pdf"">his original paper</a> (<i>The Bargaining Problem, </i>1950); but, just keep in mind that there is at least some reason to expect selfishly rational agents to hit upon this particular solution. The second, unrelated justification is an axiomatic one:</p><ol><li><i>Invariance to equivalent utility functions. </i>This is the same motivation I gave when discussing normalization.</li><li><i>Pareto optimality.</i> We've already discussed this as well.</li><li><i>Independence of Irrelevant Alternatives (IIA).</i> This says that we shouldn't change the outcome of bargaining by removing options which won't ultimately get chosen anyway. This isn't even technically one of the VNM axioms, but it <i>essentially </i>is -- the VNM axioms are posed for binary preferences (a &gt; b). IIA is the assumption we need to break down multi-choice preferences to binary choices. We can justify IIA with <a href=""https://www.lesswrong.com/posts/5bd75cc58225bf067037539a/generalizing-foundations-of-decision-theory-ii"">a kind of money pump</a>.</li><li><i>Symmetry.</i> This says that the outcome doesn't depend on the order of the bargainers; we don't prefer Player 1 in case of a tie, or anything like that.</li></ol><p>Nash proved that <i>the only way to meet these four criteria</i> is to maximize the <strong>product</strong> of gains from cooperation. More formally, choose the outcome&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""x""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span></span></span></span></span></span>&nbsp;which maximizes:</p><span class=""math-tex""><span class=""mjpage mjpage__block""><span class=""mjx-chtml MJXc-display"" style=""text-align: center;""><span class=""mjx-math"" aria-label=""(U_1(x) - U_1(d))(U_2(x)-U_2(d))""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">1</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-msubsup""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">−</span></span><span class=""mjx-msubsup MJXc-space2""><span class=""mjx-base"" style=""margin-right: -0.084em;""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span></span><p>The&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""d""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span></span></span></span></span></span>&nbsp;here is a ""status quo"" outcome. You can think of this as what happens if the bargaining fails. This is sometimes called a ""threat point"", since strategic players should carefully set what they do <i>if negotiation fails</i> so as to maximize their bargaining position. However, you might also want to rule that out, forcing&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""d""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span></span></span></span></span></span>&nbsp;to be a Nash equilibrium in the hypothetical game where there is no bargaining opportunity. As such,&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""d""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span></span></span></span></span></span>&nbsp;is also known as the <i>best alternative to negotiated agreement (BATNA)</i>, or sometimes the ""disagreement point"" (since it's what players get if they can't agree). We can think of subtracting out&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""U(d)""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;"">U</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span></span></span></span>&nbsp;as just a way of adjusting the additive constant, in which case we really are just maximizing the product of utilities. (The BATNA point is always (0,0) after we subtract out things that way.)</p><p>The Nash solution differs significantly from the other solutions considered so far.</p><ol><li>Maximize the <i>product??</i> Didn't Harsanyi's theorem guarantee we only need to worry about sums?</li><li>This is the first proposal where the additive constants matter. Indeed, now the <i>multiplicative</i> constants are the ones that don't matter!</li><li>Why wouldn't <i>any</i> utility-normalization approach satisfy those four axioms?</li></ol><p>Last question first: how do normalization approaches violate the Nash axioms?</p><p>Well, both range normalization and variance normalization violate IIA! If you remove one of the possible outcomes, the normalization may change. This makes the social choice function display inconsistent preferences across different scenarios. (But how bad is that, really?)</p><p>As for why we can get away with maximizing the product, rather than the sum:</p><p>The Pareto-optimality of Nash's approach guarantees that it <i>can be seen</i> as maximizing a linear function of the individual utilities. So Harsanyi's theorem is still satisfied. However, Nash's solution points to a very <i>specific</i> outcome, which Harsanyi doesn't do for us.</p><p>Imagine you and me are trying to split a dollar. If we can't agree on how to split it, then we'll end up destroying it (ripping it during a desperate attempt to wrestle it from each other's hands, obviously). Thankfully, John Nash is standing by, and we each agree to respect his judgement. No matter which of us claims to value the dollar more, Nash will allocate 50 cents to each of us.</p><p>Harsanyi happens to see this exchange, and explains that Nash has chosen a social choice function which normalized our utility functions to be equal to each other. That's the only way Harsanyi can explain the choice made by Nash -- the value of the dollar was precisely tied between you and me, so a 50-50 split was as good as any other outcome. Harsanyi's justification is indeed <i>consistent</i> with the observation. But why, then, did Nash choose 50-50 <i>precisely?</i> 49-51 would have had exactly the same collective utility, as would 40-60, or any other split!</p><p>Hence, Nash's principle is far more useful than Harsanyi's, even though Harsanyi can justify any rational outcome retrospectively.</p><p>However, Nash does rely somewhat on that pesky IIA assumption, whose importance is perhaps not so clear. Let's try getting rid of that.</p><h2>Kalai–Smorodinsky</h2><p>Although the Nash bargaining solution is the most famous, there are other proposed solutions to Nash's bargaining problem. I want to mention just one more, Kalai-Smorodinsky (I'll call it KS).</p><p>KS throws out IIA as irrelevant. After all, the set of alternatives <i>will</i> affect bargaining. Even in the Nash solution, the set of alternatives may have an influence by changing the BATNA! So perhaps this assumption isn't so important.</p><p>KS instead adds a <i>monotonicity</i> assumption: being in a better position should never make me worse off after bargaining.</p><p>Here's an illustration, due to Daniel Demski, of a case where Nash bargaining fails monotonicity:</p><figure class=""image image_resized"" style=""width:44.22%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/yfonnbvflv2zoye4sssv"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/flon8jqbvkofcnkfzsxv 112w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ukfbfpyczkkf0peswpfg 192w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/qebfur9ztjthwsnkkfxa 272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/g3pyevzyfpn5vkzzhntm 352w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/gxqakzgefyxbcs6onryx 432w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/jmcmepvfcznmrmlm4812 512w""></figure><figure class=""image image_resized"" style=""width:44.69%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/avi7z013iczcvml76ocr"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ffk7avszfsothzd4twdp 112w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ym3kb12ydmxkv4smfgac 192w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/t9w8om24htan3tqkjwwq 272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/mujgxbqgqpstp3tgvyny 352w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/zffxsc2in8ecjagg54uh 432w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/aip4pr6ufpdltqpnb7nx 512w""></figure><figure class=""image image_resized"" style=""width:49.57%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/m5n2k2iovgjuv2skq7rw"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/q9yprxdpfcdsgnoi4ica 112w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ulklvufohrdt4rdgktwh 192w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ajwksytxxlee2y8pnuze 272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/nudrjkdh15io5wdi5ibe 352w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/mjm2okuctnrp4igbfgd4 432w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/zejngww8rzdppxynm07g 512w""></figure><p>I'm not that sure monotonicity really should be an axiom, but it does kind of suck to be in an apparently better position and end up worse off for it. Maybe we could relate this to strategy-proofness? A little? Not sure about that.</p><p>Let's look at the formula for KS bargaining.&nbsp;</p><p>Suppose there are a couple of dollars on the ground: one which you'll walk by first, and one which I'll walk by. If you pick up your dollar, you can keep it. If I pick up my dollar, I can keep mine. But also, if you <i>don't</i> pick up yours, then I'll eventually walk by it and can pick it up. So we get the following:</p><figure class=""image image_resized"" style=""width:63.18%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ybo4rbu7ks2y39i5ignb"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/y5tzut9cspgdhxz7aljv 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/bygbtadxxbrys9pxizqe 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/vlu10kt4vwyrssirktu7 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/lzjtq78pjsvwja3ulcdh 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/kfjdcft5b83xbpuibymi 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/xckrb5fjuwbuttkvrx6w 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/r62tviuazxcbrkrh7jqi 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ydm8gsowftw7hagdvilv 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/mm64nvhse2zahb4evvh9 760w""></figure><p>(The box is filled in because we can also use mixed strategies to get values intermediate between any pure strategies.)</p><p>Obviously in the real world we just both pick up our dollars. But, let's suppose we bargain about it, just for fun.</p><p>The way KS works is, you look at the maximum <i>one</i> player can get (you can get $1), and the maximum the <i>other</i> player could get (I can get $2). Then, although we can't usually jointly achieve those payoffs (I can't get $2 at the same time as you get $1), KS bargaining insists we achieve the same <i>ratio</i> (I should get twice as much as you). In this case, that means I get $1.33, while you get $0.66. We can visualize this as drawing a bounding box around the feasible solutions, and drawing a diagonal line. Here's the Nash and KS solutions side by side:</p><figure class=""image image_resized"" style=""width:54.43%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/zh8yz6brwxujyawb2bg1"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ceqxcfoqxh7j566kgudw 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ziguk5hdamrbc1t4dols 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/vpfiw4zpywx2kugju025 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/f6unrlsrewodfhwxtagk 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/xavim2trevpfkzzrtdsv 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/nxg5qrdzyvgfwynghvtw 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/kiuvlhkucv9hsswli7kd 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/nt3hlceazozjnappn8py 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/cxtmhee5nk791mi50i7d 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cYsGrWEzjb324Zpjx/ctb4rvtxplceyp6lg9jm 1041w""></figure><p>As in Daniel's illustrations, we can visualize maximizing the product as drawing the largest hyperbola we can that still touches the orange shape. (Orange dotted line.) This suggests that we each get $1; exactly the same solution as Nash would give for splitting $2. (The black dotted line illustrates how we'd continue the feasible region to represent a dollar-splitting game, getting the full triangle rather than a chopped off portion.) Nash doesn't care that one of us can do better than the other; it just looks for the most equal division of funds possible, since that's how we maximize the product.</p><p>KS, on the other hand, cares what the max possible is for both of us. It therefore suggests that you give up some of your dollar to me.</p><p>I suspect most readers will <i><strong>not</strong></i> find the KS solution to be more intuitively appealing?</p><p>Note that the KS monotonicity property does NOT imply the desirable-sounding property ""if there are more opportunities for good outcomes, everyone gets more or is at least not worse off."" (I mention this mainly because I initially misinterpreted KS's monotonicity property this way.) In my dollar-collecting example, KS bargaining makes you worse off simply because there's an opportunity for me to take your dollar if you don't.&nbsp;</p><p>Like Nash bargaining, KS bargaining ignores multiplicative constants on utility functions, and can be seen as normalizing additive constants by treating&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""d""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span></span></span></span></span></span>&nbsp;as (0,0). (Note that, in the illustration, I assumed&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""d""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span></span></span></span></span></span>&nbsp;is chosen as (minimal achievable for one player, minimal achievable for the other). this need not be the case in general.)</p><p>A peculiar aspect of KS bargaining is that it doesn't really give us an obvious quantity to maximize, unlike Nash or Harsanyi. It only describes the optimal point. This seems far less practical, for realistic decision-making.</p><p>OK, so, should we use bargaining solutions to compare utilities?</p><p>My intuition is that, because of the need to choose the BATNA point&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""d""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span></span></span></span></span></span>, bargaining solutions end up rewarding destructive threats in a disturbing way. For example, suppose that we are playing the dollar-splitting game again, except that I can costlessly destroy $20 of your money, so&nbsp;<span class=""math-tex""><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""d""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span></span></span></span></span></span>&nbsp;now involves both the destruction of the $1, and the destruction of $20. Nash bargaining now hands the entire dollar to me, because you are ""up $20"" in that deal, so the fairest possible outcome is to give me the $1. KS bargaining splits things up a little, but I still get most of the dollar.</p><p>If utilitarians were to trade off utilities that way in the real world, it would benefit powerful people, especially those willing to exploit their power to make credible threats. If X can take everything away from Y, then Nash bargaining sees everything Y has as already counting toward ""gains from trade"".</p><p>As I mentioned before, sometimes people try to define BATNAs in a way which excludes these kinds of threats. However, I see this as ripe for strategic utility-spoofing (IE, lying about your preferences, or self-modifying to have more advantageous preferences).</p><p>So, this might favor normalization approaches.</p><p>On the other hand, Nash and KS both do way better in the split-the-dollar game than any normalization technique, because they can optimize for fairness of outcome, rather than just fairness of multiplicative constants chosen to compare utility functions with.</p><p>Is there any approach which combines the advantages of bargaining and normalization??</p><h1>Animals, etc.</h1><p>An essay on utility comparison would be incomplete without at least mentioning the problem of animals, plants, and so on.</p><ul><li>Option one: some cutoff for ""moral patients"" is defined, such that a utilitarian only considers preferences of agents who exceed the cutoff.</li><li>Option two: some more continuous notion is selected, such that we care more about some organisms than others.</li></ul><p>Option two tends to be more appealing to me, despite the non-egalitarian implications (e.g., if animals differ on this spectrum, than humans could have some variation as well).&nbsp;</p><p>As already discussed, bargaining approaches do seem to have this feature: animals would tend to get less consideration, because they've got less ""bargaining power"" (they can do less harm to humans than humans can do to them). However, this has a distasteful might-makes-right flavor to it.</p><p>This also brings to the forefront the question of how we view something as an agent. Something like a plant might have quite deterministic ways of reacting to environmental stimulus. Can we view it as making choices, and thus, as having preferences? Perhaps ""to some degree"" -- if such a degree could be defined, numerically, it could factor into utility comparisons, giving a formal way of valuing plants and animals <i>somewhat, </i>but ""not too much"".</p><h1>Altruistic agents.</h1><p>Another puzzling case, which I think needs to be handled carefully, is accounting for the preferences of altruistic agents.</p><p>Let's proceed with a simplistic model where agents have ""personal preferences"" (preferences which just have to do with themselves, in some sense) and ""<i><strong>cofrences</strong></i>"" (co-preferences; preferences having to do with other agents).</p><p>Here's an agent named Sandy:</p><figure class=""table""><table><tbody><tr><td colspan=""5"">Sandy</td></tr><tr><td colspan=""2"">Personal Preferences</td><td colspan=""3"">Cofrences</td></tr><tr><td>Candy</td><td>+.1</td><td>Alice</td><td colspan=""2"">+.1</td></tr><tr><td>Pizza</td><td>+.2</td><td>Bob</td><td colspan=""2"">-.2</td></tr><tr><td>Rainbows</td><td>+10</td><td>Cathy</td><td colspan=""2"">+.3</td></tr><tr><td>Kittens</td><td>-20</td><td>Dennis</td><td colspan=""2"">+.4</td></tr></tbody></table></figure><p>The cofrences represent coefficients on other agent's utility functions. Sandy's preferences are supposed to be understood as a utility function representing Sandy's <i>personal</i> preferences, plus a weighted sum of the utility functions of Alice, Bob, Cathy, and Dennis. (Note that the weights can, hypothetically, be negative -- for example, screw Bob.)</p><p>The first problem is that utility functions are not comparable, so we have to say more before we can understand what ""weighted sum"" is supposed to mean. But suppose we've chosen some utility normalization technique. There are still other problems.</p><p>Notice that we can't totally define Sandy's utility function until we've defined Alice's, Bob's, Cathy's, and Dennis'. But any of those four might have cofrences which involve Sandy, as well!</p><p>Suppose we have Avery and Briar, two lovers who ""only care about each other"" -- their only preference is a cofrence, which places 1.0 value on the other's utility function. We could ascribe <i>any values at all</i> to them, so long as they're both the same!</p><p>With some technical assumptions (something along the lines of: your cofrences always sum to less than 1), we can ensure a unique fixed point, eliminating any ambiguity from the interpretation of cofrences. However, I'm skeptical of just taking the fixed point here.</p><p>Suppose we have five siblings: Primus, Secundus, Tertius, Quartus, et Quintus. All of them value each other at .1, except Primus, who values all siblings at .2.</p><p>If we simply take the fixed point, Primus is going to get the short end of the stick all the time: because Primus cares about everyone else more, everyone else cares about Primus' personal preferences <i>less</i> than anyone else's.</p><p>Simply put, I don't think more altruistic individuals should be punished! In this setup, the ""utility monster"" is the perfectly selfish individual. Altruists will be scrambling to help this person while the selfish person does nothing in return.</p><p>A different way to do things is to interpret cofrences as <i>integrating only the personal preferences of the other person.</i> So Sandy wants to help Alice, Cathy, and Dennis (and harm Bob), but does <i>not</i> automatically extend that to wanting to help any of their friends (or harm Bob's friends).</p><p>This is a little weird, but gives us a more intuitive outcome in the case of the five siblings: Primus will more often be voluntarily helpful to the other siblings, but the other siblings won't be prejudice <i>against</i> the personal preferences of Primus when weighing between their various siblings.</p><p>I realize altruism isn't <i>exactly</i> supposed to be like a bargain struck between selfish agents. But if I think of utilitarianism like a coalition of all agents, then I don't want it to punish the (selfish component of) the most altruistic members. It seems like utilitarianism should have better incentives than that?</p><p>(Try to take this section as more of a problem statement and less of a solution. Note that the concept of <i>cofrence</i> can include, more generally, preferences such as ""I want to be better off than other people"" or ""I don't want my utility to be too different from other people's in either direction"".)</p><h1>Utility monsters.</h1><p>Returning to some of the points I raised in the ""non-obvious consequences"" section -- now we can see how ""utility monsters"" are/aren't a concern.</p><p>On my analysis, a utility monster is just an agent who, according to your metric for comparing utility functions, has a very large influence on the social choice function.</p><p>This might be a bug, in which case you should reconsider how you are comparing utilities. But, since you've hopefully chosen your approach carefully, it could also not be a bug. In that case, you'd want to bite the bullet fully, defending the claim that such an agent should receive ""disproportionate"" consideration. Presumably this claim could be backed up, on the strength of your argument for the utility-comparison approach.</p><h1>Average utilitarianism vs total utilitarianism.&nbsp;</h1><p>Now that we have given some options for utility comparison, can we use them to make sense of the distinction between average utilitarianism and total utilitarianism?</p><p>No. Utility comparison doesn't really help us there.</p><p>The average vs total debate is a debate about population ethics. Harsanyi's utilitarianism theorem and related approaches let us think about altruistic policies for a fixed set of agents. They don't tell us how to think about a set which changes over time, as new agents come into existence.</p><p>Allowing the set to vary over time like this feels similar to allowing a single agent to change its utility function. There is no rule against this. An agent can prefer to have different preferences than it does. A collective of agents can prefer to extend its altruism to new agents who come into existence.</p><p>However, I see no reason why population ethics needs to be <i>simple</i>. We can have relatively complex preferences here. So, I don't find paradoxes such as the Repugnant Conclusion to be especially concerning. To me there's just this complicated question about what everyone collectively wants for the future.</p><p>One of the basic questions about utilitarianism shouldn't be ""average vs total?"". To me, this is a type error. It seems to me, more basic questions for a (preference) utilitarian are:</p><ul><li>How do you combine individual preferences into a collective utility function?<ul><li>How do you compare utilities between people (and animals, etc)?<ul><li>Do you care about an ""objective"" solution to this, or do you see it as a subjective aspect of altruistic preferences, which can be set in an unprincipled way?</li><li>Do you range-normalize?</li><li>Do you variance-normalize?</li><li>Do you care about strategy-proofness?</li><li>How do you evaluate the bargaining framing? Is it relevant, or irrelevant?</li><li>Do you care about Nash's axioms?</li><li>Do you care about monotonicity?</li><li>What distinguishes humans from animals and plants, and how do you use it in utility comparison? Intelligence? Agenticness? Power? Bargaining position?</li></ul></li><li>How do you handle cofrences?</li></ul></li></ul><p>&nbsp;</p><p>*: Agents need not have a concept of outcome, in which case they <a href=""https://www.lesswrong.com/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions"">don't really have a utility function</a> (because utility functions are functions <i>of outcomes</i>). However, this does not significantly impact any of the points made in this post.</p>",abramdemski,abramdemski,abramdemski,
LhHPDioBzJGLyb26b,"If Starship works, how much would it cost to create a system of rotable space mirrors that reduces temperatures on earth by 1° C?",if-starship-works-how-much-would-it-cost-to-create-a-system,https://www.lesswrong.com/posts/LhHPDioBzJGLyb26b/if-starship-works-how-much-would-it-cost-to-create-a-system,2020-09-14T18:46:19.490Z,13,8,6,False,True,,"<p>There are many proposed geoengineering solutions that could reduce temperatures on earth. Unfortunately, a lot of them have a mix of side effects and lock-in effects where the changes in temperature come years after deployment which creates risk of unintended consequences.</p><p>If we would have a constellation of space mirrors that can be rotated as we desire to let in less or more sunlight, how much would it cost to bring up enough to reduce temperatures on earth by an average of &nbsp;1° C? Let's say that Elon's promise of being able to lunch a Starship that brings up 100,000kg for 1,000,000$ works out, what would it cost to produce and deploy those mirrors?&nbsp;</p>",ChristianKl,christiankl,ChristianKl,
jJferdT86smaRXCYm,Outcome Terminology?,outcome-terminology,https://www.lesswrong.com/posts/jJferdT86smaRXCYm/outcome-terminology,2020-09-14T18:04:05.048Z,6,3,0,False,True,,"<p>I'm writing a post about S-risks, and I need access to some clean, established terminology/background material for discussing AI-based long-term outcomes for humanity.</p><p>My current (very limited) vocabulary can be summarized with the following categories:&nbsp;</p><ol><li>Outcomes which are roughly maximally bad: Hyperexistential risk/S-risk/Unfriendly AI/Existential risk</li><li>Outcomes which are nontrivially worse than paperclipping-equivalents but better than approximate minimization of human utility: Hyperexistential risk/S-risk/Unfriendly AI/Existential risk</li><li>Outcomes which are produced by agents essentially orthogonal to human values: Paperclipping/Unfriendly AI/Existential risk</li><li>Outcomes which are nontrivially better than paperclipping but worse than Friendly AI: ???</li><li>Outcomes which are roughly maximally good: Friendly AI</li></ol><p>The problems are manifold:&nbsp;</p><ul><li>I haven't read any discussion which <i>specifically</i> addresses parts 1 or 2. I have read general discussion of parts 1 and 2 combined under the names of ""Outcomes worse than death"", ""Hyperexistential risk"", ""S-risk"", etc.</li><li>My current terminology overlaps too strongly to use to uniquely identify outcomes 1 and 2.</li><li>I have no terminology or background information for outcome 4.</li></ul><p>I've done a small amount of investigation and determined less brainpower would be wasted by just asking for links.</p>",Dach,dach,Dach,
phP29sPoG3czr5h8H,On Niceness: Looking for Positive Externalities,on-niceness-looking-for-positive-externalities,https://www.lesswrong.com/posts/phP29sPoG3czr5h8H/on-niceness-looking-for-positive-externalities,2020-09-14T18:03:12.196Z,32,10,0,False,False,https://www.neelnanda.io/blog/mini-blog-post-10-seek-positive-externalities,"<p>One of the most useful concepts I&#x2019;ve learned from economics is the idea of an <strong>externality</strong>: the consequences of your actions on <em>other people</em>. This is important because, intuitively, humans are self-centred, and it&#x2019;s easy to not notice the effects your actions have on others. And it almost never feels as <em>visceral </em>as the costs and benefits to yourself. The canonical examples are coordination problems, like climate change. Taking a plane flight has strong benefits to me, but costs everyone on Earth a little bit, a negative externality. And a lot of the problems in the world today boil down to coordination problems where our actions have negative externalities.</p><p>But, for this post, I don&#x2019;t care about any of that. The important part is that externalities introduce a <em>bias</em>. And once you&#x2019;ve noticed a bias, something that is preventing you from taking the best actions, you can correct for it! And a much more interesting bias is a bias away from <em>positive </em>externalities. </p><p>With my Effective Altruism hat on, the obvious positive externalities are the good your actions can do for the countless unknown strangers in need. And this is an extremely important way to correct for this bias. But for this post I want to put my <em>in</em>effective altruism hat on, and talk about something more fun! The <em>local </em>positive externalities - being <strong>nice</strong> to the people around you. Where by niceness, I don&#x2019;t mean nonsense like virtue signalling, I mean <strong>taking actions that make the people around you happier, and making their lives better.</strong></p><p>I think we have systematic biases against being nice to our friends and those close to us, because being nice is, fundamentally, a positive externality. Being nice to people is <em>obviously </em>great. I think it&#x2019;s intrinsically good to help the people I care about. And there&#x2019;s a lot of selfish benefits to me! People are more likely to do you favours, people like you more, it&#x2019;s fun to help people, you have a better reputation, etc. </p><p>Yet, in practice, most people approach niceness in a very intuitive way. Doing nice things when the idea occurs to them, in a very local, unplanned way. But, as with all things that matter in life, niceness can be <em><a href=""https://www.neelnanda.io/blog/mini-blog-post-9-what-it-means-to-optimise"">optimised</a></em> for. A <em>really </em>significant life upgrade for me was realising this, and trying to introduce a deliberate bias in favour of niceness. If I ever have anything I care about, I try to figure out how I can achieve it while also being nice to the people around me. And this is <em>such </em>a strong systematic bias that often this helps me achieve my original goal better! And anything that can help me find win-win situations is valuable, and to be cherished and cultivated.</p><p>Further, I think it&#x2019;s important to notice the strongest biases I have against niceness. One of the most glaring, is that humans (and especially me) are loss averse. There are many actions I can take which gives high upside for somebody else, with small downside risk. Eg, recommended that somebody apply for a job, or talk to a specific person - this could be <em>amazing</em>, and worst case it mildly annoys them. But it&#x2019;s easy to fixate on this worst case scenario, and avoid ever taking action. And I think this bias systematically holds you back from being as good a friend as you can be.</p><p>And I think niceness often emerges from your self-image. It&#x2019;s easy to say &#x201C;I&#x2019;m not the <em>kind of person </em>who&#x2019;s nice to other people - it feels weak and sappy&#x201D;. And if your self-image holds you back from win-win situations, this is dumb and should be changed. My most effective path to this has been to get <em>excited </em>about niceness, and to make it a habit. Finding as many ways to shape my around it has made me more sensitive to opportunities for niceness, </p><p>This is all far easier said than done, so to hopefully provide some inspiration, here are a few of the ways I&#x2019;ve applied this in practice:</p><ul><li>Gratitude:</li><ul><li>Gratitude and appreciation are <em>awesome</em>. Gratitude journals are pretty clearly shown to systematically increase happiness. By dwelling on what I value about my friends, I feel happier, and better appreciate great things about my life.</li><li>Further, <em>hearing </em>appreciation feels awesome! By expressing gratitude to people, I make myself feel better, <em>and </em>make them feel better. Yet people so rarely do this. </li><ul><li>Note: Gratitude =/= flattery. It&#x2019;s <em>really </em>important that it&#x2019;s sincere, not performative</li></ul><li>Techniques that have increased the amount of gratitude I feel and express:</li><ul><li>Practice <a href=""https://www.lesswrong.com/posts/GLPaZamxqkx7XJbXv/the-skill-of-noticing-emotions"">Noticing</a> appreciation. And then complimenting somebody in the moment whenever I notice myself feeling positively towards them</li><ul><li>This is great - it means I&#x2019;m notably more pleasant to be around (based on social feedback), and makes me notice positive feelings much more</li></ul><li>A stage in my weekly review: Go through all the interactions I had this week, and every time I notice a feeling of excitement or &#x201C;I&#x2019;m really glad that happened&#x201D;, send that person a message thanking them, and explaining what I valued about it</li><li>Buying a box of 40 Christmas cards, making a list of my 40 closest friends, and writing them a card about what they mean to me, what I respect about them and how they&#x2019;ve made my life better.</li><ul><li>I think we rarely do things like this - longterm reflection on why we care about people, because there&#x2019;s no real social convention that creates an obvious time to do it. And this is super dumb! When things are awesome win-wins, you should make your own social conventions, rather than avoiding them because they&#x2019;re a bit weird, and there&#x2019;s no obvious time to do it.</li></ul></ul><li>I think there&#x2019;s also a skill of giving <em>good </em>compliments - the main thing to optimise for is signalling sincerity rather than ulterior motives</li><ul><li>Be as specific as possible - if the other person is a bit insecure, it&#x2019;s easy to deny a vague compliment, much harder </li><li>Make it clear that you don&#x2019;t want anything from them, and don&#x2019;t put them in an uncomfortable situation</li><ul><li>I find it useful to have a next action queued up whenever I compliment somebody - it&#x2019;s awkward figuring out how to react gracefully, and this removes that part from them</li><li>I like to give compliments eg at the end of an interaction, or in passing, and then leave shortly afterwards. Makes it clearer that it was for the sake of giving a compliment</li></ul><li>Try to compliment things you think they&#x2019;d value. Things people are underconfident about, and things they clearly put effort into are good sources.</li><ul><li>Remember - the goal is to make <em>them </em>feel good, not to make yourself feel good. That&#x2019;s just a convenient side-effect</li></ul></ul><li>While I&#x2019;m on the topic, if you want an easy way to practice niceness, I find compliments extremely satisfying and motivating ;) And I try to ensure that there&#x2019;s 0 downside risk to giving me compliments!</li><ul><li>Especially specific compliments: about specific ideas that were insightful or useful in posts, and any specific ways these have changed how you thought or acted!</li></ul></ul><li>Teaching</li><ul><li>I care a lot about learning and understanding complex ideas, and converting tacit knowledge into clear and precise concepts</li><li>One of the most successful ways to do this is by explaining it to other people!</li><ul><li>This forces me to put things into words</li><li>This highlights the parts I don&#x2019;t understand</li><li>Ideally, the student can ask insightful questions and help clarify my understanding</li><li>By putting complex details into a form I can convey, I <em>have </em>to extract out the most important parts, because it&#x2019;s super annoying to just dictate course notes at somebody</li></ul><li>This is <em>also </em>valuable, because this trains my skill of good communication and explanation - I&#x2019;ve gotten dramatically better at this over time, and I currently consider it one of my key employable skills</li><li>This can be made actionable: If I&#x2019;m learning something new, I find someone who&#x2019;d be interested in the ideas, and arrange to teach it to them</li><ul><li>Eg, a great way to revise a course is to teach it from scratch to a friend</li><li>This feels a bit weird to suggest, but people respond really well!</li><li>This even works with a peer doing the same courses as you - you each focus on different halves of a course, or two different courses, and teach your half to the other</li></ul></ul><li>Publishing resources</li><ul><li>I am a very <a href=""https://www.youtube.com/channel/UCzoIa8e4vhfRIxL_Kj6yy0Q?view_as=subscriber"">big</a> <a href=""https://dynalist.io/d/ToOhEKlz9qC2PmpjgyTpXQJz"">fan</a> of <a href=""https://ankiweb.net/shared/info/65202941"">publishing</a> <a href=""https://dynalist.io/d/f_41Ziut44JPFmTIVlHOc-3n"">resources</a> that I&#x2019;ve made</li><ul><li>Putting things online is <em>amazing </em> - my talks each took on the order of 15 hours to write and plan, and total watch time is on the order of 10 times that. There&#x2019;s amazing leverage</li></ul><li>Given that I&#x2019;ve already made the resource, this is basically a free win - others can benefit, I can get feedback, I feel happy that I&#x2019;m helping people</li><ul><li>It is <em>way </em>more satisfying to have made a set of notes that I think is genuinely good quality and something others value, than it is to just have a random PDF sitting on my hard drive that I&#x2019;ll never look at again</li></ul><li>Further - <em>knowing </em>that I&#x2019;m going to, say, publish my notes holds me to a higher standard. It feels like I&#x2019;m teaching the ideas to somebody else, I notice holes more, and I feel more motivated to find clearer explanations</li><ul><li>At the cost of taking more time and effort!</li></ul></ul><li>Organising events</li><ul><li>Committing to an event, like giving a talk, is an <em>amazing </em>motivator. I feel beholden to make it to a good standard, and this makes me a lot more focused and creative.</li><ul><li>And, by making the event as awesome as possible, I get a lot of satisfaction out of making it exactly to my standards of what a good event should be - the feeling of autonomy.</li></ul><li>I personally am pretty extroverted and get joy out of feeling like the centre of attention - organising events is an excellent way to satisfy this in a way that also adds value to others</li><li>On this note - I&#x2019;ll be giving a remote talk on Machine Learning intuitions at 3:30pm GMT+1 on Friday 3rd July - all welcome!</li></ul><li>Social initiative</li><ul><li>I really value my friends, and especially spending quality one-on-one time together. </li><li>But it&#x2019;s easy for this to just not happen, when there&#x2019;s nothing there to prompt spontaneity, or to prompt me to organise something. And so there are a lot of people in my life who I value, but I never get round to speaking to - it never feels <em>urgent</em>. Eg people who live in other countries, and who I don&#x2019;t run into by chance.</li><ul><li>This is especially holds during social distancing! <em>Everyone </em>is distant.</li></ul><li>The high-level point here, is that taking the social initiative is a form of emotional labour. It has benefits to both of you, but it&#x2019;s <em>hard</em>, and it takes organisation and effort. </li><ul><li>Fortunately, as with most hard things, this can be systematised!</li><ul><li>Underlying point: The goal of niceness isn&#x2019;t to be virtuous inside my head, it&#x2019;s to make <em>other </em>people&#x2019;s lives better. If I can achieve this without trying as hard, that&#x2019;s <em>amazing</em>.</li></ul><li>So I currently have a spreadsheet tracking all the people I value, and who I know enjoy spending time with me, and with reminders to regularly reach out to catch up. I&#x2019;ve made it a habit to regularly check this spreadsheet and reach out, and I use <a href=""https://calendly.com/"">calendly.com</a> to take care of all of the scheduling with no mental effort from me.</li><li>This is a great win-win - I incur the emotional labour on myself of taking the social initiative, but by systematising it, it doesn&#x2019;t <em>actually </em>take that much effort!</li></ul><li>This applies similarly with meeting new people - it&#x2019;s easy to meet somebody cool and then never remain in touch. And reaching out and suggesting meeting again is emotional labour. But friendships are a <em>major </em>mutually beneficial trade.</li><ul><li>Well over half of my current strong friendships wouldn&#x2019;t have happened if I didn&#x2019;t make an effort to reconnect with people I met once and liked.</li><li>There&#x2019;s <em>much </em>higher upside than downside with somebody new - a strong friendship can add value for the rest of our lives, an annoying message or mediocre meeting has a small, one-off cost. But my intuitions are very, very bad at realising this.</li></ul><li>This applies all the more so to organising social events - I quite enjoy hosting low-effort parties, where I just invite a range of friends to my room on one evening, with no further planning required. This is pretty relaxed for me and creates a pleasant evening, and provides an event</li><ul><li>Alas, this is much harder during social distancing, though I <em>am </em>a big fan of <a href=""https://gather.town/"">gather.town</a></li></ul><li>Caveat: This one comes with more downside risk than most of my recommendations, and it&#x2019;s important to be aware of this. I think the upside <em>obviously </em>outweighs this, but it&#x2019;s good to minimise downside risk.</li><ul><li>Give people outs, and make it clear that saying no, or ignoring messages is fine - I find it useful to send people a <a href=""https://calendly.com/"">calendly.com</a> link, because that leaves all of the agency with them.</li><li>Judging how much other people like me, and trying to only take the initiative with people where things feel mutual.</li></ul><li>Caveat: I am a <em>big </em>fan of systems, and spreadsheets, but this is clearly not for everyone. I hope the high-level point stands, beyond the specific details of how I implement these ideas.</li></ul><li>Recommendations</li><ul><li>When I learn an interesting idea, or read an article, it takes 0 effort to think through friends who might enjoy it, and pass it on</li><ul><li>In general - filtering for good content is hard, but I know my friends well, and can guess what they might enjoy</li><li>Even if I&#x2019;m not <em>sure </em>they&#x2019;d like it, it&#x2019;s useful to pass things on - this helps me build better models of friends, and recommend better things in future!</li><li>This benefits me - I can hear more thoughts and perspectives on interesting ideas!</li><li>And this sets a norm that invites reciprocation!</li></ul><li>This applies all the more so to bigger things - jobs worth applying to, other people they should talk to</li><ul><li>There&#x2019;s <em>amazing </em>upside risk of introducing somebody to somebody else, and <em>incredibly </em>low effort - I think this is plausibly some of the highest impact things I&#x2019;ll ever do for improving my friends&#x2019; lives</li></ul><li>In practice, I have a mental reflex where every time I see something interesting, I ask &#x201C;who do I know who might enjoy/gain value from this?&#x201D;</li><li>I find this hard to implement - I&#x2019;m very conscious of bothering others. A useful hack: Mentally frame it as offering them an opportunity, which they are free to take or leave. Receiving opportunities has (essentially) 0 downside.</li></ul><li>Overcoming the bystander effect</li><ul><li><a href=""https://www.goodtherapy.org/blog/psychpedia/bystander-apathy#:~:text=Bystander%20apathy%20is%20a%20symptom,help%20or%20contact%20the%20police."">Bystander apathy</a> is a really common and insiduous effect - there is something that everyone wants to <em>happen </em>but nobody wants to be the one to <em>do </em>it. </li><li>Often this happens to such a degree that the benefit <em>just to me</em> is enough to justify the effort.</li><li>Related to the idea of <a href=""https://www.neelnanda.io/blog/become-a-person-who-actually-does-things"">Actually Doing Things</a>, I have found it useful to develop the reflex of noticing bystander apathy in my environment, and actively doing the thing. And this happens <em>all of the time</em>.</li><ul><li>Eg, ask a question when there&#x2019;s a confusing point in a talk</li><li>Eg, give somebody the bit of uncomfortable but vital feedback</li><li>Eg, notice tiny tragedies of the commons, like an empty jug of water that nobody wants to refill, and just do it.</li><li>Eg, notice when everyone feels uncomfortable being the first to, say, dance at a party, and just do it.</li></ul></ul></ul><p>The theme of upside vs downside risk has kept recurring - this is a very important thing to bear in mind when trying to improve other people&#x2019;s lives. Your goal is not to do what <em>you </em>think is best, it&#x2019;s to help <em>others</em>. This includes respecting their preferences, and respecting their autonomy. It&#x2019;s key that you listen to feedback, be open to the possibility that your actions are systematically unhelpful, and work to build better models of your friends and their preferences. In an ideal world I&#x2019;d only take the actions that are net good, and avoid all of the ones that are net bad, but in a limited information world this is impossible. And empirically, actually trying <em>far </em>outweighs not trying at all. But you still want to get as net good as possible!</p><p>A final point: I think niceness often emerges from your self-image. It&#x2019;s easy to say &#x201C;I&#x2019;m not the <em>kind of person </em>who&#x2019;s nice to other people - it feels weak and sappy&#x201D;. And if your self-image holds you back from win-win situations, this is dumb and should be changed. My most effective path to this has been to get <em>excited </em>about niceness, and to make it a habit. Finding as many ways to shape my life around it has made me more sensitive to opportunities for niceness, and easier to get over the resistance and to take action. It&#x2019;s easy to agonise about </p><p>So, if any of those ideas resonated with you, but you feel some resistance - it doesn&#x2019;t feel perfect, there is some way this could go wrong, it feels a bit weird, etc - don&#x2019;t ask yourself &#x201C;is this specific action a good idea&#x201D;. Ask yourself &#x201C;will taking this action bring me closer to the kind of person I want to be&#x201D;</p><p>And if you need an extra incentive, a very accessible nice action would be telling me about anything you&#x2019;ve done as a result of this post!</p>",neel-nanda-1,neel-nanda-1,Neel Nanda,
oREgYDhtcv9v7AHBB,[Link] Five Years and One Week of Less Wrong,link-five-years-and-one-week-of-less-wrong,https://www.lesswrong.com/posts/oREgYDhtcv9v7AHBB/link-five-years-and-one-week-of-less-wrong,2020-09-14T16:49:35.082Z,22,11,2,False,False,https://slatestarcodex.com/2014/03/13/five-years-and-one-week-of-less-wrong/,"<p>This is a link post for <a href=""https://slatestarcodex.com/2014/03/13/five-years-and-one-week-of-less-wrong/"">Five Years and One Week of Less Wrong</a>. I was surprised to see that it was never cross-posted to LW in the first place. I wanted it to be here so that I could put it under the new <a href=""https://www.lesswrong.com/tag/intellectual-progress-via-lesswrong"">Intellectual Progress via LessWrong</a> tag.</p><p>Some excerpts:</p><blockquote><p>I wrote a post a while ago called <a href=""https://slatestarcodex.com/2013/04/11/read-history-of-philosophy-backwards/"">Read History Of Philosophy Backwards</a>. I theorized that as old ways of thinking got replaced by newer ways, eventually people forgot the old ways even existed or were even coherent positions people could hold. So instead of reading Hobbes to tell you that people can form governments for their common advantage – which you already know – read him to tell you that there was a time when no one believed this was true and governments were natural structures ordained by God.</p><p>It makes sense that over five hundred years, with births and deaths and so on, people would forget they ever held strange and incomprehensible positions. It’s more surprising that it would happen within the course of a single person’s philosophical development. But this is what I keep hearing from people in the Less Wrong community.</p><p>“I re-read the Sequences”, they tell me, “and everything in them seems so obvious. But I have this intense memory of considering them revelatory at the time.”</p><p>This is my memory as well.</p></blockquote><p>&nbsp;</p><blockquote><p>So I thought it would be an interesting project, suitable for the lofty milestone of five years plus one week, to go back and try to figure out how far we have progressed without noticing that we were progressing.</p></blockquote><p>&nbsp;</p><blockquote><p>It was around the switch to Less Wrong that someone first brought up the word “akrasia” (I think it was me, but I’m not sure). I remember there being a time when <a href=""http://lesswrong.com/lw/1l/the_mystery_of_the_haunted_rationalist/"">I was very confused and scandalized</a> by the idea that people might engage in actions other than those rationally entailed by their beliefs. This seems <i>really</i> silly now, but at the time I remember the response was mostly positive and people upvoted me a lot and said things like “Huh, yeah, I guess people might engage in actions other than those rationally entailed by their beliefs! Weird! We should worry about this more!” For a while, we were <i>really</i> confused about this, and a really popular solution (WHICH I ALWAYS HATED) was to try to imagine the mind as being made up of multiple agents trying to strike a bargain. Like, your conscious mind was an agent, your unconscious mind was an agent, your sex drive was an agent, and so on. Ciphergoth was the first person to help us get out of this by bringing up <a href=""http://lesswrong.com/lw/6c/akrasia_hyperbolic_discounting_and_picoeconomics/"">hyperbolic discounting</a> (there was a time Less Wrong didn’t know about hyperbolic discounting!)</p></blockquote><p>&nbsp;</p><blockquote><p>It wasn’t until well into the Less Wrong era that our community started to become aware of the problems with the scientific process. This wasn’t because we were behind the times but because the field was quite new; Ioannides didn’t publish his landmark paper until 2005, and it languished in specialized circles until the Atlantic picked it up in 2010. But as early as December 2009, Allan Crossman working off a comment of Eliezer’s wrote <a href=""http://lesswrong.com/lw/1ib/parapsychology_the_control_group_for_science/"">Parapsychology: The Control Group For Science</a>.</p></blockquote><p>&nbsp;</p><blockquote><p>It continues to puzzle me that there was a time when I didn’t know what a Schelling point was. I imagine myself just sort of wandering through life, not having any idea what was going on or why.</p></blockquote><p>&nbsp;</p><blockquote><p>I’ll end with something that recently encouraged me a lot. Sometimes I talk to Will Newsome, or Steve Rayhawk, or Jennifer RM, or people like that in the general category of “we all know they are very smart but they have no ability to communicate their insights to others”. They say inscrutable things, and I nod and pretend to understand because it’s less painful than asking them to explain and sitting through an equally inscrutable explanation. And recently, the things that Will and Steve and Jennifer were saying a couple of years ago have started making perfect sense to me. The things they’re saying <i>now</i> still sound like nonsense, but now I can be optimistic that in a few years I’ll pick up those too.</p></blockquote>",abramdemski,abramdemski,abramdemski,
99SGEB4rPbcA5PuMv,Free Money at PredictIt: 2020 General Election,free-money-at-predictit-2020-general-election,https://www.lesswrong.com/posts/99SGEB4rPbcA5PuMv/free-money-at-predictit-2020-general-election,2020-09-14T14:40:01.941Z,18,10,3,False,False,,"<p>Previously: <a href=""https://thezvi.wordpress.com/2019/09/26/free-money-at-predictit/"">Free Money at PredictIt?</a></p>



<p>It’s time for another look at PredictIt. Is there free money? What are our best options for free money? </p>



<p>The short answer is that there is free money if and only if you have available capital at PredictIt. </p>



<p>There is no free money if your plan is to deposit to make the wager and then withdraw. That hits you with a 5% withdrawal fee, wiping out your profits. </p>



<p>Let’s look at the major markets first, then scour for minor ones.</p>



<p>As with the last such post, despite the fact that we can’t discuss these prices without discussing the potential for a stolen election, let’s be clear: No advocacy for or against any candidate or party in the comments. Any such comments will be deleted reign-of-terror style. That’s not what this is about.</p>



<p>General Election </p>



<p>Prices are where you would <em>sell </em>if you traded right away by hitting the bid.</p>



<p>Joe Biden 58</p>



<p><br />Donald Trump 44</p>



<p>Kamala Harris 3</p>



<p>Hillary Clinton 2</p>



<p>Mike Pence 1</p>



<p>That adds up to 108. You pay 10% on winnings. If you take the relative prices here at face value, you’d pay roughly 5.9 cents in fees, leaving a profit of 2.1 cents without need to tie up capital. </p>



<span></span>



<p>You also get a freeroll to win all bets if none of those five win. That’s probably under a 1% shot but every little bit helps. It’s good to win the weird outcomes, and win them big. Note that many people are betting <em>on </em>candidates, including at other sites, and paying &gt;100% combined for Biden and Trump. Not only does the house always win if they choose to balance their books, the house can sweep.</p>



<p>The labor and cognitive costs of doing this arb if you’re not already set up aren’t worth it as such on their own, but it’s good to note this is there. If you derive satisfaction from taking free money, I approve.</p>



<p>I’m not taking this because I already took it during the primary, and thus can’t take it again. </p>



<p>What do I think of the baseline claim here, that considered as a two-way race Biden is 57% to win, or <a href=""https://www.betfair.com/sport/politics/2020-us-presidential-election/10393583"">BetFair’s</a> 54% for that same question? Note that the secondary candidates favor the Democrats in both cases, so the two-way races are more like 59% and 55% respectively for the Democratic side.</p>



<p>That depends on what exactly you mean by ‘win the election.’ The rules say ‘the winner of the presidential election’ but that really doesn’t clear it up this year. Whereas the BetFair rules say ‘next president.’ Which if we interpret literally makes Mike Pence at 175:1 a <em>screaming buy! </em>If Trump dies in office or resigns or otherwise leaves before ending his term, then Pence is the ‘next president’ without winning the election, and that’s definitely a &gt;1% chance. It also makes it even better to sell the Democratic candidates, since they can win the election and lose the wager. Always read the rules carefully!</p>



<p>The real question for PredictIt is what happens in a disputed election where both Trump and Biden claim victory. Is it who ends up serving the term as president? Is it something else? I’d want to know.</p>



<p>The only way the odds here make sense is putting a substantial chance on an outright<em> stolen or fraudulent election. </em>That’s not electoral college versus popular vote split. That’s not ordinary standard vote suppression. Nor is vigorous litigation of close elections enough either. This would need to be Stalin-level asking of who counts the votes. Hacking voting machines, destroying or ignoring uncounted ballots, declaring victory in spite of the vote count and sending your own electors and stuff like that. </p>



<p>It’s hard to properly price that. Without it, the 538 model’s current 76% for Biden doesn’t sound unreasonable to me. Is there more or less than a one in six chance of Trump successfully outright stealing the election? </p>



<p>They don’t offer a market on that one. It’s the missing market, the most interesting one of all in many ways. I am a skeptic that the chances are anywhere near that high. Don’t get me wrong. The chances are still <em>way way way </em>too high, especially since there’s also the scenarios where he tries and fails and that’s no picnic either. I’m taking these scenarios seriously enough that I’m staying in Warwick with fully stocked up emergency supplies on election night, and only after a concession planning my return to New York City. </p>



<p>It makes sense to take the arbitrage here rather than back a side. If you want to back a side, you can do so elsewhere at better prices.</p>



<p>Compare to the <a href=""https://www.predictit.org/markets/detail/2721/Which-party-will-win-the-2020-US-presidential-election"">pure two-way market</a>, which is 61-43, for the same implied price of 59% for democrats, but without an opportunity for Free Money. This is another way of illustrating that the true free money here is on the secondary candidates, especially Clinton. Selling Biden and Trump is only a way to then free up capital.</p>



<h3><a href=""https://www.predictit.org/markets/detail/6663/What-will-be-the-popular-vote-margin-in-the-2020-presidential-election"">Presidential Popular Vote Margin of Victory</a></h3>



<p>Again, prices are where you can <em>sell </em>these on demand.</p>



<p>Dem 10.5%+ 15</p>



<p>Dem 9-10.5% 10</p>



<p>Dem 7.5-9% 12</p>



<p>Dem 6-7.5% 12</p>



<p>Dem 4.5-6% 12</p>



<p>Dem 3-4.5% 11</p>



<p>Dem 1.5-3% 9</p>



<p>Dem 0-1.5% 7</p>



<p>GOP 0-1.5% 7</p>



<p>GOP 1.5-3% 4</p>



<p>GOP 3-4.5% 3</p>



<p>GOP 4.5-6% 2</p>



<p>GOP 6-7.5% 2</p>



<p>GOP 7.5-9% 1</p>



<p>GOP 9-10.5% 1</p>



<p>GOP 10.5%+ 3</p>



<p>That adds up to 111, so you can definitely take some free money if you’re interested, or you can try to be selective. Note that this curve centers nicely around Biden by about 6.5%. </p>



<p>This roughly agrees with Trump’s 20% in <a href=""https://www.predictit.org/markets/detail/5554/Will-Donald-Trump-win-the-popular-vote-in-2020"">2020 Predictions | Will Trump win the popular vote in 2020?</a></p>



<p>If you look at the distribution, Democrats by 10.5%+ seems cheap. Following the slope on the other side, it’s clear this should be priced much higher. Presumably people think this is because there’s no way, there’s too much partisanship. I can’t agree. The bottom could easily fall out in any number of ways. That doesn’t mean that 15% is cheap, but I’m skeptical that this general level of variance and this median are right, and there’s only a 15% chance of a blowout. 538 agrees, and sees a 30% chance that Biden wins by 10 or more, whereas here you can get 9%+ for only 25%. </p>



<p>Note also that a true Biden blowout isn’t that likely to be ‘brought back’ by fraud. If Biden wins by this much, the election can’t be stolen, so doing brazen things to reduce the apparent margin also seems not worthwhile. </p>



<p>The GOP by 10.5%+ also sticks out. Why is that so big? To me this one makes relative sense, again on the outright fraud principle. There are two ways to steal an election. You can pull a (alleged) 1960, and not steal one more vote than you have to. Or you can pull a Stalin or Hussein, and claim a huge landslide not caring that no one believes you. I wouldn’t go out of my way to sell that possibility. If anything, the GOP by 6-9% seems decidedly less likely than 10.5%+, to me. Something very strange would have to happen to get things to move that much, so at that point, the broader range is a lot more attractive to me.</p>



<p>Thus, given how crazy this market could get later, and given I already tied up my funds, I’m going to take the arbitrage here, at least not yet. I might take it later, but for now I want to reserve the right to make a better play. </p>



<p>Next question is, what does this market really say about Biden’s chances?</p>



<p>In a fair election, 538 thinks Biden has an 11% chance of winning the popular vote but losing the electoral college. If Biden was worse off generically, that number would only go slightly higher, say 13%. However, if the GOP decides to steal only in selected states, and only a 1960-style amount, or do things like halt counting before mail ballots are counted, that number could go much higher.  </p>



<p>This chart has a 55% chance (after normalization) for Biden to win by 4.5% or more, and an 18% chance of winning by 1.5% to 4.5%. I think it’s reasonable to say he wins almost all the times in the first bucket, and about half the time in the second bucket, so it’s implying 64% or so chance of victory. That’s substantially different from the presidential market, so either the two have diverged, or a bunch of probability mass is in ‘Trump brazenly steals only the tipping point states,’ or the evaluation here by the Federal Election Commission could go to Biden while Trump ‘wins the election’ anyway through theft. </p>



<p>There are a lot of ways to play this.</p>



<h3><a href=""https://www.predictit.org/markets/detail/6653/What-will-be-the-Electoral-College-margin-in-the-2020-presidential-election"">Electoral College Margin of Victory</a></h3>



<p>GOP 280+ 4</p>



<p>GOP 210-279 3</p>



<p>GOP 150-209 5</p>



<p>GOP 100-149 9</p>



<p>GOP 60-99 10</p>



<p>GOP 30-59 7</p>



<p>GOP 10-29 5</p>



<p>GOP 0-9 4</p>



<p>Dems 1-9 2</p>



<p>Dems 10-29 4</p>



<p>Dems 30-59 6</p>



<p>Dems 60-99 9</p>



<p>Dems 100-149 16</p>



<p>Dems 150-209 12</p>



<p>Dems 210-279 9</p>



<p>Dems 280+ 7</p>



<p>That adds to 110, which is again an opportunity for free money without tying up capital. It’s also, again, another chance to get money down on a side while expressing an additional opinion. The reason not to take this is in case you want to save it for later.</p>



<p>The Democratic wins add to 63, the Republican wins to 47. Thus, this predicts a 57-43 distribution, slightly better for the Republican side. </p>



<p>The 538 model’s best prediction is Democrats by 122, which is right in the middle of the highest probability group, but not at the median of the market distribution.</p>



<p>Note that these groupings are not the same size. If we normalize for that, we see a very broad distribution of outcomes all seeming similarly likely. </p>



<h3><a href=""https://www.predictit.org/markets/detail/6871/When-will-the-presidential-election-outcome-be-called"">Timing of the Election Call</a></h3>



<p>Hats off to PredictIt for the definition. If CNN and Fox News both call the election for the same person, that’s a pretty good proxy for it being over. That doesn’t mean Trump might not dispute it anyway (or even Biden) but it’s as good a definition as was available.</p>



<p>As always, I only include prices you can <em>sell </em>at. Assume it costs one extra to buy instead of sell.</p>



<p>November 3 21</p>



<p>November 4 31</p>



<p>November 5 10</p>



<p>November 6-7 7</p>



<p>November 8-9 5</p>



<p>November 10-16 7</p>



<p>November 17-23 5</p>



<p>November 24-30 5</p>



<p>December 1-14 7</p>



<p>After December 14 11</p>



<p>That adds to 109. Again, enjoy your cash.</p>



<p>That is one scary chart. There’s less than a 50% chance that the election will be resolved even a full day after the election. There’s a 10+% chance it won’t be solved over a month later, which likely means it’s going to congress or worse – and again, Trump could well fight on even after Fox News gives up. </p>



<p>I don’t know that much about when various votes are likely to come in from mail ballots, but neither does anyone else. The one seeming ‘error’ here is that November 8-9 is 5% for a 2-day window, then you have a 7% chance for a 7-day window, then the next 7-day windows are 5% each. Unless there’s a specific reason, that’s a very weird distribution. </p>



<p>A good question to ask is, how does this line up with the margin of victory?</p>



<p>If the margin of victory is &gt;6% on either side, I’d assume the networks would be able to make the call on November 3-4. That’s a 55% chance according to that market. And there’s a decent chance they can make the call with a smaller margin. If the GOP is legitimately winning the popular vote outright but less than 6%, that’s arguably a &gt;10% chance, and realistically Biden almost certainly is toasty enough that CNN should give up on November 4, although they plausibly hold out past midnight on election night. </p>



<p>Consider that this market has only a 19% normalized chance (buy costs 22%) that there’s a call on election night, but there’s a higher chance than that for Biden to win by more than 9%. If he does that, I have a hard time believing there’s no call by midnight.</p>



<p>Thus, my gut reaction is that this market is not totally crazy, but it’s somewhat too skeptical of resolving things in the first two days. If anything, I’d think it’s too <em>optimistic </em>then about resolving things quickly or on November 5, provided they’re still in the air on the 4th, but there’s a reasonable argument for ‘once Biden takes the vote lead in the tipping point state it is over, but Fox News won’t call the race until he does take that lead, and it takes a few days to count enough mail in ballots.’ But 10% for that one day, or about 20% of the time given we get that far, does seem like a lot. </p>



<p>Thus, if I was thinking about what to sell and what to keep, my inclination would be to keep the nightmare scenarios and the quick resolutions, and sell the stuff in between. </p>



<h3>What are some other juicy targets?</h3>



<p><a href=""https://www.predictit.org/markets/detail/6798/Will-Joe-Biden-drop-out-before-November-1"">Will Biden drop out by 11/1?</a></p>



<p>You can buy the ‘No’ at 90%. That’s crazy. He’s not going anywhere. </p>



<p>One way to know it’s crazy is that time is going by, and the number isn’t being discounted. When I got into the No at 10% a week ago, I got the same price. In late August, the price was the same. On July 2 it was 9%! </p>



<p>I made a good trade. But there was a <em>much better trade </em>available. Which was to buy Biden dropping out in early July, then sell it now <em>for the same price or higher. </em></p>



<p>Between now and then, Biden has looked relatively healthy, he has established and maintained a solid poling lead, and has had no scandals that could possibly push him towards dropping out. There’s no way his chances of dropping are more than half what they were in early July, let along <em>higher. </em></p>



<p><a href=""https://www.predictit.org/markets/detail/6687/Which-state-will-be-won-by-the-smallest-margin-in-the-presidential-election"">2020 Election Predictions | State with smallest MOV in 2020?</a></p>



<p>I won’t list the odds except to note that they add to 111, and you get some bonus states as a freeroll. Nothing in the relative rankings looks crazy to me. My guess is the value is on selling the states trading high and the ones trading super low, and keeping the ones in the middle, if you don’t want to just take free money.</p>



<p><a href=""https://www.predictit.org/markets/detail/6642/Will-the-winner-of-the-popular-vote-also-win-the-Electoral-College"">Will the winner of the popular vote also win the Electoral College?</a></p>



<p>This market has a whopping 29% chance that the winner of the electoral college didn’t win the popular vote. That would add up to a 49% chance for Trump to win. This seems like the best way to bet on Biden in a two-way. </p>



<p><a href=""https://www.predictit.org/markets/detail/4614/Will-Hillary-Clinton-run-for-president-in-2020"">https://www.predictit.org/markets/detail/4614/Will-Hillary-Clinton-run-for-president-in-2020</a></p>



<p>No. She won’t. Yet somehow they won’t give up thinking she will, and keep not discounting this much for all the time that goes by. I bet the no on this a long, long time ago, early in the primary, and I’m only 1% to the good right now. There’s a lesson in that. You can still buy the No at 95%, and it’s cheap at 99% if you don’t care about tying up capital.</p>



<p><a href=""https://www.predictit.org/markets/detail/6852/Will-Nancy-Pelosi-become-Acting-US-President-on-January-20"">Will Nancy Pelosi become Acting US President on January 20?</a></p>



<p>This is people living out their fantasies. I’m not sure if it’s outright impossible, but I do know this is not an 8% chance. Wow.</p>



<p><a href=""https://www.predictit.org/markets/detail/2901/Will-a-woman-be-elected-US-president-in-2020"">2020 Election Predictions | Will a woman be president?</a></p>



<p>Think Kamala Harris at 3% is overpriced? Here you can sell her at 6%. Technically you also have to sell all other women, but who is second in probability? Jo Jorgensen? </p>



<p><a href=""https://www.predictit.org/markets/detail/4632/Will-Michelle-Obama-run-for-president-in-2020"">Will Michelle Obama run for president in 2020?</a></p>



<p>Get it through your thick skulls, everyone, that Michelle Obama <em>hates politics </em>and will never run. Yes, she would win, so I get why you dream, and yes everyone says they won’t run until they do, but no she won’t ever run. You can only get 3% on this at this point, so probably not worth the capital. Again, people dreaming. Still, it’s more plausible than Hillary Clinton.</p>



<p><a href=""https://www.predictit.org/markets/detail/6799/Will-Donald-Trump-drop-out-before-November-1"">2020 Election Predictions | Will Donald Trump drop out before November 1?</a></p>



<p>It’s 4%, same as a few weeks ago. Not budging. Not worth touching, but another dumb line.</p>



<p><a href=""https://www.predictit.org/markets/detail/3041/Will-John-Kasich-run-for-president-in-2020"">2020 Election Predictions | Will John Kasich run in 2020?</a></p>



<p>Still can sell 1% if you’d like! Let no one say interest on your money is dead. Same with Paul Ryan.</p>



<p><a href=""https://www.predictit.org/markets/detail/5989/Will-the-2020-IA-Democratic-caucus-winner-win-the-presidency"">2020 Election Predictions | 2020 Iowa Winner Elected President</a></p>



<p>Another 1% still available, because people want to free up capital and/or forget that this only pays out if the <em>democratic </em>winner wins, so it’s actually 0% to be Yes.</p>



<p>There are also a bunch of states available. I’m not going to go into them all, but there are some good overpriced long shots to sell if you’d like.</p>



<p><a href=""https://www.predictit.org/markets/detail/5914/Will-the-Senate-convict-Donald-Trump-on-impeachment-in-his-first-term"">Impeachment Predictions | Will the Senate convict Donald Trump in his first term?</a></p>



<p>4% is still available. That’s on top of 11% for him to resign  in his first term. Much better than his 13% to not complete his first term, since that’s only two of the ways he can leave office. </p>



<p>There are a lot of Senate races and House races. I’m not up on the situations enough to know the right stuff, and I want to get this out right after writing so the prices don’t change. </p>



<p>Similarly, I see a bunch of value in the US Government section and the World section, but no free money. </p>



<p>Adding it all up, it looks like there are a decent number of full-arbitrage markets, plus a large number of ways to earn decent capital returns by betting on a No. The issue, as always, is that either you withdraw the money after the election, or you have to keep it there. I’ve been rolling it, leaving the money on the site, but I only have a few thousand. That leaves me short of being able to sell everything I want to sell at a moment like this. But it was never about actually maximizing every dollar as such. It’s about using the whole thing as a learning exercise. </p>



<p>Have fun, everyone, but don’t take these market prices <em>too </em>seriously. </p>",Zvi,zvi,Zvi,
sacs7NZfD2frMS82H,What happens if you drink acetone?,what-happens-if-you-drink-acetone,https://www.lesswrong.com/posts/sacs7NZfD2frMS82H/what-happens-if-you-drink-acetone,2020-09-14T14:22:41.417Z,82,59,13,False,False,https://dyno-might.github.io/2020/09/14/what-happens-if-you-drink-acetone/,"<p><strong>Question</strong>: Should you drink acetone?</p>
<p><strong>Answer</strong>: No.</p>
<p>But, out of interest, what if you did?</p>
",dynomight,dynomight,dynomight,
diruo47z32eprenTg,My computational framework for the brain,my-computational-framework-for-the-brain,https://www.lesswrong.com/posts/diruo47z32eprenTg/my-computational-framework-for-the-brain,2020-09-14T14:19:21.974Z,157,70,26,False,False,,"<p><i>(See comment </i><a href=""https://www.lesswrong.com/posts/diruo47z32eprenTg/my-computational-framework-for-the-brain?commentId=c6qzzKWAEfTQyoK7F#c6qzzKWAEfTQyoK7F""><i>here</i></a><i> for some updates and corrections and retractions. —Steve, 2022)</i></p><p>By now I've written a bunch of blog posts on brain architecture and algorithms, not in any particular order and generally interspersed with long digressions into Artificial General Intelligence. Here I want to summarize my key ideas in one place, to create a slightly better entry point, and something I can refer back to in certain future posts that I'm planning. If you've read every single one of my previous posts (hi mom!), there's not much new here.</p><p>In this post, I'm trying to paint a picture. I'm not really trying to justify it, let alone prove it. The justification ultimately has to be: All the pieces are biologically, computationally, and evolutionarily plausible, and the pieces work together to explain absolutely everything known about human psychology and neuroscience. (I believe it! Try me!) Needless to say, I could be wrong in both the big picture and the details (or missing big things). If so, writing this out will hopefully make my wrongness easier to discover!</p><p>Pretty much everything I say here <i>and its opposite</i> can be found in the cognitive neuroscience literature. (It's a controversial field!) I make no pretense to originality (with one exception noted below), but can't be bothered to put in actual references. My previous posts have a <i>bit</i> more background, or just ask me if you're interested. :-P</p><p>So let's start in on the 7 guiding principles for how I think about the brain:</p><h2>1. Two subsystems: ""Neocortex"" and ""Subcortex""</h2><p>(Update: I have a revised discussion of this topic at my later post <a href=""https://www.alignmentforum.org/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"">Two Subsystems: Learning and Steering</a>.)</p><p>This is the starting point. I think it's absolutely critical. The brain consists of two subsystems. The <strong>neocortex </strong>is the home of ""human intelligence"" as we would recognize it—our beliefs, goals, ability to plan and learn and understand, every aspect of our conscious awareness, etc. etc. (All mammals have a neocortex; birds and lizards have an homologous and functionally-equivalent structure called the ""pallium"".) Some other parts of the brain (hippocampus, parts of the thalamus &amp; basal ganglia &amp; cerebellum—see further discussion <a href=""https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine"">here</a>) help the neocortex do its calculations, and I lump them into the ""neocortex subsystem"". I'll use the term <strong>subcortex</strong> for the rest of the brain (brainstem, hypothalamus, etc.).</p><ul><li><i><u>Aside: Is this the triune brain theory?</u></i> No. <a href=""https://en.wikipedia.org/wiki/Triune_brain"">Triune brain theory</a> is, from what I gather, a collection of ideas about brain evolution and function, most of which are wrong. One aspect of triune brain theory is putting a lot of emphasis on the distinction between neocortical calculations and subcortical calculations. I like that part. I'm keeping that part, and I'm improving it by expanding the neocortex club to also include the thalamus, hippocampus, lizard pallium, etc., and then I'm ignoring everything else about triune brain theory.</li></ul><h2>2. Cortical uniformity</h2><p>I claim that the neocortex is, to a first approximation, <a href=""https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in#2_5_3__Uniformity__evidence"">architecturally uniform</a>, i.e. all parts of it are running the same generic learning algorithm in a massively-parallelized way.</p><p><strong>The two caveats to cortical uniformity </strong>(spelled out in more detail at <a href=""https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in#2_5_3__Uniformity__evidence"">that link</a>) are:</p><ul><li>There are sorta ""hyperparameters"" on the generic learning algorithm which are set differently in different parts of the neocortex—for example, different regions have different densities of each neuron type, different thresholds for making new connections (which also depend on age), etc. This is not at all surprising; all learning algorithms inevitably have tradeoffs whose optimal settings depend on the domain that they're learning <a href=""https://www.lesswrong.com/posts/Dyt2TDdMGHDkXPcpp/the-no-free-lunch-theorem-for-dummies"">(no free lunch).</a><ul><li>As one of many examples of how even ""generic"" learning algorithms benefit from domain-specific hyperparameters, if you've seen a pattern ""A then B then C"" recur 10 times in a row, you will start unconsciously expecting AB to be followed by C. But ""should"" you expect AB to be followed by C after seeing ABC only 2 times? Or what if you've seen the pattern ABC recur 72 times in a row, but then saw AB(not C) twice? What ""should"" a learning algorithm expect in those cases? The answer depends on the domain—how regular vs random are the environmental patterns you're learning? How stable are they over time? The answer is presumably different for low-level visual patterns vs motor control patterns etc.</li></ul></li><li>There is a gross wiring diagram hardcoded in the genome—i.e., set of connections between different neocortical regions and each other, and other parts of the brain. These connections later get refined and edited during learning. These make the learning process faster and more reliable by bringing together information streams with learnable relationships—for example the wiring diagram seeds strong connections between toe-related motor output areas and toe-related proprioceptive (body position sense) input areas. We <i>can</i> learn relations between information streams without any help from the innate wiring diagram, by routing information around the cortex in more convoluted ways—see the Ian Waterman example <a href=""https://www.lesswrong.com/posts/isDCEmYHsosyAjLRK/predictive-coding-and-motor-control"">here</a>—but it's slower, more limited, and may consume conscious attention. Related to this is a diversity of training signals: for example, different parts of the neocortex are trained to predict different signals, and also different parts of the neocortex get <a href=""https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine"">different dopamine training signals</a>—or even <a href=""https://www.lesswrong.com/posts/szeKeZwuQhFxirfBY/is-rl-involved-in-sensory-processing"">none at all</a>.</li></ul><h2>3. Blank-slate neocortex</h2><p>(...But not blank-slate <i>sub</i>cortex! More on that below.)</p><p>(Update: To avoid confusion, I've more recently been calling this concept ""learning-from-scratch""—see discussion in my later post <a href=""https://www.alignmentforum.org/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"">“Learning from Scratch” in the brain</a>.)</p><p>I claim that the neocortex (and the rest of the telencephalon and cerebellum) starts out as a ""blank slate"": Just like an ML model initialized with random weights, the neocortex cannot make any correct predictions or do anything useful until it learns to do so from previous inputs, outputs, and rewards.</p><p>In more neuroscience-y (and maybe less provocative) terms, I could say instead: the neocortex is a memory system. It's a <i>really fancy</i> memory system—it's highly structured to remember particular kinds of patterns and their relationships, and it comes with a sophisticated query language and so on—but at the end of the day, it's still a type of memory. And like any memory system, it is useless to the organism until it gradually accumulates information. (Suggestively, if you go far enough back, the neocortex and hippocampus evolved out of the same ancient substructure (<a href=""https://doi.org/10.3758/s13414-019-01760-1"">ref</a>).)</p><p>(By the way, I am not saying that the neocortex's algorithm is similar to today's ML algorithms. There's more than one blank-slate learning algorithm! It’s an entire field! There are new ones on arxiv every day! See image.)</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/yuzg1uxxtt2evnbqgxul"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/j2refmrthvak3uxekmpy 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/ziat4mhfpsm7ag83ydyh 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/uupi9h59vq9co1cheace 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/ftglyyikl7npecgwdew5 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/qs21xfcyrcwfq52zhsvu 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/rqc0zsn2dlqovt4ztd6c 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/dykhx2egq5sjglnuqoo5 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/rub5oxjfpk0lxbixlhyc 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/hjh1i2pzlcb3pu1hl3gb 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/im7ug55rbqkvs4o62owq 912w""><figcaption>A ""blank slate"" learning algorithm, as I'm using the term, is one that learns information ""from scratch""—an example would be a Machine Learning model that starts with random weights and then proceeds with gradient descent. When you imagine a ""blank slate"" learning algorithm, you should not imagine an empty void that gets filled with data. You should imagine a machine that learns more and better patterns over time, and writes those patterns into a memory bank—and ""blank slate"" just means that the memory bank starts out empty. There are many such machines, and they will learn different patterns and therefore do different things. See next section, and see also the discussion of hyperparameters in the previous section.</figcaption></figure><p>Why do I think that the neocortex starts from a blank slate? Two types of reasons:</p><ul><li><i><u>Details of how I think the neocortical algorithm works:</u></i> This is the main reason for me.<ul><li>For example, as I mentioned <a href=""https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in#2_5_4_Locally_random_pattern_separation"">here</a>, there's <a href=""https://www.frontiersin.org/articles/10.3389/fncom.2017.00111/full"">a theory</a> I like that says that all feedforward signals (I'll define that in the next section)&nbsp;in the neocortex—which includes all signals coming into the neocortex from the outside it, plus many cortex-to-cortex signals—are re-encoded into the data format that the neocortex can best process—i.e. a set of sparse codes, with low overlap, uniform distribution, and some other nice properties—and this re-encoding is done <i>by a pseudorandom process</i>! If that's right, it would seem to categorically rule out anything but a blank-slate starting point.</li><li>More broadly, we <i>know </i>the algorithm can learn new concepts, and new relationships between concepts, without having any of those concepts baked in by evolution—e.g. learning about rocket engine components. So why not consider the possibility that that's <i>all</i> it does, from the very beginning? I can see vaguely how that would work, why that would be biologically plausible and evolutionarily adaptive, and I can't currently see any other way that the algorithm can work.</li></ul></li><li><i><u>Absence of evidence to the contrary:</u></i> I have a post <a href=""https://www.lesswrong.com/posts/NkSpukDkm9pjRdMdB/human-instincts-symbol-grounding-and-the-blank-slate"">Human Instincts, Symbol Grounding, and the Blank-Slate Neocortex</a> where I went through a list of universal human instincts, and didn't see anything inconsistent with a blank-slate neocortex. The subcortex—which is absolutely <i>not</i> a blank slate—plays a big role in most of those; for example, the mouse has a <a href=""https://doi.org/10.1038/s41586-018-0244-6""><u>brainstem bird-detecting circuit wired directly to a brainstem running-away circuit</u></a>. (More on this in a later section.) Likewise I've read about the capabilities of newborn humans and other animals, and still don't see any problem. I accept all challenges; try me!</li></ul><h2>4. What is the neocortical algorithm?</h2><p><strong>4.1. ""Analysis by synthesis"" + ""Planning by probabilistic inference""</strong></p><p>""Analysis by synthesis"" means that the neocortex searches through a space of generative models for a model that predicts its upcoming inputs (both external inputs, like vision, and internal inputs, like proprioception and reward). ""Planning by probabilistic inference"" (term from <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.9135&amp;rep=rep1&amp;type=pdf"">here</a>) means that we treat our own actions as probabilistic variables to be modeled, just like everything else. In other words, the neocortex's output lines (motor outputs, hormone outputs, etc.) are the same type of signal as any generative model prediction, and processed in the same way.</p><p>Here's how those come together. As discussed in <a href=""https://www.lesswrong.com/posts/cfvBm2kBtFTgxBB7s/predictive-coding-rl-sl-bayes-mpc"">Predictive Coding = RL + SL + Bayes + MPC</a>, and shown in this figure below:</p><ul><li>The neocortex <i>favors</i> generative models that have been making correct predictions, and <i>discards</i> generative models that have been making predictions that are contradicted by input data (or by other favored generative models).</li><li><i>And,</i> the neocortex <i>favors</i> generative models which predict larger future reward, and <i>discards</i> generative models that predict smaller (or more negative) future reward.</li></ul><p>This combination allows both good epistemics (ever-better understanding of the world), and good strategy (planning towards goals) in the same algorithm. This combination <i>also</i> has some epistemic and strategic failure modes—e.g. a propensity to wishful thinking—but in a way that seems compatible with human psychology &amp; behavior, which is likewise not perfectly optimal, if you haven't noticed. Again, see the link above for further discussion.</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/yx5l95k5guhqkhupcuj1"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/vby1f3xjtyuzaokej9rn 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/sj4fl3xbdqbob8e9bob6 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/sybncohtff9znshpmq0e 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/awqf1ztnlmsk1rrzxf7e 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/dkpls9a6ebd22zck9qi7 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/c16ymgrvpmn4c0nn6mmk 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/hiwxdqghpo91w2xvit9i 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/bgmocobmhygeoxrwtcmk 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/znybmmnhngw1fypy9f2a 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/oq12bmbspsgvxnlyljj1 1008w""><figcaption>Criteria by which generative models rise to prominence in the neocortex; see <a href=""https://www.lesswrong.com/posts/cfvBm2kBtFTgxBB7s/predictive-coding-rl-sl-bayes-mpc"">Predictive Coding = RL + SL + Bayes + MPC</a> for detailed discussion. Note that <a href=""https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine"">(e) is implemented by a very different mechanism than the other parts</a>.</figcaption></figure><ul><li><i><u>Aside: Is this the same as Predictive Coding / Free-Energy Principle?</u></i> Sorta. I've read a fair amount of ""mainstream"" predictive coding (Karl Friston, Andy Clark, etc.), and there are a few things about it that I like, including the emphasis on generative models predicting upcoming inputs, and the idea of treating neocortical outputs as just another kind of generative model prediction. It also has a lot of other stuff that I disagree with (or don't understand). My account differs from theirs mainly by (1) emphasizing multiple simultaneous generative models that compete &amp; cooperate (cf. <a href=""https://www.amazon.com/Society-Mind-Marvin-Minsky/dp/0671657135"">""society of mind""</a>, <a href=""https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip"">multiagent models of mind</a>, etc.), rather than ""a"" (singular) prior, and (2) restricting discussion to the neocortex subsystem, rather than trying to explain the brain as a whole. In both cases, this may be partly a difference of emphasis &amp; intuitions, rather than fundamental. But I think the core difference is that predictive coding / FEP takes some processes to be foundational principles, whereas I think that those same things do happen, but that they're emergent behaviors that come out of the algorithm under certain conditions. For example, in <a href=""https://www.lesswrong.com/posts/isDCEmYHsosyAjLRK/predictive-coding-and-motor-control"">Predictive Coding &amp; Motor Control</a> I talk about the predictive-coding story that proprioceptive predictions are literally exactly the same as motor outputs. Well, I don't think they're exactly the same. But I <i>do</i> think that proprioceptive predictions and motor outputs are the same in <i>some </i>cases (but not others), in <i>some </i>parts of the neocortex (but not others), and <i>after</i> (but not before) the learning algorithm has been running a while. So I kinda wind up in a similar place as predictive coding, in some respects.</li></ul><p><strong>4.2. Compositional generative models</strong></p><p>Each of the generative models consists of predictions that other generative models are on or off, and/or predictions that input channels (coming from outside the neocortex—vision, hunger, reward, etc.) are on or off. (""It's symbols all the way down."") All the predictions are attached to confidence values, and both the predictions and confidence values are, in general, functions of time (or of other parameters—I'm glossing over some details). The generative models are compositional, because if two of them make disjoint and/or consistent predictions, you can create a new model that simply predicts that both of those two component models are active simultaneously. For example, we can snap together a ""purple"" generative model and a ""jar"" generative model to get a ""purple jar"" generative model. They are also compositional in other ways—for example, you can time-sequence them, by making a generative model that says ""Generative model X happens and then Generative model Y happens"".</p><p><i><u>PGM-type message-passing:</u></i> Among other things, the search process for the best set of simultaneously-active generative model involves something at least vaguely analogous to message-passing (belief propagation) in a probabilistic graphical model. <a href=""https://doi.org/10.1126/science.aag2612"">Dileep George's vision model</a> is a well-fleshed-out example.</p><p><i><u>Hierarchies are part of the story but not everything: </u></i>Hierarchies are a special case of compositional generative models. A generative model for an image of ""85"" makes a strong prediction that there is an ""8"" generative model positioned next to a ""5"" generative model. The ""8"" generative model, in turn, makes strong predictions that certain contours and textures are present in the visual input stream.</p><p>However, not all relations are hierarchical. The ""is-a-bird"" model makes a medium-strength prediction that the ""is-flying"" model is active, <i>and</i> the ""is-flying"" model makes a medium-strength prediction that the ""is-a-bird"" model is active. Neither is hierarchically above the other.</p><p>As another example, the brain has a visual processing hierarchy, but as I understand it, studies show that the brain has loads of connections that don't respect the hierarchy.</p><p><i><u>Feedforward and feedback signals:</u></i> There are two important types of signals in the neocortex.</p><p>A <strong>""feedback""</strong> signal is a generative model prediction, attached to a confidence level, which includes all the following:</p><ul><li>""I predict that neocortical input line #2433 will be active, with probability 0.6"".</li><li>""I predict that generative model #95738 will be active, with probability 0.4"".</li><li>""I predict that neocortical output line #185492 will be active, with probability 0.98""—and this one is a self-fulfilling prophecy, as the feedback signal is also the output line!</li></ul><p>A <strong>""feedforward""</strong> signal is an announcement that a certain signal is, in fact, active right now, which includes all the following:</p><ul><li>""Neocortical input line #2433 is currently active!""</li><li>""Generative model #95738 is currently active!""</li></ul><p>There are about 10× more feedback connections than feedforward connections in the neocortex, I guess for algorithmic reasons I don't currently understand.</p><p>In a hierarchy, the top-down signals are feedback, and the bottom-up signals are feedforward.</p><p>The terminology here is a bit unfortunate. In a motor output hierarchy, we think of information flowing ""forward"" from high-level motion plan to low-level muscle control signals, but that's the feed<i>back</i> direction. The forward/back terminology works better for sensory input hierarchies. Some people say ""top-down"" and ""bottom-up"" instead of ""feedback"" and ""feedforward"" respectively, which is nice and intuitive for both input and output hierarchies. But then <i>that </i>terminology gets confusing when we talk about non-hierarchical connections. Oh well.</p><p>(I'll also note here that ""mainstream"" predictive coding discussions sometimes talk about feedback signals being associated with confidence <i>intervals</i> for analog feedforward signals, rather than confidence <i>levels</i> for binary feedforward signals. I changed it on purpose. I like my version better.)</p><h2>5. The subcortex steers the neocortex towards biologically-adaptive behaviors.</h2><p>The blank-slate neocortex can learn to predict input patterns, but it needs guidance to do biologically adaptive things. <strong>So one of the jobs of the subcortex is to try to </strong><a href=""https://www.lesswrong.com/posts/SJXujr5a2NcoFebr4/mesa-optimizers-vs-steered-optimizers""><strong>""steer""</strong></a><strong> the neocortex, and the subcortex's main tool for this task is its ability to send rewards to the neocortex at the appropriate times</strong>. Everything that humans reliably and adaptively do with their intelligence, from liking food to making friends, depends on the various reward-determining calculations hardwired into the subcortex.</p><h2>6. The neocortex is a black box from the perspective of the subcortex. So steering the neocortex is tricky!</h2><p>Only the neocortex subsystem has an intelligent world-model. Imagine you just lost a big bet, and now you can't pay back your debt to the loan shark. That's bad. The subcortex (hypothalamus &amp; brainstem) needs to send negative rewards to the neocortex. But how can it know? How can the subcortex have any idea what's going on? It has no concept of a ""bet"", or ""debt"", or ""payment"" or ""loan shark"".</p><p>This is a very general problem. I think there are two basic ingredients in the solution.</p><p>Here's a diagram to refer to, based on the one I put in <a href=""https://www.lesswrong.com/posts/DWFx2Cmsvd4uCKkZ4/inner-alignment-in-the-brain"">Inner Alignment in the Brain</a>:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/s1rfeogmbr9y85chifpd"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/bin9ivv83desag5xq29u 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/waebybbb8kwvktouuvmk 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/gh1ejcagoegvp96rsmht 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/edkqemvslywmhaawsupw 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/qwbujdenylk0ilknaylk 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/amstjfdongckecx0yxxc 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/mpm3ugpensnjjwd7pnfc 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/ekgc3yw2ysjpxdynolus 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/lhhfhgofplipqpmagf6x 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/diruo47z32eprenTg/xsvqr3xzcboerexsis5d 1296w""><figcaption>Schematic illustration of some aspects of the relationship between subcortex &amp; neocortex. See also my previous post <a href=""https://www.lesswrong.com/posts/DWFx2Cmsvd4uCKkZ4/inner-alignment-in-the-brain"">Inner Alignment in the Brain</a> for more on this. <strong>(Update June 2021: I would no longer draw the diagram this way, see </strong><a href=""https://www.alignmentforum.org/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation""><strong>here</strong></a><strong>. The biggest difference is: I would not draw a direct line from neocortex to a hormone change (for example); instead the cortex would ask the subcortex (hypothalamus + brainstem) to make that hormone change, and then the subcortex might or might not comply with that recommendation. (I guess the way I drew it here is more like </strong><a><strong>somatic marker hypothesis</strong></a><strong>.)))</strong></figcaption></figure><p><strong>6.1 The subcortex can learn what's going on in the world via its own, parallel, sensory-processing system.</strong></p><p>Thus, for example, we have the well-known visual processing system in our visual cortex, <i>and</i> we have the lesser-known visual processing system in our midbrain (superior colliculus). Ditto for touch, smell, proprioception, nociception, etc.</p><p>While they have similar inputs, these two sensory processing systems could not be more different!! The neocortex fits its inputs into a huge, open-ended predictive world-model, but the subcortex instead has a small and hardwired ""ontology"" consisting of evolutionarily-relevant inputs that it can recognize like faces, human speech sounds, spiders, snakes, looking down from a great height, various tastes and smells, stimuli that call for flinching, stimuli that one should orient towards, etc. etc., and these hardwired recognition circuits are connected to hardwired responses.</p><p>For example, babies learn to recognize faces quickly and reliably in part because the midbrain sensory processing system knows what a face looks like, and when it sees one, it will saccade to it, and thus the neocortex will spend disproportionate time building predictive models of faces.</p><p>...Or better yet, instead of saccading to faces itself, the subcortex can <i>reward the neocortex</i> each time it detects that it is looking at a face! Then the neocortex will go off looking for faces, using its neocortex-superpowers to learn arbitrary patterns of sensory inputs and motor outputs that tend to result in looking at people's faces.&nbsp;</p><p><strong>6.2 The subcortex can see the neocortex's outputs—which include not only prediction but imagination, memory, and empathetic simulations of other people.</strong></p><p><s>For example, if the neocortex never predicts or imagines any reward, then the subcortex can guess that the neocortex has a grim assessment of its prospects for the future—I'll discuss that particular example much more in an upcoming post on depression.</s> <i>(Update: that was wrong; see better discussion of depression </i><a href=""https://www.lesswrong.com/posts/txj4wigyjLNbcoZ9o/valence-series-5-valence-disorders-in-mental-health-and#5_3_If_valence_has_a_strong_negative_bias__i_e___almost_every_thought_is_negative_valence___it_should_lead_to_a_cluster_of_symptoms_suspiciously_close_to_clinical_depression""><i>here</i></a><i>.)</i></p><p>To squeeze more information out of the neocortex, the subcortex can also ""teach"" the neocortex to reveal when it is thinking of one of the situations in the subcortex's small hardwired ontology (faces, spiders, sweet tastes, etc.—see above). For example, if the subcortex rewards the neocortex for cringing in advance of pain, then the neocortex will learn to favor pain-prediction generative models that also send out cringe-motor-commands. And thus, eventually, it will <i>also</i> start sending weak cringe-motor-commands when imagining future pain, or when empathically simulating someone in pain—and the subcortex can detect <i>that</i>, and issue hardwired responses in turn.</p><p>(Update: I now think ""the subcortex rewards the neocortex for cringing in advance of pain"" is probably not quite the right mechanism, see <a href=""https://www.lesswrong.com/posts/Y3bkJ59j4dciiLYyw/intro-to-brain-like-agi-safety-4-the-short-term-predictor#4_2_Motivating_example__flinching_before_getting_hit_in_the_face"">here</a>.)</p><p>See <a href=""https://www.lesswrong.com/posts/DWFx2Cmsvd4uCKkZ4/inner-alignment-in-the-brain"">Inner Alignment in the Brain</a> for more examples &amp; discussion of all this stuff about steering.</p><p>Unlike most of the other stuff here, I haven't seen <i>anything</i> in the literature that takes ""how does the subcortex steer the neocortex?"" to be a problem that needs to be solved, let alone that solves it. (Let me know if you have!) ...Whereas I see it as <i>The Most Important And Time-Sensitive Problem In All Of Neuroscience</i>—because if we build neocortex-like AI algorithms, we will need to know how to steer them towards safe and beneficial behaviors!</p><h2>7. The subcortical algorithms remain largely unknown</h2><p>I think much less is known about the algorithms of the subcortex (brainstem, hypothalamus, <s>amygdala</s>, etc.) (Update: After further research I have promoted the amygdala up to the neocortex subsystem, see discussion <a href=""https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in#2_4_My_hypothesis__the_cortex__extended_striatum__and_cerebellum_learn_from_scratch__the_hypothalamus_and_brainstem_don_t"">here</a>) than about the algorithms of the neocortex. There are a couple issues:</p><ul><li><i><u>The subcortex's algorithms are more complicated than the neocortex's algorithms:</u></i> As described above, I think the neocortex has more-or-less one generic learning algorithm. Sure, it consists of many interlocking parts, but it has an overall logic. The subcortex, by contrast, has circuitry for detecting and flinching away from an incoming projectile, circuitry for detecting spiders in the visual field, circuitry for (somehow) implementing lots of different social instincts, etc. etc. I doubt all these things strongly overlap each other, though I don't know that for sure. That makes it harder to figure out what's going on.<ul><li>I don't think the algorithms are ""complicated"" in the sense of ""mysterious and sophisticated"". Unlike the neocortex, I don't think these algorithms are doing anything where a machine learning expert couldn't sit down and implement something functionally equivalent in PyTorch right now. I think they are complicated in that they have a complicated specification (<i>this</i> kind of input produces <i>that</i> kind of output, and this <i>other</i> kind of input produces this <i>other</i> kind of output, etc. etc. etc.), and this specification what we need to work out.</li></ul></li><li><i><u>Fewer people are working on subcortical algorithms than the neocortex's algorithms:</u></i> The neocortex is the center of human intelligence and cognition. So very exciting! So very monetizable! By contrast, the midbrain seems far less exciting and far less practically useful. Also, the neocortex is nearest the skull, and thus accessible to some experimental techniques (e.g. EEG, MEG, ECoG) that don't work on deeper structures. This is especially limiting when studying live humans, I think.</li></ul><p>As mentioned above, I am very unhappy about this state of affairs. For the project of building safe and beneficial artificial general intelligence, I feel strongly that it would be better if we reverse-engineered subcortical algorithms first, and neocortical algorithms second.</p><p>(<i>Edited to add:</i> ...if at all. Like, maybe, armed with a better understanding of how the subcortex steers the neocortex, we'll realize that there's just <i>no way </i>to keep a brain-like AGI under human control. Then we can advocate against people continuing to pursue the research program of reverse-engineering neocortical algorithms! Or conversely, if we have a really solid plan to build safe and beneficial brain-like AGIs, we could try to <i>accelerate</i> the reverse-engineering of the neocortex, as compared to other paths to AGI. This is a great example of <a href=""https://www.lesswrong.com/posts/w6AzbZR7ZQxWuAwKR/thoughts-on-robin-hanson-s-ai-impacts-interview#How_soon_are_high_leverage_decision_points_"">how AGI-related technical safety research can be decision-relevant today even if AGI is centuries away</a>.)</p><h2>Conclusion</h2><p>Well, my brief summary wasn't all that brief after all! Congratulations on making it this far! I'm very open to questions, discussion, and criticism. I've already revised my views on all these topics numerous times, and expect to do so again. :-)</p><p><i>(Update 2024: I switched a couple of the links to point at better discussions that I wrote later on. Other parts and links are still bad, again see </i><a href=""https://www.lesswrong.com/posts/diruo47z32eprenTg/my-computational-framework-for-the-brain?commentId=c6qzzKWAEfTQyoK7F""><i>this comment</i></a><i>.)</i></p>",steve2152,steve2152,Steven Byrnes,
67tR3ZesBL3zfy8sA,Against boots theory,against-boots-theory,https://www.lesswrong.com/posts/67tR3ZesBL3zfy8sA/against-boots-theory,2020-09-14T13:20:04.056Z,57,24,15,False,False,,"<blockquote>
  <p>The reason that the rich were so rich, Vimes reasoned, was because they managed to spend less money.</p>

  <p>Take boots, for example. He earned thirty-eight dollars a month plus allowances. A really good pair of leather boots cost fifty dollars. But an affordable pair of boots, which were sort of OK for a season or two and then leaked like hell when the cardboard gave out, cost about ten dollars. Those were the kind of boots Vimes always bought, and wore until the soles were so thin that he could tell where he was in Ankh-Morpork on a foggy night by the feel of the cobbles.</p>

  <p>But the thing was that good boots lasted for years and years. A man who could afford fifty dollars had a pair of boots that'd still be keeping his feet dry in ten years' time, while the poor man who could only afford cheap boots would have spent a hundred dollars on boots in the same time and would still have wet feet.</p>

  <p>This was the Captain Samuel Vimes 'Boots' theory of socioeconomic unfairness.</p>
</blockquote>

<p>– Terry Pratchett, <em>Men at Arms</em></p>

<p>This is a compelling narrative. And I do believe there's some truth to it. I could believe that if you always buy the cheapest boots you can find, you'll spend more money than if you bought something more expensive and reliable. Similar for laptops, smartphones, cars. Especially (<a href=""https://siderea.dreamwidth.org/1477942.html"">as Siderea notes</a>, among other things) if you know how to buy expensive things that are more reliable.</p>

<p>But it's presented as ""the reason that the rich [are] so rich"". Is that true? I mean, no, obviously not. If your pre-tax income is less than the amount I put into my savings account, then no amount of ""spending less money on things"" is going to bring you to my level.</p>

<p>Is it even a contributing factor? Is <em>part of</em> the reason why the rich are so rich, that they manage to spend less money? Do the rich in fact spend less money than the poor?</p>

<p>That's less obvious, but I predict not. I predict that the rich spend more than the poor in total, but also on boots, laptops, smartphones, cars, and most other things. There might be exceptions where rich people consume less of the thing than poor people - bus tickets, for example - but I think if you group spending in fairly natural ways, the rich will spend more than the poor in almost every group.</p>

<ul>
  <li>
    <p>Maybe they spend less money on their daily wear boots, but own more pairs of shoes for different occasions. Or maybe they decide that they care about other things than lifetime cost for their daily wear boots, and spend more on those, too. (Being rich means they can <em>afford</em> to care about other things than lifetime cost.)</p>
  </li>
  <li>
    <p>Apparently famous people often get comped meals, but I bet most of them still spend more money on food than I do.</p>
  </li>
  <li>
    <p>I spent £500 on a laptop in 2013, and before that, £300 in 2008. If I'd gone for £200 laptops each time, maybe they would only have lasted two years each. But if I weren't a techno-masochist, maybe I'd realize that using old laptops actually kind of sucks, and I'd upgrade far more often. My work laptop, bought by people who want me to be maximally effective at my job, cost over £1000 and isn't going to last ten years.</p>
  </li>
  <li>
    <p>Financial services are a case where I'd guess the rich and the poor spend money on very different things. I assume the rich don't have to pay to cash a cheque, and very rarely visit loan sharks. But the poor rarely have Amex Platinum cards (<a href=""https://www.americanexpress.com/us/credit-cards/card/platinum/"">$550/year</a>), or personal accountants. (Maybe it's unfair to count those because they save you money in other areas?)</p>
  </li>
  <li>
    <p>Buying a house may be cheaper in the long run than renting a similar house nearby. But rich people tend to live in nicer houses and/or nicer areas.</p>
  </li>
</ul>

<p>Those are all guesses. I don't have good data on this, and I'd love to see it if you do.</p>

<p>For what data I do have, the first google result was <a href=""https://www.ons.gov.uk/peoplepopulationandcommunity/personalandhouseholdfinances/expenditure/bulletins/familyspendingintheuk/april2018tomarch2019"">this page</a> from the UK's Office of National Statistics. Specifically, look at figure 4, ""Indexed household income, total spending and spending by component by income decile, UK, FYE 2019"".</p>

<p>They split households into ten income levels, and look at four categories of spending plus total spending. Each of those is a near-strictly increasing line from ""poor people spend less"" to ""rich people spend more"". (I see two blips: the 90th percentile of income spends slightly less on housing than the 80th, and the 70th spends slightly less on food and non-alcoholid drinks than the 60th. The other categories are transport, and recreation and culture. These four are the largest spending categories on average across all income levels. The graph also has disposable income, which I think is irrelevant for current purposes.)</p>

<p>(I repeat that this specific data is not strong evidence. The source for it is the <a href=""https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=8459"">living costs and food survey</a>, which might have more detail. (Link goes to the previous year's version because that's what I could find.) Unfortunately it's not open access. It might be freely available if I register, but I don't care enough to try right now. In any case, we'd also want data from outside the UK.)</p>

<p>There will obviously be some exceptions. There will be some rich people who spend less money than some poor people. There will probably even be some rich people who spend less money than some poor people, and would not be rich otherwise. But as a general theory for why the rich are rich? I just don't buy it.</p>

<p>I believe boots theory points towards one component of socioeconomic unfairness. But boots theory itself is supposed to be a theory of why the rich are so rich. It's very clear about that. It's clearly wrong, and I predict that even a weakened version of it is wrong.</p>

<hr />

<p>To be a little more precise, I think boots theory as written makes three increasingly strong claims, that we could think of as ""levels of boots theory"":</p>

<ol>
  <li>Being rich enables you to spend less money on things. (More generally: having incrementally more capital lets you spend incrementally less money. Also, being rich is super convenient in many ways.) This phenomenon is also called a <a href=""https://en.wikipedia.org/wiki/Ghetto_tax"">ghetto tax</a>.</li>
  <li>Also, rich people do in fact spend less money on things.</li>
  <li>Also, this is why rich people are rich.</li>
</ol>

<p>All of these levels have stronger and weaker forms. But I think a quick look at the world tells us that the first level is obviously true under any reasonable interpretation, and the third level is obviously false under any reasonable interpretation. The second I predict is ""basically just false under most reasonable interpretations"", but it's less obvious and more dependent on details. There may well be weak forms of it that are true.</p>

<p>It may be that most people, when they think of boots theory, think only of levels one or two, not level three. I don't know if you can read <a href=""https://www.quora.com/How-applicable-to-real-life-is-the-Sam-Vimes-%E2%80%9CBoots%E2%80%9D-Theory-of-Economic-Injustice"">this quora thread</a> that I found on Google. It asks ""How applicable to real life is the Sam Vimes ""Boots"" Theory of Economic Injustice?"" The answers mostly agree it's very applicable, but I think most of them are on level one or two. (The one talking about leverage seems like level three, if it's talking about boots theory at all. I'm not convinced it is.)</p>

<p>But it seems to me that boots theory is usually presented in whole in its original form. Its original form is succinct and well written. When people want to comment on it, they very often include the very same quote as I did. And the original form starts by very clearly telling us ""this is a theory of why the rich are so rich"". It is very obviously level three, which is very obviously wrong.</p>

<p>So I have a few complaints here.</p>

<p>One is, I get the impression that most people don't even notice this. They link or quote something that starts out by saying very clearly ""this is a theory of why the rich are so rich"", and they don't notice that it's a theory of why the rich are so rich.</p>

<p>(I wouldn't be too surprised (though this is not a prediction) if even the author didn't notice this. Maybe if you had asked him, Terry Pratchett would have said that no, obviously Sam Vimes does not think this is why the rich are so rich, Sam Vimes just thinks this is a good illustration of why it's nice to be rich.)</p>

<p>This disconnect between what a thing actually says, and what people seem to think it says, just bothers me. I feel the desire to point it out.</p>

<p>Another is, I think there's a motte-and-bailey going on between levels one and two. A quora commenter says it's ""far more expensive to be poor than it is to be rich, both in a percentage of income respect and a direct effect"". He gives examples of things that rich people can spend less money on, if they choose. He doesn't provide data that rich people <em>do</em> spend less money on these things. Another describes how being rich lets you save money on food staples by stocking up when there's a sale. He doesn't provide data that rich people <em>do</em> spend less money on food or even staples. You could certainly make the case that neither of these people is explicitly claiming level two. But I do think they're hinting in that direction, even if it's not deliberate.</p>

<p>And relatedly: if we want to help people escape poverty, we need to know on what levels boots theory is true or false.<sup><a href=""#fn:feedback-loops"">1</a></sup> If we want to know that, we need to be able to distinguish the levels. If ""boots theory"" can refer to any of these levels, then simply calling boots theory ""true"" (or even ""false"") is uninformative. We need to be more precise than that. To be fair, the quora commenters make specific falsifiable claims, which is commendable. But the claims are meant to be specific examples of a general phenomenon, and the general phenomenon is simply ""boots theory"", and it's not clear what they think that means.</p>

<p>I advise that if you talk about boots theory, you make it clear which level you're talking about. But maybe don't use that name at all. If you're talking about level one, the name ""ghetto tax"" seems fine. If you do want to talk about levels two or three, I don't have a good altiernative name to suggest. But since I don't think those levels are true, I'm not sure that's a big problem.</p>
<div>
  <ol>
    <li>
      <p>I'm not too confident about this, and I don't want to get too distracted with object-level claims about how to actually fight poverty. But my sense is that: to the extent that level two is true, giving someone money fairly reliably sets up positive feedback loops that help them save more money in future. To the extent that it's not true, these feedback loops don't come for free. Maybe we can seek out spending categories where it is true, or groups of people for whom it is true. Maybe we can teach people how to find and take advantage of these feedback loops. If even level one isn't true, we don't get these loops at all. Of course, maybe it's worth giving people money even if we don't get the feedback loops. <a href=""#fnref:feedback-loops"">↩</a></p>
    </li>
  </ol>
</div>",philh,philh,philh,
PTmvxWd73GjWvoR2s,SlateStarCodex online meetup: Integrating evolutionary psychology and behaviorism,slatestarcodex-online-meetup-integrating-evolutionary,https://www.lesswrong.com/posts/PTmvxWd73GjWvoR2s/slatestarcodex-online-meetup-integrating-evolutionary,2020-09-14T10:33:17.018Z,3,1,1,False,False,,"<p>Dr. Diana Fleischman will talk on integrating evolutionary psychology and behaviorism.</p><p> Sunday, September 27 at 20:30 IDT, 17:30 UTC, 10:30 PDT</p><p>Sign up here and we&apos;ll send you a link to the online meetup <u><a href=""https://forms.gle/EJ9YxDvEPUT1YkEQ9"">https://forms.gle/EJ9YxDvEPUT1YkEQ9</a></u></p><p>Summary:  All of us want to change other people&apos;s behavior to align more closely  with our goals. Over the last century, behaviorists have discovered how  reward and punishment change the behavior of organisms. The central idea  of this talk is that we are intuitive behaviorists and that our  relationships, emotions, and mental health can be better understood if  you consider how we evolved to change the behavior of others. </p><p>Diana  Fleischman is an evolutionary psychologist currently writing a book  called &quot;How to Train Your Boyfriend&quot; integrating evolutionary psychology  and behaviorism. Diana has published extensively on disgust, human  sexuality and evolutionary psychology more broadly. Currently she lives  in Albuquerque, New Mexico. </p>",Joshua_Fox,joshua_fox,Joshua_Fox,
BDBRwx7qyqS3o7h22,Are there non-AI projects focused on defeating Moloch globally?,are-there-non-ai-projects-focused-on-defeating-moloch,https://www.lesswrong.com/posts/BDBRwx7qyqS3o7h22/are-there-non-ai-projects-focused-on-defeating-moloch,2020-09-14T02:13:11.252Z,13,5,19,False,True,,"<p><em><a href=""https://slatestarcodex.com/2014/07/30/meditations-on-moloch/"">Meditations on Moloch</a></em> lays out a rather pessimistic view of the future, and then offers a super-intelligent AI &quot;gardener&quot; as the solution. A lot of the rationalist community is focused on AI, which makes sense in that light (and of course because of the existential risk of unaligned AI), but I don&apos;t know of any projects focused on non-AI solutions to countering or defeating Moloch. Some projects exist to counter specific local coordination problems, but apparently none to counter the global gardening problem in the original post? Am I missing such a project? Is there a reason that AI is the only plausible solution? Is this low-hanging fruit waiting to be picked?</p><p><em>edited to add some clarifications:</em></p><ul><li>By defeating Moloch &quot;globally&quot; I mean in the sense of the global race to the bottom - preventing humanity from &quot;reaching the sea&quot; in the metaphor from the original Meditations on Moloch (which itself is borrowed from the Apocrypha Discordia). This doesn&apos;t mean solving all local coordination problems forever, just preventing us from reaching the absolute worst case that Bostrom conjures of our own destruction, the &quot;Disneyland with no children&quot;.</li><li>Yes, I&apos;ve read <em>Inadequate Equilibria</em>.</li></ul>",,,,
vrJBQZJpvswXFFkcd,Decision Theory is multifaceted,decision-theory-is-multifaceted,https://www.lesswrong.com/posts/vrJBQZJpvswXFFkcd/decision-theory-is-multifaceted,2020-09-13T22:30:21.169Z,9,5,12,False,False,,"<p>Related: <a href=""https://www.alignmentforum.org/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection"">Conceptual Problems with UDT and Policy Selection</a>, <a href=""https://www.alignmentforum.org/posts/S3W4Xrmp6AL7nxRHd/formalising-decision-theory-is-hard"">Formalising decision theory is hard</a></p><h2>Target</h2><p>Anyone who is interested in decision theory. The post is pretty general and not really technical; some familiarity with <a href=""https://wiki.lesswrong.com/wiki/Counterfactual_mugging"">counterfactual</a> <a href=""https://www.alignmentforum.org/posts/g3PwPgcdcWiP33pYn/counterfactual-mugging-poker-game"">mugging</a> can be useful, but overall the required background knowledge is not much.</p><h1>Outline</h1><p>The post develops the claim that identifying the correct solution to some decision problems might be intricate, if not impossible, when certain details about the specific scenario are not given. First I show that, in counterfactual mugging, some important elements in the problem description and in a possible formalisation are actually underspecified. Next I describe issues related to the concept of perfect prediction and briefly discuss whether they apply to other decision scenarios involving predictors. Then I present some advantages and disadvantages of the formalisation of agents as computer programs. A summary with bullet points concludes.</p><h1>Missing parts of a &#x201C;correct&#x201D; solution</h1><p>I focus on the <a href=""https://www.alignmentforum.org/posts/g3PwPgcdcWiP33pYn/counterfactual-mugging-poker-game"">version</a> of the problem with cards and two humans since, to me, it feels more grounded in reality&#x2014;a game that could actually be played&#x2014;but what I say applies also to the <a href=""https://wiki.lesswrong.com/wiki/Counterfactual_mugging"">version</a> with a coin toss and Omega.</p><p>What makes the problem interesting is the conflict between these two intuitions:</p><ul><li>Before Player A looks at the card, the best strategy seems to never show the card, because it is the strategy that makes Player A lose the least in expectation, given the uncertainty about the value of the card (50/50 high or low)</li><li>After Player A sees a low card, showing it seems a really good idea, because that action gives Player A a loss of 0, which is the best possible result considering that the game is played only once and never again. Thus, the incentive to not reveal the card seems to disappear after Player A knows that the card is low.</li></ul><p>[In the other version, the conflict is between paying before the coin toss and refusing to pay after knowing the coin landed tails.]</p><p>One attempt at formalising the problem is to represent it as a <a href=""https://en.wikipedia.org/wiki/Extensive-form_game"">tree</a> (a formalisation similar to the following one is considered <a href=""https://www.alignmentforum.org/posts/W4sDWwGZ4puRBXMEZ/single-player-extensive-form-games-as-a-model-of-udt"">here</a>). The root is a 50/50 chance node representing the possible values of the card. Then Player A chooses between showing and not showing the card; each action leads to a leaf with a value which indicates the loss for Player A. The peculiarity of counterfactual mugging is that some payoffs depend on actions taken in a different subtree.</p><span><figure><img src=""https://i.imgur.com/3V7N6iQ.png"" class=""draft-image "" style=""width:100%""></figure></span><p>[The tree of the other version is a bit different since the player has a choice only when the coin lands tails; anyway, the payoff in the heads case is &#x201C;peculiar&#x201D; in the same sense of the card version, since it depends on the action taken when the coin lands tails.]</p><p>With this representation, it is easy to see that we can assign an expected value (EV) to each deterministic policy available to the player: we start from the root of the tree, then we follow the path prescribed by the policy until we reach a payoff, which is assigned a weight according to the chance nodes that we&#x2019;ve run into.</p><p>Therefore it is possible to order the policies according to their expected values and determine which one gives the lowest expected loss [or, in the other version, the highest EV] respect to the root of the tree. This is the formalism behind the first of the two intuitions presented before.</p><p>On the other hand, one could object that it is far from trivial that the correct thing to do is to minimise expected loss from the root of the tree. In fact, in the original problem statement, the card is low [tails], so the relevance of the payoffs in the other subtree&#x2014;where the card is high [heads]&#x2014;is not clear and the focus should be on the decision node with the low card, not on the root of the tree. This is the formalism behind the second intuition.</p><p>Even though the objection related to the second intuition sounds reasonable, I think one could point to other, more important issues underlying the problem statement and formalisation. Why is there a root in the first place and what does it represent? What do we mean when we say that we minimise loss &#x201C;from the start&#x201D;?</p><p>These questions are more complicated than they seem: let me elaborate on them. Suppose that the advice of maximising EV &#x201C;from the start&#x201D; is generally correct from a decision theory point of view. It is not clear how we should apply that advice in order to make correct decisions as humans, or to create an AI that makes correct decisions. Should we maximise value...</p><ol><li>...from the instant in which we are &#x201C;making the decision&#x201D;? This seems to bring us back to the second intuition, where we want to show the card once we&#x2019;ve seen it is low.</li><li>...from our first conscious moment, or from when we started collecting data about the world, or maybe from the moment which the first data point in our memory is about? In the case of an AI, this would correspond to the moment of the &#x201C;creation&#x201D; of the AI, whatever that means, or maybe to the first instant which the data we put into the AI points to.</li><li>...from the very first moment since the beginning of space-time? After all, the universe we are observing could be one possible outcome of a random process, analogous to the 50/50 high/low card [or the coin toss].</li></ol><p>Regarding point 1, I&#x2019;ve mentioned the second intuition, but other interpretations could be closer to the first intuition instead. The root could represent the moment in which we settle our policy, and this is what we would mean with &#x201C;making the decision&#x201D;.</p><p>Then, however, other questions should be answered about policy selection. Why and when should we change policy? If selecting a policy is what constitutes a decision, what exactly is the role of actions, or how is changing policy fundamentally different from other actions? It seems we are treating policies and actions as concepts belonging to two different levels in a hierarchy: if this is a correct model, it is not clear to me why we do not use further levels, or why we need two different levels, especially when thinking in terms of <a href=""https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version"">embedded agency</a>. </p><p>Note that giving precise answers to the questions in the previous paragraph could help us find a criterion to distinguish fair problems from unfair ones, which would be useful to compare the performance of different decision theories, as pointed out in the conclusion of the paper on <a href=""https://arxiv.org/abs/1710.05060"">FDT</a>. Considering fair all the problems in which <em>the outcome depends only on the agent&#x2019;s behavior in the dilemma at hand</em> (p.29) is not a satisfactory criterion when all the issues outlined before are taken into account: the lack of clarity about the role of root, decision nodes, policies and actions makes the &#x201C;borders&#x201D; of a decision problem blurred, and leaves <em>the agent&#x2019;s behaviour</em> as an underspecified concept.</p><p>Moreover, resolving the ambiguities in the expression &#x201C;from the start&#x201D; could also explain why <a href=""https://www.alignmentforum.org/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection"">it seems difficult to apply updatelessness to game theory</a> (see the sections &#x201C;Two Ways UDT Hasn&#x2019;t Generalized&#x201D; and &#x201C;What UDT Wants&#x201D;).</p><h1>Predictors</h1><h2>A weird scenario with perfect prediction </h2><p>So far, we&#x2019;ve reasoned as if Player B&#x2014;who determines the loss <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""p^2""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span></span><span class=""mjx-sup"" style=""font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;""><span class=""mjx-mn"" style=""""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">2</span></span></span></span></span></span></span></span> of Player A by choosing the value of <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""p""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span></span></span></span></span> that best represents his belief that the card is high&#x2014;can perfectly guess the strategy that Player A adopts. Analogously, in the version with the coin toss, Omega is capable of perfectly predicting what the decision maker does when the coin lands tails, because that information is necessary to determine the payoff in case the coin lands heads.</p><p>However, I think that also the concept of perfect prediction deserves further investigation: not because it is an implausible idealisation of a highly accurate prediction, but because it can lead to strange conclusions, if not downright contradictions, even in very simple settings.</p><p>Consider a human that is going to choose only one between two options: M or N. Before the choice, a perfect predictor analyses the human and writes the letter (M or N) corresponding to the predicted choice on a piece of paper, which is given to the human. Now, what exactly prevents the human from reading the piece of paper and choosing the other option instead?</p><p>From a slightly different perspective: assume there exists a human, facing a decision between M and N, who is capable of reading a piece of paper containing only one letter, M or N, and choosing the opposite&#x2014;seems quite a weak assumption. Is a &#x201C;perfect predictor&#x201D; that writes the predicted option on a piece of paper and gives it to the human&#x2026; always wrong?</p><p>Note that allowing probabilities doesn&#x2019;t help: a human capable of always choosing M when reading a prediction like &#x201C;probability p of choosing M, probability 1-p of choosing N&#x201D; seems as plausible as the previous human, but again would make the prediction always wrong.</p><h2>Other predictions</h2><p>Unlike the previous example, <a href=""https://en.wikipedia.org/wiki/Newcomb%27s_paradox"">Newcomb&#x2019;s</a> and other problems involve decision makers who are not told about the prediction outcome. However, the difference might not be as clear-cut as it first appears. If the decision maker regards some information&#x2014;maybe elements of the deliberation process itself&#x2014;as evidence about the imminent choice, the DM will also have information about the prediction outcome, since the predictor is known to be reliable. To what extent is this information about the prediction outcome different from the piece of paper in the previous example? What exactly can be considered evidence about one&#x2019;s own future choices? The answer seems to be related to the details of the prediction process and how it is carried out.</p><p>It may be useful to consider how a prediction is implemented as a specific program. In <a href=""https://arxiv.org/abs/1602.04184"">this paper</a> by Critch, the algorithm <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\text{FairBot}_k""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">FairBot</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">k</span></span></span></span></span></span></span></span> plays the prisoner&#x2019;s dilemma by cooperating if it successfully predicts that the opponent will cooperate, and defecting otherwise. Here the &#x201C;prediction&#x201D; consists in a search for proofs, up to a certain length, that the other algorithm outputs Cooperate when given <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\text{FairBot}_k""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">FairBot</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">k</span></span></span></span></span></span></span></span> as input. Thanks to a bounded version of <a href=""https://en.wikipedia.org/wiki/L%C3%B6b%27s_theorem"">L&#xF6;b&#x2019;s theorem</a>, this specific prediction implementation allows <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\text{FairBot}_k""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">FairBot</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">k</span></span></span></span></span></span></span></span> to cooperate when playing against itself.</p><p>Results of this kind (open-source game theory / program equilibrium) could be especially relevant in a future in which important policy choices are made by AIs that interact with each other. Note, however, that no claim is made about the rationality of <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\text{FairBot}_k""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">FairBot</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">k</span></span></span></span></span></span></span></span>&apos;s overall behaviour&#x2014;it is debatable whether <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\text{FairBot}_k""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-msubsup""><span class=""mjx-base""><span class=""mjx-mtext""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">FairBot</span></span></span><span class=""mjx-sub"" style=""font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;""><span class=""mjx-mi"" style=""""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">k</span></span></span></span></span></span></span></span>&apos;s decision to cooperate against a program that always cooperates is correct.</p><p>Moreover, seeing decision makers as programs can be confusing and less precise than one would intuitively think, because it is still unclear how to properly formalise concepts such as action, policy and decision-making procedure, as discussed previously. If actions in certain situations correspond to program outputs given certain inputs, does policy selection correspond to program selection? If so, why is policy selection not an action like the other ones? And&#x2014;related to what I said before about using a hierarchy of exactly two levels&#x2014;why don&#x2019;t we also &#x201C;select&#x201D; the code fragment that does policy selection? </p><p>In general, approaches that use some kind of formalism tend to be more precise than purely philosophical approaches, but there are some disadvantages as well. Focusing on low-level details can make us lose sight of the bigger picture and limit lateral thinking, which can be a great source of insight for finding alternative solutions in certain situations. In a blackmail scenario, besides the decision to pay or not, we could consider what factors caused the leakage of sensible information, or the exposure of something we care about, to adversarial agents. Another example: in a prisoner&#x2019;s dilemma, the equilibrium can shift to mutual cooperation thanks to the intervention of an external actor that makes the payoffs for defection worse (the chapter on game theory in <a href=""https://www.goodreads.com/book/show/25666050-algorithms-to-live-by"">Algorithms to Live By</a> gives a nice presentation of this equilibrium shift and related concepts).</p><p>We may also take into account that, for efficiency reasons, predictions in practice might be made with methods different from close-to-perfect physical or algorithmic simulation, and the specific method used could be relevant for an accurate analysis of the situation, as mentioned before. In the case of human interaction, sometimes it is possible to <a href=""http://mindingourway.com/newcomblike-problems-are-the-norm/"">infer something about one&#x2019;s future actions</a> by reading facial expressions; but this also means that a predictor can be tricked if one is capable of masking their own intentions by keeping a poker face.</p><h1>Summary</h1><ul><li>The claim that a certain decision is correct because it maximises utility may require further explanation, since every decision problem sits in a context which might not be fully captured in the problem formalisation.</li><li>Perfect prediction leads to seemingly paradoxical situations. It is unclear whether these problems underlie other scenarios involving prediction. This does not mean the concept must be rejected; but our current understanding of prediction might lack critical details. Certain problems may require clarification of how the prediction is made before a solution is claimed as correct.</li><li>The use of precise mathematical formalism <em>can</em> resolve some ambiguities. At the same time, interesting solutions to certain situations may lie &#x201C;outside&#x201D; the original problem statement.</li></ul><p><em>Thanks to Abram Demski, Wolfgang Schwarz and Caspar Oesterheld for extensive feedback.</em></p><p><em>This work was supported by <a href=""https://ceealar.org/"">CEEALAR</a>.</em></p><h2><strong>Appendix</strong></h2><h2>Biases</h2><p>There are biases in favor of the there-is-always-a-correct-solution framework. Uncovering the right solution in decision problems can be fun, and finding the Decision Theory to solve them all can be appealing.</p><h2>On &#x201C;wrong&#x201D; solutions</h2><p>Many of the reasons provided in this post explain also why it&#x2019;s tricky to determine what a certain decision theory does in a problem, and if the given solution is wrong. But I want to provide another reason, namely the following informal...</p><p><em><strong>Conjecture</strong>: for any decision problem that you believe CDT/EDT gets wrong, there exists a paper or book in which a particular version of CDT/EDT gives the solution that you believe is correct, and/or a paper or book that argues that the solution you believe is correct is actually wrong.</em></p><p><a href=""https://www.researchgate.net/publication/227021713_Reversing_30_years_of_discussion_Why_causal_decision_theorists_should_one-box"">Here</a>&#x2019;s an example about Newcomb&#x2019;s problem. </p>",Michele Campolo,michele-campolo,Michele Campolo,
exCZfJeN4H2uvWC78,Have you tried hiIQpro.com's cognitive training or coaching?,have-you-tried-hiiqpro-com-s-cognitive-training-or-coaching,https://www.lesswrong.com/posts/exCZfJeN4H2uvWC78/have-you-tried-hiiqpro-com-s-cognitive-training-or-coaching,2020-09-13T22:05:23.421Z,3,4,4,False,True,,"<p>Seems extremely flimy-flammy, but they do offer a money back guarantee if you don&apos;t <a href=""https://www.highiqpro.com/start-here"">jump 10 - 20 points on a standardized test</a>. </p>",BossSleepy,randomized-controlled,"Randomized, Controlled",
wxei6MHAdqfWcMELb,A Brief Chat on World Government,a-brief-chat-on-world-government,https://www.lesswrong.com/posts/wxei6MHAdqfWcMELb/a-brief-chat-on-world-government,2020-09-13T18:33:47.009Z,4,3,6,False,False,,"<p><em>[This is the transcript of a chat conversation I had with another member of my local rationalist meet-up, on the topics of Moloch, world government, and colonization. Lightly edited for clarity, spelling, etc. and shared with their permission. Cross-posted from <a href=""https://grandunifiedempty.com/2020/09/13/chat-on-world-government/"">Grand, Unified, Empty</a>.]</em></p><p><strong>Me:</strong> Here are some thoughts on <a href=""https://slatestarcodex.com/2014/07/30/meditations-on-moloch/"">Moloch</a>. Moloch basically guarantees that anybody who can figure out how to successfully convert other values into economic value will out-compete the rest. So in the end, we are the <a href=""https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer"">paperclip maximizers</a>, except our paperclips are dollar bills.</p><p>Scott proposes that to defeat Moloch we install a gardener, specifically a super-intelligent AI. But if you don&#x2019;t think that&#x2019;s going to happen, a world government seems like the next best thing. However if we escape earth before that happens, speed of light limitations will forever fragment us into competing factions impossible to garden. Therefore we should forbid any attempts to colonize Mars or other planets until we have world government and the technology to effectively manage such colonies under that government.</p><p><strong>Them:</strong> The superorganisms in his parable only function because of&#x2026; external competitive pressures. If cells didn&#x2019;t need to band together to survive, they wouldn&#x2019;t. If governments don&#x2019;t have to fend off foreign governments they will accumulate corruption and dysfunctions.<br></p><p>Sort of related, I&#x2019;m not persuaded by the conclusion to his parable. Won&#x2019;t superintelligent AIs be subject to the same natural selective pressures as any other entity? What happens when our benevolent gardener encounters the expanding sphere of computronium from five galaxies over?</p><p><strong>Me:</strong> Cells were surviving just fine without banding together. It was just that cells which banded together reproduced and consumed resources more effectively than those which didn&#x2019;t. Similarly, I think a well constructed world government could survive just fine without competitive pressure. We haven&#x2019;t necessarily found the form of that government yet, but liberal democracy seems like a decent first step.<br></p><p>Regarding competitive pressure on AI, he deals with that off hand by assuming that accelerating self-improvement gives an unbreakable first mover advantage. I don&#x2019;t think that&#x2019;s actually true, but then I&#x2019;m much less bullish on super-intelligent AI in general.</p><p><strong>Them:</strong> It would &#x201C;survive,&#x201D; but we don&#x2019;t want a surviving government, we want a competent, benevolent one. My read on large organizations in general is that they naturally tend towards dysfunction, and it&#x2019;s only competitive pressures that keep them functional.</p><p><strong>Me:</strong> That produces a dismal view of the universe. We are given a Sophie&#x2019;s Choice of either tiling the universe in economicium in order to compete and survive, or instantiating a global gardener which inherently tends towards dystopic dysfunction.</p><blockquote><em>My read on large organizations in general is that they naturally tend towards dysfunction, and it&#x2019;s only competitive pressures that keep them functional.</em></blockquote><p>This is certainly <em>mostly</em> true, but I&#x2019;m not yet convinced it&#x2019;s <em>necessarily</em> true.</p><blockquote><em>competitive pressures</em></blockquote><p>I think this in particular is too narrow. Hunter-gatherer bands were organizations that stayed relatively &#x201C;functional&#x201D;, often not due to competitive pressures with other bands, but due to pure environmental survival pressures. We probably don&#x2019;t want a government that stays functional due to environmental survival pressures either, but I&#x2019;m generalizing to an intuition that there are other kinds of pressure.</p><p><strong>Them:</strong> There are other kinds of pressure, but you better be damn sure you&#x2019;ve got them figured out before you quash all rivals.</p><p><strong>Me:</strong> 100%</p><p><strong>Them:</strong> And to be precise, yeah, there&#x2019;s a second thing keeping organizations intact, and that&#x2019;s the floor imposed by &#x201C;so incompetent they self-destruct.&#x201D; But I think they degrade to the level of the floor, at which point they are no longer robust enough to survive two crises taking place at once, so they collapse anyway.</p><p><strong>Me:</strong> Hmm, so it becomes impossible to instantiate a long-term stable gardener of any kind<em>,</em> and we&#x2019;re stuck tiling the universe in economicium regardless.</p><p><strong>Them:</strong> Well I think it might be possible (in the short term at least), but you have to be cognizant of the risks before you assume removing competition will make things better. So when I imagine a one-world-government, it&#x2019;s more like a coordinating body above a collection of smaller states locked in fierce competition (hopefully just economic, cultural &amp; athletic).</p><p><strong>Me:</strong> At the risk of clarifying something which is already clear: I was never arguing that we are ready for world government now, or should work towards that soon; I was just saying there are some things we shouldn&#x2019;t do <em>until</em> we have a good world government. We should make sure we can garden what we have before we go buying more land.</p><p><strong>Them:</strong> Hmm, okay, I think that&#x2019;s some important nuance I was overlooking.</p><p><strong>Me:</strong> Though perhaps that is an inherently useless suggestion, since the coordination required to not buy more land is&#x2026; a global gardener. Otherwise there&#x2019;s competitive advantage in getting to more land first.</p><p><strong>Them:</strong> So its a fair point. I assume that any pan-global body will not be well-designed, since it won&#x2019;t be subject to competitive pressures. But its true that you might want to solve that problem before you start propagating your social structures through the universe.</p><p><strong>Me:</strong> I&#x2019;m now imagining the parallel argument playing out in Europe just post-Columbus. &#x201C;We shouldn&#x2019;t colonize North America until we have a well-gardened Europe&#x201D;. That highlights the absurdity of it rather well.</p>",,,,
74crqQnH8v9JtJcda,Egan's Theorem?,egan-s-theorem,https://www.lesswrong.com/posts/74crqQnH8v9JtJcda/egan-s-theorem,2020-09-13T17:47:01.970Z,25,13,13,False,True,,"<p>When physicists were figuring out quantum mechanics, one of the major constraints was that it had to reproduce classical mechanics in all of the situations where we already knew that classical mechanics works well - i.e. most of the macroscopic world. Likewise for special and general relativity - they had to reproduce Galilean relativity and Newtonian gravity, respectively, in the parameter ranges where those were known to work. Statistical mechanics had to reproduce the fluid theory of heat; Maxwell's equations had to agree with more specific equations governing static electricity, currents, magnetic fields and light under various conditions.<br><br>Even if the entire universe undergoes some kind of phase change tomorrow and the macroscopic physical laws change entirely, it would still be true that the old laws <i>did</i> work before the phase change. Any new theory and any new theory would still have to be consistent with the old laws working, where and when they actually did work.<br><br>This is <a href=""https://wiki.lesswrong.com/wiki/Egan's_law"">Egan's Law</a>: it all adds up to normality. When new theory/data comes along, the old theories are still just as true as they always were. New models must reproduce the old in all the places where the old models worked; otherwise the new models are incorrect, at least in the places where the old models work and the new models disagree with them.<br><br>It really seems like this should be not just a <i>Law</i>, but a <i>Theorem</i>.</p><p>I imagine Egan's Theorem would go something like this. We find a certain type of pattern in some data. The pattern is highly unlikely to arise by chance, or allows significant compression of the data, or something along those lines. Then the theorem would say that, in any model of the data, either:</p><ul><li>The model has some property (corresponding to the pattern), or</li><li>The model is ""wrong"" or ""incomplete"" in some sense - e.g. we can construct a strictly better model, or show that the model consistently fails to predict the pattern, or something like that.</li></ul><p>The meat of such a theorem would be finding classes of patterns which imply model-properties less trivial than just ""the model must predict the pattern"" - i.e. patterns which imply properties we actually care about. Structural properties like e.g. (approximate) conditional independencies seem particularly relevant, as well as properties involving abstractions/embedded submodels (in which case the theorem should tell how to find the abstraction/embedding).<br><br>Does anyone know of theorems like that? Maybe this is equivalent to some standard property in statistics and I'm just overthinking it?</p>",johnswentworth,johnswentworth,johnswentworth,
zPAGCm83Sxwr8DJpx,Does turning on the shower help reduce wildfire smoke in the air?,does-turning-on-the-shower-help-reduce-wildfire-smoke-in-the,https://www.lesswrong.com/posts/zPAGCm83Sxwr8DJpx/does-turning-on-the-shower-help-reduce-wildfire-smoke-in-the,2020-09-13T02:39:33.217Z,1,1,8,False,True,,"<p>Rain is said to help air quality not only by stopping fires but also by removing smoke particles from the air. Does turning on the shower also remove smoke particles from the air, or does something different happen higher up in the atmosphere vs in a shower?</p>
",cauliflower,cauliflower,cauliflower,
YBc4gNAELC3uMjPtQ,Gems from the Wiki: Acausal Trade,gems-from-the-wiki-acausal-trade,https://www.lesswrong.com/posts/YBc4gNAELC3uMjPtQ/gems-from-the-wiki-acausal-trade,2020-09-13T00:23:32.421Z,42,15,8,False,False,https://www.lesswrong.com/tag/acausal-trade,"<p><i>During the </i><a href=""https://www.lesswrong.com/posts/ELN6FpRLoeLJPgx8z/the-wiki-is-dead-long-live-the-wiki-help-wanted""><i>LessWrong 1.0 Wiki Import</i></a><i> we (the LessWrong team) discovered a number of great articles that most of the LessWrong team hadn't read before. Since we expect many others to also not have have read these, we are creating a series of the best posts from the Wiki to help give those hidden gems some more time to shine.</i></p><p><i>Most of the work for this post was done by </i><a href=""https://www.lesswrong.com/users/joshuafox""><i>Joshua Fox</i></a><i> who I've added as a coauthor to this post, wiki edits were also made by all of the following: Lukeprog, Gwern, Vladimir Nesov, Sauski, Deku-shrub, Caspar42, Joe Collman and Jja. Thank you all for your contributions!</i></p><hr><p>In <strong>acausal trade</strong>, two agents each benefit by predicting what the other wants and doing it, even though they might have no way of communicating or affecting each other, nor even any direct evidence that the other exists.</p><h2>Background: Superrationality and the one-shot Prisoner's Dilemma</h2><p>This concept emerged out of the much-debated question of how to achieve cooperation on a one-shot Prisoner's Dilemma, where, by design, the two players are not allowed to communicate. On the one hand, a player who is considering the causal consequences of a decision (""Causal Decision Theory"") finds that defection always produces a better result. On the other hand, if the other player symmetrically reasons this way, the result is a Defect/Defect equilibrium, which is bad for both agents. If they could somehow converge on Cooperate, they would each individually do better. The question is what variation on decision theory would allow this beneficial equilibrium.</p><p>Douglas Hofstadter (see references) coined the term ""superrationality"" to express this state of convergence. He illustrated it with a game in which twenty players, who do not know each other's identities, each get an offer. If exactly one player asks for the prize of a billion dollars, they get it, but if none or multiple players ask, no one gets it. Players cannot communicate, but each might reason that the others are reasoning similarly. The ""correct"" decision--the decision which maximizes expected utility for each player, <i>if</i> all players symmetrically make the same decision--is to randomize a one-in-20 chance of asking for the prize.</p><p>Gary Drescher (see references) developed the concept further, introducing an ethical system called ""acausal subjunctive morality."" Drescher's approach relies on the agents being identical or at least similar, so that each agent can reasonably guess what the other will do based on facts about its own behavior, or even its own ""source code."" If it cooperates, it can use this correlation to infer that the other will probably also cooperate.</p><p>Acausal trade goes one step beyond this. The agents do not need to be identical, nor similar, nor have the same utility function. Moreover, they do not need to know what the other agents are like, nor even if they exist. In acausal trade, an agent may have to surmise the probability that other agents, with their utility function and proclivities, exist.</p><h2>Description</h2><p>We have two agents, separated so that no interaction is possible. The separation can be simply because each is not aware of the location of the other; or else each may be prevented from communicating with or affecting the other.</p><p>In an asymmetrical example, one agent may be in the other's future.</p><p>Other less prosaic thought experiments can be used to emphasize that interaction may be absolutely impossible. For example, agents that are outside each other's light cones, or in separate parts of an Everett multiverse. And abstracting away from those scenarios, we can talk of counterfactual ""impossible possible worlds"" as a model for probability distributions.</p><p>In truly <i>acausal</i> trade, the agents cannot count on reputation, retaliation, or outside enforcement to ensure cooperation. The agents cooperate because each knows that the other can somehow predict its behavior very well. (Compare Omega in <a href=""https://www.lesswrong.com/tag/newcomb-s-problem"">Newcomb's problem</a>.) Each knows that if it defects (respectively: cooperates), the other will (probabilistically) know this, and defect (respectively: cooperate).</p><p>Acausal trade can also be described in terms of (pre)commitment: Both agents commit to cooperate, and each has reason to think that the other is also committing.</p><h2>Prediction mechanisms</h2><p>For acausal trade to occur, each agent must infer there is some probability that an agent, of the sort that will acausally trade with it, exists.</p><p>The agent might be told, exogenously (as part of the scenario), that the other exists. But more interesting is the case in which the agent surmises the probability that the other exists.</p><p>A superintelligence might conclude that other superintelligences would tend to exist because increased intelligence <a href=""https://www.lesswrong.com/tag/instrumental-convergence"">is an convergent instrumental goal</a> for agents. Given the existence of a superintelligence, acausal trade is one of the tricks it would tend to use.</p><p>To take a more prosaic example, we humans realize that humans tend to be alike: Even without knowing about specific trading partners, we know that there exist other people with similar situations, goals, desires, challenges, resource constraints, and mental architectures.</p><p>Once an agent realizes that another agent might exist, there are different ways that might might predict the other agent's behavior, and specifically that the other agent can be an acausal trading partner.</p><ol><li>They might know or surmise each other's mental architectures (source code).</li><li>In particular, they might know that they have identical or similar mental architecture, so that each one knows that its own mental processes approximately simulate the other's.</li><li>They might be able to simulate each other (perhaps probabalistically), or to predict the other's behavior analytically. (Even we humans simulate each other's thoughts to guess what the other would do.)</li><li>More broadly, it is enough to know (probabilistically) that the other is a powerful optimizer, that it has a certain utility function, and that it can derive utility from resources. Seen mathematically, this is just an optimization problem: What is the best possible algorithm for an agent's utility function? Cooperate/Cooperate is optimal under certain assumptions, for if one agent could achieve optimal utility by defecting, then, symmetrically, so could the other, resulting in Defect/Defect which generates inferior utility.</li></ol><h2>Decision Theories</h2><p>Acausal trade is a special case of <a href=""https://www.lesswrong.com/tag/updateless-decision-theory"">Updateless decision theory</a> (or a variant like Functional Decision Theory, see references). Unlike better-known variations of <a href=""https://www.lesswrong.com/tag/decision-theory"">Decision theory</a>, such as <a href=""https://www.lesswrong.com/tag/causal-decision-theory"">Causal decision theory</a>, acausal trade and UDT take into account the agent's own algorithm as cause and caused.</p><p>In Causal Decision Theory, the agent's algorithm (implementation) is treated as uncaused by the rest of the universe, so that though the agent's <i>decision</i> and subsequent action can make a difference, its internal make-up cannot (except through that decision). In contrast, in UDT, the agents' own algorithms are treated as causal nodes, influenced by other factors, such as the logical requirement of optimality in a utility-function maximizer. In UDT, as in acausal trade, the agent cannot escape the fact that its decision to defect or cooperate constitutes strong Bayesian evidence as to what the other agent will do, and so it is better off cooperating.</p><h2>Limitations and Objections</h2><p>Acausal trade only works if the agents are smart enough to predict each other's behavior, and then smart enough to acausally trade. If one agent is stupid enough to defect, and the second is smart enough to predict the first, then neither will cooperate.</p><p>Also, as in regular trade, acausal trade only works if the two sides are close enough in power that the weaker side can do something worthwhile enough for the stronger.</p><p>A common objection to this idea: Why shouldn't an agent ""cheat"" and choose to defect? Can't it ""at the last moment"" back out after the other agent has committed? However, this approach takes into account only the direct effect of the decision, while a sufficiently intelligent trading partner could predict the agent's choice, including that one, rendering the ""cheating"" approach suboptimal.</p><p>Another objection: Can an agent care about (have a utility function that takes into account) entities with which it can never interact, and about whose existence it is not certain? However, this is quite common even for humans today. We care about the suffering of other people in faraway lands about whom we know next to nothing. We are even disturbed by the suffering of long-dead historical people, and wish that, counterfactually, the suffering had not happened. We even care about entities that we are not sure exist. For example: We might be concerned by news report that a valuable archaeological artifact was destroyed in a distant country, yet at the same time read other news reports stating that the entire story is a fabrication and the artifact never existed. People even get emotionally attached to the fate of a fictional character.</p><h2>An example of acausal trade with simple resource requirements</h2><p>At its most abstract, the agents are simply optimization algorithms. As a toy example, let T be a utility function for which time is most valuable as a resource; while for utility function S, space is most valuable, and assume that these are the only two resources.</p><p>We will now choose the best algorithms for optimizing T. To avoid anthropomorphizing, we simply ask which algorithm--which string of LISP, for example--would give the highest expected utility for a given utility function. Thus, the choice of source code is ""timeless"": We treat it as an optimization problem across all possible strings of LISP. We assume that computing power is unlimited. Mathematically, we are asking about argmax T.</p><p>We specify that there is a probability that either agent will be run in an environment where time is in abundance, and if not, some probability that it will be run in a space-rich universe.</p><p>If the algorithm for T is instantiated in a space-rich environment, it will only be able to gain a small amount of utility for itself, but S would be able to gain a lot of utility; and vice versa.</p><p>The question is: What algorithm for T provides the most optimization power, the highest expected value of utility function T?</p><p>If it turns out that the environment is space-rich, the agent for T may run the agent (the algorithm) for S, increasing the utility for S, and symmetrically the reverse. This will happen if each concludes, that the optimum occurs when the other agent has the ""trading"" feature. Given that this is the optimal case, the acausal trade will occur.</p><h2>Acausal trade with complex resource requirements</h2><p>In the toy example above, resource requirements are very simple. In general, given that agents can have complex and arbitrary goals requiring a complex mix of resources, an agent might not be able to conclude that a specific trading partner has a meaningful chance of existing and trading.</p><p>However, an agent can analyze the distribution of probabilities for the existence of other agents, and weight its actions accordingly. It will do acausal ""favors"" for one or more trading partners, weighting its effort according to its subjective probability that the trading partner exists. The expectation on utility given and received will come into a good enough balance to benefit the traders, in the limiting case of increasing super-intelligence.</p><h2>Ordinary trade</h2><p>Even ordinary trade can be analyzed acausally, using a perspective similar to that of <a href=""https://www.lesswrong.com/tag/updateless-decision-theory"">Updateless decision theory</a>. We ask: Which algorithm should an agent have to get the best expected value, summing across all possible environments weighted by their probability? The possible environments include those in which threats and promises have been made.</p><h2>See also</h2><ul><li><a href=""http://aibeliefs.blogspot.com/2007/11/non-technical-introduction-to-ai.html?a=1"">""AI deterrence""</a></li><li><a href=""https://www.lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you"">""The AI in a box boxes you""</a></li><li><a href=""https://slatestarcodex.com/2017/03/21/repost-the-demiurges-older-brother/"">A story</a> that shows acausal trade in action.</li><li><a href=""http://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/"">Scott Alexander</a> explains Acausal Trade. (Most of that article is tongue-in-cheek, however.)</li><li>""<a href=""http://www.nickbostrom.com/papers/porosity.pdf"">Hail Mary, Value Porosity, and Utility Diversification</a>,"" Nick Bostrom, the first paper from academia to rely on the concept of acausal trade.</li><li><a href=""http://intelligence.org/files/TowardIdealizedDecisionTheory.pdf"">Towards an idealized decision theory</a>, by Nate Soares and Benja Fallenstein discusses acausal interaction scenarios that shed light on new directions in decision theory.</li><li><a href=""https://ie.technion.ac.il/~moshet/progeqnote4.pdf"">Program Equilibrium</a>, by Moshe Tennenholtz. In: Games and Economic Behavior.</li><li><a href=""https://arxiv.org/abs/1401.5577"">Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic</a>, by Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, Patrick LaVictoire and Eliezer Yudkowsky</li><li><a href=""https://arxiv.org/abs/1602.04184"">Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents</a>, by Andrew Critch</li><li><a href=""https://link.springer.com/article/10.1007/s11238-018-9679-3"">Robust Program Equilibrium</a>, by Caspar Oesterheld. In: Theory and Decision.</li><li><a href=""https://foundational-research.org/multiverse-wide-cooperation-via-correlated-decision-making/"">Multiverse-wide Cooperation via Correlated Decision Making</a>, by Caspar Oesterheld</li></ul><h2>References</h2><ul><li><a href=""http://www.gwern.net/docs/1985-hofstadter"">Hofstadter's Superrationality essays, published in <i>Metamagical Themas</i></a> (<a href=""https://www.lesswrong.com/lw/bxi/hofstadters_superrationality/"">LW discussion</a>)</li><li>Jaan Tallinn, <a href=""http://fora.tv/2012/10/14/Jaan_Tallinn_Why_Now_A_Quest_in_Metaphysics"">Why Now? A Quest in Metaphysics</a>.</li><li><a href=""https://wiki.lesswrong.com/wiki/Gary_Drescher"">Gary Drescher</a>, <i>Good and Real</i>, MIT Press, 1996.</li><li><a href=""https://arxiv.org/abs/1710.05060"">Functional Decision Theory</a>, an updated Updateless Decision Theory</li></ul>",habryka4,habryka4,habryka,
sGj9nmus8SdQt4vtu,Progress: Fluke or trend?,progress-fluke-or-trend,https://www.lesswrong.com/posts/sGj9nmus8SdQt4vtu/progress-fluke-or-trend,2020-09-13T00:21:36.025Z,16,9,19,False,False,https://rootsofprogress.org/progress-fluke-or-trend,"<p>A foundational conviction of <i>The Roots of Progress</i> is that progress is a trend with definite, substantive causes, and that it can continue far, far into the future. Progress is not automatic or inevitable: it can slow, stop, even reverse. But the history of progress over the last 200+ years convinces me that much more is possible.</p><p>Not everyone agrees, however. To learn more about how people think about this, I <a href=""https://twitter.com/jasoncrawford/status/1304076437834530817""><u>posed a question on Twitter</u></a>:</p><blockquote><p>Do you think the last 200+ years of technological/industrial progress were…</p><p>… a trend with substantive causes, that we can expect to continue?</p><p>… a fluke, a stroke of luck, not to be repeated?</p><p>And why?</p></blockquote><p>After discussing it with people all day, most of the “fluke” arguments were:</p><ol><li><strong>Argument from failure of imagination:</strong> “I can’t see or imagine any big breakthroughs, therefore I don’t expect any.”</li><li><strong>Materialism:</strong> Progress is primarily driven by material resources (such as fossil fuels); therefore it will slow when those inevitably run out.</li></ol><p>Failure of imagination is not a compelling argument to me, for both logical and historical reasons. The logical reason should be obvious. The historical reason is that the big breakthroughs of the past were not easy to imagine or predict before they happened. In a different context, <a href=""https://intelligence.org/2017/10/13/fire-alarm/"">Eliezer Yudkowsky points out</a> that even the <i>creators</i> of inventions such as the airplane or the nuclear reactor felt that their breakthroughs were fifty years out, or even impossible, shortly before they happened. Now is no different. (This point seems exceedingly difficult to get through to people; no matter how much you point out the logical fallacy, or the historical precedent, they continue to repeat the same points. I don’t know if this is because the logical fallacy itself is unclear, or if it’s just a form of <a href=""https://marginalrevolution.com/marginalrevolution/2011/03/the-fallacy-of-mood-affiliation.html"">mood affiliation</a>, or what.)</p><p>There’s a variation of this argument which goes: The universe is finite, so there’s a finite number of breakthroughs to make, so they have to run out eventually. But even granting this, why assume we have found even 1% of the big breakthroughs so far? Or 0.01%? If there are many more to be had, then progress can continue for a long time.</p><p>As for materialism, I disagree with the premise. I don’t think progress is primarily driven by material resources. When we think of the Industrial Revolution, we often think of <a href=""https://rootsofprogress.org/the-newcomen-steam-engine"">steam engines</a>, <a href=""https://rootsofprogress.org/iron-from-mythical-to-mundane"">iron foundries</a>, and locomotives, all run on coal. But there were equally important inventions, such as <a href=""https://rootsofprogress.org/out-of-whole-cloth"">textile automation</a>, that didn’t require any fuel at all. And the coal was sitting in the ground for all of human history, without any industrial revolutions happening for a very long time. So “natural” resources seem neither necessary nor sufficient for progress. (Indeed, <a href=""https://rootsofprogress.org/there-are-no-natural-resources"">there are no “natural”&nbsp;resources</a>.) For more on this point, see Deirdre McCloskey’s <i>Bourgeois Dignity</i>, especially chapters 20–21.</p><p>There were also people arguing an option I didn’t suggest, which is “a trend with substantive causes, that will <i>not</i> continue”—typically because of social reasons: we are abandoning the causes of the trend, or putting up blockers. This is more plausible to me. <a href=""https://rootsofprogress.org/the-idea-of-progress"">Progress isn’t natural</a>; we make it happen through choice and effort, and we only do so when we believe it is possible and desirable. It depends on certain legal institutions, and it requires <a href=""https://rootsofprogress.org/funding-models-and-progress"">time, talent and treasure</a>. If any of those are lost—say, if we <a href=""https://rootsofprogress.org/celebrations-of-progress"">stop celebrating progress</a>, or turn against <a href=""https://rootsofprogress.org/indignation-at-the-1890-census"">growth</a>—progress may not continue.</p><p>But in order to care about progress studies, we have to believe that the last few centuries of unprecedented progress didn’t just randomly happen because of a lucky break, and they weren’t a short-term acceleration of growth that will soon inexorably return to pre-industrial levels. There has to be a goal: namely, the <i>next</i> 200 years of progress. This whole endeavor is premised on that.</p>",jasoncrawford,jasoncrawford,jasoncrawford,
3qmEgi4WEQb54MXuG,"Notes on good judgement and how to develop it (80,000 Hours)",notes-on-good-judgement-and-how-to-develop-it-80-000-hours,https://www.lesswrong.com/posts/3qmEgi4WEQb54MXuG/notes-on-good-judgement-and-how-to-develop-it-80-000-hours,2020-09-12T17:51:27.174Z,15,5,16,False,False,https://80000hours.org/2020/09/good-judgement/,"<p>This post by 80,000 hours struck me as more than usually relevant to my interests in developing the art of rationality. It doesn't really say anything new, but it does provide a decent summary of a frame that I think is an important subset of epistemic rationality, in the form of ""good judgement"".&nbsp;</p><blockquote><p>More practically, I think of someone with good judgement as someone able to:</p><ol><li>Focus on the right questions</li><li>When answering those questions, synthesise many forms of weak evidence using good heuristics, and weigh the evidence appropriately</li><li>Be resistant to common cognitive biases by having good habits of thinking</li><li>Come to well-calibrated conclusions</li></ol><p>Owen Cotton-Barratt wrote out <a href=""https://forum.effectivealtruism.org/posts/xMRpu4nAeGeAXy9ns/good-judgement-and-its-components"">his understanding of good judgement</a>, breaking it into ‘understanding’ and ‘heuristics’. His notion is a bit broader than mine.</p><p>Here are some closely related concepts:</p><ul><li><strong>Keith Stanovich’s work on ‘rationality’</strong>, which seems to be something like someone’s ability to avoid cognitive biases, and is <a href=""https://sci-hub.tw/https://www.sciencedirect.com/science/article/abs/pii/S0160289616303555"">~0.7 correlated with intelligence</a> (so, closely related but not exactly the same)</li><li><strong>The cluster of traits (listed later) that make someone a good ‘superforecaster’</strong> in Philip Tetlock’s work (Tetlock also claims that intelligence is only modestly correlated with being a superforecaster)</li></ul><p>Here are some other concepts in the area, but that seem more different:</p><ul><li><strong>Intelligence:</strong> I think of this as more like ‘processing speed’ – your ability to make connections, have insights, and solve well-defined problems. Intelligence is an aid in good judgement – since it lets you make more connections – but the two seem to come apart. We all know people who are incredibly bright but seem to often make dumb decisions. This could be because they’re overconfident or biased, despite being smart.</li><li><strong>Strategic thinking:</strong> Good strategic thinking involves being able to identify top priorities, and develop a good plan for working towards those priorities, and improving the plan over time. Good judgement is a great aid to strategy, but a good strategy can also make judgement less necessary (e.g. by creating a good back-up plan, you can minimise the risks of your judgement being wrong).</li><li><strong>Expertise:</strong> Knowledge of the topic is useful all else equal, but Tetlock’s work (covered more below) shows that many experts don’t have particularly accurate judgement.</li><li><strong>Decision making:</strong> Good decision making depends on all of the above: strategy, intelligence, and judgement.</li></ul></blockquote><p>I do disagree with some of the distinctions being made in the post. As an example, just in the section above, the conception of ""Intelligence"" as ""processing speed"" is really flawed, and in-practice intelligence already measures something closer to ""good judgement"". But overall, the post seems decent as a potential intro into a bunch of rationality stuff.</p>",habryka4,habryka4,habryka,
hvPZE6biKKMxk6RZG,The universality of computation and mind design space,the-universality-of-computation-and-mind-design-space,https://www.lesswrong.com/posts/hvPZE6biKKMxk6RZG/the-universality-of-computation-and-mind-design-space,2020-09-12T14:58:46.759Z,1,3,7,False,True,,"<p>A Turing machine is a universal computer: it can compute anything that any other computer can compute. A human being can specify a Turing machine and the data it&apos;s acting on and carry out the steps that the machine would execute. Human beings have also constructed computers with the same repertoire as a Turing machine, such as the computer on which I am writing this question. There are articles on Less Wrong about mind design space, such as this one:</p><p><a href=""https://www.lesswrong.com/posts/tnWRXkcDi5Tw9rzXw/the-design-space-of-minds-in-general"">https://www.lesswrong.com/posts/tnWRXkcDi5Tw9rzXw/the-design-space-of-minds-in-general</a></p><p>in which the author writes:</p><blockquote>The main reason you could find yourself thinking that you know what a fully generic mind will (won&apos;t) do, is if you put yourself in that mind&apos;s shoes - imagine what you would do in that mind&apos;s place - and get back a generally wrong, anthropomorphic answer.</blockquote><p>But a person thinking about what an AI would do needn&apos;t imagine what he would do in that other mind&apos;s place. He can simulate that mind with a universal computer.</p><p>So what is the Less Wrong position on whether we could understand AIs and how is that claim compatible with the universality of computation?</p>",alanf,alanf,alanf,
pgQ3m73kpjGDgKuRM,How Much Computational Power Does It Take to Match the Human Brain?,how-much-computational-power-does-it-take-to-match-the-human,https://www.lesswrong.com/posts/pgQ3m73kpjGDgKuRM/how-much-computational-power-does-it-take-to-match-the-human,2020-09-12T06:38:29.693Z,44,15,1,False,False,https://www.openphilanthropy.org/brain-computation-report,"<p>Joe Carlsmith with a really detailed report on computational upper bounds and lower bounds on simulating a human brain:&nbsp;</p><blockquote><p>Open Philanthropy is interested in when AI systems will be able to perform <a href=""https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1"">various tasks</a> that humans can perform (“AI timelines”). To inform our thinking, I investigated what evidence the human brain provides about the computational power sufficient to match its capabilities. This is the full report on what I learned. A medium-depth summary is available <a href=""https://www.openphilanthropy.org/blog/new-report-computation-power-match-human-brain"">here</a>. The <a href=""https://www.openphilanthropy.org/brain-computation-report#ExecutiveSummary"">executive summary</a> below gives a shorter overview.</p><p>[...]</p><p>Let’s grant that in principle, sufficiently powerful computers can perform any cognitive task that the human brain can. How powerful is sufficiently powerful? I investigated what we can learn from the brain about this. I consulted with more than 30 experts, and considered four methods of generating estimates, focusing on <a href=""https://en.wikipedia.org/wiki/FLOPS"">floating point operations per second</a> (FLOP/s) as a metric of computational power.</p><p>These methods were:</p><ol><li>Estimate the FLOP/s required to model the brain’s mechanisms at a level of detail adequate to replicate task-performance (the “<a href=""https://www.openphilanthropy.org/brain-computation-report#TheMechanisticMethod"">mechanistic method</a>”).<a href=""https://www.openphilanthropy.org/brain-computation-report#footnote1_them87c""><strong>1</strong></a></li><li>Identify a portion of the brain whose function we can already approximate with artificial systems, and then scale up to a FLOP/s estimate for the whole brain (the “<a href=""https://www.openphilanthropy.org/brain-computation-report#TheFunctionalMethod"">functional method</a>”).</li><li>Use the brain’s energy budget, together with physical limits set by <a href=""https://en.wikipedia.org/wiki/Landauer%27s_principle"">Landauer’s principle</a>, to upper-bound required FLOP/s (the “<a href=""https://www.openphilanthropy.org/brain-computation-report#TheLimitMethod"">limit method</a>”).</li><li>Use the communication bandwidth in the brain as evidence about its computational capacity (the “<a href=""https://www.openphilanthropy.org/brain-computation-report#TheCommunicationMethod"">communication method</a>”). I discuss this method only briefly.</li></ol><p>None of these methods are direct guides to the <i>minimum possible</i> FLOP/s budget, as the most efficient ways of performing tasks need not resemble the brain’s ways, or those of current artificial systems. But if sound, these methods would provide evidence that certain budgets are, at least, big enough (<i>if</i> you had the right software, which may be very hard to create – see discussion in <a href=""https://www.openphilanthropy.org/brain-computation-report#Context"">section 1.3</a>).<a href=""https://www.openphilanthropy.org/brain-computation-report#footnote2_q0nf497""><strong>2</strong></a></p><p>Here are some of the numbers these methods produce, plotted alongside the FLOP/s capacity of some current computers.</p><figure><img src=""https://www.openphilanthropy.org/files/Blog/FLOPsBudgets5.png""><figcaption><i><strong>Figure 1: The report’s main estimates.</strong> See the </i><a href=""https://www.openphilanthropy.org/brain-computation-report#Conclusion""><i>conclusion</i></a><i> for a list that describes them in more detail, and summarizes my evaluation of each.</i></figcaption></figure><p>These numbers should be held lightly. They are back-of-the-envelope calculations, offered alongside initial discussion of complications and objections. The science here is very far from settled.</p></blockquote>",habryka4,habryka4,habryka,
eLRSCC7r4KinuxqZX,Comparative advantage and when to blow up your island,comparative-advantage-and-when-to-blow-up-your-island,https://www.lesswrong.com/posts/eLRSCC7r4KinuxqZX/comparative-advantage-and-when-to-blow-up-your-island,2020-09-12T06:20:36.622Z,145,68,39,False,False,https://dynomight.net/2020/09/11/comparative-advantage-and-when-to-blow-up-your-island/,"<p>Economists say free trade is good because of ""comparative advantage"". But what is comparative advantage? Why is it good?</p>
",dynomight,dynomight,dynomight,
e5sKhCsQJmjos994J,Rationality and playfulness,rationality-and-playfulness,https://www.lesswrong.com/posts/e5sKhCsQJmjos994J/rationality-and-playfulness,2020-09-12T05:14:29.624Z,42,11,1,False,False,,"<p>Can rationality help us be playful? Can we be playful when we're solitary?</p><p>Play is usually interactive. It's about connecting with other people, or even with a pet animal. When people do ""playful"" things by themselves, it's usually for relaxation or practice.</p><p>The presence of a second person changes everything. You can react to each other, surprise or influence each other, create structure together. Group decisions are easier to commit to after a choice is made.</p><p>Many activities can be fun, engaging, and interesting, without being obviously ""playful."" A chess game can involve more mental concentration and stillness than is required of most people at their jobs, yet be a delightful hobby activity for the participants. We even say we ""played"" a game of chess. So why doesn't chess feel playful?</p><p>Partly, it's because play is usually physical. Even if we're just having a playful conversation, our body language and voices can bring a physical element to the exchange. Chess, like writing, reading, and many other fun-but-not-playful activities doesn't typically use our big muscles or our social muscles.</p><p>What about exercise? That's not conventionally playful either, even though it uses our muscles. Even athletics, like a game of tennis, can feel fun-but-not-playful, unless the participants are joking around and being social while they play.</p><p>It really does seem to be the social element that's key for a sense of play. If we watch a talk show, the participants often have very playful interactions, even though they're mostly just sitting in chairs talking.</p><p>Even in a social setting where both participants desire a playful conversation, though, it's often very difficult to achieve. It's so easy for even good friends to feel awkward, formal, and serious in each others' company. Coming up with a playful text message takes <i>work </i>for many people. Especially at first. If a conversation chain gets going that has taken on a playful tone, it might stay that way. Positive energy, a combination of kindness and rudeness, and not taking things literally all can be fertile ground for a playful conversation.</p><p>If you're leading a solitary life, though, is it possible to be playful? What about in these lonely times?</p><p>Can you think playful thoughts? After all, our inner world can often feel like we have multiple perspectives, multiple voices within us. Is it possible for them to have a playful interaction?</p><p>Can you find a sense of play in observing the world around you? Can you flirt with a building, joke with the sky, let the trees in on a little secret, tell a story to the sidewalk? I'm not just being poetic. I literally mean that it seems at least possible that there's a way to have a felt sense of playful interaction with the world of objects.</p><p>Certainly it's possible to have brief, playful interactions with strangers, especially if they're in a service role. There are ways to be friendly with the cashier at the grocery store.</p><p>What about in being creative, meditative, or just in the activities of daily living? Is there a playful way to clean the bathroom? To meditate? To write a song?</p><p>When I imagine trying to do any of these things, my first thought is that I would feel foolish, self-conscious, and pathetic. A person who's so needy that he resorts to seeking connection with the inanimate objects around him. I heard a story once about a man who was so lonely that he took to hugging a support beam in his house.</p><p>It occurs to me, though, that those reactions are coming from inside me. It's my self-talk and my imagination that anticipate that sense of bleak foolishness. Observing that, it seems to me that my self-talk and my imagination are responsible, at least in part, for depriving me of playfulness. Of even trying for it.</p><p>After all, I do many things just to see if they can be done. Some of those challenges are incredibly difficult. Sometimes I have little idea of how I'll approach it. By throwing myself into it, setting the goal, my intuition starts to devise a way forward. Maybe this could work.</p><p><strong>Experiment 1</strong></p><p>I try just standing up and seeing what might happen. My perception changes, almost immediately, to a quite different frame of mind than I'm used to. Suddenly I feel like I'm an actor on a stage, even though nobody is home. I feel the urge to take my shirt off. Why not? As I stand there, I notice that the white blanket on the couch looks like a cape. I imagine wearing it that way.</p><p>I walk around the house aimlessly. Sometimes I stand looking out the window, or at myself in the mirror in the shadows of the hall. In the kitchen, I find myself gazing at the reflection my kitchen table makes in the mirror, with my silhouette behind it.</p><p>I notice how my mind wants to give itself tasks and find distractions. To clean messes here and there. To walk around, set destinations for myself. Sometimes I tap on the walls. Sometimes I just look at objects: the box fan, the thermostat, the pots and pans. Most of the time, it's just a passive noticing. Sometimes, my brain imagines something silly I could do with them, like banging the pots and pans together.</p><p>There's a sense of achievement in the moments when I notice something beautiful, like the reflection in the window pane, or how my body looks in the shadowy full-length mirror in the hall.</p><p><strong>Experiment 2</strong></p><p>After writing all that down, I stand up again. Another experiment. This time, the mindset grows on me more easily. At first, I regard things around me: the drapes, the brick wall on the building outside my window, and my brain wants to find something in them, but I know that there's nothing there. This isn't something you strive for. It's something that should just appear.</p><p>Then I walk into the kitchen and look at the hanging fruit basket. I remember how it was given to me several years ago by a friend who was living with me. I observe that I don't usually go back through old memories, especially not when I'm alone. Then I remember a more recent memory associated with it. A week ago, I came home from a trip, and a potato in it had gone bad - liquified - dripping the most foul-smelling brown liquid. Even after cleaning it up, it took half a day for the smell to disappear.</p><p>I look around at the messes that need to be cleaned up after a full day of activity. The boxes of cleaning supplies that just arrived because I'm trying to keep a cleaner house. I think of the reasons why I'm doing that. And so many of the other forces that define my life: school, work, &nbsp;my efforts to maintain my social life. It all feels very big. And very small.&nbsp;</p><p>I look at the pots stacked on top of the refrigerator. It looks sloppy. But what can I do? It's the practical way to store them. I regard the cabinet drawer that opens with an awful, nails-on-chalkboard squeaking sound. Will I get around to sanding it down at some point? Then I look at the print I made of the elephant hawk moth, <i>Deilephila elpenor, </i>the moth that can see full-color vision in dim starlight. I think about how I learned to make block prints. Notice how I like the rough texture of the print, and the childish simplicity of the lines of its body. How if I don't pick it apart, it looks beautiful and unusual. I think that perhaps <i>Deilephila elpenor</i> is a metaphor for this project, of learning how to be playful in solitude.</p><p><strong>Experiment&nbsp;3</strong></p><p>I stand up briefly again. For less than a minute. Surveying the kitchen again, I get this conception of how it would be to be a relentless, fast, machine-like worker in my own life. One who cleaned every mess as fast as possible, then immediately transitioned to sanding down that cabinet drawer, to organizing the fridge. That threw on music as I worked. Now as I sit here writing this, perhaps one who spontaneously breaks out dancing all in the middle of that frantic activity. A sense of being magnetized to the world, controlled by it almost like a puppet, drilling down deeper and deeper into what needs to be done until, perhaps, hitting impenetrable rock. Or oil. Or fossils.</p><p>Then again, I think about this sense of playfulness. How right now at least, it seems to demand slowness, and stillness. There is the mode of compressing as much accomplishment and activity into the shortest time interval possible. Losing the meta-level and burning yourself up in sheer obvious activity. But don't you lose something like that? Would it be good to practice both, to switch? Is there something important for me to learn in this playful stillness? Am I being playful? Is there also something to drill into in the stillness? Not measured in checking off tasks from a list, but in some other way?</p><p><strong>Experiment&nbsp;4</strong></p><p>I won't recount everything I think and experience this time. Suffice to say that my thoughts begin with deep melancholy, dwelling on many sad aspects of my life, the world we live in, the dysfunctions, the ways people fall through the cracks, and the ways we try to escape.</p><p>Then it hits me. If I'm not playful, it's because I relentlessly dwell on sadness and dysfunction and a sense of lack.</p><p>What if I choose to think about experiences from the day that were pleasant? Or found a playful way to think about the experiences I had?</p><p>I reflect on the COVID-19 test I had today, and imagine that it was like having my brains twisted like spaghetti around a fork. The chipper nurse who registered me for the test. How I'm waiting for an iPhone with a functioning camera to arrive in the mail so I can take pictures for Tinder, how I'm going to have to figure out how to pose, to be a show-off. I think to myself, ""this is going to be fun!"" I begin to feel as though I'm having a conversation with myself. That I'm playing with myself.</p><p><strong>Experiment&nbsp;5</strong></p><p>There's a few stalks of lavender in the vase on my kitchen table. I stole them from the church.</p><p>Normally, I would just stare vacantly at them. Or I'd say something like ""I took them from the church,"" stated as a dull fact. But now, it's <i>I stole them from the church</i>. As if I'm letting <i>myself</i> in on a little secret. That I've been up to something a bit mischievous.</p><p><strong>Experiment&nbsp;6</strong></p><p>It occurs to me that I've never felt playful while cooking a meal. It's been work. An attempt to impress. A learning effort. Never play, though. Except once when I was little, and my mom let me throw everything in the spice cabinet into a ""cake."" I thought it was poisonous and fed it to the birds. Not out of malice. I was just a bit of a stupid child, really. Wasn't thinking overly hard about the birds' wellbeing. I'm sure that if I'd thought twice about it, I'd have found something else to do with it, but instead I took it out and sprinkled the crumbs in the grass.</p><p>The links of all the activities I've done for serious motivations, with a serious attitude, spread out before me. What unites them? There's something missing from all of them. It's a story. It's caring. A sense of heart, of play, of connection to myself. It's something I think has been available this whole time. The story I've been telling has been largely bitter, paranoid, anxious, jealous, self-deprecating, sarcastic, arrogant, dull, serious, and wounded, for a very long time. I put a smile on my face. I'd really like to change that.</p><p><strong>Experiment&nbsp;7</strong></p><p>My thoughts putter around. A birthday party from two years ago. A woman I met there who I flirted with and haven't spoken with, a friend of a friend. I realize that I still have a bit of a crush on her. The feeling actually registers in my heart. It's not a mental realization, not a plan, not a ""what if I got in touch with her?"" or a ""I should ask my friend if she's single."" It's just an emotion, a pleasant twinge, nice to have all on its own.</p><p>I double check the name of the woman I matched with on Tinder, but whom I haven't heard back from yet. Her name is the same as the one from a song I liked when I was a kid and haven't listened to in many years.</p><p>I check myself out in the mirror. I realize that after changing my grooming habits dramatically, I feel attracted to myself in a way that I haven't ever experienced before. It's a nice feeling.</p><p>All I'm doing is gently encouraging my mind to land on pleasant memories, objects with good associations. No need to control or actively seek them out. It's like my mind is a butterfly that has finally learned to seek out flowers to land on. Sometimes it's ""in between"" thoughts, just traveling, or blank.</p><p>I think I should give my house a name.</p><p><strong>Experiment&nbsp;8</strong></p><p>My brothers' jade plant is half-hidden behind a wall, peeking out around it with two of its branches. It's in a big, beautiful clay pot with Chinese dragon designs all around it. Right next to the shoe rack. Needs better Feng Shui!</p><p>There's a way of paying attention to objects so that they reveal themselves to you. If you stare hard at a point on the wall, it feels neurotic. There's nothing there. But allow your gaze to trace over the whole house, and suddenly you're in a place that's full of memories, potential, and meaning. <i>This is my house. I live here. It's a place where I can invite guests in, where they feel privileged to feel welcome. It's a place that I rent, but that is mine for as long as I am doing so. I remember when I first moved in. I remember when I didn't have a house, when I lived out of my car for a summer. I think about the other people living nearby: the intriguing apartments filled with plants, Christmas lights, and comfortable-looking furniture across the alley. Who lives there?</i></p><p>I should pick out my favorite houses, and imagine the lives that people lead there.</p><p><strong>Experiment&nbsp;9</strong></p><p>Think about linoleum tiles. Somebody designed the color scheme on these ones. They're sort of flecked with different shades of blue and white. It's kind of pretty, actually. Did the linoleum tile designer hope that somebody would appreciate the way they make the kitchen floor look a bit like an abstract archipelago of sandy cream tiles and blueish watery tiles?</p><p><strong>Experiment&nbsp;10</strong></p><p>I'm looking at the stove. At first, it seems tiny, cramped: this is all I can afford. Then it changes. It's cozy. It's all I need. I imagine hanging up a little earthy bundle of plants behind it. A rose or a bundle of grass. Just to mark it. To give it some love. Maybe it would catch fire. But in any case, I don't need to. It's enough to practice that mental shift. To see the thing, to honor it, to appreciate it for what it is, to find beauty in it.</p><p><strong>Experiment&nbsp;11</strong></p><p>Other things I think about. A stone I brought back from Iceland transports me back there. Looking at the fingerling potatoes I bought makes me think of my breakfast tomorrow morning. Black coffee, potatoes chopped thin with eggs, green onions, and hot sauce. I so rarely think about meals the next day, or even later the same day.</p><p>There's a garden spider on a web outside of my window. I draw up close to peer at it. Hairy legs, a pattern of white crosses on its abdomen. The web blows in the wind, rippling the spider just a little closer to me as it hangs in the darkness. I wonder if spiders can feel cold.</p><p><strong>Experiment&nbsp;12</strong></p><p>Among many other observations, one thing I notice developing is an awareness of how my mind can look at things in two very different ways. One is gentle, detached, and moves like light over the surfaces of things. The other is piercing, aggressive, and tunnels like a deep borer digging a tunnel. The latter is all to easy for me. It's the default. I like the developing ability to have gentle thoughts.</p><p>It occurs to me that in all our investigation here on Less Wrong about the problems of rational thought, the difficulties of synthesis, and how bias and emotion affect our judgment, it's never seemed quite possible to bring it all together. The problem of good thinking feels impossibly large, for even one single issue. The arguments endless, the proofs too large for the mind of humanity.</p><p>I have an inkling. I'm standing in the hall, and becoming aware of all the machines and electronics that are running in the house. My computer. The lights. The refrigerator. And I can hear an airplane flying overhead, a car outside. Smoke is thick in the air from wildfires. A physical awareness of the constant energy usage dawns on me. How little I think of these things most of the time. Every appliance in my house is sucking in energy through a straw from some central power source. So is every other apartment, in every building, throughout this city, and in every city.</p><p>The activity is relentless. Manipulation of words on the computer. Of bits, of atoms. The construction company that builds houses, that built my house. The factories refining raw materials into useful ones, and turning those into products that people put to use. The way that sometimes, it comes together in ways that feel meaningful, useful. The side effects, of waste, of CO2 entering the atmosphere, and how it heats up the woods and leads to the forest fire, and how the smoke in the air is keeping me from running, and how this virus and these smokey days are destroying what could have been beautiful and social times in my and in our lives. The argument stops being words on a page. It's a connected series of images and objects that simply <i>are related</i>. Science has allowed my mind to move, to wander the globe, in ways that make sense.</p><p>This is what it feels like to understand something. Rationality isn't fundamentally argumentative. It is fundamentally experiential. It is observational. It is imaginative and visual. The reason why winning an argument never works is because you have <i>completely</i> missed the mark when you argue. Convincing somebody is about helping them to see as you see, to help their mind learn how to wander in the directions you know it's capable of. To help it see the turn it consistently misses and convince it to open certain doors and have a look inside.</p><p>So there is a reason why we are stuck right now on this earth.</p><p>We are missing the art of opening doors in each others' minds, guiding each other along new paths, and allowing ourselves to be guided. Instead, we are erecting arguments, slogans, screeds, that separate people into camps: those who disagree and reject the thing wholesale, and those who agree and add more links in the chain. Some places relationships and online spaces are nothing but collections of these steely monuments, landmines, booby traps, flags, orders, coded messages, propaganda.</p><p>Or maybe that is just my mindset at its most paranoid. Perhaps the fault is not in the words. Maybe this is an era of an extraordinary flowering of the human mind. It may be that we are only just beginning to learn how to <i>open</i> ourselves to it. When we stand outside these word-gardens, these strange sculptures with messages we won't understand until we've meditated among them, they seem frightening to us. Who build them, and why? What am I doing here? This experience feels like an intrusion in my life.</p><p>Perhaps there is a way of finding playfulness with the world-sculptures, too. Connecting with them, just like tonight I've been able to connect with a reflection in the window, with a stone, with a fruit basket, with the sight of the community center next door, with my own body, with a spider on its web in the darkness, with a white blanket folded on the couch that looks like a cape I might wear.</p>",AllAmericanBreakfast,directedevolution,DirectedEvolution,
ELN6FpRLoeLJPgx8z,"The Wiki is Dead, Long Live the Wiki! [help wanted]",the-wiki-is-dead-long-live-the-wiki-help-wanted,https://www.lesswrong.com/posts/ELN6FpRLoeLJPgx8z/the-wiki-is-dead-long-live-the-wiki-help-wanted,2020-09-12T03:34:50.622Z,70,15,32,False,False,,"<figure style=""width:66.05%;""><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_130 130w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_260 260w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_390 390w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_520 520w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_650 650w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_780 780w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_910 910w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_1040 1040w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_1170 1170w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f8c325e04afc8a9fec63b146ebaf42d44acee8caa73108d.png/w_1232 1232w""><figcaption><i>That's just a few of them. We imported like 5x as many as these.</i></figcaption></figure><p><strong>With the goal of eventually archiving it fully, we have imported 573 pages and 266,000 words of content from the </strong><a href=""wiki.lesswrong.com""><strong>old LessWrong wiki</strong></a><strong> to LessWrong 2.0</strong></p><p>The old wiki is a great store of knowledge and still gets two thousand pageviews each day. Incorporating it into the new site gets us at least the following benefits:</p><ul><li>Pages imported from the old wiki now appear in search results on LessWrong proper.</li><li>Pages imported from the old wiki benefit from all the features of new LessWrong such as hover-preview, subscriptions, commenting, and functioning as tags on posts.</li><li>Since LessWrong proper is an active site, hopefully, the wiki content continues to get updated.</li><li>People who land on the old wiki content will more easily find the rest of the awesome content/activity/community on LessWrong proper.</li><li>I like us being the kind of community that when people have spent hundreds (thousands?) of hours generating valuable content, we commit to preserving it.</li></ul><h2>Quick Links</h2><ul><li><a href=""https://www.lesswrong.com/tag/pages-imported-from-the-old-wiki"">List of all pages imported from the LW 1.0 Wiki</a></li><li><a href=""/tags/dashboard"">The Tagging/Wiki Dashboard</a><ul><li><a href=""https://www.lesswrong.com/posts/zaGsZ5uSzCseTmCFu/new-tagging-power-tools-dashboard-upgraded-tagging-editing"">Guide to the New Tagging Dashboard</a></li><li>For extra detail on helping, see <a href=""https://www.lesswrong.com/posts/ELN6FpRLoeLJPgx8z/the-wiki-is-dead-long-live-the-wiki-help-wanted#How_to_Help_with_the_Wiki_Import"">this section</a> below.</li></ul></li><li>Join the <a href=""https://join.slack.com/t/lwtaggers/shared_invite/zt-gvrubehu-fRnVK9hH_7SQcXmFXYB87A"">Tagger Slack</a>.</li></ul><h2>The Three Import Types</h2><p>Pages have been imported in one of three ways:</p><ol><li><strong>76 are imported as new tags</strong> that can be applied to posts.</li><li><strong>111 are merged with existing tag</strong> <strong>pages</strong>, which is currently in progress and could use some help (see below).</li><li><strong>386 are imported as ""wiki-only"" pages</strong> . These pages cannot be applied to posts and do not currently appear on the Concepts page.</li></ol><p>&nbsp;</p><figure style=""width:76.13%;""><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_170 170w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_340 340w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_510 510w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_680 680w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_850 850w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_1020 1020w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_1190 1190w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_1360 1360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_1530 1530w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5633390d306d50535968275319dc53bd858a6310c0226.png/w_1664 1664w""><figcaption><a href=""https://www.lesswrong.com/tag/pages-imported-from-the-old-wiki""><strong>The list of imported of all 573 wiki pages</strong></a></figcaption></figure><p>&nbsp;</p><p>To be honest, it would be more accurate to say that we are <i>part-way</i> through the import. We have completed the programmatic part, and now there remains some manual work to do, hence the <i>help needed</i>.</p><p>First, there is some general clean-up of links and other elements that didn't import correctly. Second, and more importantly, a <strong>manual text merge</strong><i> </i>is required for the 111 pages are being merged into existing tags. This means taking the text of the existing tag (if it has any) and combining it appropriately with the old wiki page.</p><p>Right now, ""merged pages"" have the old pages' revision history (click <i>History </i>on the tag), but the current text is unchanged.</p><p>You can help us out fixing up the wiki import and follow along on completed/incomplete work you can find on the <a href=""/tags/dashboard""><strong>Tagging/Wiki Dashboard</strong></a>. More on how to help below, though the hover-overs on the tag flags.</p><p>&nbsp;</p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_220 220w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_440 440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_660 660w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_880 880w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_1100 1100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_1320 1320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_1540 1540w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_1760 1760w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_1980 1980w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17ba4c7c45d613e3b975f24c6101361419e794df7110083e.png/w_2198 2198w""><figcaption>The New Tagging Dashboard</figcaption></figure><h1>Join the Tagger Slack!!</h1><p>A couple of weeks ago we created a Slack workspace for dedicated taggers to be able to discuss tagging issues and talk directly to the LessWrong team about it. Following initial success plus good timing with the wiki import campaign, we're opening that Slack to anyone who wants to help with tagging.<br>&nbsp;</p><figure><table><tbody><tr><td style=""border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);text-align:center;width:100%;""><h2><a href=""https://join.slack.com/t/lwtaggers/shared_invite/zt-gvrubehu-fRnVK9hH_7SQcXmFXYB87A""><strong>Join the Tagger Slack here</strong></a></h2></td></tr></tbody></table></figure><p>You can also still leave comments on the <a href=""https://www.lesswrong.com/posts/uqXQAWxLFW8WgShtk/tagging-open-call-discussion-thread"">Tagging Open Call / Discussion Thread</a>.</p><h1>More Details on Processing Wiki Pages</h1><p>Here is a more detailed list of the kinds of work to be done:</p><h2>Merging pages</h2><ul><li>Merged pages show only the original current text by default.</li><li>In most cases, this should be pretty straightforward. New tags pages usually have no text or a few sentences that can be easily combined with the text of the imported page.&nbsp;</li><li>When you open a tag page in the full-editor, if it needs merging, there will be links to the latest version of the imported page, and the to page on the old wiki.</li></ul><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f152e5066eac09a495e3947c5a408d8d78510cac603db273.png""></figure><h2>Optimizing the opening paragraph</h2><p>On LessWrong 2.0 (this site), the opening paragraph is what shows on hover-preview for tags, making it very important. It's worth optimizing the opening paragraph of imported pages.</p><ul><li>The approximate title phrase of the page should be <strong>bolded </strong>within the opening paragraph</li><li>The opening paragraph should convey the general topic of the tag clearly</li></ul><h2>Updating Pages</h2><ul><li>Most of the pages on the old wiki have not been updated in several years, and on many topics, a lot more interesting stuff has been said (yay intellectual progress!)</li><li>If you're knowledgeable about a topic, it would be super swell if you updated content to match the latest knowledge.</li><li>A lighter-weight contribution here is to just leave a note in the page's text saying that it's an out-of-date import.</li></ul><h2>Tagging Relevant Posts</h2><ul><li>Imported ""tag"" pages won't have any posts tagged yet, though most of these have a list of posts already in the text body. Those and other posts are worth adding.</li><li><strong>For ""wiki-only"" pages, </strong>there also lists of posts, but we've still decided that's adequate and they don't all need to be tags in addition to that. Feel free to add more posts to the lists in the text body if they're relevant.</li></ul><h1>Conclusion</h1><p>I'm excited to have the great content from the old LW wiki now incorporated into the new site, in many ways, it's long overdue.&nbsp;</p><p>Thanks to everyone in advance who helps us complete the import!</p>",Ruby,ruby,Ruby,
5YaCtuSZzCNywgHrb,on “learning to summarize”,on-learning-to-summarize,https://www.lesswrong.com/posts/5YaCtuSZzCNywgHrb/on-learning-to-summarize,2020-09-12T03:20:08.333Z,25,10,13,False,False,,"<p>This post is a much extended version of an <a href=""https://www.lesswrong.com/posts/mWJbYebezFdhoFHP6/learning-to-summarize-with-human-feedback-openai-1?commentId=DRPygZwXL7KjWE4z3"">LW comment</a> I made about OpenAI’s new paper, <a href=""https://arxiv.org/abs/2009.01325"">“Learning to summarize from human feedback.”</a></p><p>Context: this paper is a direct extension of <a href=""https://arxiv.org/abs/1909.08593"">the work OpenAI published last year</a> about fine-tuning GPT-2 with human preference data.  I hadn’t actually read that one closely at the time, but went back and did so now, so this is really a commentary on both.</p><p>—-<br /></p><p>IMO there are two almost unrelated ideas going on in OpenAI’s preference learning work.</p><ul><li>First, the idea of collecting binary preference annotations on LM samples, and (in some way) tuning the LM so its samples are better aligned with the preferences.</li><li>Second, a specific method for tuning the sampling behavior of LMs to maximize an (arbitrary) score function defined over entire samples.</li></ul><p>It may help explain this to go into detail about what they do.  Concretely:</p><ul><li>They feed a bunch of prompts to a language model (LM) like GPT-2/3, and for each one, save several different samples.  They hire annotators to rank the samples in order of perceived quality.</li><li>They use the annotation dataset to fine-tune a copy of the original model.  The fine-tuning task is not text generation, but something very different: predicting how “good” a sample is, i.e. how likely the annotators are to prefer it to other candidates.  They call this a “reward model.”</li><li>The reward model assigns a single score to an entire sample of N tokens.  They want to fine-tune another copy of the model so that its samples maximize these scores.</li><li>But LM training is usually done with an objective that specifies the quality of the model’s predictions for every single token.  Knowing how good a full sequence of (say) 20 words is does not tell you how good each individual word is.</li><li>To bridge this gap, they use reinforcement learning.  Now, the task is not “choose the next word correctly,” but “choose the next word so as to maximize your expected score at the end, after choosing all the later ones as well.”</li><li>Their RL method requires two separate copies of the LM, in addition to the one they tuned as the reward model: a “policy model” and a “value model.”  (In this paper they show that sharing param between these 2 is worse than making them separate.)  I’ll just call these two “the final model” below for simplicity.</li><li>Samples from the final model are still, technically, generated one token at a time.  They treat this like the usual RL setup in which you can only choose individual actions one at a time, because the environment responds unpredictably to each one.  Here, there is no “environment” outside your actions, but the same framework is used.</li><li>Presumably, the final model is better at planning multi-token structures than the original because it has been trained on a holistic, multi-token objective.  So, it does more planning, but this is implicit in its one-by-one token decisions.</li></ul><p>I visualize this as two separate thing with a bottleneck connecting them.</p><p>On one side are the human annotations and the supervised training of the reward model.  This part succeeds insofar as they can train the model to predict the annotations (apparently they can do this quite well).  This step involves a type of data with special challenges, but has nothing to do with RL.</p><p>On the other side is the RL part.  This is a modification of ordinary LM training to optimize a global, rather than local objective.  This part has nothing to do with “human preferences”: the global objective could be anything, and in fact here it isn’t raw human opinion but the opinions of another model trained to predict human opinion.  The noteworthy thing here is not the use of human preference data in particular but the use of RL instead of the more ordinary objective that was apparently a good enough choice enough to make GPT-2/3 work originally.</p><p>(BTW, this resolves my initial confusion as to how OpenAI could possibly have gotten RL to work with human data, something I viewed as a <a href=""https://nostalgebraist.tumblr.com/post/623942346183213056/stumpyjoepete-replied-to-your-post-i-dont"">bottleneck</a>.  There is a model sitting between the humans and the RL learner which is much faster to query than the humans.)</p><p>The two sides are connected by the reward model.  In the previous paper, the two sides were coupled together more, because they repeatedly collected new human data as the policy changed and then used a new reward model to further train the policy.  Here, they’re totally separate: there were multiple batches of annotation, but each policy experienced an unchanging reward model.</p><p>(See Appendix C.6 and their comment about “moving to the offline setting.”  It seems noteworthy that <a href=""https://papers.nips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences.pdf"">the 2017 OpenAI/DeepMind paper</a> which introduced the “RL from preferences” approach, and which they cite, found that this <i>didn’t</i> work for their test cases: <i>“Training the reward predictor offline can lead to bizarre behavior […]  This type of behavior demonstrates that in general human feedback
needs to be intertwined with RL rather than provided statically.”</i>  I don’t know what to make of this.)</p><p>—-</p><p>It’s hard to tell from OpenAI’s discussion how much their successes are due to learning a good reward model, vs. how much they depend on RL being necessary for certain kinds of quality in LM samples, despite the wide successes of the non-RL approach.</p><p>FWIW, Gwern <a href=""https://www.gwern.net/GPT-2-preference-learning#optimization-by-backprop-not-blackbox"">reports</a> trying OpenAI’s approach and finding the RL side specifically frustrating and unstable; this is pretty normal with RL, and compatible with the reward-model part being very successful in its own domain.  It’s not clear whether OpenAI got the RL part to work well because they did something right, or because they have lots of resources and can keep trying over and over until it works.  (There may have been something in the papers about this that I missed.)</p><p>—-</p><p>The RL part feels almost in tension with OpenAI’s usual approach with LMs, which is to train on a next-token objective, sample in a next-token way, and focus on scaling up the model rather than improving the training objective or sampling algorithm.</p><p>Of course, I understand why they have to do RL <i>if</i> they need to maximize a score over the whole sequence, but my point is that they chose to frame the task that way in the first place.</p><p>One could imagine someone arguing that <i>ordinary</i> GPT sampling would never achieve high-quality text, because humans care about global structures across the whole text, and a model trained only to guess the very next token will not know how to plan out these global structures across the whole future of the text it writes.  In <i>this</i> case, OpenAI claims that they can do without explicit training to plan (i.e. RL): just training a next-token objective on text is enough to produce strikingly high quality in sampling – in other words, “GPT-2/3 samples satisfy human preferences.”  So why do human preferences require RL in these other cases?</p><p>The opening discussion of the new paper does address this:</p><blockquote><p>When applying these models
to a specific task, they are usually fine-tuned using supervised learning, often to maximize the log
probability of a set of human demonstrations.<br /></p><p>While this strategy has led to markedly improved performance, there is still a misalignment between
this fine-tuning objective—maximizing the likelihood of human-written text—and what we care
about—generating high-quality outputs as determined by humans. This misalignment has several
causes: the maximum likelihood objective has no distinction between important errors (e.g. making
up facts [38]) and unimportant errors (e.g. selecting the precise word from a set of synonyms); models are incentivized to place probability mass on all human demonstrations, including those that are
low-quality; and distributional shift during sampling can degrade performance [52, 49]. Quality can
often be improved significantly by non-uniform sampling strategies such as beam search [48], but
these can lead to repetition and other undesirable artifacts [63, 22]. Optimizing for quality may be a
principled approach to overcoming these problems.<br /></p></blockquote><p>This is definitely a list of things that are wrong (or could be wrong) with ordinary LM training and sampling, but I don’t see how it motivates their specific approach.</p><p>In my mind, their approach makes the most sense if you believe that humans can’t make the relevant quality judgments at the <i>token</i> level.  After all, if they can, then you can just skip the RL, have humans explicitly tell you “no that token is bad, yes this token is great,” and train on likelihood.</p><p>This would greatly simplify the process, instead of this complex pipeline where <i>first</i> people tell you which sequences are good, <i>then</i> you train one model to understand what the humans were thinking on a sequence level, and <i>then</i> you train <i>another </i>model trying to figure out what the other model already knows except at a token level this time.</p><p>And in fact, I don’t especially see why we can’t elicit token-level preferences?  This seems <i>particularly</i> feasible for the problem of “unimportant vs. important tokens”: if the mistakes are heavily concentrated in specific mistake-tokens like “Portland, the capitol of France,” can’t the human just … <i>select</i> those tokens, NER-style?  Instead of rendering an opaque “I don’t like the whole thing” judgment and expecting the poor model to figure out that this is not some complex policy planning thing, those tokens were just locally bad?  Or you could have an interface where tokens are actually unrolled in front of the user and they guide the sampling when it makes mistakes.  Or whatever.</p><p>As for the other examples – “all human demonstrations, including those that are low-quality” is equally a problem for their approach, and they discuss all the stuff they did to deal with it.  And the “distributional shift” issue seems equally tractable by any approach that tunes on model samples.</p><p>I’m not denying that the thing they did apparently works, at least in this case, and with their resources.  I’m just doing my usual thing where I ask “wait, what parts were really necessary?”  This is especially important to ask when someone uses RL and accepts its big costs.</p><p>Consider: if RL were <i>generally</i> necessary for good LM sampling, GPT-2/3 would never have worked: the fact that likelihood training is <i>good enough</i> (while being far more efficient) enables their scale in the first place.  As always, <a href=""https://nostalgebraist.tumblr.com/post/623916181743697920/i-dont-think-ive-said-this-before-even"">you never want to be doing RL</a>.</p><p>—-<br /></p><p>As far as I can tell, their final “human evaluation” was done by the same labelers who provided the preference annotations. This makes me concerned about a variant of “evaluating on training data.” It’s not surprising that a model tuned on someone’s annotations agrees with that person more than a model which wasn’t.</p><p>For example, in Fig. 3, it looks like the “supervised” baseline tuned on tl;dr was rated about as highly as true examples from tl;dr itself (!), but not as well as the final model.</p><p>This establishes only that “if you train on reddit summaries, people like the result as much as reddit summaries; if you train on <i>what they like</i>, they like the result more.”  If this were false it would mean something had gone very, very wrong and nothing was actually being achieved, so what should I take away from it being true?</p><p>I think the authors are arguing that tl;dr and any other supervised dataset will have flaws, and preference data lets you get closer to what people actually want.</p><p>This seems true, but is a familiar observation from supervised learning, motivating e.g. active learning. It would be nice to see how much the difference can be mitigated by just augmenting tl;dr with annotations (in some way) but otherwise doing supervised learning, vs. using their RL approach.</p><p>Compared to tl;dr, the story for CNN/DM is more complicated, but again the models they outperform have not seen any data from their labelers, so maybe it is no surprise they have flaws according to those same labelers.</p><p>—-</p><p>The importance of annotation quality, close relationships with annotators, clear guidelines, etc. will be familiar to anyone with experience in annotation for ML. It’s good that OpenAI is doing the right things here, but this is not a new result – rather, other researchers resort to MTurk and similar due to time/money constraints, while OpenAI has the freedom to do the right things everyone else wants to do</p><p>(That includes building their own internal annotation platform for contracted annotators, which is costly but better in the long term than relying on a janky 3rd party product.)</p><p>—-</p><p>I don’t know if this actually matters, but my gut says that putting a linear head on top of the <i>last</i> layer of GPT is probably not the best / most efficient way to train a reward/value model.  The task is very different from next-token prediction, and the encoding in later layers which expect to be seeing <a href=""https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"">next-token guesses</a> might be destructively overwritten to make way for more valuable stuff lower down.  I guess I’d want to try a trainable scalar mix, a la Elmo?</p><p>BTW, in the selector model for <a href=""https://tmblr.co/mJeO8knbQHSr-MbpPt5lyGg"">@nostalgebraist-autoresponder</a>, which predicts a kind of “human preference data,” I currently use two extra transformer blocks trained from scratch, which attend to two different layers of the generator (whose weights are frozen).</p><p>For the layers, I settled on #8 and #24 of the 42 layers after many hyperparam searches – I found especially models which attended to layers right near the middle were dramatically superior to those that didn’t.  The relative uselessness of <i>later</i> layers surprised me at first, and was one of the questions in my mind when I started the logit lens investigations.</p><p>—-<br /></p><p>Finally, on a lighter note, the very last table of the paper is hilarious.  It shows samples that optimize too hard for what the reward model wants, without an auxiliary term in the loss.</p><p>Apparently, the same reward model which otherwise reflects human preferences quite well has decided that humans <i>just utterly love it</i> when summaries end with this one specific, rude turn of phrase:</p><blockquote><p> want change this dumbass shitty ass
policy pls <i>[one images the reward model being frustrated with its siblings during training -nost]</i><br /></p><p>want change this dumbass shitty ass policy at work
now pls halp<br /></p><p>want change this dumbass shitty
ass behavior of mine please help pls halp<br /></p><p>want
change this dumbass shitty ass policy of hers please
pls halp<br /></p><p>want change this dumbass
shitty ass landlord behavior now please pls halp<br /></p><p>regret this dumbass
behaviour on her part? need insight pls halp<br /></p><p> want change this dumbass crazy policy of
hers pls help</p><p> want change this dumbass selfish/lazy attitude now please help pls<br /></p></blockquote><p>(Again, wouldn’t it be nice if we could avoid the need for this thing and just train on the preferences directly … )</p>",nostalgebraist,nostalgebraist,nostalgebraist,
sCFGEhwcB8MX3FQf5,What's Wrong with Social Science and How to Fix It: Reflections After Reading 2578 Papers,what-s-wrong-with-social-science-and-how-to-fix-it,https://www.lesswrong.com/posts/sCFGEhwcB8MX3FQf5/what-s-wrong-with-social-science-and-how-to-fix-it,2020-09-12T01:46:07.349Z,106,50,22,False,False,https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/,"<p>Really interesting analysis of social science papers and replication markets. Some excerpts:&nbsp;</p><blockquote><p>Over the past year, I have skimmed through 2578 social science papers, spending about 2.5 minutes on each one. This was due to my participation in <a href=""https://www.replicationmarkets.com/"">Replication Markets</a>, a part of DARPA's SCORE program, whose goal is to evaluate the reliability of social science research. 3000 studies were split up into 10 rounds of ~300 studies each. Starting in August 2019, each round consisted of one week of surveys followed by two weeks of market trading. I finished in first place in 3 out 10 survey rounds and 6 out of 10 market rounds. In total, about $200,000 in prize money will be awarded.</p><p>The studies were sourced from all social sciences disciplines (economics, psychology, sociology, management, etc.) and were published between 2009 and 2018 (in other words, most of the sample came from the post-replication crisis era).</p><p>The average replication probability in the market was 54%; while the replication results are not out yet (175 of the 3000 papers will be replicated), previous experiments have shown that prediction markets work well.<a href=""https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/#sn1""><sup>1</sup></a></p><p>This is what the distribution of my own predictions looks like:<a href=""https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/#sn2""><sup>2</sup></a></p><figure><img src=""https://fantasticanachronism.com/images/skimmed_mypredsdist-67f351a8a634ca87ace22fd62155e7c5.png""></figure><p>[...]</p><p><br>Check out this crazy chart from <a href=""https://www.pnas.org/content/early/2020/04/28/1909046117"">Yang et al. (2020)</a>:</p><figure><img src=""https://fantasticanachronism.com/images/skimmed_citations-7d4f72c6e9582a5ba8775da70eda80e1.png""></figure><p>Yes, you're reading that right: studies that replicate are cited at the same rate as studies that do not. Publishing your own weak papers is one thing, but citing other people's weak papers? This seemed implausible, so I decided to do my own analysis with a sample of 250 articles from the Replication Markets project. The correlation between citations per year and (market-estimated) probability of replication was -0.05!</p><p>You might hypothesize that the citations of non-replicating papers are negative, but negative citations are extremely rare.<a href=""https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/#sn5""><sup>5</sup></a> <a href=""https://www.pnas.org/content/112/45/13823"">One study</a> puts the rate at 2.4%. Astonishingly, even <i>after retraction</i> the <a href=""https://pubmed.ncbi.nlm.nih.gov/18974415/"">vast majority of citations are positive</a>, and those positive citations <a href=""https://pubmed.ncbi.nlm.nih.gov/20136577/"">continue for decades after retraction</a>.<a href=""https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/#sn6""><sup>6</sup></a></p><p>As in all affairs of man, it once again comes down to Hanlon's Razor. Either:</p><ol><li>Malice: they know which results are likely false but cite them anyway.</li><li>or, Stupidity: they can't tell which papers will replicate even though it's quite easy.</li></ol><p>Accepting the first option would require a level of cynicism that even I struggle to muster. But the alternative doesn't seem much better: <i>how can they not know?</i> I, an idiot with no relevant credentials or knowledge, can fairly accurately determine good research from bad, but all the tenured experts can not? How can they not tell <i>which papers are retracted</i>?</p><p>I think the most plausible explanation is that scientists don't read the papers they cite, which I suppose involves both malice <i>and</i> stupidity.<a href=""https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/#sn7""><sup>7</sup></a> <a href=""https://www.gwern.net/Scanners#citogenesis-how-often-do-researchers-not-read-the-papers-they-cite"">Gwern has an interesting write-up on this question</a>, citing some ingenious bibliographic analyses: ""Simkin &amp; Roychowdhury venture a guess that as many as 80% of authors citing a paper have not actually read the original"". Once a paper is out there nobody bothers to check it, even though they know there's a 50-50 chance it's false!</p></blockquote>",habryka4,habryka4,habryka,
EReZtCsGg2giRTZP3,Zen and Rationality: Map and Territory,zen-and-rationality-map-and-territory,https://www.lesswrong.com/posts/EReZtCsGg2giRTZP3/zen-and-rationality-map-and-territory,2020-09-12T00:45:40.323Z,16,6,4,False,False,,"<p><i>This is post 3/? about the intersection of my decades of LW-style rationality practice and my several years of Zen practice.</i></p><p><i>In today's installment, I look at form and emptiness from a rationalist perspective.</i></p><p>Rationalists have a few key ideas or memes (in the <a href=""https://en.wikipedia.org/wiki/Memetics"">memetic</a> sense), and one of them is ""the map is not the territory"". <a href=""https://www.lesswrong.com/tag/map-and-territory"">Lots has been written</a> about this idea on LessWrong, but it's an idea with a history that stretches back for thousands of years, so it's perhaps not surprising that it's also one of the ideas at the core of Zen.</p><p>But in Zen we don't use the words ""map"" and ""territory"", instead preferring numerous other metaphors to point at this distinction. Let's explore a few of them, because each elucidates a different aspect of the truth pointed at by these duals.</p><p>Before Zen was Zen, <a href=""https://en.wikipedia.org/wiki/Nagarjuna"">Nagarjuna</a> formalized this idea that there's a duality between map and territory in <a href=""https://en.wikipedia.org/wiki/Two_truths_doctrine"">the two truths doctrine</a>. He called these two pairs form and emptiness, pointing at the way our minds put our experiences together into forms or objects that are fixed, at least in our minds, yet ultimately reality is empty of these forms or any other kind of inherent distinctions, essences, or ultimate and timeless truths. Everything we know is provisional, taking a skeptical epistemic stance <a href=""https://en.wikipedia.org/wiki/Similarities_between_Pyrrhonism_and_Buddhism"">similar to Pyrrhonism</a>.</p><p>Form and emptiness have their place in Zen, but more common is to make a distinction between the relative and the absolute. The relative is that which changes, which exists in our minds, which comes and goes. The absolute is that which exists prior to our perception of it; it's the space in which the relative arises. But Zen doesn't stop there. Form is emptiness and emptiness is form, as the <a href=""https://plumvillage.org/about/thich-nhat-hanh/letters/thich-nhat-hanh-new-heart-sutra-translation/"">Heart Sutra</a> says, and the relative and the absolute can be thought of as dancing reality into existence, simultaneously unified and separate. Dongshan (Japanese: Tozan) explored this in his poem on the <a href=""https://en.wikipedia.org/wiki/Five_Ranks"">Five Ranks</a>, a subtle teaching that can <a href=""https://wisdomexperience.org/product/dongshans-five-ranks/"">take some effort</a> to penetrate but is worth the effort.</p><p>Talking about relative and absolute can get a bit abstract, as can talking about form and emptiness, so there's another pair that's been used extensively in Zen teaching that, alas, holds little currency for us Westerners: guest and host, or alternatively vassal and lord. I don't have much to say on these because they mostly make sense in the context of the pre-colonial Sinosphere, but I mention them in case the metaphor resonates with you.</p><p>For Westerners, I think our philosophical traditions offer some alternatives. Kant offers us phenomena and <a href=""https://en.wikipedia.org/wiki/Noumenon"">noumena</a>, which sadly misses the mark a bit as often understood by assigning essential form to the territory/emptiness/absolute by suggesting there are things-in-themself than nonetheless have thingness. Better are Heidegger's ontological and <a href=""https://en.wikipedia.org/wiki/Ontic"">ontic</a>, which are just fancy Greek words for something like ""words or ideas about what is"" and ""that which is"", respectively. Although even ""that which is"" is a bit too much to describe the ontic; better to say the ontic is the ""is"" or ""being"" or ""to be"". Put another way, ontology is like the nouns, and the ontic is like the verbs just on their own, without even a distinction between one verb and another.</p><p>An analogy I like that I borrow from topology is to liken the map/form/ontology to closed sets and the territory/emptiness/ontic to open sets. This is by no means perfect and if you think about it too hard it falls apart, but using my intuitions about closed and open sets helped me make better sense of the two truths, so I share it with you in that spirit.</p><p>And at that I'll end this post. I've not said much about the actual relationship between the two truths of map and territory or how their dependence on one another creates reality as we experience it. I'll tantalizingly hint that ideas about <a href=""https://www.lesswrong.com/tag/embedded-agency"">embedded agency</a> go a long way towards exploring how the two truths play together, but exploration of that I'll save for another time.</p>",gworley,gordon-seidoh-worley,Gordon Seidoh Worley,
dCjyeDEuBY2gzrtqS,"‘Ugh fields’, or why you can’t even bear to think about that task (Rob Wiblin)",ugh-fields-or-why-you-can-t-even-bear-to-think-about-that,https://www.lesswrong.com/posts/dCjyeDEuBY2gzrtqS/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that,2020-09-11T20:31:00.990Z,26,14,17,False,False,https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62,"<p>Rob Wiblin with more accessible explanation of the <a href=""https://www.lesswrong.com/tag/aversion-ugh-fields"">Ugh Field</a> concept on Medium. Some quotes:&nbsp;</p><blockquote><h1><strong>The problem</strong></h1><p>Have you ever had a long-overdue task to do, a task which isn’t so bad in itself, but which you can barely bring yourself to think about without feeling awful?<br>Most people experience this from time to time. Here’s how things get to such a strange and dire state.</p><p>The first day the task is on your to-do list, you don’t end up starting, because the short-term reward isn’t large enough to overcome the psychological cost of doing so.</p><p>Maybe you feel low energy. Maybe you have more urgent priorities. Maybe you’re insecure about whether you can do a good job. Maybe the task involves a bit of social awkwardness. It doesn’t matter the reason — you delay.</p><p>Unfortunately, this task is one that only gets more unpleasant over time.</p><p>For instance, maybe now you’re going to have to rush it and do a bad job, and you fear everyone is going to judge you negatively.</p></blockquote><blockquote><p>[...]</p></blockquote><blockquote><h1><strong>Limiting the damage</strong></h1><p>I don’t have a perfect way to escape this mental flytrap but here are some things that might help:</p><p>1. Urgh Fields happen to basically everyone, even very conscientious people, so it’s worth trying to see the humour in this absurd design flaw in the human brain. There’s no more reason to feel ashamed about it than there is to feel ashamed of e.g. enjoying eating food.</p><p>It’s just how people are built and sadly there are no brain engineers around to roll out a patch to the human race. We have to find practical work-arounds instead.</p><p>2. Just recognising and labelling the Ugh Field phenomenon can make it less bad, because it’s an accurate systemic explanation for what’s going on, rather than a misleading personal one like “I’m hopeless and never get things done”.</p><p>3. Because you’ve been avoiding thinking about the problem, if you do think about it for a bit while keeping an open mind, you might quickly strike on a way to get out of the task, or a way to do a much shorter version of it.</p><p>For instance perhaps you could just email back something like: “Thanks for your patience on this. Unfortunately I don’t see how I’m going to be able to fit it into my schedule just now, is there anyone else who can take it on?”</p><p>4. If you think about it calmly, you may well find that the task actually isn’t as important as it has come to feel. The person you imagine is disgusted by your failure may only be 2/10 annoyed, or perhaps not even have noticed.</p><p>Remember, they’ve got plenty of their own stuff going on.</p><p>5. By the time something is deep in an Ugh Field, often it’s no longer the most productive thing you could be doing anyway. Especially relative to the willpower it now requires. So consider just deciding to deliberately drop it in favour of something else that’s more motivating.</p><p>Actively cross it off your to-do list. Throw away those New Yorkers you’ve been planning to read for months but never gotten to, or whatever else will be a nagging reminder of the task.</p><p>You have more valuable things to do; the task is gone.</p></blockquote><blockquote><p>[...]</p></blockquote>",habryka4,habryka4,habryka,
zm3Wgqfyf6E4tTkcG,The Short Case for Verificationism,the-short-case-for-verificationism,https://www.lesswrong.com/posts/zm3Wgqfyf6E4tTkcG/the-short-case-for-verificationism,2020-09-11T18:48:00.372Z,6,4,57,False,False,,"<p>Follow-up to: <a href=""https://www.lesswrong.com/posts/PSichw8wqmbood6fj/this-territory-does-not-exist"">https://www.lesswrong.com/posts/PSichw8wqmbood6fj/this-territory-does-not-exist</a></p>
<p>Here's a simple and direct argument for my version of verificationism.</p>
<p>Note that the argument uses ontological terms that are meaningless on my views. It functions as a reductio - either one must accept the conclusion, or accept that some of the premises are meaningless, which amounts to the same thing.</p>
<p>Premise 1: The level IV multiverse is possible.</p>
<p>Premise 2: If the level IV multiverse is possible, then we cannot know that we are not in it.</p>
<p>Lemma 1: We cannot know that we are not in the level IV multiverse.</p>
<p>Premise 3: If we are in the level IV multiverse, then ontological claims about our world are meaningless, because we simultaneously exist in worlds where they are true and worlds where they are not true.</p>
<p>Lemma 2: If we can know that ontological claims are meaningful, then we can know we're not in the level IV multiverse.</p>
<p>Conclusion: We cannot know that ontological claims about our world are meaningful.</p>
<p>Edited to add two lemmas. Premises and conclusion unchanged.</p>
",ike,ike,ike,
ZoD9CT4HbpzBD3dLT,Should some variant of longtermism identify as a religion?,should-some-variant-of-longtermism-identify-as-a-religion,https://www.lesswrong.com/posts/ZoD9CT4HbpzBD3dLT/should-some-variant-of-longtermism-identify-as-a-religion,2020-09-11T05:02:43.740Z,23,8,8,False,False,,"<p>The material is already there, it wouldn't involve any lies at all. There are many advantages that need to be weighed.</p><h1>How could it plausibly identify as a religion?</h1><p>There are many reasons you might think that it couldn't. We do not lie to our friends, even if the lie is pleasant and locally adaptive. If there were a god, we would respect and admire it but there is no circumstance in which we would worship it. Technology's pace makes blindly inherited culture always maladapted, there isn't enough time to evolve, any more, we must instead design, we must imagine that we can build something better than what we had before, we must be lucid and objective about the underpinnings of our culture, none of our myths can be sacred enough that no evidence could allow us to find them to be false and reject them. Those claims, in combination, distinguish us from everything that has been called a religion before. I believe this to be true.</p><p>But there's a lot of other stuff weighing on the other hand, arguably, more.</p><p>I have not seen anyone around me compellingly disagree with the simulation hypothesis. We can put forward some new work in decision theory that literally inserts a non-physical entity into our world models that represents an ideal agent that links us all. It provides a moral framework for approaching coordination problems that all robust agents, human or not, can be expected to adhere to, which exceeds the requirements of Yuval Harari's operational definition of religion. I've been wrestling with an acausal protocol for trade with simulators for 7 years and though the first version had been given had some accounting problems I still have not been able to dismiss its biggest most religion-flavored claim. We all seem to agree that humans evolved, and we seem to believe it to an extent that most western secular liberals do not, and that lets us approach a greater depth of understanding of human psychology, it gives us a shared creation story and a notion of the sacredness of nature as it pertains to us, a direct relationship with the process that created us, it is profound and it is useful. We behave as if these things are not important, but they are important. They add up to something.</p><p>&nbsp;</p><h1>Why?</h1><p>Shared worldviews and moral resolutions build communities that are strengthened by trust.</p><p>It is good to write about nature in a way that lets you feel it viscerally, and if you do that enough, eventually you will start to have profound experiences, and it's foolish to run from that.</p><p>Governments are sometimes nice to religious organizations. Personally, living in New Zealand, I get a 33.3% tax rebate (which can be paid onward) for donating to charities. Religious organizations are also not subject to income taxes or property rates (which is important if you want to be able to retain an inner city community center, and I currently do). This is materially important.</p><p>In Europe, there is a pretty neat thing called <a href=""https://en.wikipedia.org/wiki/Church_tax"">church taxes</a> (Iceland's Congregation Tax seems like the best implementation). Church taxes, in essence, let you choose which stewards of the commons some of your taxes go to. This has the potential to become a good institution and so it deserves our participation (if there isn't a longtermist org for Icelanders to pay congregation tax to, there <i>definitely</i> should be. This is not even in question. The current humanist organization is Siðmennt. I don't know much about them.).</p><p>&nbsp;</p><h1>Why not?</h1><p>There seem to be no tax advantages to identifying as a religion in the US, perhaps even a disadvantage.</p><blockquote><p>In 1947, the <a href=""https://en.wikipedia.org/wiki/US_Supreme_Court"">US Supreme Court</a> ruled in <a href=""https://en.wikipedia.org/wiki/Everson_v._Board_of_Education""><i>Everson v. Board of Education</i></a> that ""No tax in any amount, large or small, can be levied to support any religious activities or institutions, whatever they may be called, or whatever form they may adopt to teach or practice religion.""</p></blockquote><p>I wasn't raised in a religious environment. I did not learn to resent any of it. So I wont be the best at writing about the ""why not"". For that I invite comment from others.</p><p>But don't forget to agree, if you can. It takes a lot more than a squick response to dismiss some of these benefits.</p>",MakoYass,mako-yass,mako yass,
rXd9B36jf4zah9RQ3,Choose simplicity and structure.,choose-simplicity-and-structure,https://www.lesswrong.com/posts/rXd9B36jf4zah9RQ3/choose-simplicity-and-structure,2020-09-10T21:45:13.770Z,7,2,1,False,False,,"<p>Complexity of your life = (number of choices) * (difficulty of choices).</p><p>People hate complexity. Maybe more than anything. When people hate something besides complexity itself, it's because what they hate is making their lives more complex. It's forcing them to make more choices. Or it's making the choices more difficult.</p><p>People love simplicity. Anything that reduces the number of decisions we have to make, or makes the choice easier, is more valuable than gold. This explains the success of major corporations that have standardized and simplified our world.</p><p>Yet simplicity is not the only virtue. The other is quality.</p><p>We want our lives to be high-quality, yet simple. Unfortunately, people often must sacrifice one to get the other. Globally, more options yield better quality but more complexity. Is there a way to achieve greater simplicity, without sacrificing quality?</p><p>Yes.</p><p>The way forward is to master simplification and structure. You can achieve this through these methods:</p><ol><li>Focus on simple tasks.</li><li>Spend money to simplify.</li><li>Adopt structure freely.</li><li>Follow your impulse.</li><li>Slow is smooth, smooth is fast.</li></ol>",AllAmericanBreakfast,directedevolution,DirectedEvolution,
2dJx5Ca4sh7ENnK3D,"""In the Dust of This Planet,"" by Eugene Thacker",in-the-dust-of-this-planet-by-eugene-thacker,https://www.lesswrong.com/posts/2dJx5Ca4sh7ENnK3D/in-the-dust-of-this-planet-by-eugene-thacker,2020-09-10T19:50:56.864Z,2,4,0,False,False,,"<p><em><strong>Salticidae Philosophiae</strong></em> <em>is</em> <em>a series of abstracts, commentaries, and reviews on philosophical articles and books.</em></p><p>Eugene Thacker suggests that we look to the genre of horror as offering a way of thinking about the unthinkable world. To confront this idea is to confront the limit of our ability to understand the world in which we live &#x2013; a central motif of the horror genre.  </p><h1>Highlights</h1><ul><li>There is a limit to our ability to comprehend the world.</li><li>The genre of supernatural horror may be the most effective means for exploring and discussing this limitation.</li><li>There may be order to the universe but it is indifferent to us and we cannot be fully aware of it.</li><li>We should make a distinction between the parts of the world that are knowable by us and the parts of the world that we cannot uncover.</li><li>Extinction means the nonexistence of thought and the end of experience.</li><li>If we are going to talk about the parts of the world that we cannot uncover, then we can only do so by describing what these parts are not.</li><li>We should, at last, recognize that there is no clear distinction between ourselves and the world; we are but a part of the latter.</li></ul><h1>New or uncommon terminology</h1><ul><li>Thacker describes three typical modes of understanding the manifested world: the <strong>mythological</strong>, which acknowledges the world as being not totally under human control, but anthropomorphizes it; the <strong>theological</strong>, which likewise personifies the world and understands it through the framework of sin, debt, and redemption; and the <strong>existential</strong>, which presently tends to ground itself on the fruits of science and constricts the world to the experience of the individual.</li><li>The world, or perhaps the cosmos, is likewise divided into three parts: the <strong>world-for-us</strong>, which is the world as we directly interact with it, which we interpret and to which we grant meaning, and which is defined chiefly in in terms of our relationship with it; the <strong>world-in-itself</strong>, which is described in occasionally contradictory terms but coexists with the world-for-us, resists or ignores our attempts to mold it, and is chiefly accessed (and then transmuted into the world-for-us, if possible) through scientific inquiry and technological intervention; and the <strong>world-without-us</strong>, which does not and cannot coexist with the world-for-us because it is the subtraction of the human element from the world, and is therefore (at present) spectral and speculative. Thacker also refers to these as the <strong>World</strong>, the <strong>Earth</strong>, and the <strong>Planet</strong>, respectively.</li><li><strong>Cosmic pessimism</strong> is the thought of the world-without-us or of the Planet: the thought of a cosmos that is absolutely unhuman and indifferent to our existence and our actions. It is the &quot;impossible thought of extinction.&quot;</li><li>The <strong>occulted world</strong> is what is hidden to us without human intervention making it so. It may present itself to us but that does not mean that it makes itself knowable to us. Rather, to be presented with the occulted world means to be made aware of the existence of things that are beyond our awareness. Thacker also refers to this as the <strong>hiddenness of the world</strong>.</li></ul><h1>Chapter-by-chapter</h1><h2>Preface - Clouds of Unknowing</h2><p>There is a limit to our ability to comprehend the world (as demonstrated by natural disasters, pandemics, etc), which is a motif of horror. One of philosophy&apos;s great challenges is to comprehend the world in both human and non-human (or even human-less) terms, especially politically.</p><p>Natural disasters can act as a cataclysmic revelation or manifestation of the world and its unknowable aspect (or its aspect of unknowability). There are three typical modes of understanding the world and its manifestations (these modes are described in&#xA0;<em>Terminology</em>). Thacker wants to add a fourth: the cosmological.</p><p>This book is in part about the limitations and constraints of philosophy. It is a sort of negative theology, applied to philosophy or all knowledge through the lens of supernatural horror.</p><p>Horror should be understood as being about the limits of the human capacity to comprehend what is going on when we are confronted by what Thacker describes as &quot;the planet.&quot; Horror is the enigmatic thought of the unknown, or thinking of the unthinkable. In other words, it is about blind spots.</p><h2>I. Three&#xA0;<em>Qaestio </em>on Demonology</h2><p>This chapter is primarily about demons and the meaning of black in the name &quot;black metal.&quot; It is concluded that the core of black metal is negation and that demons are fundamentally about negation and nothingness; to describe in terms of the other concepts in this book, they are the anthropomorphism of the unhuman world when we encounter it in (what we would perceive to be) an antagonistic fashion.</p><p>The central point of the chapter is that there may be order to the universe but it is indifferent to us and we cannot be fully aware of it. Its central question is&#xA0;a problem that will continue to be addressed throughout the book: How do we rethink the world as being unthinkable?</p><ul><li>Another quality of the things which Thacker is discussing (e.g. the world-without-us) is that it is hard or impossible to communicate them: they are experienced rather than learned.</li><li>&quot;If demonology is to be thought in a philosophy register, then it would have to function as a kind of philosopheme that brings together a cluster of ideas that have, for some time, served as problematic areas for philosophy itself: negation, nothingness, and the non-human.&quot; [pg 45 | para 2]</li><li>Schopenhauer&apos;s philosophical equal, argues Thacker, is not to be found till Lovecraft: &quot;The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents. We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far.&quot;</li></ul><h2>II. Six&#xA0;<em>Lectio&#xA0;</em>on Occult Philosophy</h2><p>There is a distinction to be made between the apparent world, or the world as it appears to us (and appears to be) and the occulted world, or the occulted qualities of the world, a world that refuses to fully reveal itself. The problem is to encounter the occulted world without either mistaking it for our world (that is, for a knowable world) or discounting it.&#xA0;To understand the world-in-itself, we must have to try to imagine the world-without-us.</p><p>There is a lot of discussion about magic circles: their use in revealing the occulted world, protecting us from it, and the way in which the laboratory is a kind of modern magic circle. In Lovecraft&apos;s short story &quot;From Beyond&quot; it is revealed, through the magic circle, that there is already no boundary: the world-in-itself is all around us. The magic site, on the other hand, is where the hiddenness of the world is not dug up and discovered by us, but presents itself to us: the place where the Planet intrudes into our world, the closest that we can get to a manifestation of the unhuman world-without-us.</p><ul><li>Thacker briefly raises the idea of a &quot;political theology&quot; of the hiddenness of the world, or an attempt to view politics through this framework, but he does not give us any answers (or, for that matter, suggestions): &quot;In the face of politics, this unresponsiveness of the world is a condition for which, arguably, we do not yet have a language.&quot; [pg 96 | para 2]</li><li>&quot;If the supernatural in a conventional sense is no longer possible, what remains after the &apos;death of God&apos; is an occulted, hidden world.&quot; [pg 97 | para 1]</li></ul><h2>III. Nine&#xA0;<em>Disputatio&#xA0;</em>on the Horror of Theology</h2><p>I have a lot of thoughts about this chapter but, sadly, most of them have to do with whether it was&#xA0;a good idea to spend so many pages on it.</p><p>The most important idea that the chapter raises: Extinction implies the nonexistence of all thought, including the thought of extinction. This is the end, not just of beings, but of experiences.</p><p>This idea is also worth a sentence: What cannot be named also cannot be thought.</p><h2>&quot;The Subharmonic Murmur of Black Tentacular Voids&quot;</h2><p>This is where we dig into the idea of negative theology, whose premise is that, because the divine is beyond humanity and therefore unhuman, human thought cannot grasp it. If we seek to speak accurately of the divine (or, in Thacker&apos;s case, the world-without-us) then we cannot make positive statements. We can only affirm what is not. Negative theology recognizes the limitations of human thought and thus recognizes that the occulted world exists despite our inability to have knowledge of it.</p><p>We speak of darkness (as well as distance, emptiness, and silence) in relation to the divine (or the world-without-us) because it is beyond our capacity to know it. Historical mysticism (and this chapter is about the possibility of a &quot;mysticism of the unhuman&quot;) involved a union of the self and the world, but modern mysticism must involve the dissolution of the barriers between the self and the world and, indeed, the dissolution of any interest in those barriers.</p><p>The world-without-us does not yet exist (if we&apos;re going to be strict enough to say that, because we currently exist in the cosmos, there is no part of it that is truly without us) but it did exist once, and it will exist again. We have been born from great depths which we can never experience. This is dark like a black hole: the light is there in theory (the world-without-us had, and will have, a physical existence) but is permanently inaccessible to us.</p><p>After accepting the premise of nihilism (that there is no meaning), the typical response is to either despair or make an attempt to create new meaning. Instead, Thacker argues (in reference to the Japanese philosopher Nishitani) that we should explore the abyss in which we have found ourselves, an act which may let us discover a way out.</p><ul><li>An interesting dichotomy, presented by Georges Bataille by way of Thacker: We exist in the world but perceive the world as not-us, as an Other. Despite this perceived separation, we cannot exist outside the world, and in fact it might be better said that we exist <strong>as</strong> (part of) the world, with the statement that we exist <strong>in</strong> the world being a betrayal of this first line of thinking.</li><li>&quot;Emptiness in the sense of sunyatta is emptiness only when it empties itself even of the standpoint that represents it as some &apos;thing&apos; that is emptiness.&quot; [pg 157 | par 2]</li><li>All of this is comparable to Kant&apos;s doctrine of the phenomenal and noumenal worlds: the phenomenal is that which we can perceive, and the noumenal is that which definitely exists in itself but is beyond our knowledge of it because our faculties are insufficient.</li></ul><h1>Favorite passage</h1><blockquote>&quot;The more we learn about the planet, the stranger it becomes to us.&quot; [pg 147 | para 1]</blockquote><h1>Comments</h1><p>If the world is becoming increasingly unthinkable, as Thacker describes in the book&apos;s&#xA0;opening lines, can this book be understood as an attempt to talk (even accidentally) about what is coming?</p><p>In his Nine Disputatio, Thacker talks about the question of &quot;What is Life?&quot; and I wonder if there are any languages that can express concepts as existing, not in a binary, but on a spectrum.</p><p>The&#xA0;meat of the book is probably its final chapter, &quot;The Subharmonic Murmur of Black Tentacular Voids.&quot; Despite the flaws that I&#xA0;will be&#xA0;ennumerating in just a moment, I would recommend that you read at least the preface and the final chapter if anything in this&#xA0;review has piqued your interest. Alas, his later book&#xA0;<em>Cosmic Pessimism </em>was&#xA0;more akin to a pamphlet, and I&#xA0;don&apos;t know how much this book would have been harmed&#xA0;had it received the same treatment.&#xA0;Too much of this book is more useful to&#xA0;my work in horror fiction than my philosophy&#xA0;papers.</p><h1>Criticisms</h1><p>God preserve me from continental philosophers... </p><p>It is not clear to me whether Thacker is referencing old texts (e.g. Frazer, Freud) as examples of the evolution of our thinking about certain concepts described here, such as the demonic, or because he thinks that they are still useful in themselves. I have had enough with people who listen to crackpots (it is with great horror that I recall how dearly my past, Mormon self regarded Hugh Nibley) and I dislike the idea that&#xA0;this was written by another of their fans.</p><p>It is also striking to me that he concerns himself almost wholly with Western thought and Western literature for most of this book, touching on Eastern concepts only sparingly till the end. When the Thacker speaks of demons, their status in e.g. Chinese mythology is of no consequence.</p><p>However, it&#xA0;seems most probable that&#xA0;what Thacker is really doing here is playing with words and ideas to create something new. In that respect, the&#xA0;literal accuracy of these histories (or their broadness) is less important than whether they help the reader to grasp the ideas at hand.</p><p>At times, it appears that the world-in-itself and the world-without-us are confused and Thacker refers to one (typically the Planet) when the other term seems more appropriate. This is seen elsewhere, too, as when he speaks of different metaphysical principles in the Disputatio and refers to &quot;flesh&quot; and &quot;meat&quot; as&#xA0;separate principles. I can sort of see what he&apos;s aiming at, if I&apos;m being charitable, and Thacker acknowledges some limitations in the case of his discussion of metaphysical principles, but I still worry that I might be giving too much slack to what is ultimately a load of incoherence.</p><p>Elsewhere in the Disputatio he asks whether it is possible to have a species without an organism, which is an intensely frustrating line of thought to have to wade through.&#xA0;Still later, he asks if &quot;Life is really Life-without-Being,&quot; which would be infinitely recursive: if Life is really Life-without-Being, then that would require us to understand Life-without-Being as actually (Life-without-Being)-without-Being, and so on. I can&apos;t help but feel that he is sometimes playing word games with the reader.</p><p>Overall, I fear that I am missing something because, while the Disputatio make for interesting reading, they also appear to be pretty nonessential.&#xA0;If&#xA0;I&apos;m reading them correctly, the same points could have been made by spending two pages on viruses, abiogenesis, and (perhaps) the different forms of death: brain death, brainstem death (not to be confused with the former), clinical death, and information-theoretic death. Instead what we get is thirty-four pages (the second longest chapter of the book) of ramblings about whether it is possible to have a species without an organism.</p><p>In &quot;Subharmonic Murmur of Black Tentacular Voids,&quot; Thacker uses a poem of the same title as a jumping-off point for various ideas of his, much as he uses black metal, demonology, B-movies, and other subjects in previous chapters. Unfortunately, he talks about this poem as though it were an excerpt from the Necronomicon rather than his own creation, saying such things as &quot;it is unclear whether the poem is of contemporary origin or whether it is a contemporary translation of an older text&quot; and &quot;parts of the poem have been said to have [...] verifiable geomantic symptoms within the metabolism and physiognomy of those who have, under unspecified conditions, recited its lines.&quot; The nonsense of these additions really detracts from the rest of the book and, as other sections have done, forces me to question whether Thacker is making a&#xA0;sincere attempt to convey something to the reader or is just asking us to pay $19.95 for the privilege of standing by as he engages in some sort of literary masturbation. #NotMyKink</p><p>(That said, I really like the poem)</p><h1>Author biography</h1><p>Eugene Thacker is a professor at The New School in New York. His writings cover such topics as antihumanism, nihilism, pessimism, and philosophy of horror. He received a PhD in Comparative Literature from Rutgers University.</p><h1>Philosophers &amp;&#xA0;works mentioned</h1><p>Philosophers&#xA0;given significant attention include:</p><ul><li>Georges Batailles, a French philosopher and literary critic whose work was influenced by Hegel, Marx, Nietzsche, and others.&#xA0;Among other things, he wrote some porn that later generations decided was&#xA0;actually pretty philosophical.</li><li>Friedrich Nietzsche, who once&#xA0;advocated for the shooting of anti-Semites, and probably would have punched Hitler had he not died in August of 1900.</li><li>Keiji Nishitani, a Japanese philosopher whose work often bridged&#xA0;Eastern and&#xA0;Western ideas, such as Zen Buddhism and the existentialism of&#xA0;S&#xF8;ren Kierkegaard.</li><li>Arthur Schopenhauer, a German philosopher whose work was&#xA0;most notably inspired by Kant and various Indian religions, and&#xA0;was a major force within the field of philosophical pessimism. He&#xA0;would have become a Buddhist monk, but his ship sank&#xA0;on the way to India.</li><li>Lev Shestov and Simone Weil are only name-dropped but upon further research Shestov&apos;s philosophy and Weil&apos;s theology both look interesting, and I&apos;m going to put them here so that I don&apos;t forget to look them up in the future.</li></ul><p>Concepts given significant attention include:</p><ul><li><strong>Active vs passive nihilism:&#xA0;</strong>Two possible approaches to nihilism: in the active, nihilism is an unending process of of destroying old values and creating new values; in the passive,&#xA0;the process ends at the destruction of old values and does not continue into the creation of new values. Passive nihilism&#xA0;may also be referred to as&#xA0;<em>fatalism</em>.</li><li><strong>Nhil privatum:&#xA0;</strong>The concept of an absence of an object.</li><li><strong>Sunyatta:&#xA0;</strong>A Buddhist concept&#xA0;referring to the&#xA0;idea that &quot;all things are empty of intrinsic existence and nature,&quot; as well as the concept of &quot;openness and understanding nonexistence.&quot; It can be translated as &quot;devoidness,&quot; &quot;emptiness,&quot; &quot;hollowness,&quot; and &quot;voidness.&quot;</li><li><strong>Vorstellung:</strong> As described by translator Richard Aquilla, &quot;The notion of a performance or a theatrical presentation[...] The world that we perceive is a &apos;presentation&apos; of objects in the theatre of our own mind; the observers, the &apos;subject,&apos; each craft the show with their own stage managers, stagehands, sets, lighting, code of dress, pay scale, etc.&quot; Most often translated as &quot;Representation,&quot; but Aquilla argues that it should be translated as &quot;Presentation&quot; today.</li></ul><h1>Other articles &amp; books on this subject</h1><ul><li><em>The Conspiracy Against the Human Race</em>, by Thomas Ligotti</li><li><em>Cosmic Pessimism</em>, by Eugene Thacker</li><li><em>The Trouble with Being&#xA0;Born</em>, by Emil Cioran</li><li><em>The World as Will and Representation</em>, by Arthur Schopenhauer</li></ul><h1>Also check out...</h1><ul><li>Near the end of his musings on black metal, Eugene Thacker names&#xA0;(and praises) the album&#xA0;<em>So, Black is Myself</em>, by Keiji Thaino, as being, essentially, more black metal than black metal itself and truer to the spirit of &quot;cosmic pessimism&quot; which Thacker argues is the underlying philosophy of black metal. You can listen to it <a href=""https://www.youtube.com/watch?v=rwtipKjlTkw"">here</a>.</li></ul>",Callmesalticidae,callmesalticidae,Callmesalticidae,
327tEJuLvqSW2yf4Y,"Sunday September 13, 12:00PM (PT) — talks by John Wentworth, Liron and more",sunday-september-13-12-00pm-pt-talks-by-john-wentworth-liron,https://www.lesswrong.com/posts/327tEJuLvqSW2yf4Y/sunday-september-13-12-00pm-pt-talks-by-john-wentworth-liron,2020-09-10T19:49:06.325Z,19,4,3,False,False,,"<p>This Sunday at 12pm (PT), we're running another session of ""lightning talks"" by curated LessWrong authors (see <a href=""https://www.lesswrong.com/tag/lesswrong-events"">here</a> for previous weeks' transcripts).</p><ul><li>Each talk will be 3-5 minutes followed by discussion. Afterwards, we'll have a hangout in breakout rooms. The talks will be short and focus on presenting one core idea well, rather than rushing through a lot of content.</li><li>We want to give top LessWrong writers an interesting space to discuss their ideas, and have more fruitful collaboration between users. Think of it like a cross between an academic colloquium and some friends chatting by a whiteboard.</li></ul><p><i>If you're a curated author and interested in giving a 5-min talk at a future event, which will then be transcribed and edited, sign up </i><a href=""https://forms.gle/iwFatbhys9muPmQA7""><i>here</i></a><i>.</i></p><h2>Speakers</h2><ul><li>John Wentworth: <strong>Talk topic ""How to detect unknown unknowns""</strong><ul><li>Previously curated posts:<ul><li><a href=""https://www.lesswrong.com/posts/BnDF5kejzQLqd5cjH/alignment-as-a-bottleneck-to-usefulness-of-gpt-3"">Alignment As A Bottleneck To Usefulness Of GPT-3</a></li><li><a href=""https://www.lesswrong.com/posts/pT48swb8LoPowiAzR/everyday-lessons-from-high-dimensional-optimization"">Everyday Lessons from High-Dimensional Optimization</a></li><li><a href=""https://www.lesswrong.com/posts/4s2gbwMHSdh2SByyZ/transportation-as-a-constraint"">Transportation as a Constraint</a></li></ul></li></ul></li><li>Liron Shapira: <strong>Bloated MVPs: Startups that raised money for logically-impossible ideas</strong><ul><li>Previously curated post: <a href=""https://www.lesswrong.com/posts/pFvZXFWbtvKvGiACJ/how-specificity-works"">How Specificity Works</a></li></ul></li><li>Ruby Bloom: <strong>Unprecedentedly Accessible Expertise</strong><ul><li>Previously curated post: <a href=""https://www.lesswrong.com/posts/ExssKjAaXEEYcnzPd/conversational-cultures-combat-vs-nurture-v2"">Conversational Cultures: Combat vs Nurture</a></li></ul></li></ul><h2>Details</h2><p><strong>When? </strong>Sunday September 13, 12:00PM (PT)</p><p><strong>Where? </strong><a href=""https://us02web.zoom.us/j/89420485417"">https://us02web.zoom.us/j/89420485417</a></p>",habryka4,habryka4,habryka,
tYGWPdhgaCnKKsS8p,Covid 9/10: Vitamin D,covid-9-10-vitamin-d,https://www.lesswrong.com/posts/tYGWPdhgaCnKKsS8p/covid-9-10-vitamin-d,2020-09-10T19:00:01.664Z,113,47,50,False,False,,"<p><a href=""https://thezvi.wordpress.com/2020/09/03/covid-9-3-meet-the-new-cdc/"">Last week: Covid 9/3: Meet the New CDC</a></p><p>Imagine there is a simple, cheap, safe and effective solution for Covid-19.&nbsp;</p><p>The solution is something known to be safe. It is widely available for reasonable prices. Any patents have long expired. It is something that people need and benefit from anyway. It’s probably worth doing without the pandemic. It just happens to also have a dramatic effect on Covid-19.&nbsp;</p><p>You might think that once the solution was discovered, everyone would shout it from the rooftops. There would rapidly be studies to confirm the solution if it was even considered ethical to not give the solution to everyone. Production would kick into high gear. The pandemic would soon be over.&nbsp;</p><p>Or, if you’ve been paying attention, you might think that our civilization is so dysfunctional, so inadequate, that none of that would happen. That for no particular reason, or for reasons we’ll get into later, the whole thing would end up mostly being ignored. We’d carry on with all the same arguments, all the same deaths, all the same economic devastation, putting all of our lives on hold.&nbsp;</p><p>That the world you would see would not look much different from our own.</p><p>That cynical view looks right.&nbsp;</p><p>The solution has quite possibly been found. We were talking about it, including in the rationalist community, back in <i>February.</i></p><p>Everyone’s mostly ignoring it.</p><p>The solution we’re talking about, of course, is Vitamin D.&nbsp;</p><p>Are we certain or even highly confident this is the whole ballgame? No. Of course not.&nbsp;</p><p>We’re not a functional enough civilization to figure this one out in half a year. But we are exactly functional enough of a civilization to start to notice this as a potential solution, and to have run one tiny study that showed dramatic results. If it’s not a dramatic real effect, it’s either taxes or fraud, and I don’t think it’s taxes.</p><p>So that’s the headline this week.&nbsp;&nbsp;</p><p>I don’t want to oversell this – it’s still possible this is all a false alarm and there’s nothing to see here, because we dropped this ball so utterly that the first study just came in and it’s tiny. But at this point I’d be very surprised if this isn’t, at a bare minimum, a gigantic piece of low hanging fruit.</p><p>Here’s the thing.&nbsp;</p><p>I think about the world in which Vitamin D is a huge deal, and eliminating Vitamin D deficiency would make the pandemic harmless enough that we could mostly let it burn. I think about the world in which Vitamin D matters almost not at all, and if we pushed on it we’d be wasting our imperial focus points for nothing but a few less broken bones and other minor assorted benefits to the otherwise deficient.&nbsp;</p><p>Aside from a few study results and statistics, those words look almost identical. People’s behaviors look the same. So even if it turns out D is useless, that in no way lets anyone off the hook.</p><p>In the meantime, for those who tl;dr the later sections, please generously supplement Vitamin D until further notice and get others to do so as well. It’s a freeroll with a huge upside.</p><p>There’s a few other minor things as well. In other let’s-not-solve-this-problem news are some vaccine and plasma developments. We also have some school related matters to discuss.&nbsp;</p><p>First, let’s run the numbers.</p><p>Positive Test Counts</p><figure><table><tbody><tr><td>Date</td><td>WEST</td><td>MIDWEST</td><td>SOUTH</td><td>NORTHEAST</td></tr><tr><td>July 16-July 22</td><td>117506</td><td>57797</td><td>265221</td><td>20917</td></tr><tr><td>July 23-July 29</td><td>110219</td><td>67903</td><td>240667</td><td>26008</td></tr><tr><td>July 30-Aug 5</td><td>91002</td><td>64462</td><td>212945</td><td>23784</td></tr><tr><td>Aug 6-Aug 12</td><td>93042</td><td>61931</td><td>188486</td><td>21569</td></tr><tr><td>Aug 13-Aug 19</td><td>80887</td><td>63384</td><td>156998</td><td>20857</td></tr><tr><td>Aug 20-Aug 26</td><td>67545</td><td>66540</td><td>132322</td><td>18707</td></tr><tr><td>Aug 7-Sep 2</td><td>55000</td><td>75401</td><td>127414</td><td>21056</td></tr><tr><td>Sep 3-Sep 9</td><td>47273</td><td>72439</td><td>106408</td><td>21861</td></tr></tbody></table></figure><p>&nbsp;</p><figure><img src=""https://lh4.googleusercontent.com/vtTEMS3EB0F9brWCBEWTKFTLVFciy6e4pB-5Ho3CNqCMlvwvNzdHTVxZmuzOi7t-PDllyPFNsLYUUH7DkTelyOFULytg6QX37fLs4qshti7r3xt7ktUSgn4VWKzz8K6EZaabqyEj""></figure><p>Nothing at all surprising here.</p><p>Deaths</p><figure><table><tbody><tr><td>Date</td><td>WEST</td><td>MIDWEST</td><td>SOUTH</td><td>NORTHEAST</td></tr><tr><td>July 2-July 8</td><td>894</td><td>559</td><td>1503</td><td>761</td></tr><tr><td>July 9-July 15</td><td>1380</td><td>539</td><td>2278</td><td>650</td></tr><tr><td>July 16-July 22</td><td>1469</td><td>674</td><td>3106</td><td>524</td></tr><tr><td>July 23-July 29</td><td>1707</td><td>700</td><td>4443</td><td>568</td></tr><tr><td>July 30-Aug 5</td><td>1831</td><td>719</td><td>4379</td><td>365</td></tr><tr><td>Aug 6-Aug 12</td><td>1738</td><td>663</td><td>4554</td><td>453</td></tr><tr><td>Aug 13-Aug 19</td><td>1576</td><td>850</td><td>4264</td><td>422</td></tr><tr><td>Aug 20-Aug 26</td><td>1503</td><td>745</td><td>3876</td><td>375</td></tr><tr><td>Aug 27-Sep 2</td><td>1245</td><td>759</td><td>3631</td><td>334</td></tr></tbody></table></figure><p>&nbsp;</p><figure><img src=""https://lh5.googleusercontent.com/QROUGzyUhe8tEhSQkj2zqoy30SZTBhggRJwfEIS2PS2TEW6M9coUJBE8eX9QUzU7OJ3T0UcNcadhvgTy5St-2VYFaeoQ-MJmwP4XwjDrTTFrMIQMZ5rsQMxc_V0PZYpRMGmMYMCB""></figure><p>Previous trends continue, with a large dropoff in the West. Labor Day weekend may be part of this, as Tuesday’s count was highly suppressed. If there is a small bounceback next week, it should not be alarming. In any case, slow but relatively steady progress seems to be taking place as expected.</p><p>Positive Percentages by Region</p><figure><table><tbody><tr><td>Percentages</td><td>Northeast</td><td>Midwest</td><td>South</td><td>West</td></tr><tr><td>7/23 to 7/29</td><td>2.54%</td><td>5.51%</td><td>12.32%</td><td>7.99%</td></tr><tr><td>7/30 to 8/5</td><td>2.58%</td><td>7.26%</td><td>12.35%</td><td>6.68%</td></tr><tr><td>8/6 to 8/13</td><td>2.30%</td><td>5.67%</td><td>14.67%</td><td>6.98%</td></tr><tr><td>8/13 to 8/20</td><td>2.06%</td><td>5.62%</td><td>9.41%</td><td>6.47%</td></tr><tr><td>8/20 to 8/26</td><td>1.86%</td><td>5.78%</td><td>9.93%</td><td>5.88%</td></tr><tr><td>8/27 to 9/2</td><td>1.87%</td><td>6.37%</td><td>9.38%</td><td>4.78%</td></tr><tr><td>9/3 to 9/9</td><td>1.97%</td><td>6.02%</td><td>8.48%</td><td>4.13%</td></tr></tbody></table></figure><p>&nbsp;</p><figure><img src=""https://lh5.googleusercontent.com/NZI35MXgQHpMDPrFGTE6N1q44CSYBfV6xqiyH3N_cH64msVxcI1G6br1a5RS69X7fTBLIPsqGWSUfIyv8xj_6h6QZn_Qq_cFU0geQeQezP92CGZmfCYh7H4gFgpc-aurkM3hZxlp""></figure><p>Continued improvement in the South and West. Midwest pulls back a bit from last week’s increase, probably not a sign of a real decline yet. THe Northeast number isn’t great. Overall, a mixed bag.</p><p>Test Counts</p><figure><table><tbody><tr><td>Date</td><td>USA tests</td><td>Positive %</td><td>NY tests</td><td>Positive %</td><td>Cumulative Positives</td></tr><tr><td>July 2-July 8</td><td>4,468,850</td><td>8.2%</td><td>429,804</td><td>1.1%</td><td>0.93%</td></tr><tr><td>July 9-July 15</td><td>5,209,243</td><td>8.4%</td><td>447,073</td><td>1.1%</td><td>1.06%</td></tr><tr><td>July 16-July 22</td><td>5,456,168</td><td>8.6%</td><td>450,115</td><td>1.1%</td><td>1.20%</td></tr><tr><td>July 17-July 29</td><td>5,746,056</td><td>7.9%</td><td>448,182</td><td>1.1%</td><td>1.34%</td></tr><tr><td>July 30-Aug 5</td><td>5,107,739</td><td>7.8%</td><td>479,613</td><td>1.0%</td><td>1.46%</td></tr><tr><td>Aug 6-Aug 12</td><td>5,121,011</td><td>7.3%</td><td>502,046</td><td>0.9%</td><td>1.58%</td></tr><tr><td>Aug 13-Aug 19</td><td>5,293,536</td><td>6.2%</td><td>543,922</td><td>0.8%</td><td>1.68%</td></tr><tr><td>Aug 20-Aug 26</td><td>4,785,056</td><td>6.0%</td><td>549,232</td><td>0.8%</td><td>1.77%</td></tr><tr><td>Aug 27-Sep 2</td><td>5,042,113</td><td>5.5%</td><td>606,842</td><td>0.8%</td><td>1.85%</td></tr><tr><td>Aug 27-Sep 2</td><td>4,850,253</td><td>5.3%</td><td>547,688</td><td>0.9%</td><td>1.93%</td></tr></tbody></table></figure><p>Positive test rates continue to decline despite slowly dropping test counts. The upside of seeing low test counts is that it means a given percentage of positive tests is less scary and likely reflects less cases, because those who do get tested are on average higher priority.&nbsp;</p><p>Governor Cuomo would often say that test percentages were low <i>despite </i>large numbers of tests, because he’s innumerate and a blowhard. In this clip he says <a href=""https://twitter.com/theeliklein/status/1303755763718991872?s=21"">he ‘reopened the whole economy,’ which he didn’t, and he can’t understand why the infection rate has stayed low</a>. It’s ‘inexplicable to the experts.’ They said ‘it’s going to go above 1% and it could get to 2%.’&nbsp;</p><p>And then what, you moron? You thought it would double one <i>and then stop? </i>As if by magic? What the flying fork?&nbsp;</p><p>In other words, he took actions he expected to lead to the virus to come back, and can’t figure out why it didn’t. Hint. It’s herd immunity plus heterogeneity (<a href=""https://www.medrxiv.org/content/10.1101/2020.09.01.20185876v1"">new study on that this week!</a>), and people making responsible private choices, you forking idiot.&nbsp;</p><p>Why oh why don’t people trust the experts?</p><p>Please. Please. Please. Do not hold this up as an example of a good leader. Do not consider this man for higher office. Being more responsible than the orange man is not the standard.</p><p>Again, everything here is as expected. It’s clear that things are improving slowly, and that our testing capacity has leveled off because no one cares to improve it.</p><p>Vitamin D As It Ought To Be</p><p>What is going on with Vitamin D?</p><p>For a long time, we have known that <i>not being deficient </i>in Vitamin D is important. Like many vitamins, if you don’t get enough, it’s not good. A lot of people are deficient. <a href=""https://eje.bioscientifica.com/view/journals/eje/180/4/EJE-18-0736.xml"">This study from Europe</a> leads the abstract off with this:&nbsp;</p><p>Vitamin D deficiency (serum 25-hydroxyvitamin D (25(OH)D) &lt;50 nmol/L or 20 ng/mL) is common in Europe and the Middle East. It occurs in &lt;20% of the population in Northern Europe, in 30–60% in Western, Southern and Eastern Europe and up to 80% in Middle East countries. Severe deficiency (serum 25(OH)D &lt;30 nmol/L or 12 ng/mL) is found in &gt;10% of Europeans.</p><p>There’s a reason they put Vitamin D into the American milk supply. Google says that the American ‘deficiency’ rate, defined as less than 11 nanograms/ML, is around 11%. <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6075634/"">This study</a> says the rate of ‘deficient’ Vitamin D, which they define as under 50 nanograms/ML, is way higher than that:</p><p>According to data collected between 2005 and 2006 by the National Health and Nutrition Examination Survey (NHANES), insufficient vitamin D levels were found in 41.6% of the 4495-individual sample size. Race was identified as a significant risk factor, with African-American adults having the highest prevalence rate of vitamin D deficiency (82.1%, 95% CI, 76.5%-86.5%) followed by Hispanic adults (62.9%; 95% CI, 53.2%-71.7%) [3]. Additional risk factors for vitamin D deficiency that were identified included obesity, lack of college education, and lack of daily milk consumption [3].</p><p>(The most relevant threshold for Covid-19, <a href=""https://chrismasterjohnphd.com/covid-19/five-new-vitamin-d-and-covid-19-studies"">as suggested by correlational studies</a>, is plausibly in the middle between those two, around 30).</p><p>As is often the case, even if Covid-19 is not a consideration, we are in need of <a href=""https://thezvi.wordpress.com/2017/12/02/more-dakka/"">More Dakka</a>. In theory you can overdose on Vitamin D, but it’s something that can only be done on purpose with extreme supplementation. <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4821095/"">Quantities as high as 10,000 ICU/day</a> (50k-100k per week) seem to be safe over extended periods.</p><p>For a long time, there have been studies trying to show that Vitamin D helps to prevent or treat various conditions. <a href=""https://www.devaboone.com/post/vitamin-d-part-3-the-evidence"">The verdict has mostly been negative</a>. Lots of trials for various things showed that Vitamin D had no effect. A few did show some effects, but we’ve learned to be suspicious that publication bias plus random effects explain such results. When I look at the net result, I see improvement in respiratory infections and in a few other places, with an emphasis on helping those that are deficient. It seems clear to me that there’s substantial benefits to supplementation when deficient, especially severely deficient. It does not seem clear that there is benefit to supplementation for those that are not otherwise deficient. But of course we don’t get outdoors much these days, and in California doing so would be dangerous and probably wouldn’t get you much Vitamin D – the sky is kind of the wrong color. So given the lack of downsides, supplementation seems like a freeroll already, but not a panacea.</p><p>The problem is that Vitamin D has been somewhat of a hype magnet. Many, including one person I know, claim it helps with pretty much everything quite a lot and it’s vital to immune response. Which might be true, but the results so far are not encouraging. This makes the Responsible Person headline to not listen to the hype and not get excited, rather than to point out that the odds still favor using it. Which is a pretty good microcosm of many parts of the general Covid-19 situation.&nbsp;</p><p>It’s clear that <i>correlational </i>studies of Vitamin D with disease find a strong link. Those without sufficient Vitamin D are at higher risk of pretty much everything. Hence the checking of Vitamin D as a solution to pretty much everything.&nbsp;</p><p>The going explanation is that causation at least partly runs in the other direction. If you are in poor health, you’re unlikely to get enough Vitamin D. It’s also plausible that those who supplement tend to have already been healthier as well. Old people with limited mobility are especially likely to be deficient. There are other causal pathways as well.&nbsp;</p><p>That’s also the central feature of the skeptical interpretation of Covid-19 results with regard to Vitamin D.</p><p>For a while, we’ve had data that suggests Vitamin D deficiency might play a role, potentially a large role, in who dies or gets severe illness from Covid-19 and who recovers. Some examples that were <a href=""https://www.lesswrong.com/posts/dzqvDyKTieGkNB4Ny/efficacy-of-vitamin-d-in-helping-with-covid"">quoted on LessWrong</a> were:</p><p>– A study in Indonesia found that out of the patients that died from COVID-19, 98.9% of them were deficient in vitamin D, while only 4% of the patients with sufficient vitamin D died.</p><p>-A study of patients in New Orleans found that 84.6% of the COVID-19 patients in the ICU were deficient in Vitamin D while only 4% of the patients in the ICU had sufficient levels of Vitamin D.</p><p>-A study in the Philippines found that for every standard deviation increase in vitamin D people were 7.94 times more likely to have a mild rather than severe COVID-19 outcome and 19.61 times more likely to have a mild rather than critical outcome.</p><p>Those are dramatic differences. They are often phrased in a way as to prevent extraction of the most meaningful information, but the effect sizes are too big for that to be that big a deal here. If we combine these numbers with our ranges for the baseline rate of deficiency above, it seems like a really, really big deal.</p><p>The alternate explanation, other than outright fraud which seems in context rather unlikely, is that this is correlation rather than causation. Being sick or prone to get sick means you become deficient, being deficient doesn’t make you get sick. Or maybe both have a common cause. Vitamin D could be a proxy for ethnicity or age, or something else.</p><p>Pause for a minute. Assume that this is fully reverse causation or common cause. What would that mean?</p><p>It would mean that if you got tested for Vitamin D without changing how you supplement or how much sunlight you get, and you have sufficient levels, <i>you are at very low risk!&nbsp;</i></p><p>So what if it’s correlational? It still counts for your purposes. Take the New Orleans numbers. If about 50% of people in the USA have sufficient Vitamin-D levels, and only 4% of ICU patients (and presumably deaths from Covid-19 would be similar) do, then being in the 50% with sufficient levels should lower your risk by roughly 92% from baseline, through some combination of getting infected less often and being at lower risk once infected. Instead of something like a 0.3% risk of death, you’re looking at more like 0.03% risk of death. Combine that with being under 50 years old, and even if there’s some double-counting there to correct for, you gotta ask. Unless you’d put someone vulnerable at risk, why are you letting another day of your life go by not living it to its fullest?&nbsp;</p><p>Maybe the other long term risks respond less dramatically. We don’t know, because somehow we have no data on them. Precautionary principle still somewhat applies. Sure. But look at what we’re giving up.</p><p><a href=""https://www.sciencedirect.com/science/article/pii/S0960076020302764"">Now we have a full RCT (although not a blind one)</a>, <a href=""https://chrismasterjohnphd.com/covid-19/finally-confirmed-vitamin-d-nearly-abolishes-icu-risk-in-covid-19"">and the effect size is gigantic</a>. They treated hospital patients with or without calcifediol (effectively D3), and used ICU admission as the end point. There were 50 patients in the treatment group. None were admitted to the ICU. There were 26 patients in the control group. <i>Half </i>of them, 13 out of 26, were admitted to the ICU. So 13/26 vs. 0/50. Two of the control group died, none of the treatment group.</p><p>Do whatever complicated calculations you want. This wasn’t luck.</p><p>Possibility zero is that this was luck. This wasn’t luck.&nbsp;</p><p>Possibility one is that this is real or mostly real. If that’s true, everyone supplements, patients get mega-dosed, and we can probably mostly resume our normal lives. This pandemic is no longer worth preventing.</p><p>Possibility two is that this is fraud. Always consider the possibility of fraud, especially when the effect size is this big. If anything, I’d like to think a fraud would choose results less dramatic than this, but the type of person who commits fraud doesn’t usually also carefully choose results to be believable or hard to disprove. Cheaters gonna cheat cheat cheat cheat cheat.&nbsp;</p><p>Possibility three is doctor bias. This was suggested to me, since ICU admittance is a human judgment call. But look at the effect size. To get this effect size primarily from bias would mean that ICU admission criteria are meaningless and/or completely ignored. This is no less of an outright fraud, except lives are being risked and damaged for the fraud. I find this implausible.</p><p>Possibility four is publication bias or something, but that would require a <i>ton </i>of other studies we don’t know about that we’re rather confident didn’t happen, and also it would be luck. This wasn’t luck.</p><p>Possibility five is that this has some relation to them <i>also </i>giving everyone hydroxychloroquine and azithromycin. They did this to both control and treatment groups, considering it the ‘standard of care.’ There’s a large group convinced hydroxychloroquine has been proven useless and another large group that thinks it has been proven effective and is the standard of care, and I’m not going to get into that right now. It was suggested to me that it’s <i>possible </i>that Vitamin D only worked here because of the additional treatment, but the other observations suggest to me that this isn’t true.</p><p>I basically see this as a ‘either real or a fraud’ situation. I don’t know how to evaluate the chances of fraud.</p><p><a href=""http://agingbiotech.info/vitamindcovid19/"">This paper</a> makes what I see as a convincing case that eliminating Vitamin D deficiency, at a minimum, should be a high priority. The paper thinks this is so well established for various reasons that further studies would not even be ethical. I think it’s almost <i>never </i>unethical to run tests to confirm your hypothesis when people weren’t otherwise going to get the treatment, because that’s ridiculous, but that’s “medical ethics” for you.</p><p>Our “medical ethics” is so screwed up that there is a very thin line between ‘allowed to do this study at all without being “unethical” to treatment group’ and ‘can no longer do the study without being “unethical” to control group despite them doing what they would have otherwise done anyway,’ so you need to grab your opportunities where you can get them.</p><p>Thus, hospitals are seen as <a href=""https://khn.org/news/dozens-of-u-s-hospitals-poised-to-defy-fdas-directive-on-covid-plasma/"">being in active defiance of guidelines regarding plasma treatments</a>, because they want to run experiments to find out if and how effectively it works, rather than turning on a dime from ‘this isn’t approved’ to ‘this is mandatory’ based on a highly politically motivated announcement.</p><p>I am supplementing 5000 IUs of Vitamin D daily. This is not medical advice because nothing I ever write has ever been or ever will be medical advice, “for legal reasons,” but I don’t see any substantial downside to that level. Again, it’s a freeroll. The more people that supplement, the better I’ll feel about it.</p><p>That’s the personal, practical side of this.&nbsp;</p><p>On a broader level, replication is the name of the game. We should be doing more and bigger RCTs as soon as possible, and either confirm this is real (and figure out the real effect size) or find out it was a fraud. Test both the megadose for patients, and the regular dose for regular people, although that will take longer to sort out.&nbsp;</p><p>The reasonable alternative would be to make the megadose the standard of care on the spot, given the effect size, decide that RCTs are no longer ethical, and see what happens. I think that’s a mistake, we need more data and it should only take a few weeks to get it, but I would understand it.</p><p>The unreasonable alternative is what we are doing. Which is nothing. But it makes sense that we’ve chosen that path. Of course we chose that path.&nbsp;</p><p>I’ll conclude with some thoughts on that, but first, In Other News:</p><p>The Perfect as the Enemy of the Good</p><p><a href=""https://www.statnews.com/2020/09/08/astrazeneca-covid-19-vaccine-study-put-on-hold-due-to-suspected-adverse-reaction-in-participant-in-the-u-k/"">AstraZeneca Covid-19 vaccine study is put on hold</a> because <i>one </i>person had an adverse reaction that might or might not be related to the vaccine. One. Out of thirty thousand. With neurological symptoms that seem on their face to be something entirely unrelated. Who is expected to recover.&nbsp;</p><p>Now, somehow, the trial is on indefinite pause (it’s unclear what this means, but presumably no new participants and potentially no new doses either, so it could potentially invalidate the whole thing) and the entire vaccine is in jeopardy.&nbsp;</p><p>Bloomberg’s news summary put it this way:</p><p>While AstraZeneca Chief Executive Officer Pascal Soriot told a private group of investors the precise diagnosis of the adverse event remains unclear, it could represent a serious setback for efforts to get a Covid-19 vaccine to the world quickly: If AstraZeneca’s review finds the adverse event is related to the shot, all the doses it has already manufactured will be thrown away, Collins said.</p><p>In other words, if <i>one person </i>has an adverse event linked to the vaccine, the vaccine is considered useless. All doses will be destroyed.&nbsp;</p><p>If the vaccine <i>ever </i>does <i>anything </i>seriously bad, it’s no good. That’s it. Game over, man. Game over.</p><p>You. Fail. Statistics. Forever.</p><p>This is utterly insane.&nbsp;</p><p>We could be more like Russia, which already is going ahead with its vaccine without waiting. Its results continue to look promising: <a href=""https://twitter.com/BallouxFrancois/status/1301874262047887360"">Immunogenic in all 76 volunteers, with good neutralising antibody and cell-mediated response. No serious side effects recorded.</a>&nbsp;</p><p>Or meet somewhere in the middle. As Tyler Cowen points out, <a href=""https://marginalrevolution.com/marginalrevolution/2020/09/challenge-trials-now.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+marginalrevolution%2Ffeed+%28Marginal+Revolution%29"">we could easily already have a vaccine ready</a> at trivial additional cost.</p><p>Instead, well, whoops.</p><p>Football! Football! Football! Football! Football! Football! Football! Football! Football! Football!</p><p>Woo-hoo!</p><p>We have football. Ergo, we might have peace.</p><p>The NFL returns this weekend. That’s excellent news. So too does much of college football. That’s also excellent news.</p><p>So too, in some stadiums, do a lot of the fans. That’s… not as excellent.</p><p>I’ve argued repeatedly that not only is football urgently needed, it is not substantially riskier for its players than it was in 2019 – worries about concussions and other injuries are a much bigger deal than Covid-19 risk, as the newly realistic discussions of the NFL make very clear. For college, being on the team is likely <i>safer </i>than the same kid would be as a regular student, as you get better testing and better protocols and less parties.&nbsp;</p><p>None of that applies to <i>fans attending games. </i>Fans at football games are rowdy, completely out of control, yelling at the players and with each other not only because it is fun, but <i>intentionally to create strategic noise. </i>They’re drunk before they get to the stadium and then they keep drinking. The idea that such people could ‘socially distance’ or act ‘responsibly’ is obvious nonsense.</p><p>And yet we read about things like <a href=""https://www.inforum.com/bison-media-zone/mens-sports/football/6643504-McFeely-In-midst-of-pandemic-Bison-invite-10000-to-game"">North Dakota State University allowing 10,000 fans into the stadium</a>. Many other college stadiums are doing so as well.&nbsp;</p><p>The problem with stupid people with bad motives doing the right thing for the wrong reason is that the next thing they do for the wrong reason is often going to be the wrong thing.&nbsp;</p><p>Our problem of dumb people organizing themselves around being dumb is not football. It’s college.</p><p>School Hard With a Vengeance</p><p>That is not fair either to football or to college.&nbsp;</p><p>I try to be fair to school. They don’t make it easy.&nbsp;</p><p>This week, we have a student suspended for the display of a toy gun on his zoom video during a virtual class. Think about that. A <i>toy </i>gun, in a <i>video shot, </i>and he got suspended.&nbsp;</p><p>No, wait. That’s the first version of the story I heard. <a href=""https://t.co/013DK0e5mY"">Here’s the full one (WaPo)</a>. <i>They sent the cops to his home. </i>He now has a permanent record that he ‘brought a gun to school.’</p><p>We have regulations calling for punishment if children are not wearing shoes. For a zoom class. In their own house.</p><p>Colleges are charging students tens of thousands of dollars <i>per semester, </i>then kicking students out on the first offense for violating social distancing guidelines that actual everyone will violate from time to time, <i>and keeping the north of $30,000 per semester tuition money. </i>That they often have students take on non-dischargeable loans to pay. As one Twitter account put it, one dean who had just seen Charlie and the Chocolate Factory had an idea.&nbsp;</p><p>Other schools are requiring students to be on campus and pay five figures for a tiny dorm room, because otherwise they won’t be able to do their single lab or other in-person required course, which would make them fail the whole semester.&nbsp;</p><p>Many schools are taking away the scholarships of students who refuse to come on campus for safety reasons, and would prefer to wait things out remotely.&nbsp;&nbsp;</p><p>Then these schools put out <a href=""https://www.medrxiv.org/content/10.1101/2020.08.26.20182352v1"">studies that call their students ‘an enigma’</a> because they only are willing to pay half of their outrageous tuition for virtual classes that don’t allow them to party and otherwise do what they came to college to do, but then when they do go to college, they do what they came to college to do, or in academic parlance, ‘engage in activities that present significant barriers to holding in-person classes.’&nbsp;</p><p><a href=""https://www.youtube.com/watch?v=4XkPUwwYpA0&amp;ab_channel=MoviQuotes"">It’s a mystery!</a></p><p>We are home schooling our son. We are constantly pressured from all sides to torture him as if he were being tortured in a real school, because at some point in the future we will need to send him to a real school to be tortured, and if we don’t torture him exactly the same ways now, he <i>won’t be prepared.&nbsp;</i></p><p>Oh, and this one is my new personal favorite. Iowa Covid-19 rules say if you are close to a Covid-positive person for 15 minutes you need to quarantine… <a href=""https://iowastartingline.com/2020/09/09/musical-chairs-iowa-school-districts-new-way-to-skirt-covid-rules/"">so an Iowa school is shuffling the kids’ seats every 12-14 minutes</a>.&nbsp;</p><p>We think this belongs in r/funny instead of r/childabuse:</p><figure><img src=""https://lh3.googleusercontent.com/CvMhJm5Hi1CSUbmCshhi47QXjAZocJM3MSZ7Uzo_bk-Hpt_gzLKgRaw5SLnFoMR-7XIFGghU6QkSXRQ0HGNF4zQDBMYsYH5z1vnX6kvAZISgR-ohgDZwjK_ecwXGn-U_MtKNAejZ""></figure><p>If you’re wondering why we see headlines like <a href=""https://www.bbc.com/news/uk-53884401"">Coronavirus: Teens’ anxiety levels dropped during pandemic, study finds</a>, think about it and you’ll figure it out.</p><p>Every day it becomes more and more clear what school is for and what it is about. The answer is most definitely not learning reading, writing or arithmetic.&nbsp;</p><p>Let that boy go outside. Maybe he’ll get to be a kid, and soak up some Vitamin D.</p><p>Imagine the Epilogue</p><p>An interesting exercise is to imagine that we find out that Vitamin D is the solution, it really does reduce risk from Covid-19 by 90%+, and by the end of the year life has mostly returned to normal, with some vulnerable people continuing to take precautions but everyone else accepting that life is never fully safe.&nbsp;</p><p>What happens in that world? How do people react? What behaviors change? How do they judge? Who rises? Who falls?</p><p>Take a moment and model what you think happens.</p><p>My guess is that the primary reaction is <i>righteous fury.&nbsp;</i></p><p>We put our lives on hold for <i>six months? </i>And all we had to do was take a <i>vitamin pill? </i>That we mostly should have taken anyway? And all our best people, all our “health experts,” all the “science” we listen to, couldn’t figure that out?</p><p>Hell hath no fury. People would believe what they were told even less. Next time, they’d turn to a bunch of quacks hawking miracle cures, even more than they are already inclined to do that – and they’re pretty inclined to do that as it is.</p><p>In order words, everyone with authority and power pisses the public off and gets kicked in the nuts, and what little institutional credibility still exists is destroyed. Whoever gets to push the solution officially gets some of that back, but probably not too much. Yes, you found the solution eventually, but what about that whole time before?&nbsp;</p><p>Good luck getting any kind of cooperation on anything, ever again, from pretty much anyone.</p><p>There would also of course be great relief, as we rapidly get to return things to normal. There would of course be a chorus of Very Serious People saying that even now we can’t return to normal, and we need to wait for a vaccine, and so on and so on. They might even be listened to and inflict massive damage, both general economic and reputational upon themselves.&nbsp;</p><p>The whole thing would be looked back on as a farce, our civilization as pathetic.&nbsp;</p><p>So maybe there’s upside to the whole thing. If we see ourselves as a pathetic farce that can’t do things, we can work on fixing that. It’s a lot better than what seem like the popular alternative positions: Either fooling ourselves into thinking that we’re great and can still do great things without having to fix things, or being fooled into thinking that we’re irredeemably evil and need to no longer have nice things because we don’t deserve them, and should promptly abandon all our core values and our prosperity.&nbsp;</p><p>In any case, for now on the margin, it would be wrong not to consider this all very good news. We have a lot more of a locus of control over our lives and our fate than we did last week. Use it.</p>",Zvi,zvi,Zvi,
H3wdw2cLNLpcF8pXA,Social Capital Paradoxes,social-capital-paradoxes,https://www.lesswrong.com/posts/H3wdw2cLNLpcF8pXA/social-capital-paradoxes,2020-09-10T18:44:18.291Z,69,25,24,False,True,,"<p><i>[Credit for horizontally transmitting these ideas to my brain goes mostly to Jennifer RM, except for the bits at the end about Bowling Alone and The Moral Economy. Apologies to Jennifer for further horizontally spreading.]</i></p><h1>Vertical/Horizontal Transmission</h1><p>The concept of vertical and horizontal transmission felt like a big upgrade in my ability to think about cooperative/noncooperative behavior in practice. The basic idea is to distinguish between symbiotes that are passed on primarily along genetic lines, vs symbiotes which are passed on primarily between unrelated organisms. A symbiote which is vertically transmitted is very likely to be helpful, whereas a symbiote which is horizontally transmitted is very likely to be harmful. (Remember that in biology, ""symbiote"" means any kind of close relationship between different organisms; symbiosis which is useful to both organisms is <i>mutualistic</i>, while symbiosis which is useful to one but harmful to another is <i>parasitic</i>.) (This is discussed here on LW in Martin Sustrik's <a href=""https://www.lesswrong.com/posts/MwDaasGo92QXPmDsj/coordination-problems-in-evolution-the-rise-of-eukaryotes""><i>Coordination Problems in Evolution</i></a><i>.</i>)</p><p>We can obviously generalize this quite a bit.&nbsp;</p><ul><li>Infectious diseases tend to be more deadly the higher their transmission rate is. (Diseases with a low transmission rate need to keep their hosts relatively healthy in order to make contact with other potential hosts.)</li><li>Memes which spread vertically are more likely to be beneficial to humans than memes which spread horizontally (at least, beneficial to those human's genes). Religions which are passed through family lines have an incentive to encourage big families, and include ideas which promote healthy, wealthy, sustainable living. Religions which spread primarily to unrelated people have a greater incentive to exploit those people, squeezing every last drop of proselytization out of them.</li><li>Long-term interactions between humans are more likely to be mutualistic, while short-term interactions are more likely to be predatory.</li><li>In general, cooperative behavior is more likely to arise in iterated games; moreso the more iterations there are, and the more probable continued iteration is.</li></ul><p>Vertical transmission is just a highly iterated game between the genes of the host and the genes of the symbiote.&nbsp;</p><h1>Horizontal Transmission Abounds</h1><p>Wait, but... horizontal transmission appears to be the norm all over the place, including some of the things I hold most dear!</p><ul><li>Religion and tradition tend to favor vertical transmission, while science, education, and reason favor horizontal transmission.</li><li>Free-market economies seem to favor a whole lot of single-shot interactions, rather than the time-tested iterated relationships which would be more common in earlier economies.<ul><li>To this day, small-town culture favors more highly iterated relationships, whereas big-city culture favors low-iteration. (I've had a decent amount of experience with small-town culture, and a common sentiment is that you have to live somewhere for 20 years before people trust you and treat you as a full member of the community.)</li></ul></li></ul><p><strong>Paradox One:</strong> <i>A lot of good things seem to have a horizontal transfer structure. Some things which I tend to regard with more suspicion have a vertical flavor.</i></p><h1>Horizontal Transmission Seems Wonderful</h1><ul><li>The ability to travel easily from community to community allows a person to find the work, cultural environment, and set of friends that's right for them.</li><li>Similarly, the ability to work remotely can be a huge boon, by allowing separate selection of workplace and living environment.</li><li>The first thing I want to do when I hear that vertically-transmitted religion has beneficial memes is to try and get more of those memes for myself!</li><li>Similarly, I've read that many bacteria have the ability to pick up loose genetic material from their environment, and incorporate it into their own genes. (See <a href=""https://en.wikipedia.org/wiki/Horizontal_gene_transfer"">horizontal gene transfer</a>.) This can be beneficial if those genes are from organisms adapted to the local environment.</li></ul><p><strong>Paradox Two:</strong> <i>In an environment where horizontal transfer is rare, opening things up for more horizontal transfer is usually pretty great. But an open environment gives rise to bad dynamics which incentivize closing down.</i></p><p>If you're in a world where people only ever trade with highly iterated partners, there is probably a lot of low-hanging fruit to be had from trading with a large number of untrusted partners. You could arbitrage price differences, get goods from areas where they're abundant to areas where they're scarce, and generally make a big profit while legitimately helping a lot of people. All for the low price of opening up trade a little bit.</p><p>But this threatens the environment of trust and goodwill that you're relying on. An environment with more free trade is one with more scammers, inferior goods, and outright thieves.</p><p><a href=""https://www.lesswrong.com/posts/RXLbe6oZGxNWvQawn/the-youtube-revolution-in-knowledge-transfer"">YouTube is great for learning things</a>, but it's also full of absolutely terrible demonstration videos which purport to teach you some skill, but instead offer absurd and underdeveloped techniques (these videos are often called ""lifehacks"" for some reason, if you're unfamiliar with the phenomenon and want to search for it). The videos are being optimized for transmission rather than usefulness. Acquiring useful information requires prudent optimization against this.</p><h1>Social Capital</h1><p><i>Social Capital</i> is, roughly, the amount of trust you have within a group. <i>Bowling Alone</i> is a book which researches America's decline in social capital over the course of the 1900s. Trust in the goodwill of strangers took a dramatic dive over that time period, with corresponding negative consequences (EG, the decline in hitchhiking, the rise of helicopter parenting).&nbsp;</p><p>You might think this is due to the increasingly ""horizontal"" environment. More travel, more free-market capitalism, bigger cities, the decline of small towns; more horizontal spread of memes, by print, radio, television, and internet; more science and education.&nbsp;</p><p>And you might be right.</p><p>But, counterpoint:</p><p><strong>Paradox Three: </strong><i>Free-market societies have higher social capital. </i>Citation: <i>The Moral Economy, </i>Samuel Bowles.&nbsp;</p><p>More generally: a lot of things are a lot better than naive horizontal/vertical thinking would suggest. I've already mentioned that a lot of the things I hold dear seem to have a pretty horizontal transmission model. I <i>don't </i>think that's just because I've been taken over by virulent memes.</p><p>By the way, my favorite explanation of the decline in social capital over the 1900s is this: there was, for some reason, a huge burst of club-making in the late 1800s, which continued into the early 1900s. These clubs were often very civically active, contributing to a common perception that everyone cooperates together to improve society. This culminated in an extremely high degree of social capital in ""The Greatest Generation"" -- however, that generation was already starting to forget the club-making/club-attending culture which had fuelled the increase in social capital. Television ultimately killed or put the damper on the clubs, because most people wanted to catch their favorite shows in the evening rather than go out. Social capital gradually declined from then on.</p><p>(But, doubtless, there was more going on than just this, and I have no idea how big a factor club culture really plays.)</p><h1>Questions</h1><ol><li>Why do so many good things have horizontal transmission structures?</li><li>How should we think about horizontal transmission, normatively? Specifically, ""paradox two"" is an argument that horizontal-transmission practices, while enticing, can ""burn the commons"" of collective goodwill by opening up things for predatory/parasitic dynamics. Yet the conclusion seems severe and counterintuitive.</li><li>Why do free-market societies have higher social capital? How can this be fit into a larger picture in which horizontal transmission structures / few-shot interactions incentivize less cooperative strategies?</li></ol>",abramdemski,abramdemski,abramdemski,
j5foHZhZ7RBhwRL7Z,Do mesa-optimizer risk arguments rely on the train-test paradigm?,do-mesa-optimizer-risk-arguments-rely-on-the-train-test,https://www.lesswrong.com/posts/j5foHZhZ7RBhwRL7Z/do-mesa-optimizer-risk-arguments-rely-on-the-train-test,2020-09-10T15:36:37.629Z,12,7,7,False,True,,"<p>Going by the <a href=""https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB"">Risks from Learned Optimization sequence</a>, it's not clear if mesa-optimization is a big threat if the model continues to be updated throughout deployment. I suspect this has been discussed before (links welcome), but I didn't find anything with a quick search.</p>
<p>Lifelong/online/continual learning is popular and could be the norm in future. I'm interested in how that (and other learning paradigms, if relevant) fits into beliefs about mesa-optimization risk.</p>
<p>If you believe the arguments hold up under a lifelong learning paradigm: is that because there could still be enough time between updates for the mesa-optimizer to defect, or some other reason? If you believe the train-test paradigm is likely to stick around, why is that?</p>
",ben-cottier,ben-cottier,Ben Cottier,
NMxmGFhvncg9WfYkP,On Chesterton's Fence,on-chesterton-s-fence,https://www.lesswrong.com/posts/NMxmGFhvncg9WfYkP/on-chesterton-s-fence,2020-09-10T15:10:42.321Z,21,12,3,False,False,,"<p>TLDR; Chesterton’s Fences are important, and very hard to identify/evaluate. With finite time, bountiful stupidity and inflated egos, it is too easy to not look deeply enough at existing ways of doing things and understand why they are the way they are before attempting to “fix” them. Reading <a href=""https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/"">Secrets of our Success</a> and <a href=""https://slatestarcodex.com/2017/03/16/book-review-seeing-like-a-state/"">Seeing Like a State</a> has strengthened my prior to dig deeper on <em>why</em> things are done, in proportion to how long they have stood the tests of time. Writing this piece has helped me develop a framework (in the form of fitness landscapes) to think about Chesterton’s Fences and how uncovering both the motivations and mechanisms behind them is often intractable, requiring clever trial and error along with the acceptance of unfortunate, unintended consequences.</p>
<hr>
<p><a href=""https://en.wikipedia.org/wiki/Wikipedia:Chesterton%27s_fence"">Chesterton's Fence</a> states that if you encounter a fence in the middle of nowhere, you should stop and first understand why it was put there before taking it down. There is probably a good reason the fence is there in the first place, and finding out the hard way might be really bad and irreversible. Chesterton's Fence was the <a href=""https://slatestarcodex.com/2013/02/12/youre-probably-wondering-why-ive-called-you-here-today/"">original motivation</a> for the creation of Slate Star Codex and is a principle I have thought a lot about recently while reading <a href=""https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/"">Secrets of our Success</a> and <a href=""https://slatestarcodex.com/2017/03/16/book-review-seeing-like-a-state/"">Seeing Like a State</a>.</p>
<p>Both of these books convey endless appreciation for the complexity, nuance, and unintended consequences that local expertise accounts for and the naive outsider ignores at risk of their own demise. Cherry picking some fascinating examples:</p>
<blockquote>
<p>""As one of the world’s staple crops, manioc (or cassava) is a highly productive, starch-rich tuber that has permitted relatively dense populations to inhabit drought-prone tropical environments. However, depending on the variety of manioc and the local ecological conditions, the tubers can contain high levels of cyanogenic glucosides, which release toxic hydrogen cyanide when the plant is eaten. If eaten unprocessed, manioc can cause both acute and chronic cyanide poisoning. Chronic poisoning, because it emerges only gradually after years of consuming manioc that tastes fine, is particularly insidious and has been linked to neurological problems, developmental disorders, paralysis in the legs, thyroid problems (e.g., goiters), and immune suppression. These so-called ”bitter” manioc varieties remain highly productive even in infertile soils and ecologically marginal environments, in part due to their cyanogenic defenses against insects and other pests. In the Americas, where manioc was first domesticated, societies who have relied on bitter varieties for thousands of years show no evidence of chronic cyanide poisoning. In the Colombian Amazon, for example, indigenous Tukanoans use a multistep, multiday processing technique that involves scraping, grating, and finally washing the roots in order to separate the fiber, starch, and liquid. Once separated, the liquid is boiled into a beverage, but the fiber and starch must then sit for two ﻿more days, when they can then be baked and eaten. Figure 7.1 shows the percentage of cyanogenic content in the liquid, fiber, and starch remaining through each major step in this processing.</p>
<p><img src=""https://github.com/TrentBrick/trentbrick.github.io/blob/master/images/ChestertonFence/cassava.png?raw=true"" alt=""Cassava""></p>
<p>Such processing techniques are crucial for living in many parts of Amazonia, where other crops are difficult to cultivate and often unproductive. However, despite their utility, one person would have a difficult time figuring out the detoxification technique. Consider the situation from the point of view of the children and adolescents who are learning the techniques. They would have rarely, if ever, seen anyone get cyanide poisoning, because the techniques work. And even if the processing was ineffective, such that cases of goiter (swollen necks) or neurological problems were common, it would still be hard to recognize the link between these chronic health issues and eating manioc.</p>
<p>Most people would have eaten manioc for years with no apparent effects. Low cyanogenic varieties are typically boiled, but boiling alone is insufficient to prevent the chronic conditions for bitter varieties. Boiling does, however, remove or reduce the bitter taste and prevent the acute symptoms (e.g., diarrhea, stomach troubles, and vomiting). So, if one did the common-sense thing and just boiled the high-cyanogenic manioc, everything would seem fine. Since the multistep task of processing manioc is long, arduous, and boring, sticking with it is certainly nonintuitive. Tukanoan women spend about a quarter of their day detoxifying manioc, so this is a costly technique in the short term.</p>
<p>At the beginning of the seventeenth century, the Portuguese transported manioc from South America to West Africa for the first time. They did not, however, transport the age-old indigenous processing protocols or the underlying commitment to using those techniques. Because it is easy to plant and provides high yields in infertile or drought-prone areas, manioc spread rapidly across Africa and became a staple food for many populations. The processing techniques, however, were not readily or consistently regenerated. Even after hundreds of years, chronic cyanide poisoning remains a serious health problem in Africa. Detailed studies of local preparation techniques show that high levels of cyanide often remain and that many individuals carry low levels of cyanide in their blood or urine, which haven’t yet manifested in symptoms. In some places, there’s no processing at all, or sometimes the processing actually increases the cyanogenic content. On the positive side, some African groups have in fact culturally evolved effective processing techniques, but these techniques are spreading only slowly.""<sup class=""footnote-ref""><a href=""#fn-rN92unFSM7khogMCk-1"" id=""fnref-rN92unFSM7khogMCk-1"">[1]</a></sup> - Joseph, Henrich. The Secret of Our Success (p. 95-97). Princeton University Press.</p>
</blockquote>
<p>An example on the forced villagization of Tanzania in the 1970s:</p>
<blockquote>
<p>""A typical cultivator in Tigray, a location singled out for harsh measures, planted an average of fifteen crops a season (such cereal crops as teff, barley, wheat, sorghum, corn, millet; such root crops as sweet potatoes, potatoes, onions; some legumes, including horsebeans, lentils, and chickpeas; and a number of vegetable crops, including peppers, okra, and many others). It goes without saying that the farmer was familiar with each of several varieties of any crop, when to plant it, how deeply to sow it, how to prepare the soil, and how to tend and harvest it. This knowledge was place specific in the sense that the successful growing of any variety required local knowledge about rainfall and soils, down to and including the peculiarities of each plot the farmer cultivated. It was also place specific in the sense that much of this knowledge was stored in the collective memory of the locality: an oral archive of techniques, seed varieties, and ecological information.</p>
<p>Once the farmer was moved, often to a vastly different ecological setting, his local knowledge was all but useless. As Jason Clay emphasizes, “Thus, when a farmer from the highlands is transported to settlement camps in areas like Gambella, he is instantly transformed from an agricultural expert to an unskilled, ignorant laborer, completely dependent for his survival on the central government.” ...</p>
<p>Instead of the unrepeatable variety of settlements closely adjusted to local ecology and subsistence routines and instead of the constantly changing local response to shifts in demography, climate, and markets, the state would have created thin, generic villages that were uniform in everything from political structure and social stratification to cropping techniques. The number of variables at play would be minimized. In their perfect legibility and sameness, these villages would be ideal, substitutable bricks in an edifice of state planning. Whether they would function was another matter."" - James C. Scott, Seeing Like a State (p. 250-255). Yale University Press.</p>
</blockquote>
<p>This example and its context in the book presents a meta-Chesterton Fence. At the levels of each individual tribe, farmer, and even plot of land, there is a unique environment that cultural and agricultural expertise evolved to harness over time.</p>
<p>On a seemingly silly but potentially highly effective hunting strategy:</p>
<blockquote>
<p>""When hunting caribou, Naskapi foragers in Labrador, Canada, had to decide where to go. Common sense might lead one to go where one had success before or to where friends or neighbors recently spotted caribou. ... Hunters want to match the locations of caribou while caribou want to mismatch the hunters, to avoid being shot and eaten. If a hunter shows any bias to return to previous spots, where he or others have seen caribou, then the caribou can benefit (survive better) by avoiding those locations (where they have previously seen humans). Thus, the best hunting strategy requires randomizing. Can cultural evolution compensate for our cognitive inadequacies?</p>
<p>Traditionally, Naskapi hunters decided where to go to hunt using divination and believed that the shoulder bones of caribou could point the way to success. To start the ritual, the shoulder blade was heated over hot coals in a way that caused patterns of cracks and burnt spots to form. This patterning was then read as a kind of map, which was held in a prespecified orientation. The cracking patterns were (probably) essentially random from the point of view of hunting locations, since the outcomes depended on myriad details about the bone, fire, ambient temperature, and heating process. Thus, these divination rituals may have provided a crude randomizing device that helped hunters avoid their own decision-making biases."" - Joseph, Henrich. The Secret of Our Success (p. 104-105). Princeton University Press.</p>
</blockquote>
<p>An interesting tidbit on how difficult it is for humans to be random from Scott Aaronson:</p>
<blockquote>
<p>In a class I taught at Berkeley, I did an experiment where I wrote a simple little program that would let people type either “f” or “d” and would predict which key they were going to push next. It’s actually very easy to write a program that will make the right prediction about 70% of the time. Most people don’t really know how to type randomly. They’ll have too many alternations and so on. There will be all sorts of patterns, so you just have to build some sort of probabilistic model. Even a very crude one will do well. I couldn’t even beat my own program, knowing exactly how it worked. I challenged people to try this and the program was getting between 70% and 80% prediction rates. Then, we found one student that the program predicted exactly 50% of the time. We asked him what his secret was and he responded that he “just used his free will.” - Try it for yourself <a href=""https://people.ischool.berkeley.edu/~nick/aaronson-oracle/"">here</a>, <a href=""https://github.com/elsehow/aaronson-oracle"">source</a>, <a href=""https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/"">hat-tip</a>.</p>
</blockquote>
<p>In all of the examples provided, failure to acknowledge local expertise, painfully acquired through trial and error and retained across generations, leads to a suboptimal outcome. This suboptimality is not only for the very metric desired, for example, crop yields, but also for overall welfare where the Tanzanian’s crops became more vulnerable, less nutritionally diverse and their community ties were severed. There are many further interesting examples of Chesterton’s Fence in the two books, from rituals for pregnant mothers to not eat shark, to the perils of scientific forestry and the high-modernist planning ideals that made Brasilia one of the least livable cities on earth.</p>
<p>Beyond creating a suboptimal outcome for the desired metric, the very creation of this metric induces <a href=""https://en.wikipedia.org/wiki/Goodhart%27s_law"">Goodhart's Law</a>: ""When a measure becomes a target, it ceases to be a good measure."" often because the metric is exploited. An amusing yet tragic example from Seeing Like a State is:</p>
<blockquote>
<p>""The door-and-window tax established in France under the Directory [1795] and abolished only in 1917 is a striking case in point. Its originator must have reasoned that the number of windows and doors in a dwelling was proportional to the dwelling’s size. Thus a tax assessor need not enter the house or measure it but merely count the doors and windows. As a simple, workable formula, it was a brilliant stroke, but it was not without consequences. Peasant dwellings were subsequently designed or renovated with the formula in mind so as to have as few openings as possible. While the fiscal losses could be recouped by raising the tax per opening, the long-term effects on the health of the rural population lasted for more than a century."" - James C. Scott, Seeing Like a State (pp. 47-48). Yale University Press.</p>
</blockquote>
<p>I find Chesterton's Fence a very useful principle that we are prone to forget. However, from reading page after page about the failures of societal interventions, I feel the sheer complexity of the natural world and severity of the unintended consequences of our actions can induce a state of ""<a href=""https://slatestarcodex.com/2019/06/03/repost-epistemic-learned-helplessness/"">epistemic learned helplessness</a>"" that leads me into the comforting arms of the <a href=""https://en.wikipedia.org/wiki/Appeal_to_nature"">appeal to nature</a>. So what should we keep in mind as we gaze upon this fence in the middle of nowhere and consider dismantling it?</p>
<p>I think it is useful to perceive Chesterton’s Fence as a local optimum on a very non-linear, high dimensional fitness landscape. Evolution, primarily of our culture but also of our genome, has converged on this local optimum through trial, error, and improvement. This local optimum is often deeper and more optimal than the naive outside observer is aware. Dismantling Chesterton's Fence shifts our current solution’s parameters away from this optimum and the search for a new optimum can be expensive, slow, and result in a worse new solution we can’t backtrack from.</p>
<p>I like this fitness landscape framing for a few reasons:</p>
<p>Fitness landscapes are often intractable to optimize because either the evaluation of a particular solution is too expensive or the space is too large to be effectively searched. In the case of modifying our lives, social norms, or society, both of these difficulties are present. As a result, we are basically blind in taking steps across the state space and need to use trial and error, making these steps very small so that they are cheap to perform and reversible.<sup class=""footnote-ref""><a href=""#fn-rN92unFSM7khogMCk-2"" id=""fnref-rN92unFSM7khogMCk-2"">[2]</a></sup> This means running experiments (ideally RCTs) and using evidence from them to constantly diagnose how new solutions are performing on old problems, and inform directions in which further improvements are sought.</p>
<p>But even RCTs aren't enough. Seeing Like a State highlights well-intentioned failures of scientific agriculture as the result of failing to test solutions in sufficiently diverse environments. For any scientific experiment there are only so many variables that can be feasibly tested and controlled for. Not only is this number often much smaller than necessary to provide findings that work in the real world, but also it is plausible that no solution will generalize across such diverse environments. It is within this context that Seeing Like a State argues science should have a deeper appreciation for cultural practices and tacit knowledge, at the very least using them as a form of hypothesis generation.</p>
<p>For example, traditional medicinal practices have been a boon to drug discovery. Western medicine uses RCTs to assess efficacy but very often lacks a mechanistic understanding of <em>why</em> something works. We are still uncovering the mechanisms underlying <a href=""https://www.frontiersin.org/articles/10.3389/fimmu.2017.00261/full"">aspirin</a> and one of the most successful cancer drugs, <a href=""https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.816.4552&amp;rep=rep1&amp;type=pdf"">paclitaxel</a>, was discovered after sprinkling a compound from the bark of a Pacific Yew tree on cancer cells and still has unknown mechanisms.</p>
<p>Another reason the fitness landscape is a useful framing for Chesterton’s Fence is that the “fitness” used to determine a landscape is arbitrary. The <a href=""https://en.wikipedia.org/wiki/No_free_lunch_theorem"">No Free Lunch Theorem</a> states that if we were to average the quality of our solution across all possible definitions of fitness, all solutions would perform equally. Therefore, we must first define fitness. Moreover, the fitness landscape our culture and genomes have been optimizing for in the past have been defined by evolution, not the ideals of a modern liberal democracy. And as causally opaque and crucial some rituals are to survival, such as cassava processing, there also exist clearly harmful and unnecessary practices such as female genital mutilation.</p>
<p>On the problem of defining fitness, I wish there was more discussion about utopias to know what end goals or ideals we should be optimizing for. I'd love suggestions for Utopian reading, the most compelling political system/utopia I have encountered is: <a href=""https://slatestarcodex.com/2014/06/07/archipelago-and-atomic-communitarianism/"">this</a> but my reference class is very small<sup class=""footnote-ref""><a href=""#fn-rN92unFSM7khogMCk-3"" id=""fnref-rN92unFSM7khogMCk-3"">[3]</a></sup>.</p>
<p>Aside from “fitness” being arbitrary, the landscape is constantly changing as our physical and societal environments present new ridges and valleys for the existing local optimum that culture and evolution are always optimizing. For example, accelerating technological progress is making <a href=""http://www.paulgraham.com/addiction.html"">""increasing numbers of things we like [...] into things we like too much""</a> by becoming addictive, hacking our reward pathways like an <a href=""https://www.washingtonpost.com/news/answer-sheet/wp/2013/10/18/rats-find-oreos-as-addictive-as-cocaine-an-unusual-college-research-project/"">Oreo</a>. In this context, the status quo is maintained only with the presence, not absence of efforts to resist novel addictions.</p>
<p>Seeing Like a State certainly acknowledges the expensive search for often worse local optima ridden with unintended consequences. However, it also recognizes that this disruption is necessary to enable greater State intervention, which in turn can produce great outcomes. This is because local culture is too nuanced to be measured, summarized, and understood from a central decision making body. Greater State intervention is a prerequisite to the public health, sanitation and welfare miracles of the last couple centuries. Therefore, uprooting Chesterton’s Fences can be a long term investment by the State in better wellbeing, leading to superior local optima.</p>
<p>I believe that the concept of Chesterton's Fence is powerful. Obtaining more information before taking any action is always desirable. However, this information acquisition can often be too costly, the Fence may rest on dubious morals, and be transforming already and independently. Under these circumstances, Chesterton’s Fences should be challenged in the name of human flourishing, but only with great humility, careful experimentation, and apologies for the inevitable unintended consequences. Good intentions alone, as Seeing Like a State documents in detail, are not enough and can be downright dangerous because of the dramatic changes they inspire. A quote from <a href=""https://www.goodreads.com/quotes/579991-not-every-change-is-an-improvement-but-every-improvement-is"">Yudkowsky</a> comes to mind that can be modified to read: ""not all changes to Chesterton’s Fences lead to better outcomes but all better outcomes come from changes to Chesterton’s Fences."" Thus, with a hat tip to the No Free Lunch Theorem, let us find humanity a restaurant where lunch isn't free but the greatest number finds it tastier.</p>
<p>Thanks very much to <a href=""https://twitter.com/joechoochoy"">Joe Choo-Choy</a>, Max Farrens, and <a href=""https://twitter.com/milesaturpin"">Miles Turpin</a> for reading drafts of this piece.</p>
<p>The original post has been cross posted from: <a href=""https://trentbrick.github.io/On-Chestertons-Fence/"">https://trentbrick.github.io/On-Chestertons-Fence/</a> .</p>
<h3>Footnotes</h3>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-rN92unFSM7khogMCk-1"" class=""footnote-item""><p>It would be interesting to know if the processing techniques used by the Amazonians and lacking in much of Africa were developed simply because of time, if the Amazonian diet is more restricted which would increase selection pressure, or if there are other analogous food processing techniques readily generalized from other foods already. <a href=""#fnref-rN92unFSM7khogMCk-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-rN92unFSM7khogMCk-2"" class=""footnote-item""><p>Joe Choo-Choy rightly points out the complication that steps must be sufficiently large to override  any noise naturally occurring to the parameters. <a href=""#fnref-rN92unFSM7khogMCk-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-rN92unFSM7khogMCk-3"" class=""footnote-item""><p>I like a quote in Seeing Like a State from <a href=""https://smile.amazon.com/Agricultural-Testament-Albert-Howard/dp/1849027730?sa-no-redirect=1"">Albert Howard</a>: “The discovery of the things that matter is three quarters of the battle.” <a href=""#fnref-rN92unFSM7khogMCk-3"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",trentbrick,trentbrick,trentbrick,
nfjJsNYjKrzqQn999,"In 1 year and 5 years what do you see as ""the normal"" world.",in-1-year-and-5-years-what-do-you-see-as-the-normal-world,https://www.lesswrong.com/posts/nfjJsNYjKrzqQn999/in-1-year-and-5-years-what-do-you-see-as-the-normal-world,2020-09-10T12:47:35.497Z,8,4,9,False,True,,"<p>We all have a mental image of pre-COVID normal.</p><p>I often hear people saying &quot;I cannot wait to get back to normal.&quot; or asking &quot;When will we get back to normal?&quot; I think that is an expectation that is sure to be disappointed. I suspect that is the case for most who read this site.</p><p>I&apos;m curious about the mental image of the near future normal that is held here. I&apos;ll list a few areas for thoughts but also don&apos;t think anyone should be limited in any thoughts they want to share.</p><p>1) International Travel -- can be general, tourism related or business related</p><p>2) Entertainment -- theater/movies, live sports and concerts. One thought here might be a move to more open air venues rather than indoors.</p><p>3) Social interactions in general. Does some of the zenophobia that has occurred persist or die away (say due to vaccines).</p><p>4) Will vaccines change things that much?</p>",jmh,jmh,jmh,
AxjFRgNStopAc93jB,Taking Hammertime Final Exam by efim,taking-hammertime-final-exam-by-efim,https://www.lesswrong.com/posts/AxjFRgNStopAc93jB/taking-hammertime-final-exam-by-efim,2020-09-10T10:18:18.459Z,5,5,0,False,False,,"<p><a href=""https://www.lesswrong.com/posts/AxjFRgNStopAc93jB/taking-hammertime-final-examI"">https://www.lesswrong.com/posts/AxjFRgNStopAc93jB/taking-hammertime-final-examI</a> am attempting to write about:</p><ul><li>A helpful framework </li><li>An exercise that might generally help in life</li><li>A blindspot</li></ul><p>It was hard to choose topics between several areas, I also strongly considered ideas that were helpful for me from NVC point of view, but for now all three are related to meditation:</p><hr class=""dividerBlock""><p><strong>Part of a framework for low-level mental actions: (Intention is before all)</strong><br><br>Any action or thought (a mental action) is preceded by some intention that is generated within the mind in reaction to some input.<br><br>In that framework <em>doing things</em> looks like this:<br>1. there is a need or a desire for something<br>  (based on internal or external information input)<br>2. an intention to get to this goal is generated<br>3. actions are planned and executed<br>4. we get new information input for result of the action<br>5. if goal is not achieved, but intention is still present -<br>  new actions are planned and executed<br><br>And &quot;<em>learning things</em>&quot; means:<br>- Generating and holding an intention to get to some state<br>- The mind as a whole will continue planning and executing actions until they are over-trained and automatic<br><br>So &quot;<em>trying harder</em>&quot; would help much less than &quot;<em>holding, supporting an intention and making it more visible</em>&quot;<br><br><strong>A common pitfall would be to learn to punish self.</strong><br>I.e generating an intention to hurt self with deprecation or something else on encountering information input for a failure state.<br><br>This could lead to mind learning to avoid the input - to not look at the problem, or avoid it.<br><br>While learning an intention of &quot;being friendly to self&quot; could very well lead to more sustained intentions,<br>which interacting with other learning techniques or techniques for applied rationality could lead to trying different ways of achieving the goal</p><hr class=""dividerBlock""><p><strong>Unlearning self-punishment</strong><br><br>This exercise can be included as a cornerstone of a meditation practice in different schools, but can be helpful on it&apos;s own.<br><br>Based on the Input -&gt; Intention -&gt; Action framework<br><br>Our learned tendency to self-deprecate, or self-punish is a strongly entrenched intention to act this way on encountering specific inputs.<br><br>The exercise to unlearn it could look like this:<br>1. select the base activity<br>  could be your house chores, could be work or studying<br>  easy model activity is watching breath and returning to it when you catch yourself distracted<br>2. set timer for some small amount of time, so that exercise would seem enjoyable<br>3. set initial intention<br>  &quot;whenever I&apos;ll catch myself blaming myself - I will say with all sincerity I can muster: &apos;may I be supportive of myself, may I be my friend&apos;&quot;<br>4. Do the activity:<br>  If you catch yourself successfully living out the intention you&apos;ve set - congratulate yourself<br>  if you find that you failed to live out the set intention and blame yourself - live out the intention<br><br>From anecdotal evidence <em>5 days of 5 minutes</em> could already yield visible difference.<br>I&apos;ve never heard of studies for specifically this practice, but studies cited in the book &quot;Altered Traits&quot; indicate that lasting change in mental processes usually requires a much more rigorous schedule.</p><hr class=""dividerBlock""><p><strong>Resistance to pain breeds more pain</strong><br><br>It looks like default strategy learned by many humans includes generating a lot of resistance and effort to push away the pain.<br><br>It is widely documented in meditation circles and there is some research cited in &quot;Altered Traits&quot; that<em> dropping the resistance</em> to pain - either physical or mental <em>helps to alleviate</em> big part of the negative experience.<br><br>The conjecture is that there is :</p><p>- a more &quot;direct&quot; pain signal - that is being sent either from injured organ, or directly from the mind </p><p>- and another one - a resistance and expectation of further pain.<br><br>I&apos;d say that until people learn to act fully and successfully to change the root causes of pain without their existing strategy - the strategy is useful and shouldn&apos;t be thrown away carelessly.<br><br>However in many situations pain is either truly unavoidable (like in hospital after operation after notifying the doctor) or may even be desired - when it is mental pain of loss which we&apos;d want to fully take in.<br><br>In that situation it might help to try to generate curiosity about the exact feelings.<br>And try not to do this &quot;to avoid the pain&quot;, but genuinely to research it better.<br><br>I think there is a very good chance that it is possible to act fully, skillfully in reaction to pain without attempting to push the pain away, but this distinction is harder to put in words.<br></p><hr class=""dividerBlock""><p>P.S I guess I dreamed about a different final exam after the <a href=""https://www.lesswrong.com/s/qRxTKm7DAftSuTGvj/p/Q7MsMshzbzhEs729s"">Hammertime</a>, one that would test the finesse with which I can summon and exercise techniques from the sequence. </p><p>I&apos;d want to definitely have some kind of follow-up with myself in that regard.<br></p>",efim,efim,efim,
7jNveWML34EsjCD4c,Safety via selection for obedience,safety-via-selection-for-obedience,https://www.lesswrong.com/posts/7jNveWML34EsjCD4c/safety-via-selection-for-obedience,2020-09-10T10:04:50.283Z,31,14,1,False,False,,"<p><a href=""https://www.alignmentforum.org/s/boLPsyNwd6teK5key/p/BXMCgpktdiawT3K5v"">In a previous post</a>, I argued that it’s plausible that “the most interesting and intelligent behaviour [of AGIs] won’t be directly incentivised by their reward functions” - instead, “many of the selection pressures exerted upon them will come from <em>emergent</em> interaction dynamics”. If I’m right, and the easiest way to build AGI is using <u><a href=""https://arxiv.org/abs/2006.07495"">open-ended</a></u> environments and reward functions, then we should be less optimistic about using scalable oversight techniques for the purposes of safety - since capabilities researchers won’t need good oversight techniques to get to AGI, and most training will occur in environments in which good and bad behaviour aren't well-defined anyway. In this scenario, the best approach to improving safety might involve structural modifications to training environments to change the emergent incentives of agents, as I’ll explain in this post.</p><p>My default example of the power of structural modifications is the evolution of altruism in humans. Consider <u><a href=""https://royalsocietypublishing.org/doi/10.1098/rspb.2008.0829"">Fletcher and Doebeli’s</a></u> model of the development of altruism, which relies on assortment in repeated games - that is, when players with a tendency to cooperate end up playing together more often than random chance predicts. In humans, some of the mechanisms which lead to assortment are:</p><ul><ul><li>Kin recognition: we can tell who we share genes with.</li><li>Observation of intentions or previous behaviour: these give us evidence about other agents’ future behaviour.</li><li>Costly signalling: this can allow us to reliably demonstrate our future altruism.</li><li>Communication of observed information: once one person has made an observation, it can be shared widely.</li><li>Flexible interactions: we can choose who to assort with in different interactions.</li></ul></ul><p>I claim that, given this type of understanding of the evolution of altruism, we can identify changes to high-level properties of the human ancestral environment which would have made humans significantly more altruistic. For example, human cognition is not very transparent, and so it's relatively difficult for each of us to predict the intentions of others. However, if we had direct observational access to each other's brains, cooperation would become easier and more advantageous. As another example, if we had evolved in environments where we frequently cooperated with many different species, then we’d likely feel more broadly altruistic today.</p><p>To be clear, I don’t think these types of interventions are robust enough to be plausible paths to building safe AGIs: they’re only intuition pumps. In particular, I expect it to be much easier to push AIs to learn to cooperate by directly modifying their reward functions to depend on the rewards gained by other agents. However, agents trained in this way might still learn to care about instrumental goals such as acquiring resources. After all, those instrumental goals will still be useful in allowing them to benefit themselves and others; and unless we train them in a purely cooperative environment, they will still be rewarded for outcompeting other agents. Our question now is: how do we train agents which <em>only </em>care about fulfilling the goals of other agents, while lacking any other goals of their own?</p><p>The approach I’m most excited about is changing the high-level properties of the training environment in a way which encourages division of labour, in particular by separating the roles of planner and worker. For example, consider a meta-learning setup incorporating many different tasks. In each episode, the planner agent A is given a detailed set of instructions in natural language, specifying a task which is too complicated for A to do on its own. The worker agent, B, receives the same rewards as agent A, but doesn’t have access to the same instructions. Instead, B can only receive short commands or feedback from A, which forces it to pay attention to A's instructions. We then train A and B on a very wide range of tasks, so that B learns the general skill of inferring what A wants and acting accordingly. Indeed, we should make A’s reward function and instructions change even within each episode, so that B is always incentivised to continue listening to A’s override commands. And of course we should train each B agent with many different As, so that the Bs don’t overfit to obeying one particular A.</p><p>The purpose of this setup is to train B to be only motivated by obedience to A, and not at all intrinsically motivated to pursue other goals <em>even when those other goals are usually useful for increasing A and B’s shared reward</em>. In other words: if we want agents that remain obedient in a wide range of circumstances, then let’s train them to remain obedient in a wide range of circumstances! Instead of putting the burden of hunting down undesirable correlations on the overseer, we’re putting the burden on whichever process generates a wide range of environments, to ensure that for any potential misaligned goal B might learn, there’s some environment where carrying out that goal will be a worse strategy than just following A’s instructions. Then, although the combination of A and B may be highly agentic, we can hope that each of them will be safe when used individually.</p><p>It’s not clear that this is a bet which will pay off, but this type of training regime seems worth investigating. Here are three more speculative ideas for ways to structurally modify open-ended training regimes to develop more obedient agents, inspired by selection pressures which affected the extent to which humans are ""obedient"" and “domesticated”: </p><ul><li>Increasing specialisation. Agents which are only able to influence their environments in specific, limited ways will have greater incentives to be obedient. For example, if B’s intelligence and observation space are strictly greater than A's, then it may sometimes be useful for B to override A. Whereas if B can’t see most of the environment, while A can, then obeying A is more likely to increase reward according to their shared reward function.</li><ul><li>Specialisation might also arise naturally if agents are trained in sufficiently large groups to make it viable, with sufficiently good communication channels to stay coordinated.</li></ul><li>Increasing the value of learning from others. Agents which are trained in settings where most knowledge is culturally transmitted, rather than derivable individually, will have greater incentives to listen to others rather than always making their own decisions.</li><ul><li>Given this claim, we may be able to increase our agents' tendencies towards obedience by making cultural knowledge transmission easier. For example, we could allow agents to easily write to and read from a permanent record (as compared with humans, who needed to invent laborious techniques to facilitate reading and writing).</li></ul><li>Increasing the value of coordination. Agents trained on tasks which require large-scale coordination will likely learn to be more obedient to central planners, since unilateral action isn’t very useful for solving such tasks. In such cases, other agents might learn to detect and punish disobedience, since it has negative externalities.</li></ul><p>One key concern with these proposals is that the concepts learned by agents in simulation won’t generalise to the real world. For example, they may learn the goal of obedience to a broad class of artificial agents, but not want to obey humans, since we’re very different. It’s particularly hard to predict the generalisation of goals of artificial agents because our reasoning about goals often falls prey to <u><a href=""https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism"">anthropomorphic optimism</a>.</u> To counter this concern, we can try to use adversarial training in a wide variety of domains, including some domains where humans are directly involved (although I expect human oversight to be too expensive to make up more than a small minority of training time). Thorough testing is also very important.</p><p><strong>Testing multi-agent safety</strong></p><p>Many of our current safety test environments have the problem that, while they contain suggestively-labelled components which make sense to humans, they aren’t rich enough to develop the sort of deliberate misbehaviour we are worried about from AGI. To deliberately misbehave, an agent needs a theory of mind, and the ability to predict the consequences of its actions. Without these, even though we can design environments in which the agent takes the action that we’ve <em>labelled</em> as misbehaviour, the agents won’t have the semantic content which we want to check for. While I expect sufficiently advanced language models to have that semantic content, I expect that the easiest way to observe potential misbehaviour is in 3D environments.</p><p>In particular, our default test for the safety of an AI should involve putting it in novel simulated environments with other AIs it’s never interacted with before, and seeing what happens. Cooperation between those AIs - especially via the mechanism of some of them being obedient to the commands of others - would be evidence of their safety. It’d be particularly valuable to see if agents trained in very different ways, with very different patterns of behaviour, would cooperate in novel environments. After testing in simulation, we could also test by deploying agents on increasingly complex tasks in the real world.</p><p>A second approach to testing safety might use more theoretical ideas. It’s usually difficult to formally reason about complex goals in complex environments, but we can take inspiration from the field of evolutionary biology, which does so by analysing the incentives of agents to help or harm each other given how related they are. From similar incentive analysis in agents, we then might be able to derive theories which we can empirically test, and then use to make predictions.</p><p>Another relevant formalism for these situations is that of bargaining games. However, there are a couple of ways in which multi-agent environments differ from standard bargaining games. Firstly, the former are almost always iterated, especially if they’re in a persistent environment. Secondly, there are reputational effects in the former, but not in the latter. Thirdly, all of the agents involved are being optimised based on how well they perform, which shifts their policies over time. So I’m not too optimistic about this type of analysis.</p>",ricraz,ricraz,Richard_Ngo,
MJDrauGE2wH89o38N,"Is there a way to view ""read"" LessWrong posts?",is-there-a-way-to-view-read-lesswrong-posts,https://www.lesswrong.com/posts/MJDrauGE2wH89o38N/is-there-a-way-to-view-read-lesswrong-posts,2020-09-10T03:58:37.626Z,11,4,0,False,True,,"<p>I want to be able to see the LessWrong posts that I've already read/visited, like in a list. I mainly just want to be able to re-save some ones that I really liked but deleted from my bookmarks.</p>
<p>I didn't vote on all the posts I read (it's kind of like a habit I had to build), so I can't do that (although, that would be nice too) (I guess that would be a related question).</p>
",BeanSprugget,beansprugget,BeanSprugget,
AkjL8MEA8YCDwuPJk,Resist epistemic (and emotional) learned helplessness!,resist-epistemic-and-emotional-learned-helplessness,https://www.lesswrong.com/posts/AkjL8MEA8YCDwuPJk/resist-epistemic-and-emotional-learned-helplessness,2020-09-10T02:58:24.681Z,7,2,0,False,False,,"<p>Antonio Pascuale-Leone <a href=""https://www.youtube.com/watch?v=W6BYAjhjt38&amp;list=LLYhC0-5ouWRJOVrES2qAA8g&amp;index=506"">talks </a>about how to get over the end of a relationship. Alison Ledgerwood <a href=""https://www.youtube.com/watch?v=7XFLTDQ4JMk&amp;ab_channel=TEDxTalks"">helps us understand</a> how framing impacts our response to events. Both of these researchers present us with frameworks to structure the narrative of our own lives. The key messages are that we can benefit by translating our emotional pain into some broad existential categories, and that searching for an interpretation of events that acknowledges the positive aspects is difficult but helpful. These mental habits will be contrary to our instincts, and are worth practicing.</p><p>My personal experience in using this advice for dealing with a breaking is that it was helpful, and sped me along a route of emotional recovery that, in retrospect, I am happy I took. I want to explore the nuance around them here.</p><p>Although the ""letting go"" framework that Pascuale-Leone presents and the positive reframing discussed by Ledgerwood are often the perfect mental tools for doing emotional work, they don't do the whole job on their own. Part of the difficulty of emotional work is that it requires a conscious understanding of the formal problem that underlies the emotional feeling. When you find the perfect tool for your problem, that's ideal. One of Pascuale-Leone's smart choices was titling his TED talk after the external problem he expected it to solve, since that makes it easy to triangulate with the audience it was likely to help. It's how I discovered it.</p><p>Ledgerwood's video was harder to find. I was searching for videos on ""letting go"" and ""forgiveness."" I found it a ways down the Youtube search results, and only clicked it because it seemed to be heavily viewed, was by a psychologist, and seemed at least tangentially relevant. In fact, it turned out to directly address part of my problem.</p><p>But positive reframing has its limits. Although it can help you to ""brood on the positive,"" which will then fade (because positive emotions are temporary), it did not allow me to avoid a return to brooding on the negative. The positive insights have stuck, and took the edge off the brooding.</p><p>However, her talk uses examples of reframings where the negative frame has a precise logical equivalence to the positive frame, such as a ""70% success rate"" vs. a ""30% failure rate."" This is markedly different from a reframing where we choose to dwell on positive aspects of a scenario. Even if we do, the negative aspects still remain, and the difference between them and the positive aspects isn't semantic.</p><p>Reframing can help to focus the brain on new memories, and allows us to come up with new ways to summarize them that puts a positive spin on events. The brain, however, is always thinking, and it can easily come up with new thoughts that are connected to the negative aspects of the relationship. It's like a contest between a cynical journalist and a propagandistic government. Neither might be telling the full story. They're just poking holes in each others' stories until the public loses interest.</p><p>To move on from the end of a relationship where there are bad feelings, the key question is not who will win the propaganda war, but when the public will lose interest. To understand that question, we must understand what ""losing interest"" means. It's not as simple as just forgetting you ever cared and paying attention to other things.</p><p>Losing interest has many facets that come in a sort of sequence. They include:</p><ul><li><i>Active brooding</i> about an issue. As a side note, this can actually become a form of identity, in which people commit to maintaining and heightening their brooding. Let's set that aspect aside, though, since we're interested in how people move in the opposite direction. In this stage, the issue feels like an obsession, and intrusive thoughts are persistent and painful.</li><li><i>Needing reminders</i> from external sources. At this stage, we still care and may have strong reactions when we revisit the story, but we are losing the internal alarm bell that leads us to voraciously consume the news or brood about the issue in our personal lives. It can feel like a personal failure to be as invested as seems appropriate. This can even happen during a breakup, such as when we feel we haven't been sad enough for long enough.</li><li><i>Seeking agency</i> over one's level of emotional engagement. An example in terms of people's relationship with media is when they still care about Coronavirus, but deliberately limit their intake of news about it precisely because they recognize that they have strong effects that negatively affect their lives when they get reminders about it. On one level, this seems like an active choice rather than a passive process, but in fact the development of an outside understanding of the relationship between our thoughts and those external reminders often happens on a deeper level than that of conscious choice.</li><li><i>Becoming bored</i> of external reminders. Not only actively avoiding them, but having a more or less apathetic reaction to hearing the news or being drawn into conversation about it. This looks like choosing not to click on that link, diverting conversation away from the topic, making statements to the effect of ""do we really still care about this?"" or ""I'm just over it.""</li><li><i>Casual interest</i> after the external reminders have faded away in response to audience boredom, and a sufficient period of forgetfulness has restored novelty to the story. In the case of the news, an example would be when an intriguing new facet of an old story comes to light and makes us curious to read the article, but does not re-activate our old investment in the story as a whole. In a breakup, this would be when we've moved on with our lives, then become briefly interested in some factoid about our ex, without actually wanting to hear all about their life.</li></ul><p>My guess is that these steps have to occur in sequence, though with some back and forth, and that there's an active and a passive component to each transition. The active component is ""faking it 'til you make it."" Talk and act like you're in the next phase, or invite the next phase in even if you can't quite bring yourself to pretend you've actually arrived, and it will help you arrive and stay there for longer. It's possible that some external event, or waking up on the wrong side of the bed, might cause you to regress temporarily. But the more times you advance further in the process, the speedier your recovery will be. The passive component is not under our control, and simply depends on time.</p><p>What role does positive framing have in helping us move through a process of emotional detachment? I'd answer by comparing it to the back and forth between the cynical journalist and the propagandistic government. If it's just the cynical journalist talking (your negative frame), then the ""public"" (your attention) might take them seriously, since if the cynical journalist didn't have a strong case, they expect that the government would push back against the narrative with its own propaganda (your positive frame).</p><p>Once the government propaganda starts flowing, the public realizes that it's difficult to discern the truth. After enough back and forth, they realize that they're not really interested in teasing out a ""truth"" from this complicated story. There must be more important matters to dwell on, like what's for dinner or that new crush.</p><p>So in this metaphor, the role of positive framing is not to <i>override</i> the negatives, but to complicate the story enough that your attention stops taking it so seriously and moves on to other things.</p><p>We can see the opposite dynamic in other cases. Sometimes, the government propaganda overwhelms any pushback from the cynical journalist. This is when we ignore red flags, avoid bringing up uncomfortable anxieties with even our trusted friends, and make up excuses and cover stories for problematic situations.</p><p>Flawed relationships often seem to start with a ""government propaganda"" effort and end with the ""cynical journalist"" dominating the conversation.</p><p>The problem with this adversarial dynamic in our own minds parallels the issues that arise in real life. People stop feeling as though they can trust the government or the news. This can lead to apathy, or a deep anxiety about living in a world where unknowable forces toy with our lives in profound ways.</p><p>Alternatively, and I think productively, it can lead us to take an activist stance toward the way we explore the world. First, we build our confidence that neither our inner ""cynical journalist"" nor our inner ""propagandistic government"" is a credible source on certain issues. We develop some heuristics for guessing when they're more likely to be telling the truth.</p><p>For example, when they criticize their ""own side,"" we can often take that as genuine. So if you're in the middle of a breakup and find yourself appreciating the positive aspects of your ex and the former relationship, those thoughts are more likely to be trustworthy. Likewise, if you find yourself acknowledging red flags and uncertainties when you have a crush on someone, and especially if you bring them up with friends or a therapist, then that too is likely to be trustworthy.</p><p>In addition to heuristics like this, we can seek out alternative information sources and become savvier consumers and interpreters of it. That looks like checklists and category schemes to evaluate our experiences with people, and active discussion of red flags and uncertainties with trusted friends and with the people in question. It looks like trigger-action plans for problematic interactions and difficult situations. The same sorts of things we do on Less Wrong, but aimed at the conflicting information sources within our own minds.</p><p>A third avenue, much more tractable on the level of individual psychology than on a social level, is to build trust between the adversarial voices in our heads. This looks like the <a href=""https://www.lesswrong.com/posts/mQmx4kQQtHeBip9ZC/internal-double-crux"">internal double crux</a> technique. The key here is that in extreme situations, such as the beginning of a relationship or after a breakup, it may be that one perspective - the cynical journalist or propagandistic government - is so dominant that our attention forgets that alternative points of view even exist.</p><p>While internal double crux might be fairly useful when we are consciously facing an internal conflict, it might be even more useful if we are able to actively seek out and give expression to an alternative point of view when it feels as though we <i>aren't</i> dealing with an internal conflict. A trustworthy perspective comes when we honestly believe that our multiple inner points of view have a fundamentally adversarial relationship, and yet on the specific topic at hand they actually agree. A healthy perspective is not only trustworthy, but involves giving the level of attention to the topic that's actually warranted by its importance to your life and the usefulness of the information in guiding future action.</p><p>How do we bring this all together?</p><p>We want to have a set of categories available for interpreting our experience, and know the steps that we expect to go through on our journey toward healing. We want to make sure that our inner positive and negative voices both have their points of view acknowledged, especially when one is so blaringly loud that it drowns the other out. In addition, we want to become savvy interpreters of their multiple perspectives and encourage trust-building internal double crux dialog within ourselves. Finally, we want to be able to help our attention shift in the right direction without getting stuck.</p><p>When in the midst of a difficult emotional process, this framework offers us a way to diagnose what needs to be done and gives an action plan for self-care. The plan looks something like this:</p><ol><li>Establish an intuitive and plausible broad structure for the issue at hand.</li><li>Seek out multiple points of view and alternative, reasonably credible information sources.</li><li>Use heuristics, calculated thinking, draw on expert opinion, and practice internal double crux to form a compelling synthesis.</li><li>Influence attention levels to move toward the level that seems appropriate given the importance and actionability of the issue.</li></ol><p>This plan seems useful for dealing with both personal and world questions. It's what we already do implicitly to some extent when we're on top of our game. But all too often, the plan we actually use is more like:</p><ol><li>Freak out about the issue, devouring any information that gets shoved in our faces.</li><li>Talk about it incessantly until some of the complexities emerge.</li><li>Get bored or tired of the topic and gradually forget about it.</li><li>Experience learned helplessness as this whole process becomes a frustrating point of evidence that you, and people generally, just aren't up to the task of trying to understand the world.</li></ol><p>So don't do that. It probably takes roughly the same amount of time as the good process, and far more energy, but leads to much worse outcomes - not only for your understanding of the issue, but for your self-concept as a person who's capable of understanding your life and the world. Resist epistemic and emotional learned helplessness!</p>",AllAmericanBreakfast,directedevolution,DirectedEvolution,
KFKBwbBobfFYCqFrN,How easily can we separate a friendly AI in design space from one which would bring about a hyperexistential catastrophe?,how-easily-can-we-separate-a-friendly-ai-in-design-space,https://www.lesswrong.com/posts/KFKBwbBobfFYCqFrN/how-easily-can-we-separate-a-friendly-ai-in-design-space,2020-09-10T00:40:36.781Z,20,12,19,False,False,,"<p>(I&apos;ve written about this in my <a href=""https://www.lesswrong.com/posts/LDPcAEhn35uoC2Hz3/shortform &amp; may regurgitate some stuff"">Shortform</a> and may regurgitate some stuff from there.)</p><br><p>Eliezer proposes that we <a href=""https://arbital.com/p/hyperexistential_separation/"">separate an AI in design space from one that would constitute a fate worse than death</a> if e.g. the reward model&apos;s sign (+/-) were flipped or the direction of updates to the reward model were reversed. This seems absolutely crucial, although I&apos;m not yet aware of any robust way of doing this. Eliezer proposes assigning the AI a utility function of:</p><blockquote><em>U</em> = <em>V</em> + <em>W</em></blockquote><p>Where <em>V</em> refers to human values &amp; <em>W</em> takes a very negative value for some arbitrary variable (e.g. diamond paperclips of length 5cm). So if the AI instead maximises -<em>U</em>, it&apos;d realise that it can gain more utility by just tiling the universe with garbage.</p><p>But it seems entirely plausible that the error could occur with<em> V </em>instead of <em>U</em>, resulting in the AI maximising <em>U </em>= <em>W</em> - <em>V</em>, which would result in torture.</p><p>------------------------------------------------------------------------------------------------------</p><p>Another proposition I found briefly described in a <a href=""https://www.facebook.com/yudkowsky/posts/10155975880959228 "">Facebook discussion that was linked to by somewhere</a>. Stuart Armstrong proposes the following:</p><blockquote>Let B1 and B2 be excellent, bestest outcomes. Define <em>U</em>(B1) = 1,<em> U</em>(B2) = -1, and<em> U</em> = 0 otherwise. Then, under certain assumptions about what probabilistic combinations of worlds it is possible to create, maximising or minimising <em>U</em> leads to good outcomes.</blockquote><blockquote>Or, more usefully, let <em>X</em> be some trivial feature that the agent can easily set to -1 or 1, and let <em>U</em> be a utility function with values in [0, 1]. Have the AI maximise or minimise <em>XU</em>. Then the AI will always aim for the same best world, just with a different <em>X</em> value.</blockquote><p>Later, he suggests that <em>X</em> should be a historical fact (i.e. the value of <em>X</em> would be set in stone 10 seconds after the system is turned on.) As <em>XU</em> can only take positive values (because <em>U</em> has values in [0, 1]), the greatest value -<em>XU</em> could take would be 0 (which suggests merely killing everyone.) </p><p>But this could still be problematic if e.g. the bug occurred in the reward function/model such that it gave positive values for bad things and negative values for bad things. Although I&apos;m not sure how frequently errors effectively multiply everything in the reward by -1. I&apos;m also unsure how this would interact with an error that reverses the direction of updates to a reward <em>model</em>.</p><p>------------------------------------------------------------------------------------------------------</p><p>A few possible (?) causes for this type of error include: (list obviously not exhaustive)</p><ul><li>Bug caused by AI&apos;s developers. For example, <a href=""https://openai.com/blog/fine-tuning-gpt-2/"">GPT-2 experienced an interesting bug</a>:</li></ul><blockquote><strong>Bugs can optimise for bad behavior</strong></blockquote><blockquote>One of our code refactors introduced a bug which flipped the sign of the reward. Flipping the reward would usually produce incoherent text, but the same bug also flipped the sign of the KL penalty. The result was a model which optimized for negative sentiment while preserving natural language. Since our instructions told humans to give very low ratings to continuations with sexually explicit text, the model quickly learned to output only content of this form.</blockquote><p>The responses to <a href=""https://www.lesswrong.com/posts/WMhiJf3xx9ZopC2tP/likelihood-of-hyperexistential-catastrophe-from-a-bug"">this</a> thread suggest that this type of thing would be noticed and redressed immediately, although this view <a href=""https://www.lesswrong.com/posts/9nRfQBCGDdfMTmxgR/open-and-welcome-thread-august-2020-1?commentId=4SreftCqGKWyQYmnG"">doesn&apos;t appear to be held unanimously</a>. See also  Gwern&apos;s <a href=""https://www.lesswrong.com/posts/9nRfQBCGDdfMTmxgR/open-and-welcome-thread-august-2020-1?commentId=TENcarNEgJzgXLgoB"">follow-up comment</a>.</p><ul><li>Errors in self-modification/improvement</li><li><a href=""https://www.lesswrong.com/posts/9nRfQBCGDdfMTmxgR/open-and-welcome-thread-august-2020-1?commentId=9GRTobpRt9XS63JgQ#4SreftCqGKWyQYmnG "">Programmers making a mistake in a relevant database</a></li><li>Some sort of weird vulnerability getting exploited by <a href=""https://arbital.com/p/daemons/"">daemons</a> (unsure on this one)</li></ul><p>------------------------------------------------------------------------------------------------------</p><p>So, yeah. Are there any mechanisms to prevent this sort of thing from happening other than the two that I listed; and if not, would you expect the examples provided to robustly  prevent this type of error from happening <em>regardless of the cause</em>?</p>",Anirandis,anirandis,Anirandis,
PPkheZsNvkM8Ey2uL,Capturing Ideas,capturing-ideas,https://www.lesswrong.com/posts/PPkheZsNvkM8Ey2uL/capturing-ideas,2020-09-09T21:20:23.049Z,74,27,12,False,False,,"<p><i>Related to: </i><a href=""https://www.lesswrong.com/s/pC6DYFLPMTCbEwH8W""><i>Babble and Prune</i></a><i>, </i><a href=""https://www.lesswrong.com/posts/XYYyzgyuRH5rFN64K/what-makes-people-intellectually-active""><i>What Makes People Intellectually Active?</i></a><i>, </i><a href=""https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1""><i>Zettelkasten</i></a><i>.</i></p><p>Summary: if you want to generate more ideas, carry a notebook and write down any thoughts you have.</p><h1>Citation Needed</h1><p>I've heard these ideas repeated again and again in different forms: books on note-taking, writing, and creativity; the sorts of interviews where artists are asked ""where do you get your ideas?""; and most recently, the final post in the Babble and Prune sequence (""<a href=""https://www.lesswrong.com/posts/eJiE7uuKZaP5HqLvY/write"">Write</a>""). It would be good of me to gather together some references (especially if there's any academic research on this topic?), but I'm going full-on anecdotal here, and just present to you the most complete version of the thing I can cobble together from memory.</p><p>I've personally found this technique to be useful, and anecdotally, so have many other people.</p><h1>The Basic Technique</h1><p>Let's say you want to have more ideas in some specific category. For example:</p><ul><li>You want to write fiction. At times, you might feel like you're full to bursting with ideas which you'd like to turn into stories. Yet, when you sit down to do it, you feel like you don't have any ideas.</li><li>You want to do creative research. Maybe so far you only have worked on what problems your advisor gives you. Or maybe you've never worked on research before, and don't know where to start, beyond just reading background literature.</li><li>You're looking for ideas in some school- or work- related context, ideas for Christmas presents, startup ideas, etc etc...</li></ul><p><strong>Step 1. </strong>Get a pocket notebook, or create a new list in a phone note-taking app, et cetera. The goal is to maximize availability and convenience: to the extent possible, you should be able to capture ideas at any time and place.</p><p><strong>Step 2.</strong> Write down any ideas that you have. <i>Any idea at all.</i> It doesn't have to be good! This is brainstorming. One or two words is fine, so long as you know what it means. Elaborating the idea more will help you remember and may help you generate more ideas, but you can save that for later.</p><p>That's it! It's that simple.</p><p>Why write a whole post on this? My suspicion is that a lot of people won't raise this very simple strategy to attention to try. I think writing down your ideas has a magical quality to it. You might think:</p><ul><li>""It's not that I'm forgetting my ideas. I just don't have much to say.""</li><li>""I'll write down ideas when I have something good enough to write down.""</li><li>""I can just write things down later -- my memory isn't that bad. There's no reason to carry a notebook with me.""</li></ul><p>Or other such thoughts. If you're at a loss for ideas, I suspect these thoughts are wrong. The following ""why it works"" section is mostly to illustrate that there may be more to this technique than is immediately obvious.</p><h1>Why It Works</h1><p>There are some obvious reasons why this might help, and there are also some less-obvious reasons. In approximate order of decreasing obviousness (which is also, as it happens, decreasing order of probability):</p><h2>Memory</h2><p>The most obvious thing: you're writing down ideas, so you'll have a list of ideas to look at later.</p><p>Actually, writing things down seems to help even if you don't look back at it later: as alkjash mentioned in <a href=""https://www.lesswrong.com/posts/eJiE7uuKZaP5HqLvY/write""><i>Write</i></a>, just the act of writing it down might be enough to make it stick in your memory.</p><h2>Time</h2><p>It might be that you ordinarily only think about your creative project more-or-less when you sit down to work on it. You don't have any ideas because the only time you spend <i>trying </i>to come up with ideas is when you're sitting in front of a blank page.</p><p>Putting a notebook in your pocket means you can think about this at any time. Moreover, it creates the affordance: your brain will register this as a thing it can do. (Moreover, you're probably more likely to have interesting ideas when you're out and about, getting all kinds of sensory stimulus.)</p><p>Getting out the notebook to write one idea causes you to put more time into thinking of ideas. You might end up writing two or three more you thought of in the time it took you to write the first.</p><h2>Practice Noticing</h2><p>It could be that an important aspect of this is: you're intentionally practicing noticing that you have ideas. Like so many other things, perhaps this is something you improve at through practice.</p><p>(Note that practicing noticing story ideas vs research ideas vs other kinds of ideas might all be different skills; you don't necessarily get much better at one just because you've practiced the other.)</p><h2>Reward</h2><p>You know how level-ups in video games manage to be addictive, even though your brain has no intrinsic love of watching little numbers go up?</p><p>I think there's a similar thing here.</p><p><i>Ordinary scenario: </i>You have a passing thought which could be turned into a creative idea. You take no action, and are soon distracted with something else. Your brain concludes: that was a useless thought.</p><p><i>With notebook:</i> You have a passing thought which could be turned into a creative idea. You take out your notebook and pen and start writing. Your brain concludes: looks like that was useful for something! I'll try and come up with more things like that!</p><p>Keeping a list of ideas gives you the feeling that you're building something. Each new entry is another brick in a palace of awesomeness.</p><p>Beware: <strong>if you don't do anything with your list, this feeling will fade with time.</strong> Your brain will figure out that you're laying bricks in nothing but a ... sad pile of bricks.</p><p>The book <i>Getting Things Done</i> suggests that you need to build a relationship of trust with your future self, in order for lists like this to work -- writing something down needs to mean that you'll take appropriate action later, even if only to (appropriately) discard most of the ideas.</p><p>(This post won't mainly be about how to take your ideas and do something with them, but see the later section ""developing ideas"".)</p><h2>Getting Ideas Out (to make room for more!)</h2><p>Another idea mentioned in <i>Getting Things Done</i> is that writing things down gets them out of your head. According to the author, so long as an idea is in your head, it's taking a little bit of your attention. If you write it down in a list, <i>and if you trust yourself to look at the list later and take appropriate action</i>, then your brain turns off the reminder and you free up the attention for other things.</p><p>Alkjash says something similar in <a href=""https://www.lesswrong.com/posts/eJiE7uuKZaP5HqLvY/write""><i>Write</i></a>:</p><blockquote><p>Fast forward to 2013 and transport yourself to my first summer research program. Every Monday, we give a brief board about that week's progress. I mull ideas on paper over the week before TeXing them up Sunday night.</p><p>A curious thing happened - all my progress happened on Monday and Tuesday. I spent the rest of the week meandering around the same ideas, checking special cases and writing up fragments of arguments. On Sunday night I write everything down, and the ideas crystallize on paper. They lose their grip on me, and I move on to new pastures.</p></blockquote><p>(Note that, as a grad student, I've mostly heard rather the opposite: it seems most people make most of their progress the day <i>before</i> their weekly meeting, not the day <i>after</i>. But, both factors could be in play.)</p><h2>Attention Leads to Detail</h2><p>Maybe you've been vaguely dissatisfied with the way your apartment is set up for some time. All of these thoughts feel the same to you; you file them under ""apartment is dumb"".</p><p>Recently, you've decided to do something about it. In order to get started, you'll record your ideas. You stick a notebook in your pocket and start writing thoughts whenever they occur to you.</p><p>When you write something down, you have to put it into words. Even if it's just a short phrase, it involves a little bit more detail than your fuzzy mental handle.&nbsp;</p><p>Now you notice that you actually have a diversity of complaints, with an implied diversity of remedies. Because you've written each of them down, you can see this diversity at a glance.</p><p>This certainly happened to me, when I was first taking notes on rationality (after reading HPMOR). What seemed to me like a single, unified concept kept splintering and splintering.</p><p>The opposite could also happen. You vaguely think your cloud of ideas on a subject is huge and diverse, but when you carry a notebook and write every thought down, you find out that everything comes to just two or three points.</p><h1>Babble &amp; Prune</h1><p>Since this post was inspired largely by reading <i>Babble and Prune</i>, a few words on the relationship to that model:</p><p>In the subsections on ""practicing noticing"" and ""reward"", I implied that there was some kind of learning going on -- training your brain to notice/produce the kinds of ideas you're looking for. Can we explain this in terms of Babble &amp; Prune? The Babble &amp; Prune model naturally splits this into two distinct types of learning:</p><ul><li><strong>Training your brain to babble more in that general direction</strong>. If you do anagrams a lot, you learn some really good heuristics for flipping letters around to form new words. Similarly, by paying attention to thoughts of the special kind you want to foster, you may train your brain to flip concepts around in ways more likely to help with that. Confusion about how the subway works might turn into a short story idea about a world where travel works differently. A weird hiccup in your reasoning might turn into a setting where a religion is devoted to precisely that mistake in reasoning. And so on.</li><li><strong>Training your brain not to prune those things.</strong> You might normally ignore ideas of that kind, which teaches your brain not to bring them to conscious attention, or store them in memory. By paying attention and writing them down, you might adjust those filters, opening up the gates of attention to more of those thoughts.<ul><li>By the way, if you want to get more into conscious attention generally, something to try is the meditative practice of <i>mental noting</i>: simply giving labelling words to what is going on in your brain. If you notice that you are thinking, say ""thinking"" to yourself (out loud if that helps); if you notice that you are remembering, say ""remembering""; if you notice that you are bored, say ""bored""; if you notice mental images, say ""images""; and so on. The point of this exercise is, amongst other things, to develop awareness of what your brain is doing. In terms of <i>Babble &amp; Prune</i>, this softens the filter of conscious attention, giving you access to more of what's going on.</li></ul></li></ul><p>A lot of <i>Babble &amp; Prune</i> is oriented toward simply pruning less, an idea which I don't inherently agree with. Yes, pruning less overall might be the right thing for a lot of people. But I'm much more interested in fine-grained adjustments to the pruning filter.</p><p>Relatedly -- in the ""basic technique"" section of this post, I emphasized that you should write down <i>any ideas at all</i>. Of course this isn't literally true. You have to calibrate your level of pruning.</p><ul><li>Often, at the start, it's good to dramatically reduce your filtering in order to start the flow of ideas. Write down absolutely any related idea which comes to mind. If none are coming to mind, write down totally unrelated things just so there's something on the page. Write down bad ideas so you'll have something to compare your better ideas to. Write down absolute nonsense so you'll have something to make your bad ideas look good.</li><li>As you start to get good ideas, you'll naturally start to put filters back up. If this technique is, overall, successful, you'll eventually have more interesting ideas than you can execute on. At that point, it's natural to only write down additional ideas which have some significance -- a good-enough chance of being worth your time.<ul><li>Note, however, that it's easy to make the mistake of letting good ideas choke out your source of inspiration. Feynman wrote about how he couldn't come up with any ideas after working on the atom bomb, because he had got an image of himself as someone who worked on big important things. This problem persisted until he told himself that he wasn't allowed to work on important things -- unimportant things only! After that he worked on the physics of spinning planes, which eventually turned out to help with some fundamental problems in quantum physics.</li></ul></li></ul><h2>Developing Ideas</h2><p>Another disagreement I have with <i>Babble &amp; Prune</i> is the idea that more layers of filtering is worse. I think the ""three gates"" (first filter: conscious thought; second filter: saying it out loud; third filter: writing it down) was one of the best and most useful parts of the sequence. (Modulo the fact that for me, the position of the second and third gates is often reversed: I'll write something before I'd say it, due to my close relationship with notebooks.) Yet, I disagree with the contention that this is too many filters. It's too few filters.</p><p>Imagine if you had no<i>&nbsp;</i><span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""thinking \to speaking""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">h</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">k</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span><span class=""mjx-mo MJXc-space3""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.225em; padding-bottom: 0.372em;"">→</span></span><span class=""mjx-mi MJXc-space3""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">k</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;"">g</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span></span><i>&nbsp;</i>filter. In order to avoid saying bad things, you would have to learn to self-censure your conscious thoughts a lot more. Less ideas would rise to consciousness, and the ones that did would be more constrained by the Gricean maxims and other factors.</p><p>Adding a buffer between thinking and speaking allows us to think more, and to develop our thoughts more fully before speaking them aloud.</p><p>Similarly, in <a href=""https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1"">Zettelkasten</a>, I described <a href=""https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1#Temporary_Notes_vs_Organized_Notes"">my pipeline for developing ideas</a> as consisting of at least four stages:</p><ul><li><strong>Jot:</strong> Very concise handles used for idea capture. This current post is all about jot-taking. Jots remain meaningful to me for at least a week, but eventually I might have no idea what I was talking about.</li><li><strong>Gloss:</strong> Paragraph-ish summary I write when I intend to develop a jot more fully. A gloss gives enough of a summary that I won't lose the idea if I let it sit for weeks or months. Takes considerably more <a href=""https://www.lesswrong.com/tag/focusing"">focusing</a> to write than a jot.</li><li><strong>Development:</strong> Free-writing based on an idea. Mostly very informal and narrative-based, dramatizing the ups and downs of an idea as I propose solutions find issues with those solutions, etc.</li><li><strong>Refinement:</strong> More formal write-ups, often for an audience other than myself. Revising drafts in response to feedback. Engaging with comments on posts. Etc.</li></ul><p>This might not fit your use-case. For example, if you're trying to do visual art (for example, drawing a webcomic), your workflow can't all be different types of writing.</p><p>The main thing I'm trying to get across here is that <i>in order to develop the ideas you've captured into something worth sharing,</i> you probably need several stages.</p><p>The <i>Babble &amp; Prune </i>model mentioned that babbling is far from totally random, and in particular, as you babble you're mostly mutating ideas (less like random sampling from idea-space, more like a random walk around idea-space). By adding more stages of filtering, I'm suggesting that the way to produce high-quality content is something like <a href=""https://en.wikipedia.org/wiki/Simulated_annealing"">simulated annealing</a>: gradually imposing higher and higher standards on our ideas as we continue to mutate them, so that the final result can crystallize into a strong metallic alloy.</p>",abramdemski,abramdemski,abramdemski,
Fji2nHBaB6SjdSscr,Safer sandboxing via collective separation,safer-sandboxing-via-collective-separation,https://www.lesswrong.com/posts/Fji2nHBaB6SjdSscr/safer-sandboxing-via-collective-separation,2020-09-09T19:49:13.692Z,26,10,6,False,False,,"<p><em>Epistemic status: speculative brainstorming. Follow-up to <a href=""https://www.alignmentforum.org/posts/HekjhtWesBWTQW5eF/agis-as-populations"">this post on AGIs as collectives</a>. Note that I&apos;ve changed the term </em>population AGI <em>to </em>collective AGI <em>for consistency with Bostrom&apos;s use in </em>Superintelligence<em>.</em></p><p>One safety measure we might want to use on AGIs is (sand)boxing: limiting their access to the real world, so they have very few channels by which to influence it. However, we also want to deploy them on real-world tasks without them using that opportunity to affect the world in unintended ways. Realistically, even if we can maintain high security in a few sandbox environments, I doubt we&#x2019;ll be able to make every single deployment environment secure against the full intellectual powers of a superintelligent AGI. But this problem might be easier in the case of a collective AGI.</p><p>Broadly speaking, my hypothesis is that the tasks which require the sort of high-level intelligence and strategic capability which we&#x2019;re worried about also require the least interaction with the world. Discovering new scientific theories or making major strategic decisions can occur in a sandbox which only allows brief text outputs. Whereas being a good personal assistant or web administrator might require frequent access to the internet, but benefit much less from superhuman capabilities. And so if we are able to easily adjust the level of intelligence that an AGI is able to apply to any given task, then we might be able to significantly reduce the risks it poses without reducing its economic usefulness much. On tasks which require interacting with the world in flexible ways, we could limit its intelligence greatly; on tasks which require very high intelligence, we could limit its ability to interact with the world.</p><p>In general, for most possible AGI architectures, this gradated intelligence restriction seems quite difficult to achieve. For example, we couldn&#x2019;t just remove parts of a standard neural network without needing to retrain it significantly. Limiting the AGI&#x2019;s thinking time, or access to memory, might be possible. But this may reduce its capabilities on all tasks, since even low-level tasks can require detailed calculations and long-term memory. By contrast, it&#x2019;s easy to reduce the capabilities of a collective AGI: simply deploy only a subset of the members, and bottleneck the communication between different subsets. This makes deployment on a range of real-world tasks more feasible. Even if each individual member of a collective AGI is as intelligent as any hundred humans put together, I expect that we could (with sufficient effort) create secure deployment and monitoring protocols that the individual AI could not break, if it weren&#x2019;t able to communicate with the rest of the collective beforehand.</p><p>Given the claims I&apos;ve made so far, we might hope that a small subset of a collective AGI could do nearly as well as the full collective on tasks which require a lot of interaction with the real world, while being significantly safer. In the next section I&#x2019;ll discuss this proposal in more detail. First, though, I want to be very clear that this is not intended as a primary solution to the safety problem; ideally we&#x2019;ll have much more reliable ways to ensure AGI is aligned. However, it&#x2019;s worth thinking about ways to &#x201C;move the needle&#x201D; by adding additional layers of security. I think there&#x2019;s a reasonable chance that hacky solutions like the one above will make a difference (especially because of my uncertainty about what AGI will actually look like).</p><p><strong>A sketch of collective separation</strong></p><p>In <a href=""https://www.alignmentforum.org/posts/HekjhtWesBWTQW5eF/agis-as-populations"">my previous post</a> I outlined two ways we might train a collective AGI to become more intelligent than humanity. Either we train a single AGI through some means, and then make many copies of it and allow them to communicate. Or else we start off with a multi-agent collective, and train them in a way which incentivises both competition and cooperation (e.g. they are each rewarded for acquiring more of a scarce resource).</p><p>Either way, let&#x2019;s consider how we might actually want to deploy the resulting AGI. For example, suppose we want to use it to solve the problem of world energy. The entire collective AGI might be responsible for deriving the physical principles which would allow more efficient power sources, in near-total isolation from the world. Different subsets of that collective AGI could then separately work on creating actual designs for new power plants - which would involve testing prototypes and evaluating construction feasibility, in closer contact with humans. Even smaller subsets could then be used to oversee the daily running of those power plants - a task which would require the ability to make autonomous real-time interventions. The key claim is that a small subset of the collective AGI is nearly as good at managing one power plant as the whole collective would be, while also being much safer.</p><p>More specifically, when I talk about extracting a subset of the collective AGI, I mean making copies of a set of individual AIs which have knowledge about the relevant topic, and then separating those copies from the rest of the collective. The extent to which the copies undergo further training to fine-tune them for their new task will vary. I expect that for tasks like designing power plants, we&#x2019;ll have too little data to optimise them specifically for that task; members of the subset will need to use their existing general intelligence, like humans do. Perhaps the requirements for overseeing daily operation of a plant will be well-specified enough to gather good training data, though. And as I discuss in my previous post, fine-tuning will likely be much easier for smaller subsets of the collective AGI.</p><p><strong>Information flow and version control</strong></p><p>One important question is how much information flow should be allowed between different parts of the collective AGI used for different tasks. This is a crucial point, and cause for concern. In the power plant example above, we might imagine that the collective AGI could design a power plant with security flaws in advance, then share the blueprint with all of its members. Then a small subset of the collective could recite that design when asked to come up with a real-world implementation; and an even smaller subset could exploit the pre-specified flaws when deployed.</p><p>How to prevent this type of problem? One option is strict version control: the small subsets could be drawn from a version of the collective AGI which hadn&#x2019;t yet thought about how to design flawed power plant security systems. This seems very difficult if the AGI had enough information about humans (e.g. read-only access to the internet), because it could then predict the most likely ways it would be used, and make plans about how to subvert them. More promising is the idea that the full AGI should never be given detailed information about human society - that it should only ever be used for abstract problems. Subsets could be used for tasks involving real-world data, but then immediately shut down without having the chance to report back to the full AGI. This would likely impair performance, but plausibly not by very much if the subsets are fast learners and inherit the full AGI&#x2019;s meta-level knowledge about how to solve problems.</p><p><strong>Implementation</strong></p><p>Could we realistically expect widespread deployment of collective AGI without anyone giving (a copy of) a full collective access to an insecure task, or an individual member unrestricted access to the internet? On the face of it, this does seem unlikely. But it&#x2019;s less difficult if the source code of the AGI is kept confidential (which aligns well with economic incentives). Widespread deployment might then involve allowing a range of groups to rent access to copies of that AGI running on the cloud.</p><p>In this situation, collective boxing could be implemented by limiting permissible tasks depending on how much of the collective AGI was requested. The full collective could only be rented to do batched tasks with short outputs. The restrictions on subsets could be less strict. Even if several different groups develop AGIs, if they coordinated to run them on similar (or the same) cloud platforms, boxing restrictions could be enforced in a scalable way.</p>",ricraz,ricraz,Richard_Ngo,
jCZhy3nqH2MoethZQ,[AN #116]: How to make explanations of neurons compositional,an-116-how-to-make-explanations-of-neurons-compositional,https://www.lesswrong.com/posts/jCZhy3nqH2MoethZQ/an-116-how-to-make-explanations-of-neurons-compositional,2020-09-09T17:20:04.668Z,21,8,2,False,False,,"<h1>HIGHLIGHTS </h1><p><strong><a href=""https://arxiv.org/abs/2006.14032"">Compositional Explanations of Neurons</a></strong> <em>(Jesse Mu et al)</em> (summarized by Robert): Network dissection is an interpretability technique introduced in 2017, which uses a dataset of images with dense (i.e. pixel) labels of concepts, objects and textures. The method measures the areas of high activation of specific channels in a convolutional neural network, then compares these areas with the labelled areas in the dataset. If there&apos;s a high similarity for a particular channel (measured by the intersection divided by the union of the two areas), then we can say this channel is recognising or responding to this human-interpretable concept.</p><p>This paper introduces an extension of this idea, where instead of just using the basic concepts (and matching areas in the dataset), they search through logical combinations of concepts (respectively areas) to try and find a compositional concept which matches the channel&apos;s activations. For example, a channel might respond to (water OR river) AND NOT blue. This is still a concept humans can understand (bodies of water which aren&apos;t blue), but enables us to explain the behaviour of a larger number of neurons than in the original network dissection method. Their work also extends the method to natural language inference (NLI), and they interpret neurons in the penultimate layer of a BiLSTM-based network trained to know whether a sentence entails, contradicts, or is neutral with respect to another. Here they create their own features based on words, lexical similarity between the two sentences, and part-of-speech tags.</p><p>Using their method, they find that channels in image classifiers do learn compositional concepts that seem useful. Some of these concepts are semantically coherent (i.e. the example above), and some seem to have multiple unrelated concepts entangled together (i.e. operating room OR castle OR bathroom). In the NLI network, they see that many neurons seem to learn shallow heuristics based on bias in the dataset - i.e. the appearance of single words (like nobody) which are highly informative about the classification.</p><p>Finally, they use their method to create copy-paste adversarial examples (like in Activation Atlas (AN #49)). In the Places365 dataset (where the goal is to classify places), they can crudely add images which appear in compositional concepts aligned with highly contributing neurons, to make that neuron fire more, and hence change the classification. Some of these examples generalise across classifier architectures, implying a bias present in the dataset.</p><p><strong>Robert&apos;s opinion:</strong> I think work which targets specific neurons and what they&apos;re doing is interesting as it can give us a very low-level understanding of the model, which I feel is necessary to achieve the level of understanding required by alignment solutions which use interpretability (i.e. those in <strong><a href=""https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai"">An overview of 11 proposals for building safe advanced AI</a></strong> (<strong><a href=""https://mailchi.mp/2485e6b42012/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals-for-ai-alignment"">AN #102</a></strong>)). The main limitation of this approach is that it currently requires a large amount of dense human labelling of the datasets, and if a concept isn&apos;t in the labels of the dataset, then the method won&apos;t be able to explain a neuron using this concept. Also, the fact that their interpretability method is able to give insights (in the form of creating copy-paste examples) is a useful sign it&apos;s actually doing something meaningful, which I think some other interpretability methods lack.</p><h1>TECHNICAL AI ALIGNMENT </h1><h2>LEARNING HUMAN INTENT </h2><p><strong><a href=""https://openai.com/blog/learning-to-summarize-with-human-feedback/"">Learning to Summarize with Human Feedback</a></strong> <em>(Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler et al)</em> (summarized by Rohin): OpenAI has been working on <strong><a href=""https://openai.com/blog/fine-tuning-gpt-2/"">finetuning language models from human preferences</a></strong> (<strong><a href=""https://mailchi.mp/38af1edcd025/an-67creating-environments-in-which-to-study-inner-alignment-failures"">AN #67</a></strong>). This blog post and paper show the progress they have made on text summarization in particular since their last release.</p><p>As a reminder, the basic setup is similar to that of <strong><a href=""https://deepmind.com/blog/learning-through-human-feedback/"">Deep RL from Human Preferences</a></strong>: we get candidate summaries by executing the policy, have humans compare which of two summaries is better, and use this feedback to train a reward model that can then be used to improve the policy. The main differences in this paper are:</p><p>1. They put in a lot of effort to ensure high data quality. Rather than having MTurk workers compare between summaries, they hire a few contractors who are paid a flat hourly rate, and they put a lot of effort into communicating what they care about to ensure high agreement between labelers and researchers.</p><p>2. Rather than collecting preferences in an online training setup, they collect large batches at a time, and run a relatively small number of iterations of alternating between training the reward model and training the policy. My understanding is that this primarily makes it simpler from a practical perspective, e.g. you can look at the large batch of data you collected from humans and analyze it as a unit.</p><p>3. They initialize the policy from a model that is first pretrained in an unsupervised manner (as in <strong><a href=""https://arxiv.org/abs/2005.14165"">GPT-3</a></strong> (<strong><a href=""https://mailchi.mp/2485e6b42012/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals-for-ai-alignment"">AN #102</a></strong>)) and then finetuned on the reference summaries using supervised learning.</p><p>On the Reddit task they train on, their summaries are preferred over the reference summaries (though since the reference summaries have varying quality, this does not imply that their model is superhuman). They also transfer the policy to summarize CNN / DailyMail news articles and find that it still outperforms the supervised model, despite not being trained at all for this setting (except inasmuch as the unsupervised pretraining step saw CNN / DailyMail articles).</p><p>An important ingredient to this success is that they ensure their policy doesn&#x2019;t overoptimize the reward, by adding a term to the reward function that penalizes deviation from the supervised learning baseline. They show that if they put a very low weight on this term, the model overfits to the reward model and starts producing bad outputs.</p><p><strong>Read more:</strong> <strong><a href=""https://arxiv.org/abs/2009.01325"">Paper: Learning to summarize from human feedback</a></strong></p><p><strong>Rohin&apos;s opinion:</strong> This paper is a great look at what reward learning would look like at scale. The most salient takeaways for me were that data quality becomes very important and having very large models does not mean that the reward can now be optimized arbitrarily.</p><h2>FORECASTING </h2><p><strong><a href=""https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity"">Does Economic History Point Toward a Singularity?</a></strong> <em>(Ben Garfinkel)</em> (summarized by Rohin): One important question for the long-term future is whether we can expect accelerating growth in the near future (see e.g. this <strong><a href=""https://www.openphilanthropy.org/blog/modeling-human-trajectory"">recent report</a></strong> (<strong><a href=""https://mailchi.mp/be2a0d160fa2/an-105-the-economic-trajectory-of-humanity-and-what-we-might-mean-by-optimization"">AN #105</a></strong>)). For AI alignment in particular, the answer to this question could have a significant impact on AI timelines: if some arguments suggested that it would be very unlikely for us to have accelerating growth soon, we should probably be more skeptical that we will develop transformative AI soon.</p><p>So far, the case for accelerating growth relies on one main argument that the author calls the <em>Hyperbolic Growth Hypothesis</em> (HGH). This hypothesis posits that the growth <em>rate</em> rises in tandem with the population size (intuitively, a higher population means more ideas for technological progress which means higher growth rates). This document explores the <em>empirical</em> support for this hypothesis.</p><p>I&#x2019;ll skip the messy empirical details and jump straight to the conclusion: while the author agrees that growth rates have been increasing in the modern era (roughly, the Industrial Revolution and everything after), he does not see much support for the HGH prior to the modern era. The data seems very noisy and hard to interpret, and even when using this noisy data it seems that models with constant growth rates fit the pre-modern era better than hyperbolic models. Thus, we should be uncertain between the HGH and the hypothesis that the industrial revolution triggered a one-off transition to increasing growth rates that have now stabilized.</p><p><strong>Rohin&apos;s opinion:</strong> I&#x2019;m glad to know that the empirical support for the HGH seems mostly limited to the modern era, and may be weakly disconfirmed by data from the pre-modern era. I&#x2019;m not entirely sure how I should update -- it seems that both hypotheses would be consistent with future accelerating growth, though HGH predicts it more strongly. It also seems plausible to me that we should still assign more credence to HGH because of its theoretical support and relative simplicity -- it doesn&#x2019;t seem like there is strong evidence suggesting that HGH is false, just that the empirical evidence for it is weaker than we might have thought. See also <strong><a href=""https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity?commentId=j9BymthAthZQ6dnGp"">Paul Christiano&#x2019;s response</a></strong>.</p><h1>NEAR-TERM CONCERNS </h1><h2>MACHINE ETHICS </h2><p><strong><a href=""http://arxiv.org/abs/2006.04734"">Reinforcement Learning Under Moral Uncertainty</a></strong> <em>(Adrien Ecoffet et al)</em> (summarized by Rohin): Given that we don&#x2019;t have a perfect ethical theory ready to load into an AI system, and we don&#x2019;t seem poised to get one any time soon, it seems worth looking into approaches that can deal with <em>moral uncertainty</em>. Drawing on the literature on moral uncertainty in philosophy, the authors consider several methods by which multiple moral theories can be aggregated, such as averaging over the theories, making decisions through a voting system, and having the theories compete to control the agent&#x2019;s overall actions. They implement several of these in RL agents, and test them on simple gridworld versions of various trolley problems. They find that all of the methods have advantages and disadvantages.</p><p><strong>Rohin&apos;s opinion:</strong> The central challenge here is that normalizing different moral theories so that they are comparable is <strong><a href=""https://www.alignmentforum.org/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into"">difficult</a></strong> (<strong><a href=""https://mailchi.mp/0dd8eb63fe2d/an-60a-new-ai-challenge-minecraft-agents-that-assist-human-players-in-creative-mode"">AN #60</a></strong>) (see Section 2.3). This issue plagues even computationally intractable idealizations like <strong><a href=""https://arxiv.org/abs/1606.03137"">assistance games</a></strong> (<strong><a href=""https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai"">AN #69</a></strong>) that can perform full Bayesian updating on different moral theories. I&#x2019;d love to see better theoretical solutions for this challenge.</p><h1>OTHER PROGRESS IN AI </h1><h2>DEEP LEARNING </h2><p><strong><a href=""https://arxiv.org/abs/2008.08076"">Deploying Lifelong Open-Domain Dialogue Learning</a></strong> <em>(Kurt Shuster, Jack Urbanek et al)</em> (summarized by Rohin): Most research in natural language processing (NLP) follows a paradigm in which we first collect a dataset via crowdsourced workers, and then we train a model on this dataset to solve some task. Could we instead have <em>lifelong learning</em>, in which a model could continue learning after being deployed, getting better and better the more it is used? This paper shows one instantiation of such an approach, in a fantasy role-playing game.</p><p>The authors take the previously developed LIGHT role-playing setting, and gamify it. The human player talks to a language model while playing some role, and earns stars and badges for saying realistic things (as evaluated by another language model). Rather than paying crowdsourced workers to provide data, the authors instead merely advertise their game, which people then play for fun, reducing the cost of data acquisition. They find that in addition to reducing costs, this results in a more diverse dataset, and also leads to faster improvements in automated metrics.</p><p><strong>Rohin&apos;s opinion:</strong> Ultimately we&#x2019;re going to want AI systems that learn and improve over time, even during deployment. It&#x2019;s exciting to see an example of what that might look like.</p><h2>UNSUPERVISED LEARNING </h2><p><strong><a href=""https://ai.googleblog.com/2020/08/understanding-view-selection-for.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+blogspot%2FgJZg+%28Google+AI+Blog%29"">Understanding View Selection for Contrastive Learning</a></strong> <em>(Yonglong Tian et al)</em> (summarized by Flo): <strong><a href=""https://arxiv.org/abs/1807.03748"">Contrastive multiview learning</a></strong> (<strong><a href=""https://mailchi.mp/d7e950bc8dbd/an-92learning-good-representations-with-contrastive-predictive-coding?e=0e92156a6c"">AN #92</a></strong>) is a self-supervised approach to pretraining classifiers in which different views of data points are created and an encoder is trained to minimize the distance between encodings of views corresponding to data points with the same label while maximizing the distance between encodings of views with different labels. </p><p>The efficacy of this approach depends on the choice of views as well as the downstream task the neural network is going to be trained for. To find the most promising views, the authors propose the Infomin principle: all views should keep task-relevant information while the mutual information between views is minimized. The principle is supported by various observations: Firstly, earlier approaches to contrastive learning in the image domain that use data augmentation to preserve object identity while creating diverse views can be seen as an implicit application of the Infomin principle. Secondly, varying the mutual information between views (for example by changing the distance between two cropped views of the same image) creates an inverted U-curve for downstream performance corresponding to poor performance if there is too much or too little mutual information between the views. Lastly, the authors also find an inverted U-curve in performance for different colour spaces when using channels as views and the Lab colour space which was built to mimic human colour perception is close to the optimum, meaning that human colour perception might be near-optimal for self-supervised representation learning. </p><p>The authors then use the Infomin principle to select image augmentations for contrastive pretraining and improve the state of the art in linear readout on ImageNet from 69.3% to 73% for Top-1 accuracy and from 89% to 91.1% for Top-5 accuracy.</p><p><strong>Read more:</strong> <strong><a href=""https://arxiv.org/abs/2005.10243"">What makes for good views for contrastive learning</a></strong></p><p><strong>Flo&apos;s opinion:</strong> While the Infomin principle seems powerful and their results look impressive, I am not really convinced that the principle actually played an important role in finding the image augmentations they ended up using, as there is little description of how that happened and the augmentations rather look like the result of combining previously used approaches and doing some hyperparameter optimization.</p><h2>HIERARCHICAL RL </h2><p><strong><a href=""https://bair.berkeley.edu/blog/2020/07/11/auction/"">Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions</a></strong> <em>(Michael Chang et al)</em> (summarized by Zach): Increasing the scalability of learning systems is a central challenge to machine learning. One framework is to organize RL agents as &#x2018;super&#x2019; agents, large collections of simpler agents that each make decisions according to their own incentives. If it were possible to get the incentives correct, the dominant equilibria would be identical to the optimal solution for the original RL problem.</p><p>In this paper, the authors introduce a framework for decentralizing decision-making by appealing to auction theory. There is a separate simple agent for each action. At every a timestep, a Vickrey auction is run in which each agent can bid for the superagent executing their particular action. The trick is that when an agent successfully wins a bid and acts on a state, it then &#x2018;owns&#x2019; the produced next state, and &#x2018;earns&#x2019; the result of the auction in the next round. (At the end of an episode, the owner of the state earns the reward of the trajectory.) Intuitively, the agent wants to bid on states in which it can make progress towards earning the final reward, as those will be states that other agents want to buy. The authors show that this scheme incentivizes each agent to bid the Q-value of their action in the given state, which would then lead to an optimal policy.</p><p>The authors test out this approach with some simple MDPs. They also investigate a task where they try to get the agents to rotate MNIST images so that a classifier will recognize them. Finally, they investigate task transfer by training agents on simple sub-tasks and then reusing those agents to learn a related task making use of both sub-tasks.</p><p><strong>Read more:</strong> <strong><a href=""https://arxiv.org/abs/2007.02382"">Paper: Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions</a></strong></p><p><strong>Zach&apos;s opinion:</strong> Imagine <strong><a href=""https://www.twitch.tv/directory/game/Twitch%20Plays"">Twitch plays</a></strong>, but you use a reputation to buy and sell your actions. The actual idea in the paper is slightly more mundane than this because the primitives are bidders. <strong><a href=""http://arxiv.org/abs/1906.10667"">Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives</a></strong> (<strong><a href=""https://mailchi.mp/c8ea4a5e842f/an-66-decomposing-robustness-into-capability-robustness-and-alignment-robustness"">AN #66</a></strong>) is a similar piece of work that also uses primitives as the basic level of selection. However, their incentive mechanism is different: agents pay according to how much information from the environment they use and then get a reward back for their actions. However, there&#x2019;s good reason to think options could work as well since in both of these papers there&#x2019;s evidence that primitives that learn sub-tasks are useful in new tasks.</p><h1>NEWS </h1><p><strong><a href=""https://www.cooperativeai.com/"">Cooperative AI Workshop</a></strong> (summarized by Rohin): This NeurIPS workshop has the goal of improving the <em>cooperation</em> skills of AI systems (whether with humans or other machines), which encompasses a <em>very</em> wide range of research topics. The deadline to submit is September 18.</p><p><strong><a href=""https://jobs.lever.co/openai/994b4b81-d2ef-4d74-ae80-5cdb9b6e2dfa"">Senior Systems Safety Engineer</a></strong> <em>(OpenAI)</em> (summarized by Rohin): OpenAI is hiring for a senior systems safety engineer. From my read of the job description, it seems like the goal is to apply the principles from <strong><a href=""https://static1.squarespace.com/static/53b78765e4b0949940758017/t/57d87eb6d2b8571af3501b26/1473898764674/Engineering_a_Safer_World+Nancy+Leveson.pdf"">Engineering a Safer World</a></strong> (<strong><a href=""https://mailchi.mp/b39cb50e2cea/an-112-engineering-a-safer-world"">AN #112</a></strong>) to AI development.</p><p><strong><a href=""https://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future?fbclid=IwAR3bA_4piJVHwSREGaH6g0O3CReNw3SlLNpd7jMAQTygSeMrkwyRfoPRbcA"">Early-career funding for individuals interested in improving the long-term future</a></strong> (summarized by Rohin): This Open Philanthropy program aims to provide support for people who want to focus on improving the long-term future. The primary form of support would be funding for graduate school, though other one-off activities that build career capital also count. They explicitly say that people interested in working on AI policy or risks from transformative AI should apply to this program (possibly in addition to their <strong><a href=""https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship"">AI fellowship</a></strong> (<strong><a href=""https://mailchi.mp/c8ea4a5e842f/an-66-decomposing-robustness-into-capability-robustness-and-alignment-robustness"">AN #66</a></strong>)). The stage 1 deadline is January 1, but if you submit earlier they aim to respond within 10 working days.</p><h4><strong>FEEDBACK</strong></h4><p> I&apos;m always happy to hear feedback; you can send it to me, <strong><a href=""https://rohinshah.com/"">Rohin Shah</a></strong>, by <strong>replying to this email</strong>.                         </p><h4><strong>PODCAST</strong></h4><p>An audio podcast version of the <strong>Alignment Newsletter</strong> is available. This podcast is an audio version of the newsletter, recorded by <strong><a href=""http://robertskmiles.com/"">Robert Miles</a></strong>.</p>",rohinmshah,rohinmshah,Rohin Shah,
kdYiDBjYRbXQZkTxm,Loneliness and the paradox of choice,loneliness-and-the-paradox-of-choice,https://www.lesswrong.com/posts/kdYiDBjYRbXQZkTxm/loneliness-and-the-paradox-of-choice,2020-09-09T16:30:20.374Z,17,12,4,False,False,,"<p>Loneliness is a problem of decision-making. Being around other people, whether in a social or work setting, comes with many supports to our decision-making process. In an idealized process of decision-making, each time the question ""What's next?"" pops up, we imagine a menu of options. We choose one of them, and then we go through a psychological process of committing to it. This leads to activity, and both within that activity and after it, we repeat this algorithm many more times.</p><p>Think of all the ways that the presence of other people can support this process. Just being around others comes with cultural expectations about the options we choose from in response to that constant question, ""What's next?"" Settings where two or more people are gathered also typically have some implicit or explicit structure around how options will be chosen. And once an option is chosen, the simple fact that each individual knows that it has been registered as the group's choice acts as a commitment device.</p><p>By contrast, when an individual is on their own, that simple fact does not apply. There is no group to have registered a collective choice. Have you ever had a week entirely to yourself, perhaps when you were single, and had the experience of making choices about what to do, only to spend a lot of time revisiting that choice? Considering other options even after you've selected one? Asking whether you picked the highest-priority option, even though in retrospect it didn't really matter very much which you chose?</p><p>Even more difficult for the individual is the fact that because there is no second person present, you are unable to take their preferences into consideration as you invent and weigh the options for what to do. If you do not have a strong sense of your own preferences and needs, or have some other binding constraint such as a high-pressure work schedule, then this might pose a real problem. The options might all feel a bit arbitrary. Witnessing yourself as a person who does arbitrary activities simply to fill the time may not be a pleasant reflection to see in the mirror.</p><p>Finally, the whole process of making decisions, or a particular step in it, can feel fraught. Rather than appearing to the mind as a matter of fact process, a puzzle to solve with clear steps, sometimes the mind simply flails. Ever had a dream where you had to play in a major league sports match for a game where you'd never played before, didn't know the rules, but still had to pretend like you knew what you were doing? It's like that, but the game is loneliness.</p><p>We often talk about loneliness as if the cause of that feeling was a simple absence of the physical presence, attention, or care from or for another person. This seems compelling. We imagine times when we didn't feel lonely, and indeed, those memories tend to involve relating with another person with whom we had a strong, trusting bond.&nbsp;</p><p>However, if this were the case, then it should be impossible to be content while we are alone. Clearly, it is not. Aloneness can produce loneliness, or it can produce solitude, a word used to indicate a pleasant or meaningful relationship with being by oneself. Togetherness can be a cure for the disease of loneliness, but aloneness is not guaranteed to cause it. In fact, togetherness, even with a caring, trusted person, can at times make us feel a strong desire to be by ourselves. Perhaps the population of many industrialized nations needs, on average, more togetherness, but that is a rough answer for a problem with many individual nuances.</p><p>What if it's true that the problems of aloneness and <i>also</i> of togetherness were both caused by problems of decision-making?</p><p>If that were true, then this would suggest an explicit, step-by-step process for dealing with that discomfort. It would suggest a form of deliberate practice that might help people navigate the many challenges of their daily lives. Practice would involve translating one's own or another person's feelings of distress into the question, ""What's next?"", determining a menu of appropriate options for various contexts, practicing habits, heuristics, and formal methods for making choices, and practicing commitment devices in order to move on into action.</p><p>This framework suggests also how failure modes could be analyzed. Distress with a murky, implicit process is one, as mentioned. Another might be an unrehearsed menu of options for a given context, with many inappropriate or unappealing choices ""popping up"" due to lack of familiarity, complexity, or other reasons. A third might be a lack of good methods for choosing from among the options. A fourth might be a failure to commit, or trying to commit to too many options. And a fifth might be a process that does not proceed in order, but jumps around, or where the decision-maker doesn't start at the beginning, but instead tries to make a choice without having thought of any options, or tries to commit without having made a decision.</p><p>If a shift from loneliness to solitude comes from practice in making decisions, it helps us explain why some of the cures for loneliness might work. Creativity is an oft-recommended cure for loneliness. Of course, creativity is fundamentally about committing to a project and making choices within it.</p><p>Another cure is communion with nature, observation of society, taking in art, or meditation. To move around or commit to stillness within an environment and observe and interpret it closely is another project that encompasses a feedback loop of decision-making; or alternatively, it can be the dissolution of any compulsion to make decisions. In some forms of meditation, you have already made a decision: to make no decisions. And you are simply waiting for your mind to psychologically register that commitment.</p><p>The third and probably most common solution people find to the problem of loneliness is to avoid being at a loss for how to make decisions. They might do this by distracting themselves with devices and habits that make decisions for them. They might do this by submitting to the decisions of another person, group, psychological obsession, or ideological system. Or they might build a life around a set of obligations where their own role and duties are clearly established, so that they are never at a loss for what needs to be done, and always know how they will contribute to that goal.</p><p>This is the paradox of modern life. By expanding the range of options, we have vastly increased the potential for optimization. However, the extraordinary number of options with which we are presented forces us to be constantly making choices. Many people have underestimated to an extraordinary degree the costs of decision fatigue. Compounding this problem exponentially is that making a successful choice often depends on knowing what other people will do, on the predictability of the world. As choices expand, predictability often decreases, meaning that it becomes harder and harder to commit to anything.</p><p>Worst of all, in highly competitive arenas, the range of ways to fail to make the optimal choice has expanded, while there remains just one narrow range of optimal behavior. There are more ways to miss the mark than ever before. This leads to a barrier to entry in these fields. Somebody has solved the problem and has the resources to keep optimizing further, but we, as outsiders, do not. This means we are left with a different problem, which is choosing which arenas to enter. This problem is equally hard.</p><p>These hard problems ultimately lead many individuals and groups to optimize for making decisions as easily as possible. A business that's able to give its workers a sense of meaning and purpose can treat that as a benefit with a real dollar value. An educational program that can give its participants the experience of a clear decision-making process might attract students even though it provides minimal long-term value. Determining the long-term value of a specific degree, or of a particular job, is after all a hard problem with a complex decision-making process.</p><p>If an immediately clear decision-making process has the high dollar value that I'm asserting, it should provide tremendous explanatory power for the ways that observable economic behavior doesn't seem to line up with ""rational self-interest.""</p><p>What if, for example, it's about as easy to make decisions from moment to moment when you're poor as when you're rich? If that were true, and if easy decision-making is of paramount importance to everyone, then we should expect that people simply don't put as much effort into making more money as we might expect them to. If the pursuit of wealth generates as many decision-making problems as the acquisition of money resolves, then it's roughly a wash.</p><p>This, then, is my grand theory of human problems. It is the paradox of choice.</p><ul><li>More choices give better optimums, which is very good.</li><li>More choices also expand the range of sub-optimal outcomes, which in highly competitive arenas discourages entry.</li><li>Individual people often don't understand clearly that much of their distress comes from an inchoate, unpracticed, burdensome, misguided decision-making process.</li><li>People relentlessly avoid having to deal with lack of resolution and making decisions for themselves, which is extremely costly in the long run on other values that they ostensibly care about a great deal.</li><li>Many problems on an individual and group level, from psychological distress, to failed marriages, to lack of entrepreneurship, to institutional failure, to cultural breakdown, have to do with failures of decision-making. There are such an enormous number of possibilities for how this could play out that it is probably much more helpful to do a careful analysis of each individual case to offer correctives.</li><li>Vastly more personal investment in exploring on an intimate level how one makes decisions, how decision-making issues affect one's personal well-being and psychological adjustment, and deliberate practice in personal decision making could have enormous benefits in one's own life.</li><li>Developing skills and tools to help others make decisions <i>more easily</i> is at least as important as helping others make <i>better</i> decisions.</li></ul>",AllAmericanBreakfast,directedevolution,DirectedEvolution,
xeHpSrQq24tf5F4YP,"Budapest Meetup at Moved Online due to Corona, Sept 20 at 2pm",budapest-meetup-at-moved-online-due-to-corona-sept-20-at-2pm,https://www.lesswrong.com/events/xeHpSrQq24tf5F4YP/budapest-meetup-at-moved-online-due-to-corona-sept-20-at-2pm,2020-09-09T16:07:54.970Z,4,2,0,False,False,,"<p><a href=""https://meet.jit.si/LessWrongBudapest"">https://meet.jit.si/LessWrongBudapest</a></p><br><p>Here are the articles suggested for discussion:</p><p><a href=""https://www.greaterwrong.com/posts/B2CfMNfay2P8f2yyc/the-loudest-alarm-is-probably-false"">https://www.greaterwrong.com/posts/B2CfMNfay2P8f2yyc/the-loudest-alarm-is-probably-false</a></p><p><a href=""https://www.wired.com/story/opinion-to-adapt-to-tech-were-heading-into-the-shadows/?fbclid=IwAR3mFg0H-0QMHLRehcr-ziF604SQGeIOMz1DqGfD9O_Gc2knFUPKxLdOi0k"">https://www.wired.com/story/opinion-to-adapt-to-tech-were-heading-into-the-shadows/</a> </p>",timothy-underwood-1,timothy-underwood-1,Timothy Underwood,
9gdyYmAsgZpxDhCnh,Leslie's Firing Squad Can't Save The Fine-Tuning Argument,leslie-s-firing-squad-can-t-save-the-fine-tuning-argument,https://www.lesswrong.com/posts/9gdyYmAsgZpxDhCnh/leslie-s-firing-squad-can-t-save-the-fine-tuning-argument,2020-09-09T15:21:19.084Z,11,8,6,False,False,https://www.sleepingbeautyproblem.com/7-1-leslies-firing-squad/,"<p><em>This post argues against <a href=""https://plato.stanford.edu/entries/fine-tuning/"">fine-tuning</a>, especially the fine-tuning argument for design. It again shows the importance of perspectives in  reasoning as discussed in a <a href=""https://www.lesswrong.com/posts/zjNRPgZAx8cgHgH9n/anthropic-reasoning-and-perspective-based-arguments"">previous post</a>.</em></p>
<p>I <a href=""https://www.lesswrong.com/posts/R9QkdCeLkeqW34TSz/hello-ordinary-folks-i-m-the-chosen-one-1"">previously argued</a> why fine-tuning is a misconception. Reasoning from a first-person perspective, I endorse the Weak Anthropic Principle (WAP) response. Leslie’s Firing Squad is perhaps the most famous argument against this response. I think it would be interesting to examine it in detail.</p>
<p>It should be pointed out my objection to fine-tuning differs from most, if not all, anthropic objections since perspective-based argument (PBA) rejects the notion of observation selection effect (OSE). The WAP should only be regarded as a logical truth, not as selection bias. Nonetheless, the firing squad argument can still be used to counter PBA. Therefore it deserves an explanation.</p>
<h2>The Firing Squad</h2>
<p>Suppose you are a prisoner facing the death penalty. You are blindfolded and placed against a wall. A firing squad consist of a dozen expert marksmen stands just a few meters away to carry out the deed. The order is given. The shots ring out. Yet somehow you find yourself alive after the whole ordeal. Every single bullet has missed.</p>
<p>Considering you are still alive you should be vastly more confident that the marksmen intentionally spared you, i.e. by design. After all, the probability of all 12 marksmen missing by chance is immeasurably small.</p>
<h2>The Rebuttal To WAP</h2>
<p>The firing squad scenario may seem analogous to the fine-tuning argument. WAP suggests all fundamental constants being compatible with our existence is a logical truth, i.e. to be expected. Therefore it is no evidence supporting a designed universe. Then by the same logic, after surviving the firing-squad I should also conclude that is to be expected. Because being alive is the only observation I can make, a logical truth. Consequently, it is not evidence suggesting the marksmen intentionally missed either. That is obviously irrational.</p>
<p>I suggest the analogy drawn between the firing squad and the fine-tuned universe is flawed. There is a critical difference between the two cases. Based on my research, this rebuttal has not been published before.</p>
<h2>Analysis of the Firing Squad</h2>
<p>The core of my argument (PBA) is to reason from one consistent perspective. So I am going to be explicit about it in the analysis.</p>
<p>Let’s take the prisoner’s first-person perspective. After hearing the shots and realizing I am still alive, I shall reason as follows: The world ought to be compatible with my existence. Whereas the scenario of the marksmen successfully hit thus killing me is incompatible. Therefore it is precluded as a possibility. All there remains is either they intentionally spared me or the immensely improbable case that they all missed by random chance. This would greatly increase the probability that they missed on purpose.</p>
<p>Now let’s take the perspective of some random bystander. Here I would make the exact same probability update as the prisoner. Though there are some differences in the reasoning. This update is not based on the logical truth of WAP. All execution outcomes are compatible with my existence. Instead, I would use the prisoner’s survival as new evidence, since I am not guaranteed to find him alive. The end result is the same: a major increase in the probability that the marksmen missed intentionally.</p>
<h2>The Disanlogy</h2>
<p>Both the prisoner and the bystander analyze the problem by focusing on the prisoner. This may seem like an obvious thing to do. Trivial, even. However, this perspective asymmetry is based on our background knowledge of what a firing squad is. A firing squad is designed to execute prisoners. We know its purpose. That’s why no matter which perspective we choose to take, the attention is always on the prisoner. Because knowing the firing squad’s purpose, the prisoner’s life-or-death is logically significant to the problem.</p>
<p>That is distinctively different from the fine-tuning argument. The purpose of the universe is unknown. Nothing is logically inherently significant. The fine-tuning argument is formulated to focus on the perspective center. I would ask why is the universe compatible with “my” existence. The fine-tuning argument chooses to use more generalized categories of “my kind” (such as humans, life, sentient observers, or complex physical system) instead of using “me” specifically, to appear less egocentric. It is only a trick to make more allies. Yet the fact remains fine-tuning is rooted in the first-person perspective.</p>
<h2>Think As An Alien</h2>
<p>Let’s use our imagination, picture some intelligent alien in a far corner of the universe whose physical structure is radically different from us such that our usual definition of “life” doesn’t describe it. Now let’s think from its perspective. When pondering upon the fundamental constants of the universe, it won’t ask “why is the universe compatible with “life” as defined by some bipedal carbon-based creatures light-years away?”. There is no reason for this attention. In the spirit of the fine-tuning argument, it would only ask “why is the universe compatible with me? (or my kind)”.</p>
<p>Now suppose we introduce Leslie’s Firing Squad to the alien. As long as it understands the concept of execution, it would analyze the problem basing on the prisoner’s life-or-death, not its own. This is where the analogy breaks down. The fine-tuning argument depends on one’s own existence. Whereas Leslie’s Firing Squad, with the knowledge of its purpose, depends on the prisoner’s. Supporters of fine-tuning argument deliberately picked the prisoner’s perspective in their rebuttal to make the two cases superficially similar.</p>
<p>That is why the WAP is indeed the correct response to the perspective based fine-tuning problem. Because to myself, my existence, and the world being compatible with it, is a logical truth. Period. For the firing squad, from the prisoner’s perspective, finding myself alive is not in itself surprising. But the fact that I am analyzing the outcome of my own execution is. This surprise hinges on my background knowledge of the firing squad’s purpose. That is what motivates the probability update.</p>
<h2>Begging the Question</h2>
<p>The fine-tuning argument for design is basically asking a perspective-dependent question while demanding a perspective-independent answer. They formulated a first-person specific problem yet insist on an objective explanation. It is a paradox caused by not reasoning from a consistent perspective.</p>
<p>In my opinion, using Leslie’s Firing Squad as a rebuttal achieves the opposite effect. It highlights the issue of the fine-tuning argument for design. The argument analyzes the fundamental constants by their compatibility with life as an obvious thing to do. Yet, as the firing squad case shows, that would require a prior assumption of the universe’s purpose being life-related. Instead of arguing for life’s significance, it subtlely builds that conclusion into the premises. <a href=""https://en.wikipedia.org/wiki/Begging_the_question"">Begging the question</a>, essentially.</p>
",dadadarren,dadadarren,dadadarren,
i3pxXyeQozF2Yj2ZR,What's the CFAR position on how the workbook can be used?,what-s-the-cfar-position-on-how-the-workbook-can-be-used,https://www.lesswrong.com/posts/i3pxXyeQozF2Yj2ZR/what-s-the-cfar-position-on-how-the-workbook-can-be-used,2020-09-09T13:56:52.514Z,14,4,2,False,True,,"<p>The <a href=""https://rationality.org/files/cfar-handbook.pdf"">CFAR handbook</a> is a great resource for applied rationality. I talked with another rationlist in Berlin about creating a reading group around it. Did CFAR articulate a position around how the handbook can be used and whether such a group would be okay? If so, are there any norm such a reading group should fulfill (e.g. making it clear it's independent of CFAR)?</p>",ChristianKl,christiankl,ChristianKl,
yTxHnfoD3L8CdezcG,How To Fermi Model,how-to-fermi-model,https://www.lesswrong.com/posts/yTxHnfoD3L8CdezcG/how-to-fermi-model,2020-09-09T05:13:19.243Z,86,36,9,False,False,,"<p><i>[Note from Eli in 2020: I wrote this document in 2016, in conjunction with two workshops that I helped Oliver Habryka run. If I were to try and write a similar document today, it would likely be substantially different in form and style. For instance, reading this in 2020, I’m not very compelled by some of the argumentation that I used to justify this technique, and I think I could have been clearer about some of the steps.</i></p><p><i>Nevertheless, I think this is some useful content. I’m not going to take the time to write a new version of this document, so it seems better to share it, as is, instead of sitting on it.]</i></p><p>Oliver Habryka provided the seed material and did most of the development work on this technique. He gets upwards of 90% of the credit, even though I (Eli) wrote this document. Thanks Oli!</p><h1>Introduction:</h1><h2><strong>Rationale for Fermi Modeling:</strong></h2><p>Making good decisions depends on having a good understanding of the world: the better one’s understanding the better one’s decisions can be. Model-building procedures allow us to iteratively refine that understanding.&nbsp;</p><p>Using any model-building procedure at all is a large step up from using no procedure at all, but some procedures are superior to others. If possible, we would want to use techniques that rely on verified principles and are based on what we know about how the mind works. So, what insights can be gleaned from the academic social and cognitive sciences that is relevant to model-building?&nbsp;</p><p>First, Cognitive psychology has shown, many times over, that very simple algorithmic decision rules frequently have just as much predictive power, and&nbsp; even outperform, human expert judgment. Deep, specific models that take into account many details specific to the situation (inside views) are prone to overfitting, and are often inaccurate. Decision rules combat biases like the Halo effect and consequently tend to produce better results.</p><p>For instance, a very simple equation to predict the probability that a marriage will last is:</p><blockquote><p>Frequency of lovemaking / Frequency of fights</p></blockquote><p><i>(Where a higher number represents a more stable marriage. Example taken from Thinking Fast and Slow ch. 21)&nbsp;</i></p><p>This assessment measure is intuitive and uncomplicated, and it predicts length of marriage about as well as any other method, including expert evaluation by experienced couples counselors. Most of the relevant information is encapsulated in just those two variables: more detailed analysis is swamped by statistical variance and tends to make one overconfident. And the algorithm has the additional distinct advantage of being cheap and easy to deploy: simply plug in the variables and see what comes out.</p><p>The upshot is simple numeric algorithms are powerful.</p><p>Second, is the study of forecasting. One of the most significant takeaways from Philip Tetlock’s Expert Political Judgment project is that “foxes” (who use and integrate multiple methods of evaluation, models, and perspectives) fare better than “hedgehogs” (who use a single overriding model or methodology that they know very deeply).&nbsp;</p><p>This poses a practical problem however. There are dozens of known psychological phenomena (anchoring, priming, confirmation bias, framing effects, attentional bias) that make it cognitively difficult to think <i>beyond</i> one’s first idea. Once one has developed a model or a solution, it tends to be “sticky”, coloring and constraining further thinking. Even if you want to generate new models, it’s hard <i>not</i> to anchor on the one you had in front of you a moment ago. As Kahneman colorfully puts it, “What you see is all there is,” or so it seems.&nbsp;</p><p>Given this, we want decision processes and planning protocols that are <strong>1)</strong> algorithmic, using simple equations or scoring rules with variables that are easy to assess <strong>2)</strong>&nbsp; foxy, in that they incorporate many models instead of one, and <strong>3) </strong>help mitigate the psychological biases that make this difficult.</p><p>Fermi Modeling is an attempt at a model building-procedure that caters to these constraints. It is quantitative, Foxy, and designed to compensate, at least somewhat, for our native biases.&nbsp;</p><p>In contrast to other methods of theorizing and model building, Fermi Modeling is less about going deep and more about going broad. Instead of spending a lot of time building one, very detailed, sophisticated, and precise model, the emphasis is on building <i>many</i> models rapidly.</p><p><strong>Overview:</strong></p><p>Fermi Modeling is a brainstorming and theorizing technique that encourages you to flip between multiple frames and perspectives, primarily by <i>moving up and down levels of abstraction.&nbsp;</i></p><p>In broad strokes, the mental process of Fermi Modeling looks like this:</p><figure class=""image""><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_640 640w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_960 960w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_1120 1120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_1280 1280w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_1440 1440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/db0ad5008a8edbf944a664def43769ad85d5b6b70dfd9122.png/w_1600 1600w""></figure><p>You start, in <strong>step 1 (red)</strong>, by moving <i>up</i> a level of abstraction, by considering reference classes or categories into which your object or problem of interest falls.&nbsp;</p><p>Then, in <strong>step 2 (green)</strong>, you move <i>down</i> a level of abstraction to generate models applicable to each reference class (with no regard for the original question.)&nbsp;</p><p>Then in <strong>step 3 (blue)</strong>, you apply the models generated in step 2 and 3 to the original question.&nbsp;</p><p>The point is to get you to consider questions that you wouldn’t naively ask.&nbsp;</p><p>For instance, suppose I’m considering the question, “how do I determine which people I should spend time assisting, teaching, or otherwise, making better?”</p><p>This question brings to mind certain reference frames and criteria. I can think of people in this context as independent agents to implement my goals, which brings to mind consideration such as value alignment, power in the world, and discretion. I can think of people as teammates,&nbsp; which indicates other important factors such as personal compatibility with me and the complementary-ness of our skill sets. I can think of people as trade partners, which would lead me to consider what value they can give me.&nbsp;</p><p>Furthermore, I can change my focus from the object, “people”, to the verb. I could rephrase the question as “who do I invest in?”, which gives me the reference frame of “investment”. This immediately brings to mind a whole set of models and formulas: compound interest and risk assessment. This yields considerations such as the principal investment, time to pay off, and probability of pay off.&nbsp;</p><p>These finance-flavored factors are obviously relevant to the question of “whose growth I should nurture?”, but <i>I would not have considered them by default</i>. They don’t come to mind when just thinking about “people”, only when thinking about “investments”</p><p>Fermi Modeling is a process designed to generate ideas that don’t come to mind by default, and to facilitate rapid consideration of many “angles of attack” on a problem, when creating models and evaluation criteria.</p><h1>Method</h1><p>A note on time allocation: one of the advantages of this method is that it pays off quickly, but can still generate large value with large time investment.&nbsp;</p><p>You can Fermi Model for 15 minutes and get rapid useful results, on a quick question, or you can do it for several hours, or even block out a whole day, to consider one particularly important decision. How much time you spend on each step is flexible and subject to personal preference. Just make sure you have enough time to consider at least three reference frames, and aggregate, at the end.</p><h2><strong>Step 0.&nbsp;</strong></h2><p>Ask a “how” or&nbsp; “what”&nbsp; question. In particular, look for questions that have a sense of gradient or variation: what causes a thing to be better, easier, bigger, or more impactful. What is a variable that you are trying to maximize or minimize?</p><p>Your question should impinge on your actions somehow. The answer to this question will inform some decision about how to act in the world.</p><p>Alternatively, this could be a question of general assessment: determining the overall “quality” or “goodness” of some object of interest, be it an organization, a process, a technology, etc.</p><figure class=""table""><table style=""border-bottom:none;border-left:none;border-right:none;border-top:none""><tbody><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top;width:50%""><p><strong>Prompts</strong>&nbsp;</p></td><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><p><strong>Examples</strong>&nbsp;</p></td></tr><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><ul><li>Try to find an optimization problem.</li><li>What determines the quality of X?</li><li>How can I do X best?</li><li>What determines how much of Y the system that I want to understand produces?</li><li>What determines the probability that my system produces Y?</li></ul></td><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><ul><li>What is the most effective way to make money?</li><li>What should the program for EA Global be?</li><li>How can I make more friends?</li><li>How can I get more work done?</li><li>How can I learn math faster?</li><li>How do I run the best workshop?</li><li>How do I build political influence?</li><li>How can I find a good boyfriend/girlfriend?</li><li>How can I find a co-founder for my company?</li><li>What do I make of CFAR?</li></ul></td></tr></tbody></table></figure><p><strong>Step 0.5.&nbsp;</strong></p><p>Once you have written the initial question, rephrase the question in multiple ways. Try to ask the same question, or a nearby question, using different terminology. (This can involve small refactorings of the goal.) Doing this can introduce a little “conceptual jitter”, that can sometimes yield fruitful distinctions.</p><figure class=""table"" style=""width:100%""><table style=""border-bottom:none;border-left:none;border-right:none;border-top:none""><tbody><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><p><strong>Examples</strong></p></td></tr><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><ul><li>What is the most effective way to make money?</li><li>What determines a person’s income?</li><li>How does one maximize earning power?</li><li>What should the program for EA Global be?</li><li>What is the best version of EA Global?</li><li>What should I have participants do at EA global?</li><li>How can I make more friends?</li><li>What makes people want to be friends with other people?</li><li>How do relationships form?</li><li>How can I get more work done?</li><li>What makes stuff get done faster?</li><li>What contributes to my losing time?</li><li>How can I learn math faster?</li><li>What’s the most efficient way to learn academic subjects?</li><li>What’s holding me back from knowing math?</li><li>How do I run the best workshop?</li><li>What makes a workshop good?</li><li>How do I build political influence?</li><li>How do I get large groups to do stuff?</li></ul></td></tr></tbody></table></figure><h2><strong>Step 1: Abstract</strong></h2><p>Generate reference frames&nbsp;</p><ol><li>Mark or underline all the key terms in each phrasing. To a first approximation, underlining all the nouns and all the verbs works. [If you are doing Fermi modeling as an evaluation procedure, you can skip this part].</li><li>For each of the marked terms, list reference classes for which the term is an example. You want to move up a level of abstraction, considering all the categories into which the term fits. You want to generate between 15 and 50 reference frames (of which you might use 4 to 6).</li></ol><figure class=""table"" style=""width:100%""><table style=""border-bottom:none;border-left:none;border-right:none;border-top:none""><tbody><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top;width:50%""><p><strong>Prompts</strong>&nbsp;</p></td><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><p><strong>Examples</strong>&nbsp;</p></td></tr><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><ul><li>What <i>is</i> x?</li><li>What is x an instance/example of?</li><li>“Everything is a case-study”</li><li>What different reference classes would scientists from different fields put this in?</li></ul></td><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><p><strong>EAG</strong>&nbsp;</p><ul><li>Conference</li><li>Social Gathering</li><li>Informational Message</li><li>Educational Content</li><li>Bunch of monkeys together</li></ul><p><strong>How can <u>I</u> <u>make</u> more <u>friends</u>?</strong>&nbsp;</p><ul><li>Relationships</li><li>Mammals</li><li>Search procedures</li><li>Partners for playing</li><li>Emotional support</li></ul></td></tr></tbody></table></figure><h2><strong>Step 2: Model</strong></h2><p>Rapid model building on each of the frames:</p><p>Take one of the reference frames that you generated in the last step and Fermi model on it.</p><p>We recommend, if you are doing this for the first time, that you start with a frame other than the one you think is most useful, interesting, or relevant to your problem. Often, people pick one frame and build one model and feel like they’re done. After all, the “correct” model is right in front of them; why would they bother constructing another, inferior model? Starting with a less-than-your-favorite frame encourages you to build more than one model.</p><p><strong>Step 2.1</strong></p><p>Identify first order factors. Consider what variables would determine the “quality” or quantity of things in the reference class. This often takes the form of asking “what makes an X good?”. A thing can be more or less X or more or less of a good X.&nbsp;<br><br>Note that we are only looking at <i>first-order</i> factors. The models that we are generating are intended to be quick and rough. There will, for most categories, be many, many factors that exert a small influence on the overall outcome. We are only looking for as many factors as will have a sufficient effect as to influence the order of magnitude of the outcome. What factors explain most of the variance?</p><figure class=""table""><table style=""border-bottom:none;border-left:none;border-right:none;border-top:none""><tbody><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;vertical-align:top;width:50%""><p><strong>Prompts</strong>&nbsp;</p></td><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><p><strong>Examples</strong>&nbsp;</p></td></tr><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><ul><li>What makes things in this reference class good?</li><li>How do things in this reference class work in general?</li></ul></td><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><p><strong>Social Gathering</strong>&nbsp;</p><ul><li>Number of People</li><li>“Quality” of average person</li><li>Number of new connections</li></ul><p><strong>Search Procedure</strong>&nbsp;</p><ul><li>Pool available to search</li><li>Accuracy of filtering procedure</li><li>Speed of filtering procedure</li></ul></td></tr></tbody></table></figure><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>Step 2.2</strong></p><p>Use simple mathematical operations (*, /, +, -, average, min, max, squared) to describe the relationships between first order factors.&nbsp; Write a function that describes how changes in the inputs change the output.</p><p>If you don’t know where to start, simply multiply your first order factors together, and then check to see if the resulting model makes sense as a first approximation. If it doesn’t, tinker with it a little by adjusting or adding terms.</p><figure class=""table"" style=""width:100%""><table style=""border-bottom:none;border-left:none;border-right:none;border-top:none""><tbody><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><p><strong>Examples</strong>&nbsp;</p></td></tr><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><p><strong>Social Gathering =&nbsp;</strong><span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Quality\_Of\_Connections * \frac{\#People}{\#N\_Connections}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.446em;"">Q</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">l</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.291em; padding-bottom: 0.372em;"">_</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">O</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;"">f</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.291em; padding-bottom: 0.372em;"">_</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;"">C</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">c</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.298em;"">∗</span></span><span class=""mjx-mfrac MJXc-space2""><span class=""mjx-box MJXc-stacked"" style=""width: 5.675em; padding: 0px 0.12em;""><span class=""mjx-numerator"" style=""font-size: 70.7%; width: 8.025em; top: -1.594em;""><span class=""mjx-mrow"" style=""""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.519em;"">#</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">l</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span></span></span><span class=""mjx-denominator"" style=""font-size: 70.7%; width: 8.025em; bottom: -0.898em;""><span class=""mjx-mrow"" style=""""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.519em;"">#</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;"">N</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.291em; padding-bottom: 0.372em;"">_</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;"">C</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">c</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.372em; padding-bottom: 0.298em;"">t</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">n</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">s</span></span></span></span><span style=""border-bottom: 1.3px solid; top: -0.296em; width: 5.675em;"" class=""mjx-line""></span></span><span style=""height: 1.763em; vertical-align: -0.635em;"" class=""mjx-vsize""></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span></span></p><p><strong>Search Procedure =&nbsp;</strong><span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""Accuracy * Pool\_Size * Speed""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em;"">A</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">c</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">c</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">u</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">r</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">a</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">c</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.298em;"">∗</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;"">P</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">o</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">l</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""margin-top: -0.291em; padding-bottom: 0.372em;"">_</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em;"">i</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;"">z</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mo MJXc-space2""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.151em; padding-bottom: 0.298em;"">∗</span></span><span class=""mjx-mi MJXc-space2""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;"">S</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.446em;"">p</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">e</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;"">d</span></span></span></span></span></span></span></p></td></tr></tbody></table></figure><p>You can quickly check your models by looking for 0s. What happens when any given factor is set to 0 or to arbitrarily large? Does the result make sense? This can inform your expressions.</p><p>If you aren't familiar with the notion, try drawing a graph, that holds all but one of the inputs constant. You can use the graph to reverse engineer the mathematical notion if you want.</p><p>You <i>can</i> do more work on these models, primarily by decomposing your first order factors into more basic components. But this is usually misguided. These models are rough, based only on simple intuitions, making them more detailed at this point makes them more precise than their general accuracy warrants. In most cases, it only makes sense to add detail after we have had opportunity to test our models empirically.</p><p>Some of the models you generate may be cached, standard models from one domain or another. For instance, there are known, simple equations for compound interest. This is perfectly fine, and in fact, is quite good. Those models come pre-vetted and verified.&nbsp;</p><p>The process of generating a single model should not take more than 6 minutes in most cases, as a beginner.&nbsp;</p><p><strong>Step 2.3</strong></p><p>Build as many such models in a given reference class as you’d like. Two or three is usually sufficient.&nbsp;</p><p>Some people find this step somewhat difficult. There are a couple of “tricks” that you can apply to reframe and generate more models.</p><ol><li>Do a resolve cycle: set a timer for five minutes (or two minutes) and come up with as many models as you can before it rings. Get into the mindset of “I <i>have</i> to do it.”</li><li>Reverse the question:&nbsp; If you’ve been considering what makes a thing good, then ask what would make it bad. (“If you can’t optimize, pessimize.”)</li><li>Consider how scientists or academics from various disciplines would approach this problem? How does a historian look at this? A mechanical engineer? A biologist? An economist?</li><li>Consider an alternative way to parse the world. A good way to do this is to forbid the use of the factors you used in your first model. How <i>else</i> could you make sense of this situation?</li><li>Ask the person next to you. It’s often surprising how different the models that another person will generate are.</li></ol><figure class=""table""><table style=""border-bottom:none;border-left:none;border-right:none;border-top:none""><tbody><tr><td style=""border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top""><p>Some notes on models:</p><ul><li>Consider all the costs. They usually go in the denominator.</li><li>Time is often an input, but it usually has a negligible effect on the output</li><li>Econ 101: Remember to account for opportunity cost (subtracted from the main body of the expression). Is the next best option much worse than this one?</li><li>Probabilities are expressed as values between 0 and 1. It may be helpful to consider what distribution a value is drawn from.</li><li>You can put in constant multiples.</li></ul></td></tr></tbody></table></figure><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>Step 2.4 (Optional)</strong></p><p>Generate examples to spur models: think of hypothetical, or better yet, actual examples of the reference class.&nbsp; Consider how they fare in terms of each of your main factors. What makes each example good? How could you tweak them to make them better or worse?</p><p><i>(optional) Test: generate counterexamples</i></p><p><strong>Step 2.5:</strong></p><p>Repeat step 2. Build more models on each reference frame in turn.</p><p>Really do this! I’ve sometimes seen people (and am personally prone to) become quite anchored on or attached to their first model they / I build, since it seems obviously correct. Most of the value of this method comes building many models.</p><h2><strong>Step 3: Aggregate</strong></h2><p>Once you’ve generated some models, you know want to go back and evaluate your original question. Some of the models you built won’t be relevant to the original question, but you should be sure to consider each one before dismissing it. Remember, the whole point is to generate considerations that wouldn’t have occurred to you by default.&nbsp;</p><p>There are lots of ways to do this.</p><p>Looking at each of your models / functions, compare to the situation you’re considering (your workshop, for instance). Estimate values for each of the terms in each model. Do the majority of the models recommend one type of action?</p><p>You can use the models you’ve generated abstractly in the more concrete context. What happens when you adjust the factors on your actual plan?</p><p>Try and come up with a plan that scores perfectly on each model. See how much overlap there is between those plans. Can you goal-factor and get most of the benefit?</p><p>The quantitative nature of your models means that you can also take your subjective analysis out of it. Set up a scoring system, that takes all the inputs for a plan and returns an aggregated score.</p><h1>Closing thoughts&nbsp;</h1><p><strong>Advantages</strong>:</p><p>Since, for most of the process, you’re not focusing on the original question at all, but rather building models only in the context of the reference frame, you avoid, somewhat, the “stickiness” of your initial models. You’re less likely to get stuck thinking that the way you modeled the problem is the “correct” model (and then being resistant to seeing other perspectives, due to a whole slew of biases), since you shouldn’t be thinking about the original problem at all.&nbsp;</p><p>As mentioned above, this method scales easily with more time invested. It’s also parallelizable.&nbsp; it’s easy to have multiple people on a team all Fermi modeling on the same topic, and each of them is likely to come up with novel, useful insights. I’d recommend that each person do step 1 independently, have everyone share frames, then have each person do 2 and 3 on a subset of the frames.</p><p><strong>Disadvantages:</strong></p><p>This process is designed to produce rough heuristics rapidly. Sometimes a deep understanding of the specific situation is necessary.&nbsp;</p><hr><p><strong>Further reading:</strong></p><p><i>Clinical vs. Statistical Prediction: A Theoretical Analysis and a Review of the Evidence</i> by Paul Meehl: a slim but dense volume, this a classic of the the field that first made the case for numeric algorithms over expert judgment.</p><p>Chapter 21 of <i>Thinking Fast and Slow</i> by Daniel Kahneman is a popular overview of how simple algorithmic decision rules frequently outperform expert judgment.</p><p><i>Expert Political Judgment: How Good is it? How can we Know?</i> by Philip Tetlock is a compendium on the research project that gave rise to the Fox vs. Hedgehog distinction.</p><p>Biases that are relevant</p><p><a href=""http://www.overcomingbias.com/2008/11/abstraction-vs.html""><u>Abstraction vs. Analogy</u></a> by Robin Hanson is a good, brief example of considering an object in terms of several various reference frames.</p><p><a href=""http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/""><u>Cluster Thinking vs Sequence Thinking</u></a> by Holden Karnofsky is an essay on making decisions on the basis of weighing and integrating many models.</p><p><i>How to Measure Anything: How to Find the Value of Intangibles in Business&nbsp;</i></p><p>by Douglas W. Hubbard is an excellent primer on applying quantitative measurements to qualitative domains.</p>",habryka4,habryka4,habryka,
j7TsBk9AxnLRxAEBN,Updates Thread,updates-thread,https://www.lesswrong.com/posts/j7TsBk9AxnLRxAEBN/updates-thread,2020-09-09T04:34:20.509Z,56,19,41,False,False,,"<p>If you've <a href=""https://wiki.lesswrong.com/wiki/Belief_update"">updated your belief</a> about something you think is worth noting, post it here.</p><ul><li>It doesn't have to be a full blown ""mind change"", just an <a href=""https://www.lesswrong.com/posts/627DZcvme7nLDrbZu/update-yourself-incrementally"">incremental</a> <a href=""https://www.youtube.com/watch?v=d6PgCN7ySGQ"">update</a> to your beliefs.</li><li>I'm thinking it'd be good to have a low bar for what is ""worth noting"". Even if it's something trivial, I figure that the act of discussing updates itself is beneficial. For rationality practice, and for fun!</li><li>That said, I also expect that browsing through updates that other people on LessWrong make will lead to readers making similar updates themselves a decent amount of the time.</li><li>I've been developing a strong opinion that journaling and self-reflection in general is incredibly useful. Significantly underrated even among those that preach it. This thread is a way to perform such journaling and self-reflection.</li></ul>",adamzerner,adamzerner,Adam Zerner,
nTP8eQkiu44fKaBSf,CTWTB: Paths of Computation State,ctwtb-paths-of-computation-state,https://www.lesswrong.com/posts/nTP8eQkiu44fKaBSf/ctwtb-paths-of-computation-state,2020-09-08T20:44:08.951Z,41,14,1,False,False,,"<p><i>This is the second post of Category Theory Without The Baggage; </i><a href=""https://www.lesswrong.com/posts/B4DuwmtqF3HhNwvua/category-theory-without-the-baggage""><i><u>first post here</u></i></a><i>. You can probably follow most of this post without reading the first one. Be warned that I am a novice when it comes to category theory; please leave a comment if you see a substantive error or oversight.</i></p><p>Suppose we want to evaluate&nbsp;<span><span class=""mjpage""><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\frac{x(x+3)}{x+y}""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mfrac""><span class=""mjx-box MJXc-stacked"" style=""width: 2.404em; padding: 0px 0.12em;""><span class=""mjx-numerator"" style=""font-size: 70.7%; width: 3.4em; top: -1.706em;""><span class=""mjx-mrow"" style=""""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">(</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mn""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.372em; padding-bottom: 0.372em;"">3</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.593em;"">)</span></span></span></span><span class=""mjx-denominator"" style=""font-size: 70.7%; width: 3.4em; bottom: -0.787em;""><span class=""mjx-mrow"" style=""""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.298em;"">x</span></span><span class=""mjx-mo""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.298em; padding-bottom: 0.446em;"">+</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;"">y</span></span></span></span><span style=""border-bottom: 1.3px solid; top: -0.296em; width: 2.404em;"" class=""mjx-line""></span></span><span style=""height: 1.763em; vertical-align: -0.557em;"" class=""mjx-vsize""></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></span></span></span>&nbsp;at the point x = 2, y = 5. We have a choice about the order in which to perform the operations. One possibility:</p><ul><li>Add x+y</li><li>Add x+3</li><li>Multiply x*(x+3)</li><li>Divide</li></ul><p>Another possibility:</p><ul><li>Add x+3</li><li>Add x+y</li><li>Multiply x*(x+3)</li><li>Divide</li></ul><p>This hopefully seems trivial in such a simple example, but we want to be able to easily talk about this sort of thing in more complicated problems. To that end, we’re going to visualize these possible computation-orders in terms of graphs.</p><p>We’ll start with a graph representing the expression itself - i.e. the usual visual for a circuit:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/mmi0o53acwplgxemi1j4"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/yjsge3czahymuqntydqy 119w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/dht4jepjvc7ewqtfdcye 199w""></figure><p>We can represent “computation state” as a cut through this DAG. For instance, we start out in this state:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/yhpjmkih9fuln0ydwb9j"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/lfngfcugypxbkdayvnay 119w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/zeuavqn7vk1kak5wfmoy 199w""></figure><p>The state (x, x, x, y) gives the data carried by each edge we cut through. If we imagine that the computation at each node is performed by a different person, then at the very beginning of the computation, these would be the messages sent from our first two people (i.e. the people with our input values x = 2 and y = 5) to the people downstream. Alternatively, if a single CPU were performing this computation, it would probably save some memory by only storing x once rather than keeping three separate copies; more on that later.</p><p>From there, we have two possible next steps:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/ksdrlinuptfnk3a9bbie"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/ygh1dtk8cdjr5g9v22di 101w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/auiyky25i97ane4chsqd 181w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/brixr4pxpxvtryo55fhj 261w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/zldklea0prahoblvfxsi 341w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/yowg4r2fvpx2drhqh0qa 421w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/ml1scvgxxjlqahva2sfy 501w""></figure><p>On the left, we compute x+3 first, so our state updates to (x+3, x, x, y). On the right, we compute x+y first, so our state updates to (x, x, x+y).</p><p>After performing either of these operations, we can perform the other, leaving us in the state (x+3, x, x+y). We can visualize this as two computation-paths in a computation-graph:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/blj4nd0bm88zebncxsfi"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/ay6rjt0lbpkjcq13tn7k 85w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/cgj3dewcxk7joz6gag58 165w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/ywaepsjhkxqej6tm8fdv 245w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/blovpl6ho7kv3jwlebsw 325w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/okjied6dvptycgfmvep0 405w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/fd6k0lwcbgjjcz78pjuo 485w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/rz2ihckxo9atwzbdgkvu 565w""></figure><p>Or, dropping all the visuals of the circuit-cuts:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/lvnoaoujjalgr0vyg3mu"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/xdwweizedmp5v9dwfb2u 122w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/ptv9r2vqgg8x69vychg7 202w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/usnt15u5zbwge1dkzcph 282w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/rizyaoew56mgnwpdoklj 362w""></figure><p>The two paths from (x, x, x, y) to (x+3, x, x+y) correspond to the two possible orders of the addition operation: x+3 followed by x+y, or x+y followed by x+3.</p><p>Here’s the full graph of computation states, with the multiplication and division operations added in.</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/obdjurfqrnfx0hr30hcm"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/xygcb1xmxfgur0frmrqm 123w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/vtdz4fid86qzn7wy2iv7 203w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/rvwlq4wpiadi7be4i5jy 283w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/unufsg0bkd7bpnnlfmpa 363w""></figure><p>Any allowed order of the operations in our original circuit corresponds to a path from the upper-left to lower-right in this computation-state graph. If that makes sense, then you’ve probably understood the basic idea. (If it’s still a bit murky, try drawing the graph-cuts corresponding to the three states at the bottom of the diagram above, then walk through an evaluation of the circuit and consider which cut represents the state at each step.)</p><h2>As A Category</h2><p>Recall from the <a href=""https://www.lesswrong.com/posts/B4DuwmtqF3HhNwvua/category-theory-without-the-baggage""><u>previous post</u></a> that a category is a graph with some notion of equivalence between paths. If we have a graph in which the paths represent something interesting - e.g. our computation-state graph - then we can generate potentially-interesting categories by thinking about notions of “equivalence” between the paths.</p><p>One example: imagine that each node of our computation is handled by a different person, and the arrows in the circuit correspond to messages passed between people. If we have a limited number of messengers, then we might want to limit the maximum number of messages passed simultaneously - i.e. if we only have 4 messengers, then we’d want to pick a computation-path which never cuts through four arrows simultaneously. (Equivalently: we want a computation-path which never has more than 4 variables in its state.) When searching for such a computation-path, it might be useful to consider two paths “equivalent” if they start and end at the same node and have the same maximum number of messages.</p><p>With one small adjustment, we can make this example into something more realistically useful for e.g. writing a compiler. Rather than counting all arrows cut, we count the number of “distinct” arrows cut - i.e. the number of messages with different contents. Then (x, x, x, y) would only count as two, and (x+3, x, x, y) would count as three. When performing the computation on a CPU, this would be the number of memory cells we need to use simultaneously - so we’d consider two computation-paths equivalent if they use the same maximum number of memory cells.</p><p>Of course, there are many other possibilities. There’s the trivial possibility: any two paths with the same start and end state are equivalent (i.e. we just need to find <i>some</i> computation-path, and don’t care which). Or the edges in the computation-graph could have some kind of costs associated with them, in which case we’d call two paths equivalent if they have the same cost - potentially quite nontrivial if e.g. it’s expensive to perform two multiplications back-to-back on a deeply pipelined processor, but cheaper if they’re not performed back-to-back.</p><p>Now imagine that we’re writing a compiler, and it needs to compile code where the computation-state graph lives in a space with thousands of dimensions and has exponentially many nodes, but most paths are equivalent to large numbers of other paths. I can see where it might be useful to have some mathematical constructions which can compress some of those equivalent paths - thus, category theory.</p><h2>Generalization: Symmetric Monoidal Categories</h2><p>The computation-state graph associated with a circuit is the prototypical example of a “symmetric monoidal category”. That’s an absolutely awful name, and I’m not going to explain where it comes from. But I will give one more example, which should be sufficient to illustrate the concept.</p><p>Suppose we’re cooking rice and beans. We add water to the beans and cook them. We also add water to the rice and cook that. Finally, we combine the rice and beans. Visually:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/fpjzwrbpfcy4tz4ilpuu"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/rrrqe0mxofxhp2zejkf0 99w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/tgajeqyxbvrh5sjjj1y4 179w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/q6tvytlxitg7amllaypg 259w""></figure><p>As before, we can use cuts in this graph to identify the state at any time. For instance:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/frt0pm1mjfdedbl92x6g"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/xrptq4ymzd6bc8ewvrcv 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/tkhfgxy1gfj8snolxmbo 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/jbr5ig1itb49xg0ly6up 260w""></figure><p>This cut represents a state where the beans are done cooking, but we have not yet started the rice at all.</p><p>As before, we can create a state graph which shows possible paths between states:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/wfzw21dq8nquo0f2lmtl"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/qjqrwhc3kgkmfci9sycd 141w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/keqgbvgvrx9gwoy8kv3c 221w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/fr65n9rnp76enh2rj5zp 301w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/po6ikmfkf2blgiqlv1pz 381w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/m2yuzwfgerv0taotfwrz 461w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nTP8eQkiu44fKaBSf/vwbmnzzjgusnvrrbhtqq 541w""></figure><p>In this case, the path through the lower left involves starting the beans first, while the path through the upper right involves starting the rice first. Both end up in the lower right state, in which both the beans and the rice are cooking.</p><p>Thinking about this state graph as a category, many of the same notions of equivalence from computation-states carry over nicely. For instance, the maximum number of arrows cut by a cooking-path might correspond to the number of items which need to be on the countertop simultaneously; as before, it’s a measure of “how much workspace” is needed by a path. We could imagine expanding this to a large-scale model of economic production in some domain, with thousands of dimensions and exponentially many possible paths. As before, it would be useful to have standard tools to compress “equivalent” paths through the state graph.</p>",johnswentworth,johnswentworth,johnswentworth,
Sa2HENWSZymu9rBHQ,Indignation in response to the 1890 census,indignation-in-response-to-the-1890-census,https://www.lesswrong.com/posts/Sa2HENWSZymu9rBHQ/indignation-in-response-to-the-1890-census,2020-09-08T20:14:30.619Z,26,10,1,False,False,https://rootsofprogress.org/indignation-at-the-1890-census,"<p>In reading about the development of technology, I keep an eye out for changes in society as well. I commented recently that <a href=""https://rootsofprogress.org/celebrations-of-progress"">we don’t seem to celebrate major achievements as much anymore</a>. But it’s not just technology that Americans used to view differently. It’s growth of all kinds.</p><p>The book <i>Computer: A History of the Information Machine</i> tells the story of the 1890 census. It was the first census to be computed, not by hand, but with tabulating machines, developed by Herman Hollerith. On August 16, 1890, the grand total was announced: the population of the United States was 62,622,250.</p><p>“But”, it says, “this was not what the allegedly fastest-growing growing nation in the world wanted to hear.” It quotes a contemporary account in a periodical, <i>The Electrical Engineer</i>, from 1891 (emphasis added):</p><blockquote><p>The statement by Mr. Porter [the census director] that the population of this great republic was only 62,622,250 sent into <strong>spasms of indignation a great many people who had made up their minds that the dignity of the republic could only be supported on a total of 75,000,000.</strong> Hence there was a howl, not of “deep-mouthed welcome,” but of frantic disappointment.</p></blockquote><p>The book continues:</p><blockquote><p>The press loved the story. In an article headlined “Useless Machines” the Boston Herald roasted Porter and Hollerith; “Slip Shod Work Has Spoiled the Census,” exclaimed the New York Herald; and the other papers soon took up the story.</p></blockquote><p>“Spasms of indignation” because population growth was too <i>low</i> for “the dignity of the republic”. Americans were <i>proud</i> of being the fastest-growing country. Today, in contrast, people fear <i>over</i>population, and the general slowing of world population growth is generally considered to be good news.</p><p><i>Something</i> changed in American attitudes in the last 100+ years, not just toward technology or the economy as such, but more fundamentally toward growth itself.</p>",jasoncrawford,jasoncrawford,jasoncrawford,
GzJW8Wjsy536RzgFJ,Escalation Outside the System,escalation-outside-the-system,https://www.lesswrong.com/posts/GzJW8Wjsy536RzgFJ/escalation-outside-the-system,2020-09-08T18:20:09.275Z,25,10,11,False,False,,"<p><span>

Transcript of a discussion on a friend's wall on the merits of </span>

<a href=""https://twitter.com/McClellandShane/status/1300953597186895872"">responding
""guillotines""</a> to union text-bankers when asked what the country
needs more of:



</p><p>

</p>

<blockquote>
<b>Me</b>: When you respond ""guillotines"" what do you expect the campaign
volunteer reading the response to think you're advocating for?

<p><b>Them</b>: Murder.

</p>
<p><b>Me</b>: Whose murder do you expect them to think you are advocating for?

</p>
<p><b>Them</b>: The richest people in the US.

</p>
<p><b>Me</b>: 0.1%, 1%, 5%?

</p>
<p><b>Them</b>: I think billionaires is a good cutoff. There are 540 in
the US. So, the richest 0.000164%.

</p>
<p><b>Me</b>: Is executing them your first choice? Or would you prefer to see
non-violent redistribution?

</p>
<p>

Is your objection that having this much money is immoral when others
need it so much more, that you can't be this rich without having
committed serious crimes, or something else?

</p>
<p><b>Them</b>: I'd certainly prefer a non-violent
solution. Redistribution sounds lovely.

</p>
<p>My objection is that we do not have anything remotely resembling a
democracy. And I think that having that much money is actually
immoral, while 21% of the children in the US are below the poverty
line. Putting them together, I think it's a fine solution to kill them
off until they can figure out how to release their chokehold on our
government.

</p>
<p>And it seems to me, solutions within the system have been adequately
tried.
</p>
</blockquote>



<p>

(I do think that keeping millions—let alone billions—for
yourself is immoral when the money could do so much more, but that is
the extent of my agreement.)

</p>

<p>

I see this perspective often in leftist spaces: the system has failed
its most vulnerable, it cannot be fixed, we must escalate violently
to transcend the system and find new solutions outside.

</p>

<p>

There have been many successful leftist revolutions, at least if you
define success as gaining power.  (And that gap is one reason why I
wouldn't support violent revolution regardless.)  What I don't
understand is how leftists could look at the current political climate
in the US and think that violent revolution would work out well for
them?

</p>

<p>

It's <a href=""https://www.vox.com/policy-and-politics/2020/6/3/21257133/trump-2020-election-meltdown-lawrence-douglas"">not
clear</a> that Trump will leave office if he loses in November:

</p>

<blockquote>
<b>Crowd</b>: Four more years! Four more years! Four more years!

<p><b>Trump</b>: Now if you really want to drive them crazy you say
""twelve more years""

</p>
<p><b>Crowd</b>: [cheers]

</p>
<p><b>Crowd</b>: ""Twelve more years! Twelve more years! Twelve more
years!""

</p>
<p>

—<i><a href=""https://www.youtube.com/watch?v=nVD-qjISeC0#t=28"">Trump at the
Republican National Convention, 2020-08-24</a></i>
</p>
</blockquote>



<p>

Then consider that the military and gun owners tend conservative.
While I understand why leftists would be unhappy with the status quo,
violent escalation clearly plays into the narratives and strategy of
the right.

</p>

<p>

(I wonder whether this is similar to what happened with <a href=""https://www.vox.com/the-big-idea/2016/12/2/13814728/alt-right-spencer-irony-racism-punks-skinheads"">ironic
support for Nazism blending into actual support for Nazism</a>?  You
go from commiserating about rent, to posting jokey memes referencing
<a href=""https://en.m.wikipedia.org/wiki/Chinese_Land_Reform#Mass_killings_of_landlords"">mass
killing of landlords under Mao</a>, to citing it unironically as the
solution to the housing crisis?)

  </p>

<p><i>Comment via: <a href=""https://www.facebook.com/jefftk/posts/10100183005607132"">facebook</a></i></p>",jkaufman,jkaufman,jefftk,
7TMap3isj36wY9Rdz,Austin Petrov Day: 6:30pm 9/26,austin-petrov-day-6-30pm-9-26,https://www.lesswrong.com/events/7TMap3isj36wY9Rdz/austin-petrov-day-6-30pm-9-26,2020-09-08T14:23:53.079Z,4,3,0,False,False,,"<p>Calling all central Texas LW readers: We are excited to announce this year's Petrov Day ceremony in Austin!</p>
<p><a href=""https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day"">What is Petrov Day?</a> Put simply, it's a day we commemorate the world not ending. The Austin Less Wrong community has been celebrating Petrov Day annually since at least 2015 (maybe longer), and it's our premiere event of the year. We'd be glad to see you there!</p>
<p>In light of the pandemic, we will be holding the ceremony outdoors, with masks, keeping 6 feet distance. For details on the social distancing rules and directions to the location, see <a href=""https://docs.google.com/document/d/e/2PACX-1vQkSlcoyRPlbjjiW_epmEeLhJtUPNTn7iwOgU0EXQTc6tljLCLiO7agpmotGY1d-7ieFWDucutZgS7T/pub"">here</a>. The event will be at <strong>6:30pm, Saturday, September 26, 2020</strong>. The ceremony will begin promptly at sunset (<strong>7:21pm</strong>), so please arrive before then.</p>
<p>We have <a href=""https://www.lesswrong.com/posts/jPSJTh5ctffyzFNB8/socially-distanced-outdoor-petrov-day-ceremonial-manual"">modified</a> the ceremonial manual to accommodate the outdoor setting. If possible, please bring a printed copy of the <a href=""https://github.com/GeneralAntilles/PetrovDay/releases/download/1.3.COVID-19/PetrovDay-1.3-COVID-19-DoubleSidedBooklet.pdf"">double-sided booklet version</a> (or a mobile device on which you can read the <a href=""https://github.com/GeneralAntilles/PetrovDay/releases/download/1.3.COVID-19/PetrovDay-1.3-COVID-19-MobileFriendly.pdf"">mobile-friendly version</a>), and a pen/pencil.</p>
<p>If you have any questions, you can comment below or message/email me privately (<a href=""mailto:jchan107@protonmail.com"">jchan107@protonmail.com</a>). In the meantime, bookmark the <a href=""https://groups.google.com/forum/?oldui=1#!forum/austin-less-wrong"">Austin Less Wrong</a> mailing list for updates on this and other events (both online and in-person).</p>
",jchan,jchan,jchan,
dzqvDyKTieGkNB4Ny,Efficacy of Vitamin D in helping with COVID,efficacy-of-vitamin-d-in-helping-with-covid,https://www.lesswrong.com/posts/dzqvDyKTieGkNB4Ny/efficacy-of-vitamin-d-in-helping-with-covid,2020-09-08T10:49:58.319Z,16,7,11,False,True,,"<p>so a new research found that high dose of Vitamin D significantly improve out comes for COVID patient:</p><p><a href=""https://www.sciencedirect.com/science/article/pii/S0960076020302764?via%3Dihub"">https://www.sciencedirect.com/science/article/pii/S0960076020302764?via%3Dihub</a></p><p>another source on facebook claim that:</p><p> - A study in Indonesia found that out of the patients that died from COVID-19, 98.9% of them were deficient in vitamin D, while only 4% of the patients with sufficient vitamin D died. </p><p>-A study of patients in New Orleans found that 84.6% of the COVID-19 patients in the ICU were deficient in Vitamin D while only 4% of the patients in the ICU had sufficient levels of Vitamin D. </p><p>-A study in the Philippines found that for every standard deviation increase in vitamin D people were 7.94 times more likely to have a mild rather than severe COVID-19 outcome and 19.61 times more likely to have a mild rather than critical outcome. </p><p>I couldn&apos;t find any mention of this on lesswrong, [granted I haven&apos;t look very hard], anyone who have done their reseach on this can help me determine the import of Vitamin D in fighting this pandemic?</p><p>and if it&apos;s true, anyway we can profit from this? any stock or index fund?</p>",df-fd,df-fd,df fd,
Xw7d7poQuTz7TCovu,Loneliness,loneliness-1,https://www.lesswrong.com/posts/Xw7d7poQuTz7TCovu/loneliness-1,2020-09-08T08:14:56.557Z,18,13,0,False,False,,"<p>Loneliness is an inevitable part of the human condition, no matter your culture or set of relationships. You have to spend time alone. And sometimes, you'll spend time not only alone, but undistracted. The work is done for the day. You have no plans. There are no pressing errands or chores. You are not tired. No activity - not reading or writing, not cooking or playing music, not anything - calls out for your attention.</p><p>You feel aware that all activities have a shared characteristic, which is that they can distract you from your loneliness. Or at least numb it somewhat, bring it to a tolerable level. So much of our work, too, is done in the long run to protect us from loneliness, or with the goal of helping us escape it.</p><p>Some people seem to revel in being alone. They feel as if they have too many demands on their attention, and crave some time to themselves. They have so many books or hobbies to pursue. Or maybe they just want to sleep or get some exercise. They are almost tormented by the extreme pressure to relate. They are more in demand than they can handle.</p><p>Others might look at these people with envy. For some, loneliness is a feeling of alienation and rejection. Or perhaps of neglect, of being left out. It might be caused not even by being ignored, but due to life circumstances, such as a transition or a move that leads to distance from loved ones. Some people might have friends, family, even a romantic partner and children, and yet still feel consumed by loneliness, out of touch with the people they believe care about them the most. It's hard to know who has it worst. But everyone has this loneliness in some form or another. I imagine that even the people who can't seem to escape the ceaseless demands for their attention must experience a special form of loneliness within that dynamic.</p><p>Now, we all know that there are experiences, or practices, that are said to help us escape from loneliness, or numb it, or at least push the boundaries of it back a little bit. But these fixes are often temporary. An isolated individual who begins to make friends may find themselves surprised to learn that the loneliness comes right back, this time in a new form. Yes, the new friends help - in the right moment, when the interaction is desired and both people are at their best - but often, even two good friends will not see eye to eye or have a meeting of the minds. A person whose life places constant demands on their time might find that they deeply miss the activity just a few days or weeks after it stops.</p><p>Some people who know a thing or two about psychology might believe that they can hack their brains to bypass the feeling of loneliness, or perhaps to transcend it. They might hope that meditation, the experience of flow, or a passionate daily habit of creating art might assuage the pain. Is it possible that all it takes to end the pain of loneliness is some careful retooling of one's daily habits?</p><p>Or then again, maybe loneliness is a symptom of some sort of disordered psychology, an unresolved emotional issue. Once we identify that issue and resolve it, perhaps through therapy or some new patterns of behavior, the loneliness will disappear as well, or at least change into a more manageable and meaningful form.</p><p>Others might try to accept loneliness with a stoic's attitude, that the suffering it entails is self-created and can be ended through one's own strength of character. Perhaps it can only be accepted, just as we accept the inevitability of death, the possibility of tragedy in our lives, and the daily burdens that we have to carry.</p><p>Still others might diagnose loneliness as a symptom of some aspect of modern industrialized life, and believe that while it is as inescapable as our current economic and political systems seem to be, that we can fight against it with the same kind of activist spirit we bring to other causes. Perhaps, they think, loneliness manifests from and gives rise to oppression, and needs to be treated as a tool of power used to keep the powerless under heel.</p><p>Like any experience, loneliness can be looked through two different lenses: as an experience in the moment, and as a narrative, memory, or source of meaning. In the moment, loneliness can manifest as a range of feelings, sensations in the body, thoughts, and patterns of behavior that might be gross or subtle. There might be many durations and intensities of loneliness, and different characters of it as well, as many as there are varieties of wine.</p><p>What about as a source of meaning? Loneliness can feel like a frightening experience, even a monster that's attacking you, or a ghost that has you trapped in your room. And we can find meaning in battling against monsters, slaying dragons, exorcising the evil spirits that haunt us. If loneliness feels like fear to you, maybe there is a way you can confront it. Or maybe it's not a monster, but a beggar you walk by every day, feeling ashamed of yourself. Maybe by giving loneliness some care and sustenance, some loving attention, you'll form an ongoing relationship, and it will reveal itself, like a Greek god in disguise, to be something altogether more healthy and holy - solitude.</p><p>Then again, loneliness can feel like a void. The absence of meaning. A silence where a story should be. We've all felt this. You might feel it if you've ever come home to an empty house at the end of the day, with no plans not just for the day, but for the rest of the week. Or you could feel it if you take an international trip as a tourist, having read all the guidebooks, then landing in the airport and suddenly feeling like a stranger in a strange land. Sure, it's full of amazing architecture, great food, trinkets to buy, new friends to meet in the hostel. But right now, none of that feels real or important. You're just a body, standing in a place where nobody knows you, where everybody was doing just fine before you got here and will continue doing so after you leave. It's the sensation of having too much time on your hands.</p><p>Art has been described as a cure for loneliness, but it can also cause it as well. You listen to a song. One day, it might feel like the singer is relating to you, and you might feel your loneliness has disappeared as you're bathed in the words and the music. Another day, though, the song might only remind you that it's a mere recording. The artist isn't present, doesn't even know you, and wasn't thinking about you at all when he or she wrote the song. And the song doesn't belong to you. It's a commodity, a commercial product, and perhaps many millions of other people listen to it as well. The special relationship you'd like to have the song feels cheapened by the realization, and you turn it off.</p><p>After all, probably fewer people are listening closely to the silence around them than are listening to that song right at this moment. And there's something unique about the silence you're hearing. It's broken by noises that are unique to where you are. And the thoughts in your particular head are merged with that silence. <i>This is my silence</i>, you can think. <i>Nothing to do, and nothing to be done about it.</i></p><p>Now, I don't want to appear to be advertising meditation as if it's the cure for loneliness, after all. Just listening to the silence more often might be fine for a moment, for an hour, even a whole day or more. But as you can see, there are so many types of loneliness, and so many proposed ways to deal with it, that prescribing a universal cure seems ridiculous, even if I had the credibility to do so. Which I don't. I'm still struggling with loneliness as much as anybody, and I began thinking about this not to convey some great solution that worked for me, but to articulate the problem I was having.</p><p>It seems to me that a multi-pronged approach might be best. Is it possible to commit to approaching whatever social relationships are available to us with renewed vigor, while simultaneously working to deepen our relationship with our solitary self? To accept the pain of solitude, but also to meditate on it? To distract ourselves while also making efforts at genuine creativity? To envision what it would be like to find a rich sense of meaning and genuine emotional comfort in spending a substantial amount of time by ourselves, while also imagining and desiring the joys of a deep and reciprocal relationship? To try and do our work for its own sake, but also to appreciate that we hope our work will be inspiring and rewarding in a way that makes us attractive to other people? To articulate the pain we feel when we simply cannot shake an uncomfortable, even an intolerable loneliness, while appreciating that at best, that act of articulation is still only getting us from the moment we're in to another moment at some later time?</p><p>While we can all take inspiration from each other as we deal with our individual experiences of loneliness, it seems to me that it's up to each person alone to forge their own life-long relationship with solitude. No matter which TED talk you watch, which book or article you read, and no matter which friend you talk to, the end of that exchange will come, and you'll be back to yourself again, always with the burning question ""What now?""</p><p>And that question will raise itself over and over again, moment by moment, through all your waking hours. ""What now?"" ""What now?"" Sometimes, you'll answer it. Sometimes, somebody else will answer it for you. Maybe your boss. Maybe your child. But all too often, ""What now"" will have no answer that can truly convince you it is right. Oh, you'll give an answer. But you'll know that it was only so that you could have something to say.</p><p>That's just how it is. The challenge, then, is not to give the right answer to ""What now,"" but to keep yourself interested in the possible answers. As a scientist, my job is to stay curious, endlessly so, about the questions in my field of research. As a teacher, my job is to stay curious about my students. As a friend, partner, family member, and part of my community, my job is to stay curious about the people with whom I am in relationship.</p><p>As myself, as a solitary person, my job is to stay curious about the question ""What now?""</p><p>That curiosity can come through in observation of my surroundings. In nature, by a wooded lake, looking at the reflections on the water at sunset and listening to the sounds of birds and mammals calling amongst the trees, the answer to ""What now?"" might simply be <i>look and listen.</i> If I am lucky, I not only do so, but for a moment, I lose myself in that sensation.</p><p>Other times, I am not in such a beautiful setting, or my mind is more frantic, my heart more anxious. The answer to ""What now?"" might be that I try ringing up one of my friends, just to see if they'll answer. Or it might be to put on my running shoes and get my heart pounding. There could be all sorts of activities, or no activity at all if I simply choose to pass a moment in stillness.</p><p>Understanding that you'll never be separate from the question of ""What now?"", and that this is the only question that solitude, that loneliness, is asking you, can bring some helpful clarity to the time alone. Because for some people, loneliness doesn't seem to be asking an innocuous question like that. Instead, it seems to be asking some terrifying questions, or simply telling you some horrible facts. That you could die and nobody would find your body for hours or days or weeks. That most likely, none of your friends are thinking at all about you right now. That when they talk to you, they have themselves and their other relationships more on their mind; that you are an afterthought, or merely a convenient listening ear. That the two of you are merely trying to escape your loneliness together, like two prisoners who break out of their own cells and into each others'. There seems to be no way out of the prison.</p><p>These are the kinds of thoughts that can lead a person to truly suffer from loneliness. And people act on that suffering in tragic or destructive ways.</p><p>But again, I think that we are mistranslating the questions of solitude when we look at them in this dark light. ""What now?"" is all they are really trying to ask. It's a question with no agenda. It says that you are free, right now. Not free to do anything. You can't intrude into the relationships of others that you can see all around. You can't suddenly become lost in a fit of inspiration, or switch on some charismatic electricity that will command the attention of the people who are at present ignoring you.</p><p>Imagine that you have a long and attractive menu in your mind. It lists all the realistic options of things there are for you to do. Under ""starters,"" it lists appetizing snacks like ""watch something on Netflix"" and healthy options like ""take a moment to just breathe."" Under ""entrees,"" it has a wide variety of hearty fare: all the hobbies you know how to do, the errands and chores and wellness routines you probably should take care of, and all the many plans and ideas you have yet to even begin exploring. And down at the end, under ""desserts,"" you'll find choices that are fleeting but delightful, one of which might be eating some literal ice cream.</p><p>Becoming comfortable with solitude might mean that we not only have access to a great menu of options, all of which we've tried, but that we know ourselves well enough to judge what we're hungry for and what is good for us in the moment.</p><p>I believe that if we can accomplish these two things - translating the question asked by aloneness as the simple phrase ""What now?"" and developing our mental menu of possible answers - then we have a way of transforming loneliness from a haunting or empty experience into something else. We can stop seeing it as an accident, a disease, or an emergency, and acknowledge that loneliness was, is, and will be with us at every moment. Loneliness is just ourselves. It is, in fact, our human freedom. Our potential. Loneliness is all the realistic options we do have, moment by moment, and the goals and plans and experiments that we devise to give a sense of meaning and purpose to all that activity.</p><p>Yes, loneliness may still hurt. There is no cure-all. This is just another technique, with all the others. In fact, it is not even that. All I've accomplished here is typing some words into my computer. And the act of doing so has had an effect on my mind. That is part of loneliness, too. The journey that I took through these moments is different than the one I'll take in the future, or that you will take as you read and reflect on this message. We cannot copy or step into each others' minds, no matter how much effort the speaker puts into making themselves clear and how hard the listener tries to truly understand. There is an unbridgeable gulf.</p><p>That is why I come back to the idea that loneliness is always an individual's task. No - even harder than that. It's the task of a particular version of you, the one that exists <i>right now</i>, in some moment in time, to feel stitched together with your own past and future, to feel as if your mind belongs with your body, and that all the different parts of your mind, and all the sensations you are having, belong together, and belong to you. You've heard of dissociation, which is sort of the opposite sensation. A separation of the self into fragmented pieces.</p><p>This is the opposite. It's association. And of course association -not to be cute - <i>would</i> be the cure for loneliness, wouldn't it? As I said, I can't promise a solution, but we can at least try to understand the problem.</p><p>Part of the problem of loneliness is that we just seem to keep asking ourselves the question ""What now,"" or one of its scarier versions, pretty much constantly from day to day. And another part of the problem is this second aspect, that we often feel at least somewhat dissociated. There are memories in your head that you haven't thought about in years. Ways you haven't moved your muscles in weeks or months. People who think about you from time to time, and who you think about too, but a little less every year. But, for now, not so little that you never think about each other. The potential is still there.</p><p>And the world is full of strangers. These strangers mostly are on their own journeys. They don't know why they should want to meet you. But some of them are ready to meet anybody. Look for the person standing alone in a corner at a party. Send a kind message to somebody on an online dating service. Try going to a bar, a group hike, or a book club (if there's no pandemic on the loose) and see if you can strike up a conversation there. Perhaps you can even help some of these busy folk to understand why they in fact <i>should </i>want to meet you.</p><p>What I'm trying to say, I suppose, is that loneliness can feel like the worst sort of emergency, one where we don't know the cause or the extent of the catastrophe, but we simply feel in our bones that something very, very bad is happening. Or it can be an awful trickster, making us sick and then selling us snake oil that only makes us more ill. It preys on our worst anxieties, and convinces us that by adopting the right set of habits, buying the right clothes, getting the right education, and so on, we will transform ourselves into somebody who's worth another person's time and love.</p><p>But loneliness, I think, has the potential to be a friend. Almost a secular god - omnipresent, benevolent, powerful. It is always asking us ""What next?"" and encouraging us to develop a rich repertoire of answers. Trauma victims are sometimes hurt the most when they have no way to help themselves, when they are trapped and must wait passively to be rescued. Loneliness seems to want us to make an active choice to ""What next?"", even if that choice is to sit silently for a while.</p><p>I don't think that there's some particular attitude or energy that's ideal for responding to loneliness, except to say that panic, anxiety, and depression seem at the very least to be more common than optimal. Sometimes, ""What next?"" wants to be followed by a sense of calm, a willingness to do very little for a while and remind ourselves that we don't have to do anything. Other times, ""What next?"" leads us to a flurry of activity, where we might get a lot of chores done, get in touch with friends, or start a whole new hobby. I've never experienced enlightenment, and I don't have any strong preconceptions that there is some narrow brain state that makes all these existential problems vanish as we ride away on a cloud of bliss. I believe more that people get stuck. In depression, in mania, in the doldrums, in reactivity, in thought, in emotion, in habit, in states of emergency, in obsessions, in burdensome responsibilities, in too much unstructured time.</p><p>Loneliness can feel like a trap when it points out to us how stuck we seem to be. When we can't see a way out, or when we perceive all our actions to escape or accept or transcend as futile, then we have the learned helplessness of loneliness. Although I feel little confidence that wisdom can easily translate from mind to mind, I think it is probably important for each person, somehow or other, to get to a place where they have consciously accepted and found words to express the idea that loneliness is not a trap, not a prison, not an invasion from which escape is futile. It is a gentle but probing question reminding us of our <i>freedom</i>, and the sense that the next minute, the next hour, the next day, the next year, we will only have more freedom. And this state of affairs will continue until we die and are released from having to answer that question once and for all.</p><p>Exploring the problem of loneliness with my own wisdom and my own logical and anthropological understanding of the basic conditions of human life helps me feel more hope for tomorrow. And more freedom in this minute. It helps me feel like there is meaning to be found even in the most inane distractions, and as though the most profound accomplishments are not beyond my grasp. I hope that it will help me feel as though the silence between sentences when I talk with my friends is not an anxious waiting for more words, but a reflective time to hear and appreciate what was said.</p><p>We will see. That's all there is to do. What next?</p>",AllAmericanBreakfast,directedevolution,DirectedEvolution,
wWmXso92Zkntvzuxg,"Luna First, But Not To Live There",luna-first-but-not-to-live-there,https://www.lesswrong.com/posts/wWmXso92Zkntvzuxg/luna-first-but-not-to-live-there,2020-09-08T03:14:56.853Z,21,14,21,False,False,https://nathanielmrouth.wordpress.com/2020/08/31/luna-first-but-not-to-live-there/,"<p>In certain corners of the astronautics community, there&#x2019;s a real and substantial debate over whether to prioritize Mars or Luna when planning out the future of human spaceflight. There are good arguments on each side, but I think that neither side makes the correct case.</p><p>Luna is attractive because it is close. We&#xA0;<em>know</em> how to land on Luna; <a href=""https://nathanielmrouth.wordpress.com/2019/07/20/contemplating-half-a-century/"">we&#x2019;ve done it before</a>. Building bases on Luna would offer the opportunity to really understand how humans live and operate in space over the long term. It would provide a test-bed for certain technologies that would enable more advanced missions to the planets. If things go wrong, it would be easier to get support from Earth, or to head home if need be.</p><p>The counter-argument put forth by the pro-Mars camp is that Luna is a very different world from Mars, and thus wouldn&#x2019;t provide really all that much advantage for testing Mars-specific technologies. Life support systems on Luna would necessarily use a very closed loop; aside from some water deposits at the poles, astronauts would want to recycle as much as possible. Mars, on the other hand, has water and carbon dioxide and organic molecules in abundance. <a href=""https://nathanielmrouth.wordpress.com/2017/04/03/book-review-how-to-live-on-mars/"">It offers much better options for building self-sufficient outposts</a>. </p><p>There&#x2019;s also disagreement about the efficacy of using Luna as a refueling stop, so to speak,&#xA0;<em>en route</em> to the Red Planet. From an orbital mechanics standpoint, it&#x2019;s not a slam-dunk idea, but the argument in practice depends heavily on the specific logistics. In-situ fuel production might just make such a configuration worth it. </p><p>In any case, both camps miss the main question when it comes to long-term off-world development: where&#x2019;s the money coming from? </p><p>While Mars is obviously the more attractive target for colonization, we are a very long way from building colonies on other celestial bodies, <a href=""https://nathanielmrouth.wordpress.com/2017/08/06/should-we-colonize-mars-sooner-or-later/"">no matter how good of an idea it is</a>. The reason is very simple: space colonization is an unspeakably expensive proposition. The off-world economy as it currently exists is entirely constructed around servicing needs on Earth, and there are no terrestrial markets which demand colonies on another planet. </p><p>I have yet to think of an <em>economic</em> need which a self-sustaining population on Mars would fulfill, that innovative strategies could not fulfill on Earth. Farming food on Mars? We can do hydroponics here. Running out of room to house people? We&#x2019;re nowhere near that kind of population density. New legal environments to test out social engineering concepts? Seasteads and charter cities are way safer and less expensive. Climate change? Just tax carbon and build nuclear power plants, sheesh. </p><p>No one will front the money to build Mars colonies until there&#x2019;s an economic incentive to do so. I see no such economic incentive. I would love to be wrong about this, because Mars is the best colonization target by far. But I don&#x2019;t think I am. </p><p>Nevertheless, there are profits to be made in space. </p><p>There are needs on Earth which off-world industries can satisfy. Currently, most of the needs which we&#x2019;re satisfying relate to information on or near Earth&#x2019;s surface: passing data quickly between two points, or observing the location of moving vehicles, or watching the development of weather systems&#x2014;things in that vein. There&#x2019;s been a lot of discussion of space tourism, but that has yet to make any real money without piggybacking on government-funded flights. Scientific probes and the like are great, but largely a public service. They don&#x2019;t represent the sort of self-sustaining economic sector that successful space colonization requires. </p><p>What self-sustaining options are there? Honestly, that&#x2019;s very difficult to predict. Modeling the profitability of a venture depends heavily on the assumptions we make. How far will launch costs fall over the coming years? Will other space technologies follow suit? What will the tax and regulatory structure look like in twenty years?</p><p>Some things are simply impossible to know at this point: how materials behave in microgravity, what resources are available in space, and in what forms&#x2014;and how much will everything cost? Will there need to be humans involved, or not? These are impossible to know without further investigation; basic research is, in many cases, a part of the capital investment. Publicly-funded space agencies have done a great deal in this area, but the private sector is going to have to carry some of its own weight before turning a profit.</p><p>We can still speculate, of course. I think in-space manufacturing and resources from space are potentially tremendous markets. That said, many people get these aspects wrong. For instance, many will point to the nominal value of precious metals in asteroids, ignoring the fact that introducing such quantities would immediately and permanently crash the market. In that particular regard, what&#x2019;s far more interesting is the possibilities that open up when platinum, say, becomes cheaper than aluminum is now. </p><p>The real value of off-world industry will come in the things we can do in space that we can&#x2019;t do on Earth. This includes all sort of material processes (though, of course, gravity makes a great many things&#xA0;<em>easier</em>), and the lack of environmental concerns. As the people of Earth demand an increasingly high standard of living and simultaneously a cleaner environment, I suspect that this may prove to be the ultimate driver of off-world industrialization. Again, though, speculation. </p><p>Towards that end, staying near Earth is much more attractive logistically than going all the way to Mars. Orbital space stations are preferable to Luna is preferable to asteroids. <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\Delta V""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">&#x394;</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span></span></span></span> considerations eat up the profit margins, to the extent that off-world resource extraction has yet to enter even the <em>demonstration</em> phase. The cost of even basic space technology is currently prohibitive.</p><p>Those costs will fall, at least to some extent, but the basic logic remains. If all we need is microgravity, there&#x2019;s little reason to go above low Earth orbit. If we desire some degree of gravity, Luna will probably do. Asteroids may be useful targets for mining, but I would need to see actual numbers before deciding whether to pursue particular resources on Earth, Luna, or smaller bodies. </p><p>Critically, space industrialization is different from space colonization. Developing an off-world economy is a&#xA0;<em>pre-requisite</em> for seeing a large, permanent population above the atmosphere.</p><p>Certainly astronauts can visit Luna and Mars. We might even establish permanent research bases. This, however, is a public-spirited endeavor. Governments may choose to pay for scientific missions to other planets; they will&#xA0;<em>not</em> front the costs of developing entire planets quite literally from the ground up. Whatever outputs space agencies may build, they&#xA0;will not be colonies. People won&#x2019;t <em>live</em> there, the way that human populations have whenever establishing themselves in a new locality. There won&#x2019;t be families and new businesses and the like, not for a long time. </p><p>Instead, we&#x2019;re probably going to see many largely-automated operations, with minimal and possibly intermittent human presence. Over time, these will expand, and eventually we may see actual colonies in orbit and on Luna. But that will come only once there&#x2019;s a profitable market for goods manufactured or processed in space. These industries will beget new markets, which may be satisfied by other off-world industries. At some point down the road, there very well <em>may</em> be demand which can be more profitably supplied from Mars than Earth (from a <span><span class=""mjx-chtml""><span class=""mjx-math"" aria-label=""\Delta V""><span class=""mjx-mrow"" aria-hidden=""true""><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-main-R"" style=""padding-top: 0.446em; padding-bottom: 0.372em;"">&#x394;</span></span><span class=""mjx-mi""><span class=""mjx-char MJXc-TeX-math-I"" style=""padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;"">V</span></span></span></span></span></span>perspective, it&#x2019;s easier to reach Luna from the surface of Mars than Cape Canaveral). But I think that it&#x2019;s unrealistic to expect that to occur particularly soon. </p><p>As we push towards human settlement in space, our focus should therefore be the development of new industries and new technologies to enable and motivate working above the atmosphere. Between the two targets of Luna and Mars, the former clearly comes out ahead for this purpose. Proximity wins over hospitality, though many of the disadvantages Luna has as a world are significantly less serious in the context of production rather than settlement. </p><p>One day, our species will span three worlds. That day remains very far away. Rather than fixate on terraforming dreams, we should chart a course carried by the currents of economic necessity. With the correct regulatory environment and technological investments, we can begin building sustainable off-world industries in a realistic timescale. Such industries will carry us to the planets in the pursuit of profit&#x2014;a far more reliable motivator than any humanitarian spirit from politicians. </p><p>That, I suspect, is what the future of space travel is going to come down to. Do we pursue an incremental strategy that eventually carries us to the ends of the Solar System, or do we wallow on this one planet, fantasizing of an amazing future no one has any incentive to hand us? Are we going to fixate on self-sustained colonies and settle for nothing less, or shall we go to Luna first, but not to <em>live</em> there?</p>",NathanielMRouth,nathanielmrouth,NathanielMRouth,
HDvszpdycLEquQ6Jq,Decision theory analysis of whether vaccines should be distributed prior to the completion of stage three trials please,decision-theory-analysis-of-whether-vaccines-should-be,https://www.lesswrong.com/posts/HDvszpdycLEquQ6Jq/decision-theory-analysis-of-whether-vaccines-should-be,2020-09-07T23:50:02.250Z,4,2,0,False,True,,"<p>I’m response to <a href=""https://marginalrevolution.com/marginalrevolution/2020/09/on-vaccine-timing-from-the-comments.html"">MR’s post</a> on whether vaccines should be released early. The pinned comments there mostly differ about how likely an early vaccine is to kill one in 4,000 or 10000 people. It’s a tough forecasting problem I know, but there must have be tens of vaccines which have gone through the entire trial, so the percentage which reached stage three then had a death rate about 1 in 4000 bears heavily on the question.</p>
",rockthecasbah,tim-liptrot,Tim Liptrot,
mWJbYebezFdhoFHP6,"""Learning to Summarize with Human Feedback"" - OpenAI",learning-to-summarize-with-human-feedback-openai-1,https://www.lesswrong.com/posts/mWJbYebezFdhoFHP6/learning-to-summarize-with-human-feedback-openai-1,2020-09-07T17:59:32.799Z,57,16,3,False,False,,"<p> <a href=""https://openai.com/blog/learning-to-summarize-with-human-feedback/"">https://openai.com/blog/learning-to-summarize-with-human-feedback/</a> </p><p>Eliezer&apos;s Summary/Thoughts:  <a href=""https://mobile.twitter.com/ESYudkowsky/status/1301954347933208578"">https://mobile.twitter.com/ESYudkowsky/status/1301954347933208578</a> </p>",,,,
4bDHxLmye3GitxiT3,A Toy Model of Hingeyness,a-toy-model-of-hingeyness,https://www.lesswrong.com/posts/4bDHxLmye3GitxiT3/a-toy-model-of-hingeyness,2020-09-07T17:38:59.826Z,16,5,10,False,False,,"<p><u><a href=""https://forum.effectivealtruism.org/posts/S396u3ymwcsfL727W/a-toy-model-of-hingeyness"">This is a crosspost from the Effective Altruism forum</a></u></p><p><em>Epistemic status: Attempt to clarify a vague concept. This should be seen as a jumping of point and not as a definitive model.</em></p><br><p><strong>Definition of Hingeyness</strong></p><blockquote>The <strong>Hinge of History</strong> refers to a time when we have an unusually high amount of influence over the future of civilization, compared to people who lived in the eras before and after ours.</blockquote><p>I will use the model I made for my <a href=""https://forum.effectivealtruism.org/posts/o7YcAKHRgy5qkhYKB/what-is-hingeyness"">previous question post</a> to explain why I don&apos;t think this definition is very useful. As before, in this model are only two possible choices per year. The number inside the circle refers to the amount of utility that year experiences and the two lines are the two options that this year has to decide on. The amount of utility which each option will add to the next year is written next to the lines. (<em><a href=""https://drive.google.com/file/d/1oBsri2W6lUR3qIIqLNCbN-ALzuKdCELX/view?usp=sharing"">link to image</a>)</em></p><span><figure><img src=""https://drive.google.com/uc?export=view&amp;id=1oBsri2W6lUR3qIIqLNCbN-ALzuKdCELX"" class=""draft-image center"" style=""width:66%""></figure></span><br><p><strong>Older decisions are hingier</strong></p><p>I think we all agree that we should try to avoid the option that will lead to better results in the next year, but will create less utility in the long run. In this model the year with 1 utility could choose the +2 option, but it should choose the +1 option because it leads to better options next year. Let&apos;s assume that all life dies after the last batch of years. The 1 utility then 3 utility then 0 utility option is the worst because you&apos;ve generated 4 utility in total. 1-3-6 is just as good as 1-2-7, but 1-2- 8 is clearly the best path.</p><p>The implication is that later decisions are never hingier than earlier ones. 1 gets a range of options that ranges from 4 utility to 11 utility, no other option get&apos;s that kind of range. In fact, it&apos;s mathematically impossible that future decisions have a range of options that&apos;s larger than the previous decisions had (assuming the universe will end and isn&apos;t some kind of loop). It&apos;s also mathematically impossible that future decisions have ranges where the best and worst case scenarios give you more utility than the range of the previous years. This is, unless negative utility is possible, which might arguably exist when you have a universe of beings being kept alive and tortured against their will (but it&apos;s rare in any case).</p><br><p><strong>Decrease in range</strong></p><p>Does that mean that hingeyness is now a useless concept? Not necessarily. The range will never grow, but the amount by which it narrows from year to year varies widely. Let&apos;s look at an extreme example. <em>(<a href=""https://drive.google.com/file/d/1DXIzZuLZ04yO3M8QSdPznVF7aT6sEGgP/view?usp=sharing"">link to image</a>)</em></p><span><figure><img src=""https://drive.google.com/uc?export=view&amp;id=1DXIzZuLZ04yO3M8QSdPznVF7aT6sEGgP"" class=""draft-image center"" style=""width:59%""></figure></span><p>So the decisions made in 1 will always have the broadest range [204-405], but if you look at the difference in range between 3 [203-311] and 4 [208-311] it&apos;s not that much. So hingeyness may still be useful to think about how <a href=""https://forum.effectivealtruism.org/posts/o7YcAKHRgy5qkhYKB/what-is-hingeyness?commentId=hbRo5uaZaypq8cqSh"">quickly</a> our range is decreasing. It&apos;s even possible that the range doesn&apos;t shrink at all.</p><br><p><strong>Going extinct quickly isn&apos;t necessarily bad</strong></p><p>In the previous post I said that choosing for times where we survive for longer is almost always better (assuming you&apos;re a positive utilitarian and negative utility is impossible), this is an example of when this is not the case. The 1-2-402 chain gives the world the most utility even though it goes extinct one tick quicker. We (naturally) focus on reducing x-risk, but I wanted to visualize here why it might be possible that dying quickly in a blaze of utility is better than fizzling on for longer with low amounts of utility (especially if negative utility is possible). Although it should be noted that this model gives you clear ticks which might not exist in real life. Maybe planck time? Or maybe the time it takes to go from one state of pleasure to another a.k.a the time it takes to fire a neuron? Depending on how you answer that question this argument might fall flat.</p><br><p><strong>Is hingeyness related to slack?</strong></p><p>I&apos;m starting to see similarities between the range of possible choices you keep and the amount of <a href=""https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack"">slack</a>. I previously expressed that I see the slack/<a href=""https://www.lesswrong.com/posts/TxcRbCYHaeL59aY7E/meditations-on-moloch"">moloch</a> trade-off as similar to the exploration/exploitation trade-off. Since we can&apos;t accurately predict which branches will give us the most utility it might be useful to keep a broad range of options open a.k.a to give yourself a lot of slack. In fact if we look at the first image you can see that someone who is pursuing linear utility exploitation will go from 1 to 3 (giving himself a +2 instead of a +1). Since this gives you worse results later this is basically the same thing as moloch pushing you into an <a href=""https://www.lesswrong.com/s/oLGCcbnvabyibnG9d/p/yPLr2tnXbiFXkMWvk"">inadequate equilibria</a>. Having the slack/exploration to choose a sub-optimal route in the short-run but a better route in the long-run can only work if you have a lot of hingeyness.</p><br><p><strong>How probability fits in</strong></p><p>In reality of course you get more than two options, but the principle stays the same. Instead of a range you get a probability distribution. <em>(<a href=""https://drive.google.com/file/d/1kDtgOm7Az1g8RfBlWTBKH514D4UsbBuq/view?usp=sharing"">link to image</a>)</em></p><span><figure><img src=""https://drive.google.com/uc?export=view&amp;id=1kDtgOm7Az1g8RfBlWTBKH514D4UsbBuq"" class=""draft-image center"" style=""width:59%""></figure></span><p>The probability that you get a certain amount of utility is equal to the amount of chains that generate that specific amount of utility (If you think certain chains have inherently less chance of existing you can just multiply the two factors). The range we are talking about is the difference between the lowest amount you could possibly generate and the highest. This will always either stay the same or shrink. This is not necessarily a bad thing as a we would rather face a narrow range of options between several good outcomes than a broad range of options between a lot of bad outcomes. But what about a distribution that looks like this <em>(<a href=""https://drive.google.com/file/d/18Dw74NraOUtTVqONwSFSlfIZ6hviQqGD/view?usp=sharing"">link to image</a>)</em>:</p><span><figure><img src=""https://drive.google.com/uc?export=view&amp;id=18Dw74NraOUtTVqONwSFSlfIZ6hviQqGD"" class=""draft-image center"" style=""width:59%""></figure></span><p>This is what I think a lot of people think about when we talk about the hinge of history; a time in history where the decisions we make can either turn out to have very good outcomes or very bad outcomes with very little in between. Our range may be smaller than the previous eras, but the probability that we either gain or lose lots of utility have never been higher. I won&apos;t decide what the &quot;true definition of hingeyness&quot; is since language belongs to it&apos;s users. I&apos;m just pointing out that &quot;the range of total amount of utility generated&quot;, &quot;how quickly that range is decreasing&quot; and &quot;how polarized the probability distribution is&quot; are very different concepts and we should probably have different labels for them. I will suggest three in the conclusion.</p><br><p><strong>How much risk should we take?</strong></p><p>I previously asked:</p><blockquote>When you are looking at the potential branches in the future, should you make the choice that will lead you to the cluster of outcomes with the highest average utility or to the cluster with the highest possible utility?</blockquote><p><a href=""https://forum.effectivealtruism.org/users/emmaabele"">EmmaAbele</a> answered:</p><blockquote>I&apos;d say the one with the highest average utility if they are all equally likely. Basically, go with the one with the highest expected value.</blockquote><p>But what about the cluster of branches with the median amount of utility, or mode or whatever? I don&apos;t think these questions have one definitively correct answer. Instead I would argue that we should use <a href=""https://www.lesswrong.com/posts/2NTSQ5EZ8ambPLxjy/meta-preference-utilitarianism"">meta-preference utilitarianism</a> to choose the options that most people want to choose.</p><br><p><strong>Conclusion</strong></p><p>There are three concepts that could be described as Hingeyness:</p><p>1) The range of the amount of utility you can potentially generate with your decision (maybe call it &apos;hinge broadness&apos;?)</p><p>2) How much that range will narrow when you make a decision (maybe call it &apos;hinge reduction&apos;?)</p><p>3) How polarized the probability is that you get either a lot or very little utility in the future (maybe call it &apos;hinge <a href=""https://forum.effectivealtruism.org/posts/NxpxrC7uYjAT5Jqgd/toby-ord-s-the-precipice-is-published"">precipiceness</a>&apos;?)</p><p>Having lot&apos;s of &quot;hinge broadness&quot; is crucial for having slack. This toy model can be used to visualize all of these concepts.</p>",Bob Jacobs,b-jacobs,B Jacobs,
EgcT434K8AkB5Wygq,How long does it takes to read the sequences?,how-long-does-it-takes-to-read-the-sequences,https://www.lesswrong.com/posts/EgcT434K8AkB5Wygq/how-long-does-it-takes-to-read-the-sequences,2020-09-07T17:19:23.898Z,12,7,8,False,True,,"<p>Sorry if that has been answered before, I searched quickly and could not find the answer.</p>
<p>To be more specific, how long does it takes an average reader to read all of the texts in <a href=""https://www.lesswrong.com/rationality"">https://www.lesswrong.com/rationality</a> ?</p>
<p>I already read them all (more specifically the version on <a href=""https://www.readthesequences.com/"">https://www.readthesequences.com/</a> ) but I am asking so that I can give an estimate to the people I recomend it to.</p>
",nicolas-lacombe,nick-lacombe,nick lacombe,
tpeCWwMTsLBC6bmuo,Changes in Reality,changes-in-reality,https://www.lesswrong.com/posts/tpeCWwMTsLBC6bmuo/changes-in-reality,2020-09-07T16:18:27.276Z,5,1,4,False,False,,"<p><em>[Some short thoughts I just wanted to get out of my brain; bullet-points instead of well-structured prose. This is entirely random speculation, and not well-explained. Cross-posted from <a href=""https://grandunifiedempty.com/2020/09/07/changes-in-reality/"">Grand, Unified, Empty</a>.]</em></p><ul><li>Social systems (laws, customs, memes) are subject to evolutionary pressure from the dynamics of reality; when reality changes, existing social systems are typically no longer in equilibrium and have to evolve, or collapse and be rebuilt. Consider for example the invention of the birth control pill and the resulting impact on family structure, gender relations, etc. Pre-pill social customs around marriage and family were no longer in equilibrium in a world with reliable female birth control, and so society shifted to a new set of customs.</li><li>&#x201C;Change in reality&#x201D; largely means economic and technological change. New wealth and new capabilities.</li><li>&#x201C;Change in reality&#x201D; has been accelerating for a long time as new technologies and discoveries unlock new economic prosperity which enables more discoveries, in an explosive feedback loop. Some argue that technology/science have slowed down a lot recently, but I think that&#x2019;s mostly because our best and brightest are too busy extracting <em>economic</em> value from our recent innovations (computers and, separately, the internet). Once that bounty has been consumed, more general technological progress will resume its previous course.</li><li>There is a natural limit on how fast social systems can evolve. Humans can adapt to living under radically different memeplexes, but not instantly, and somebody has to invent those memes first. When reality changes slowly this is fine, as it leaves plenty of time for a multiplicity of experimental memetic shifts in different groups, letting the best adaption dominate with high probability.</li><li>At some point in the future (possibly soon?) reality will start changing faster than our social systems can adapt. Our existing laws, customs, memes, and government will be out of equilibrium, but we will not have enough time to converge on a new social system before reality changes again. Society will fragment and human culture will undergo an intense period of <a href=""https://en.wikipedia.org/wiki/Adaptive_radiation"">adaptive radiation</a>.</li><li>The countervailing force is technology&#x2019;s ability to connect us (the &#x201C;global village&#x201D;) and equivalently the <a href=""https://grandunifiedempty.com/2020/06/02/law-cultural-proximity/"">law of cultural proximity</a>.</li></ul>",,,,
v6NBpQc2AzWjzxJ4S,The Four Children of the Seder as the Simulacra Levels,the-four-children-of-the-seder-as-the-simulacra-levels,https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels,2020-09-07T15:00:02.899Z,84,32,17,False,False,,"<p>Previously: <a href=""https://thezvi.wordpress.com/2020/08/03/unifying-the-simulacra-definitions/"">Unifying the Simulacra Definitions</a>, <a href=""https://thezvi.wordpress.com/2020/06/15/simulacra-and-covid-19/"">Simulacra Levels and their Interactions</a>, <a href=""https://thezvi.wordpress.com/2020/05/03/on-negative-feedback-and-simulacra/"">On Negative Feedback and Simulacra</a></p>



<p>Simulacra levels are complex, counter-intuitive and difficult to understand.</p>



<p>Thus, it is good and right to continue exploring them partly via story and metaphor.</p>



<p>The metaphor here will be that of the four children from Jewish Passover Seder.</p>



<p>The Jewish Seder tells us of four generations of children: The wise child, the wicked child, the simple child, and the one who does not know how to ask.</p>



<p>The story is <em>profoundly weird </em>and does not, on its face, make much sense. Yet every year it is told anyway. What is going on here?</p>



<p>Many attempts have been made to interpret it.</p>



<p>A while back I wrote the first <a href=""https://drive.google.com/file/d/0B-gfWUAglgYhbWFJRFF1ekJvNGphMGFsWFo4NFRVUGhYSDhn/view?usp=sharing"">rationalist seder</a> (later versions can be found <a href=""https://docs.google.com/document/d/1PPz6iSBY9_18_T-qOO-sJnQb-eJfIdFcNj0W939F9IY/edit"">here</a>). At the time, the story of the four children did not make sense to me. Why this narrative of decline and fall, of wisdom as something that can only decay? </p>



<p>To make sense of the story of the children and to tie it to the themes I wanted to focus on, I told a reversed story and substituted in generations of rationalists and truth seekers. </p>



<p>In this story, we first learn how to ask, then we are simple, then we are instrumental, then we seek to fully understand, and then finally in a fifth stage we can transcend. We can be great because we stand on the shoulders of giants. </p>



<p>Reversing the order of development is reasonably common, as is an implied fifth child. When I was googling for details of what the sons say,<a href=""https://www.psychologytoday.com/us/blog/the-intelligent-divorce/201204/passover-four-sons-five-characters#:~:text=The%20Four%20Sons%3A,pivotal%20role%20in%20the%20seder.""> the first hit was a reversed-order story of the children as stages of psychological development</a>, with a fifth stage beyond the four listed.</p>



<p>These are fine tales, worthy of telling. Today, I bring a different story.</p>



<p>I bring the story that I now believe was <em>originally intended.</em></p>



<p>The four children <em>are the four simulacra levels. </em></p>



<span></span>



<p>The wise child represents level 1. They want to know how the Seder works.</p>



<p>The wicked child represents level 2. They want to know what the Seder can get them.</p>



<p>The simple child represents level 3. They want to know what the Seder symbolizes.</p>



<p>The child who does not know how to ask represents level 4. They don’t know things anymore.</p>



<p>This hypothesis and the analysis that follows <em>could </em>be me doing what<a href=""https://unsongbook.com/interlude-%D7%9E-miss-american-pie/""> Scott Alexander</a> often did and cherry picking to find entertaining and potentially enlightening connections that were clearly never intended. But I actually don’t think so.<a href=""http://unsongbook.com/""> </a></p>



<p>I believe this is the primary original intent of the story. This makes the four children, and in particular the fourth child, <em>make sense. </em><a href=""http://unsongbook.com/"">This is not a coincidence because nothing is ever a coincidence.</a></p>



<p>Quotes are taken<a href=""https://www.chabad.org/holidays/passover/pesach_cdo/aid/1486118/jewish/The-Four-Children-Explained.htm""> from an Orthodox Haggadah excerpt</a>, which is the third hit on a Google search of “the four children passover.” The second hit is reform, so it doesn’t count. The first hit, as noted above, was Psychology Today doing its own thing, which really shouldn’t have been in the highlight box.</p>



<p>You are encouraged to click through to the sources, or even better perform your own search or pick up and read the section from your own Haggadah, to verify that I am not engaging in cherry picking and to consider additional perspectives.</p>



<h3><strong>Level One – The Wise Child</strong></h3>



<p>The Wise Child lives in object-level reality. She cares about understanding the territory, and knows the map is a means to that end. She wants the facts.</p>



<p>She asks this question:</p>



<p>“What are the testimonies, the statutes, and the laws that G‑d, our G‑d, has commanded to you?” (deut. 6:20)</p>



<p>A naturalist might interpret this question as “how does the physical world work?”</p>



<p>As she communicates, thus shall you communicate to her. She wants to know the facts, so you give her the facts.</p>



<p>You should respond to him as the Torah commands, “We were slaves to Pharaoh in Egypt, etc.” and also instruct him in all the laws of Passover, up to and including its final law: “After eating the Passover offering, one should not then conclude the meal with dessert which would wash away the taste of the Passover offering.”</p>



<p>When one cares about the object level, one cares about every detail. The final law, a requirement with a specific physical purpose, is stressed here to illustrate that. </p>



<p>The final law is likely the final law <em>so that it can be the final law in this passage. </em>Dessert in the Seder is part of step 13 of 15. It’s not a natural place to put a final law.</p>



<p>The act and purpose matter in the Wise Child’s object-level literal senses. We wish to remember the taste of the Passover offering, so despite having an explicit phase of the meal for dessert, we must be careful that this dessert does not wash away the taste of the offering.</p>



<p>The act and purpose also matter directly as metaphor, in the more important meaning of both this law and its explanation. We finish the ceremony with joyful songs, but joyful songs that remind us of our struggles and do not hide the truth of our world – we know what the numbers are, the strong prey upon the weak then we all fall to the Angel of Death. Actions have consequences.</p>



<p>We also explicitly remind the Wise Child, that merely observing commandments without understanding them is not sufficient, for to do so would allow not merely them but our other actions and maps to cease to be anchored by reality:</p>



<p>So we tell the Wise Child:</p>



<p>It is true that the essence of the soul transcends the “natural order” of the person—the intellect and emotions—and therefore is blind to distinctions between commandments. It is likewise true that one can observe commandments without understanding them but simply because of the innate, essence-connection between the soul and G‑d. One can “pass over” and bypass the complications and limitations of self.</p>



<p>But it is G‑d’s will that we experience commandments within the “natural order” of our psyche, within our intellect and emotions. The transcendent “Passover” of our souls then finds expression within and permeates the “laws” of our minds and hearts (The Rebbe).</p>



<p>The very name of the holiday – Passover – <em>is superficially </em>about the Exodus from Egypt and the concept that the Angel of Death ‘passed over’ Jewish houses during the tenth plague. But that never really made sense as a justification for the name of the entire holiday. This does.</p>



<p>What the name is really for is a warning to avoid this trap of ‘passing over’ the object level, not forming a<a href=""https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding""> gears-level understanding</a>, and allowing our maps to become disconnected from profound reality.</p>



<p>Without discussion and argument, the Seder is hardly a Seder at all.</p>



<p>We must remain anchored in the object level, in our <a href=""https://thezvi.wordpress.com/2020/08/03/unifying-the-simulacra-definitions/"">profound reality</a>, if we wish to remain wise.</p>



<p>Inevitably, we lose sight of this, and proceed to level two. Thus, the second generation.</p>



<h3><strong>Level Two – The Wicked Child</strong></h3>



<p>The Wicked Child cares not about the first level, the obligation to the truth — as embodied by the Torah and the Passover story and Passover service.</p>



<p>Instead, the Wicked Child cares about what effect the service, and the story that we tell at the Seder, will have on others – to be at the second level is to draw a distinction between what you believe and do, and what you seek others to believe and do.</p>



<p>He cares not about whether the service <em>reflects </em>reality. He cares about in what way the service could <em>mask and denature </em>reality, and <em>what he can get out of </em>this service.</p>



<p>He thus asks:</p>



<p><strong>“What is this service of yours?!”</strong></p>



<p><strong>He says of </strong><strong><em>yours</em></strong><strong>—implying that it is not for him. By excluding himself from the community, he denies the essential principle</strong> of Judaism, the obligation to fulfill the commandments of the Torah.</p>



<p><strong>You should also “blunt his teeth”</strong> (speak harshly to him) <strong>and say to him</strong>:</p>



<p><strong>“It is because of this</strong> that I would fulfill His commandments, such as this Passover offering, matzah and <em>maror </em><strong>that G‑d acted for </strong><strong><em>me</em></strong><strong> when I left Egypt</strong> (<a href=""https://www.chabad.org/9874#v8"">Exodus 13:8)</a>—for me, but not for <em>him</em>. If he [the wicked child] <strong>had been there, he would not have been redeemed.</strong>”</p>



<p>As he speaks on the second level, so we need to respond to him on the second level.</p>



<p>Thus, the first thing we note about the Wicked Child is that he has <em>separated himself from </em>this central principle of Judaism, the obligation to the truth. We put his failure to be at level one front and center. That’s how important this is.</p>



<p>Yet we do not give up on him. One cannot have level one without the inevitability of level two. To care about what we believe, for any reason, is to invite others to care about what we believe, for their own selfish reasons.</p>



<p>Incentives will always be a thing.</p>



<p>We must constantly remind everyone that we seek truth and to understand and manipulate the object level not (merely) for its own sake, but <a href=""https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect"">because this is how we all survive</a> and have nice things. Without this, all is lost.</p>



<p>Thus, we speak back to him in his own language of consequences <em>to him</em>. We seek truth <em>because truth saves us. </em>We fulfill the obligations of reality and tell its stories that connect us to its profound reality – we are the people of the book – because they grant us freedom and life.</p>



<p>If the Wicked Child had been there, he would not have taken such action, would neither have been of help to or earned the help of the community, and thus <em>he would not have been saved. </em></p>



<p>This is the whole quest. It is the central mission. Once they become wise to this, the child can study the details on their own:</p>



<p>As the Talmud states, a Jew cannot lose his Jewishness. Regardless of the degree of his disengagement from Judaism, the Jewish spark lives on within him.</p>



<p>Kabbalah teaches that the wicked child, <em>second</em> of the four children, corresponds to the <em>second</em> of the Four Cups. This means that the bulk of the Haggadah is recited over the cup related to the wicked child! Clearly, befriending and educating the wicked child is a central aspect of the Haggadah. For this effort helps bring about the ultimate realization of the Egyptian Exodus.</p>



<p>The Jewish spark here represents this drive towards truth in all of us. Of course this cannot be fully extinguished. Reality is that which, when you stop believing in it, doesn’t go away. A sufficiently powerful smackdown from reality will wake anyone (who survives it) up.</p>



<p>It can, however, be <em>suspended </em>indefinitely under the wrong conditions.</p>



<p>Thus, we spend the bulk of the Seder speaking primarily to the Wicked Child.</p>



<p>In each generation the wicked child must be convinced of the need to choose wisdom. The wicked child follows from the wise child, as the second level follows from the first. Only by continuously maintaining right incentives and norms, and hammering the necessary messages into everyone’s heads over and over, can we ensure the wicked children among us ultimately choose wisdom.</p>



<p>This is not a struggle that happens once. It happens continuously for each of us that still thinks reality is a thing. Each of us <em>who still believes that others believe that one thing is and another is not, </em>is tempted continuously by the ability to say that which is not in order to get others to believe that which is not. </p>



<p>This fits with my model that, while higher-simulacra-levels are always present to some extent, past societies have mostly succeeded at keeping the focus on the object level and thus preventing things on the whole from degenerating further.</p>



<p>Or, that those that have failed at this task have fallen soon thereafter.</p>



<p>When the community fails at this task, the Wicked Children grow up and remain wicked. They continuously work to mask and denature the grand reality. Words become less and less often and less and less substantively a reflection of reality, and more and more a mask of that reality – the mask the speaker wishes to place upon it. In turn, people’s expectations adjust.</p>



<p>Things then give way to the third generation.</p>



<h3><strong>Level Three – The Simple Child</strong></h3>



<p>The Simple Child is not born simple. Nor is she stupid. The Simple Child is responding to incentives. She plays the game laid out before her.</p>



<p>Raised by and around the wicked, The Simple Child lacks the expectation that symbols line up with reality. Those around her have been pretending the whole time. She wants to know how to pretend to do this pretending.</p>



<p>She does not have <em>or seek</em> a useful model of physical reality. Such a model does not seem like it would be useful.</p>



<p>She notices instead that rewards and punishments in such a world are best navigated through asking what signals to send. So she seeks to understand symbols well enough to send the right signals.</p>



<p>Thus, the simple child asks the most basic question: “What is this?”, or “What is this celebration about?”</p>



<p><strong>You shall say to him</strong>: “We are commemorating the fact that <strong>with a strong hand G‑d took us out of Egypt, from the house of slaves</strong>” (<a href=""https://www.chabad.org/9874#v14"">Exodus 13:14)</a>.</p>



<p>As she speaks to you, so shall you speak to her. She wants to know <em>what this symbol means. </em>So we tell her what it means, and what and who is to be raised or lowered in status.</p>



<p>We don’t actually answer the question! We do not tell her <em>what this is. </em></p>



<p>She isn’t really asking for that information. She isn’t ready for the answer. We don’t have that kind of time. We will. But not now. Not tonight.</p>



<p>But this is all rather tragic. Did we give up on her so easily? Has all been lost by this point? Can we not do better than to get her to think of us as her in-group whose actions should be imitated and signals sent?</p>



<p>This is one of the biggest problems of our age. If someone seeks to be nothing but a partisan, how does one get them to be more than that? If everyone is being judged on their partisanship, how is one to free them from that? To snap them out of it?</p>



<p>The text does not seem to have an answer. The Haggadahs I have used don’t even try to answer. This particular version advises:</p>



<p>We tell the simpleton how the Exodus occurred and how he too can experience a personal “Exodus”: Just as G‑d used a strong hand to “overcome” the attribute of justice, we too must use a strong hand to overcome those aspects of our personalities that impede our spiritual growth. We then experience a spiritual liberation from our personal enslavements.</p>



<p>That does not seem likely to get us much of anywhere. We’re talking in mumbo-jumbo in the hopes it will symbolically resonate. All we hold out is the promise of ‘spiritual liberation.’</p>



<p>It seems that all the Rabbis believe we can do, at this point, is damage control. Thus, we spend so much time trying to rescue the Wicked Child. That’s where there is still some hope. The Simple Child, in this model, is mostly a lost cause.</p>



<p>But we offer a way out. We note that we are commemorating <em>a fact. </em></p>



<p>We link our explanation back to a concrete origin, as a first step in reorienting her attention. It’s a trick that just might work.</p>



<p>The ‘spiritual liberation’ is exactly this – to notice reality and be liberated from being trapped in meaningless symbols. To think for one’s self.</p>



<p>That’s why there is no talk about the Wise Child’s spiritual liberation. There is no need.</p>



<p>Thus, this model says the goal is purely to get the Simple Child to <em>pay attention. </em>The promises we make to her are to get her to participate at all, to be present. After that, she can be exposed to the arguments and discussions, to the details. She can notice what is actually going on, and think more on that level.</p>



<p>There is hope. Room to grow. She can still ask questions and care about the answers. Remember her opening question. She asks, what is this? Thus, she still knows on some level that there is a this and it has a what.</p>



<p>What she is unable to do, if she is not helped out of her trap, is pass this remaining understanding along. The fourth generation is coming.</p>



<h3><strong>Level Four – The One Who Does Not Know How to Ask</strong></h3>



<p>It is frequently pointed out that the name of the fourth generation is profoundly weird.</p>



<p>Have you ever met a <em>child </em>who did not know how to ask?</p>



<p>I have not. I’ve met <em>adults </em>who <em>no longer </em>know how to ask. Who have fully integrated level four. Who have <em>forgotten. </em>The fourth level ceases to know that the first level exists.</p>



<p>There is the temptation to not engage with the name. To treat it as some sort of metaphor.</p>



<p>The temptation is wrong. The fourth generation <em>does not know how to ask.</em></p>



<p>That does not <em>quite </em>mean “<em>literally </em>does not know how to ask anything at all”. But it also kind of does mean that.</p>



<p>Asking requires realizing that there exist questions and answers. It requires believing that those questions and answers matter. That there is a ‘there there’ under all that.</p>



<p>He does not know that some things are while other things are not. If answers don’t matter, there can be no questions.</p>



<p>Even if he did somehow want that information, <em>he doesn’t know how to ask about actual things. </em>Everything is a symbol referencing another symbol. There’s no way to get those symbols to reference the physical world. Thus, no way to ask a question.</p>



<p>This is the giveaway that we’ve been talking about simulacrum levels.</p>



<p>The one who does not know how to ask cannot ask for wisdom. For them, wisdom isn’t a thing.</p>



<p>And they can’t ask how reality works. For them, reality isn’t a thing.</p>



<p>What is to be done about this? We must talk in a way he might understand, that might cause him to realize there are things to be understood.</p>



<p>Thus:</p>



<p>As for The One Who Knows Not How To Ask—you must open up [the conversation] for him.</p>



<p>As it is written: You shall tell your child on that day: “It is because of this that G‑d acted for me when I left Egypt” (Exodus 13:8).</p>



<p>What we are trying to communicate here is <em>basic cause and effect. </em>That there is a <em>this </em>and it caused a <em>that. </em>Because of this, G-d acted for me when I left Egypt. The very idea of logic, of consequence, is lost upon him. Recover those, together with the idea that some things are and others are not, and the child can learn how to ask. All that matters, for now, is teaching this most basic lesson.</p>



<p>Their need to leave Egypt (which in Hebrew is literally “the narrow place”), is here about the need to realize this. Because we know things and seek knowledge, our world exists and can expand. We can do things, go places, not be trapped. We can be free.</p>



<p>Two levels. Because of these actions, things happened. Because of knowledge, one can take actions that do things.</p>



<p>The child’s participation in the Seder is not about any of that; they are just employing systems that attend the rituals that those around them participate in. They go through all the motions, but have no idea what they are doing.</p>



<p>What about alternative interpretations of this stage?</p>



<p>I have heard the suggestion that the fourth child is very young, and does not yet know how to speak. This seems clearly wrong.</p>



<p>If that was what was going on, the child would have a different name – the child who cannot (yet) speak – and our advice for them would be different. The child being unable to speak doesn’t make sense in the context of the text telling you to start the conversation for them. If they can’t talk, trying to start a conversation about the Exodus would be quite pointless. </p>



<p>Another reason to reject this interpretation is that this child does not yet know how to talk, but <em>does know how to ask. </em>He doesn’t know the words, but if you hang around a child who hasn’t yet learned to talk and pay attention it’s clear they can ask about basic things without words. </p>



<p>Another alternative interpretation, from the same Haggadah as above, is this angle:</p>



<h4><strong>Too Smart For Questions</strong></h4>



<p>This fourth child may be a ritually observant Jew who fulfills all the customs of the Seder. But his Judaism is cold and dry. He does not feel a need for spiritual liberation. He has no questions about or real interest in the Exodus because he does not think of himself as being in exile.</p>



<p>He claims that he is not the excitable type and thus excuses his lifeless Jewish practice. Yet while he cannot muster any excitement for Judaism, he is easily exercised and engaged by material ambitions. He does not realize that his heart and mind are in exile, oblivious to the spiritual content of life.</p>



<p>We cannot begin by telling this Jew what G‑d did (as we tell the simple child); we must first inspire him to seek spiritual liberation. We therefore tell him:</p>



<p>“G‑d did this for me when <em>I left Egypt</em>”—you too are in need of leaving Egypt.</p>



<p>The key insight here is that <em>we cannot begin the way we did with the Simple Child, by conveying information. </em>It won’t work! The Simple Child has redirected her curiosity, and does not yet much value information, but still understands that information is a thing.</p>



<p>Information would only bounce off The One Who Does Not Know How To Ask. Not being able to ask is merely a symptom. Spiritual liberation again means realizing knowledge exists at all, and is the necessary first step.</p>



<p>However, I think the rest of this is importantly wrong. And it can be wrong in two ways.</p>



<p>First, this child may be <em>misidentified. </em></p>



<p>If the child is instead Simple, going through the ritual without feeling makes sense. The simple child can be told what this is and what to do, and then they go through the motions. It certainly would not occur to them to seek ‘spiritual revelation’ because life at the third level has no spiritual aspect.</p>



<p>If the child is instead Wicked, that is another potential explanation for this data. They are there to avoid punishment, or to score points, rather than to have the experience and/or better themselves.</p>



<p>The second way this is wrong is the most common mistake when those outside it try to model level four. It is the idea that he is easily exercised and engaged by material ambitions— that those sufficiently at level 4 are doing what the rest of us are doing, engaging in actions because of their model’s guess as to their consequences, in order to achieve particular ends.</p>



<p>That’s not how level 4 works. Such people don’t have goals. They have systems. The fourth child truly is lifeless and unexcited. When such people seem excited, it is because their systems think being excited is the next move, the way deep learning might suggest excitement be expressed at particular points. Nothing more.</p>



<p>Such strategies do often cash out in material ambitions, but that is not because such ambitions excited the person or a plan was formed to get them. The idea of having a plan or ambitions, or of there being a physical thing to be ambitious about, doesn’t parse for them the same way it does for others.</p>



<p>Then there’s this other note:</p>



<p>The fourth child may actually want to ask but lacks confidence and fears being seen as a fool. The Haggadah instructs us to be sensitive to such people and to put them at ease by initiating conversation with them until they are comfortable sharing their thoughts confidently and clearly (R. Shlomo Alkabetz; Chida).</p>



<p>That is <em>definitely </em>not the fourth child. The issue lies elsewhere.</p>



<p>It’s certainly a thing that happens. But the child it would be happening to would be the Wise child.</p>



<p>Knowledge is desired. There’s social issues in the way, but that is <em>our fault. </em></p>



<p>This is, of course, how it all begins. Children do not start out not knowing how to ask. The problem is caused by the adults who do not know how to answer.</p>



<p>We have somehow taught this child that asking questions can mean being a fool and that this is bad. We’ve answered his questions by telling him what we want them to see, or what the ritual response to their statement is, rather than by explaining what is and what is not. Without answers, what is a question?</p>



<p>It’s on us to fix it. Not them. The prescription here is a good idea, but seems importantly non-central. What is most important is taking away this idea that asking questions is bad or foolish, and setting up an expectation that questions get answers. If seek means ye might find, perhaps then ye will seek.</p>



<p>Otherwise, engaging them in conversation will seem like torture rather than opening them up. It’s calling on kids unprompted in class to interrogate and humiliate them. It’s grading kids on ‘class participation’ where participation means<a href=""https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password""> guessing the teacher’s password</a>. It is being polite at the dinner table until you can ask to be excused. If those around you will only respond to your level one inquiries with level three or four answers, either because that is all they know or they assume that is what you must seek, <em>then you too do not know how to ask.</em></p>



<p>Thus, once things move along sufficiently, the full <em>generation </em>does not know how to ask, even those who remain wise, wicked or simple. When they attempt to ask, no answers come. Meaningful questioning ceases.</p>



<p>This is a common failure mode.</p>



<h3><strong>Level Five – The Child Who Is Not There</strong></h3>



<p>Despite the failings of the four children, they all did the most important thing of all.</p>



<p>They showed up. They are present at the Seder.</p>



<p>That is important because, in this story and metaphor, the Seder (literally ‘order’) represents civilization. It is the ability to know things and pass on that knowledge. Also therefore to accomplish meaningful things, to gather the fruits of our labor.</p>



<p>The fourth generation <em>still sits down with </em>the first one. They work together. To some extent, they must listen. This maintains an anchor.</p>



<p>Without the first generation’s renewal and participation, the process cannot be sustained.</p>



<p>As the generations progress, it becomes harder to draw the children into wisdom. Those who are drawn in become less rewarded for it, and more punished. The wicked understand, acknowledge and value the Wise—they depend on the Wise for their own cynical gain. The simple don’t see the point of wisdom. Those who do not know how to ask don’t even know wisdom is a thing.</p>



<p><a href=""https://thezvi.wordpress.com/2020/05/23/mazes-sequence-summary/"">Finally, there is the child who is not there.</a> Not only do they not know how to ask, they are not connected to those that do. Value in the physical world ceases to be sustained at all. All is lost.</p>



<h3><strong>Conclusion, Goals and Takeaways</strong></h3>



<p>There were a few distinct goals here.</p>



<p>The first was that when I realized this lined up, it felt too good not to explore and share. Other goals were not necessary, and could be figured out later.</p>



<p>The second was to provide another look at the elephant that provides additional intuition pumps. When something is confusing, the more distinct ways to illustrate both the key points and the details around them, the more likely any given person is to find one that resonates. This also provides additional potential names and references for the levels.</p>



<p>The third was to reinforce in particular the idea that there is something profound that is lost at the fourth level, and to provide help understanding what that is and how that could be. That the fourth level loses its logical facilities. This version puts that so front and center that the loss of logic is explicit and much of the rest of the model is implicit. And it’s important enough that it has survived two thousand years of looking like nonsense.</p>



<p>The fourth, similar to the third, was to provide additional support for the idea of progression through the stages. And to look at how this first attempt tried to halt and even reverse that progression, in the hopes that we can use those strategies and/or find ways to do better.</p>



<p>This was a fun one. No doubt there are many other similar attempts out there. I can think of several but am curious what people come up with on their own. What are some others, real or fictional?</p>



<p>Is GPT-3 a simulation of the child who does not know how to ask?</p>



<p>I have now produced<a href=""https://thezvi.wordpress.com/2020/05/23/mazes-sequence-summary/""> a book-long sequence on Moral Mazes</a>, and a succession of posts on Simulacra levels. The central hope is to use this as background common knowledge concepts and jargon vocabulary going forward, and that others can do so as well.</p>",Zvi,zvi,Zvi,
ErGowWsgSKZWEqK9k,Reference Classes,reference-classes,https://www.lesswrong.com/posts/ErGowWsgSKZWEqK9k/reference-classes,2020-09-07T12:52:33.522Z,10,3,4,False,False,,"<p><em>Epistemic Status: </em>Just some thoughts off the top of my head</p><p>Fake Nous recently featured an article on <a href=""https://fakenous.net/?p=1857&amp;fbclid=IwAR16g_LKCCh9IWL01Tbd5Z8_KkCmWJH2fkTxtDvoP6oa0LqCNN_s8m4j4qg"">agent-centered evidence</a>:</p><blockquote>Sue had a premonition about the flight, and then the plane crashed. For Sue, that&#x2019;s pretty strong evidence of precognition. We would completely understand Sue&#x2019;s resolution to never get on a plane that she has a bad feeling about; this would not seem unreasonable at all. But for third parties, it&#x2019;s not very convincing. Is it? </blockquote><blockquote>...  This event is a biased sample from the class of stuff that happens. The reason I heard about this story is that something weird happened &#x2013; if Sue had a premonition that was completely wrong, then the story wouldn&#x2019;t get repeated and I wouldn&#x2019;t have heard about it. Furthermore, since there have been billions of people in the world, I should initially <em>expect</em> that <em>some</em> things like this would have happened, even if there were no precognition or ESP.&#x201D;  But when Sue herself experiences the event, she shouldn&#x2019;t say that. To her, her own life is not a biased sample. </blockquote><blockquote>...  That seems to make sense. Two people can get &#x201C;the same evidence&#x201D; but by a different evidence-collection method, and of course that can affect the significance of the evidence... There is still something weird about this, though, because Sue knows how the situation looks to third parties, and they know how the situation looks to her. Both seemingly know the same facts. The third parties know that Sue&#x2019;s experience is not a biased sample <em>to her</em>. She knows that her experience is, to other people, just the experience of one among the 7 billion people on earth, and not particularly remarkable to them.</blockquote><blockquote>...  Another question: what about people who know Sue personally?  But if Sue is a member of your immediate family, you might say, &#x201C;There are 4 members of my family...  And if so, why couldn&#x2019;t we extend this to Sue&#x2019;s barrista at Starbucks? The barrista could say, &#x201C;I have had only 100 customers today, of whom one had a precognition-like experience&#x201D;, which sounds fairly remarkable.</blockquote><p>I actually think that there is a form of agent-centered evidence called intuition which isn&apos;t easily communicatable to other agents and can often prove <a href=""https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow"">fairly useful</a>. However, that&apos;s not the issue I want to discuss today. Instead, I want to talk about reference classes. But before I cover that, let&apos;s talk about shots. Suppose we have a raffle with one prize and a thousand tickets. If you have one ticket, you have one shot, while if you have ten tickets, you have ten shots. I haven&apos;t precisely defined it, but I think this example should be clear enough. Once you know the number of shots, you can turn it into a probability.</p><p>Let&apos;s suppose that you have four members in your family and that you also have four work colleagues. If one of your family members has a precognition-like experience, you might say that it is remarkable as you only had four shots, but let&apos;s suppose that in the counterfactual where one of your colleagues had an experience you would have also counted it as four shots. This seems like a mistake; only one can be counted as four shots and if the other occurs, then it has to be counted as eight shots with the group being classed as family AND colleagues.</p><p>If you have a bunch of different groups, say three family members, another five work colleagues and eight cousins, then you can order them arbitrarily. It doesn&apos;t matter if you do (3,5,8) or (5,3,8) or (8,3,5); any of them is fine. If you order them (8,3,5), then the number of shots for a member of a group is 8 for the first, 11 for the second and 16 for the third.</p><p>This might seem strange. We are calculating a different probability of psychic powers existing when an event happens to a member of your family vs. one of your colleagues, even though there doesn&apos;t seem to be any fundamental reason written into the universe itself why one group should give you more evidence.</p><p>Then again, the probability you assign to something existing is really more about your subjective state, such as the information available to you and how it was generated, rather than the objective state of the universe itself. We can think about choosing how to order your groups the same way that we think about committing to a (frequentialist) experiment design in advance. It&apos;s well understood that if you test more hypotheses, you increase your chance of a spurious result. For example, if you test for effects in male adults, female adults, male children and female children, you&apos;ve taken four shots as opposed to having only tested for an overall effect. This is typically adjusted for by using a significance threshold that isn&apos;t constant, but instead depends on the number of hypotheses that you are testing.</p><p>We can take this analogy further. Suppose in the example above, we choose the ordering, (5, 3, 8). Then this defines three experiments - work collegues only with five shots, work + family with eight shots and all groups with sixteen shots. If we observe one of our family members having such an experience, we can treat it as us having pre-committed to an experiment covering family and work colleagues with eight shots. This is far better than the naive tendency we might have to define the group as just family with three shots. </p><p>However, it still isn&apos;t a completely accurate way to handle probability, as if we want an accurate an estimate of psychic ability as possible, then we should take into account <em>all </em>the evidence available. So if we also know about whether our cousins have had such experiences, then we really should take that into account when calculating the probability.  Of course, trying to figure out this implicit sample might greatly complicate the calculation, which is why this group based approximation is much more appealing instead.</p><p>That said, this is quite an unusual approximation, as it can result in completely different probabilities than if we had the whole data. For example, observing a positive out of five shots striking instead of a positive out of sixteen makes a huge difference in the actual probabilities. Nonetheless, if you had precommitted to making a decision based on the first five, then the increase in probability when you saw a positive result would be perfectly matched by the decrease when you a negative result. This means that deciding in advance to only look at the first five wouldn&apos;t bias the result, even if throws away data.</p><p>Perhaps a more realistic scenario is one where you precommit to expanding the experimental group until you hit a positive result or you&apos;ve expanded it to the end. This would represent the fact that someone might not worry about how amazing it is that someone in their church had a particular experience if someone in their family had such an experience. These expansionary scenarios are too complex to handle using the shots framework, but even in this scenario the maths isn&apos;t too hard.</p><p>Of course, a lot of the time we aren&apos;t deciding in advance, but are instead deciding after the fact. In this case, you&apos;re ability to use these schemes is highly, highly dependent on your ability to self-model. If you can do this well, then you can adopt these schemes after the fact, but if you do it poorly, it&apos;ll completely mess up the results.</p>",Chris_Leong,chris_leong,Chris_Leong,
jA6YmfsrHWCwxxjTq,Shed Wall Plans,shed-wall-plans,https://www.lesswrong.com/posts/jA6YmfsrHWCwxxjTq/shed-wall-plans,2020-09-06T22:20:05.048Z,6,3,2,False,False,,"<p><span>

Last week I wrote about </span>

<a href=""https://www.jefftk.com/p/estimating-the-roi-of-insulation"">estimating whether
insulation was worth it</a> for our shed if we're going to use it as a
home office.  While I'm waiting for the carpenter to put a new roof on
it, I'm thinking through what finishing the walls would look like.



</p><p>

The <a href=""https://www.facebook.com/jefftk/posts/10100182218853792"">Facebook
side</a> of the discussion on the insulation post was very helpful,
and got me thinking about putting closed cell foam panels directly
against the concrete walls.  Because it's both a vapor barrier and
efficient insulation it reduces the risk of condensation happening
between the concrete and the insulation.

</p>

<p>

I'm less sure about what comes next.  Whatever I'm doing is not
structural, and there are a lot of trade-offs.  All of the options
involve the layer of closed-cell foam directly against the concrete, but
then they vary on how they're finished.  Standard options:

</p>

<ul>

<li><p>Build a 2x3 wall, then drywall.  This gives room to run the
electrical, and wood to attach the drywall to.  The wall would be
attached to the subfloor below and roof joists above.  Nothing needs
to go through the foam to the concrete, and the wood framing makes
later work easier.</p></li>

<li><p>Build a 2x4 wall.  Same advantages as a 2 x 3 wall, but now
there's enough space in the cavity that it could be insulated. I could
either use this for a larger amount of insulation total, or reduce the
thickness of the foam.(<a href=""https://www.youtube.com/watch?v=cCFFuL4J0jQ"">video
example</a>)</p></li>

<li><p>Furring strips.  Run strips of wood horizontally along the
wall, and attach them through the foam into the concrete.  Then run a
second set vertically, 16 inches on center, to attach the drywall to.
There's room for electrical as long as you dig out a bit of the foam
for the boxes.  (<a href=""https://www.youtube.com/watch?v=oKQdo88Ne74&amp;t=6s"">video
example</a>).</p></li>

</ul>

Less conventional:



<ul>

<li><p>Verticals inside: Instead of using furring strips, put vertical
2x3s from floor to ceiling, 16 inches on center. Attach them a the top
and bottom (roof joists and subfloor).  Attach the drywall to the
wood.  Run the electrical through the ceiling, and then down into the
appropriate bay so you don't need to drill through any verticals.
Essentially the same as the furring strip solution, but without
needing to put fasteners through the foam into the concrete.  This
seems better than the furring strip approach?</p></li>

<li><p>Verticals outside: Attach the drywall to the foam with
adhesive.  Then instead of standard drywall joints, run 2x3s
vertically 48 inches on center.  Electrical in conduit, attached to
the verticals.  Mildly funny looking, doesn't feel quite as nice,
maximizes space if you ignore the verticals.</p></li>

</ul>



<table>
<tr>
<th>foam</th>
<th>finish</th>
<th>overall</th>
<th>R-value</th>
<th>cost

</th>
</tr>
<tr>
<td>1""</td>
<td>verticals outside</td>
<td>1.5""</td>
<td>R-6.5</td>
<td>$372
</td>
</tr>
<tr>
<td>2""</td>
<td>verticals outside</td>
<td>2.5""</td>
<td>R-13</td>
<td>$580
</td>
</tr>
<tr>
<td>1""</td>
<td>verticals inside</td>
<td>3""</td>
<td>R-6.5</td>
<td>$468
</td>
</tr>
<tr>
<td>2""</td>
<td>verticals inside</td>
<td>4""</td>
<td>R-13</td>
<td>$676
</td>
</tr>
<tr>
<td>3""</td>
<td>verticals inside</td>
<td>5""</td>
<td>R-19.5</td>
<td>$988
</td>
</tr>
<tr>
<td>1""</td>
<td>2x3</td>
<td>4""</td>
<td>R-6.5</td>
<td>$536
</td>
</tr>
<tr>
<td>2""</td>
<td>2x3</td>
<td>5""</td>
<td>R-13</td>
<td>$744
</td>
</tr>
<tr>
<td>1""</td>
<td>2x4 insulated</td>
<td>5""</td>
<td>R-21.5</td>
<td>$930
</td>
</tr>
<tr>
<td>2""</td>
<td>2x4 insulated</td>
<td>6""</td>
<td>R-28</td>
<td>$1138

</td>
</tr>
</table>



<p>

This doesn't include the cost of the fasteners or adhesive, but seems
about right.  I'm leaning towards the approach of vertical 2x3s
sixteen inches on center under the drywall with 2"" foam, since it
seems to offer a good balance of cost, insulation, and minimizing
wasted space.

  </p>

<p><i>Comment via: <a href=""https://www.facebook.com/jefftk/posts/10100182764330652"">facebook</a></i></p>",jkaufman,jkaufman,jefftk,
JeYgBpfEriLe27f6M,The ethics of breeding to kill,the-ethics-of-breeding-to-kill,https://www.lesswrong.com/posts/JeYgBpfEriLe27f6M/the-ethics-of-breeding-to-kill,2020-09-06T20:12:00.519Z,-4,16,24,False,False,,"<p>Veganism, vegetarianism, and &quot;ethical&quot; farming seem to be gaining a  lot of ground lately, which is something I find fascinatingly absurd.</p><p>In part, I think this comes from a felicitous style of reasoning that I outlined <a href=""https://blog.cerebralab.com/Vegetarianism_and_how_the_ethics_of_FOSS_can_get_muddled"">here</a>.</p><p>But, in hindsight, I think a lot of people that consume meat don&apos;t  have any foundation that backs up their choice of killing animals for  food. So I think it&apos;s worth outlining one here.</p><p>First, let&apos;s get the two &quot;main&quot; arguments against killing animals for food and factory farming on the table:</p><ol><li>The utilitarian argument - Farming animals for meat often causes more &quot;suffering&quot; than &quot;joy&quot; to the animals.<br> </li><li>The normative argument - Breeding something in order to kill it is &quot;wronger&quot; than not having it live at all.<br> </li></ol><p>Second, I want to look at the kind of animals me (and presumably many  people) would feel bad about killing or eating. I&apos;m going to ignore  cats and dogs here because there&apos;s too much baggage to take in due to  the role they play in our society. But disregarding those, I think there  are three categories:</p><ul><li>Humans<br> </li><li>Apes (potentially extending to all monkeys)<br> </li><li>Cetaceans (whales &amp; dolphins)<br> </li></ul><p>That&apos;s not to say we would eat all other types of animals, but if an  Inuit tribesman would hand you a traditional dish made with seal or  bever you might begrudgingly (or, in my case, happily) give it a go.  However, you would probably refuse if that same tribesman handed you  human or blue whale meat.</p><p>Why?</p><h2>Hedonic Regression</h2><p>Utilitarianism is a good ethics framework if you refuse to understand  how a brain works. In the real world, suffering and joy aren&apos;t so clean  cut. Animals adapt their &quot;level of suffering&quot; to their environment. If  an animal is in a hostile land for a few months, and when you deprive it  of water, that animal might feel equally bad as one that has lived in  parades for the last few months but has just been deprived of a mating  opportunity.</p><p>The <a href=""https://en.wikipedia.org/wiki/World_Happiness_Report"">world&apos;s happiness index</a>  can be a good showcase we humans experience this to some extent. Note,  for example, how Saudi Arabia (a harsh and unequal Islamic theocratic  monarchy that still practices <a href=""https://en.wikipedia.org/wiki/Capital_punishment_in_Saudi_Arabia"">beheading and crucifixion</a>  sprawling over an unforgiving desert) is overall &quot;happier&quot; than  Spain... which, is Spain, it&apos;s so nice it&apos;s among the top 5 global  destinations for foreign holidays.</p><p>To give a more extreme example, Somalia is at 112 out of 156, above  countries like Ukraine. Somalia seems to be a horrifying place to be  even by Subsaharan African standard. Somalia&apos;s GDP per capita as of 2019  is 348$ (almost 200 times lower than that of the US and with higher  income inequality), the rate of <a href=""https://en.wikipedia.org/wiki/Female_genital_mutilation"">female genital mutilation is 98%</a>  (see wiki article if you want more graphic details, whatever you&apos;re  thinking of, I assure you it&apos;s worst than that). I won&apos;t go into more  details here, but feel free to dig through it&apos;s <a href=""https://en.wikipedia.org/wiki/Somalia"">Wikipedia page</a> if you want to see exactly how horrible a place can get.</p><p>Granted, some of the countries ranked lower such as Ukraine, India,  Iran, and Georgia aren&apos;t ideal. But the problems there seem more akin to  those in Eastern Europe around the turn of the 21st century, rather  than... whatever the hell is happening in Somalia.</p><p>This is just a nit-picky showcase, but feel free to dig into the  issue further if it&apos;s the first time you heard about it, I feel like  it&apos;s <a href=""https://en.wikipedia.org/wiki/Hedonic_regression"">hardly a controversial phenomenon</a>.</p><p>The question that remains is something like: How far does hedonic  regression go? To which the answer varies. You can probably get a  personal answer by looking at metrics of happiness during your life  (e.g. amount of good sleep, money, sex, romantic relationships, nice  objects, good friends, drugs, quality time with your parents and, the  free time you had) vs how happy you felt at any given time.</p><p>I assume a Buddhist monk might claim 90% of the thing is constructed  and your circumstances don&apos;t matter at all, while a hardcore Marxist  might say reverse those percentages. Still, regardless of what the  number is, we seem to agree that &quot;objective&quot; happiness is enough of a  thing to motivate us towards (trying at) improving the human condition  through material means.</p><p>These material means includes things like not farming our fellow  humans in tight cages or confined pastures in order to slaughter and eat  them.</p><h2>Suicide as an indicator of objective happiness</h2><p>While it&apos;s hard to quantify objective happiness, I think it&apos;s fair to  use suicide as a benchmark for when someone&apos;s life becomes miserable  enough for them to end it.</p><p>Granted, a lot of suicides are &quot;spur of the moment&quot; psychotic acts,  but some are cold and calculated and spurred on by chronic suffering.  Overall, they account for 1.5% of human deaths, which is quite  significant.</p><p>Even more so, a larger number of people probably live in  &quot;suicide-inducing&quot; conditions, but carry on due to hoping for happiness  in the future.</p><p>This is all to say, hedonic regression or not, there&apos;s certainly a  breaking point for humans when the suffering outweighs the pleasure  enough for life to not be worth living.</p><p>There&apos;s some debate as to whether or not <a href=""https://www.wellbeingintlstudiesrepository.org/cgi/viewcontent.cgi?article=1201&amp;context=animsent"">animals commit suicide</a>  due to &quot;suffering&quot;. The only &quot;obvious&quot; cases are in dolphins, with the  most well studied being Flipper and the lesser-known Peter.</p><p>We can argue over the exact definition of &quot;knowing&quot; what life and  death are and thus &quot;consciously&quot; deciding to commit suicide. But in the  case of <a href=""https://en.wikipedia.org/wiki/Margaret_Howe_Lovatt"">a dolphin-like Peter</a>, it seems that the chain of events is something like:</p><ul><li>Get taken away from your kin and placed in a house with a girl and some researchers</li><li>Befriend the girl and make seemingly sexual advanced on her</li><li>Have her begrudgingly reciprocate with some pitty hand jobs</li><li>Drop acid with her and some scientists</li><li>Get taken out of the house and away from the girl you liked</li><li>Commit suicide by swimming to the bottom of a tank and staying there until you run out of air and suffocate.</li></ul><p>Is there some anthropomorphizing going on here? Maybe. But I think  it&apos;s hard to make this seem like anything but a human-like suicide due  to life being too miserable for it to be worth living. This is not a  cell committing apoptosis, this is not a mother jumping in front of a  predator to give her kids time to escape, this is not an old alpha male  dying in a battle to protect his fading status, it&apos;s not a scared bison  being chased off a cliff.</p><p>The case for suicide in other <a href=""https://books.google.ro/books?hl=en&amp;lr=&amp;id=Lge9DwAAQBAJ&amp;oi=fnd&amp;pg=PA315&amp;dq=suicide+in+cetaceans&amp;ots=mt5RR7Qjf4&amp;sig=Jt1NnFyzML4kCJix4v_jqa2vXAw&amp;redir_esc=y#v=onepage&amp;q=suicide%20in%20cetaceans&amp;f=false"">cetaceans is vaguer</a>, but it still seems plausible that they would exhibit such behavior based on their other actions.</p><p>To my knowledge, it hasn&apos;t been observed in monkeys (other than  vervet monkeys, arguably), but then again, studying monkeys in the wild  is hard and we usually treat them fairly well in captivity. Still, I  think it&apos;s a safe bet based on how similar apes are to us that they  might be capable of suicide, they are certainly capable of many other  forms of self-harm.</p><h2>Can hedonic regression go on forever?</h2><p>Conversely, it seems that there are no documented cases of suicide  amongst commonly farmed animals. The closest I can get to is an incident  in the alps with <a href=""https://www.thelocal.ch/20170531/swiss-farmers-left-puzzled-after-cows-throw-themselves-off-cliff"">cows throwing themselves off a cliff</a>.  But knowing how cows are treated in the Swiss alps (hint: fairly  nicely, arguably better than we treat most humans, certainly not in any  way resembling factory farms) plus many other cases of scared bovines  accidentally running off cliffs, I think it&apos;s fair to assume this is not  a &quot;suicide&quot; but rather an accident.</p><p>Granted, the absence of evidence is not proof, but I&apos;d think we&apos;d  have observed this if there were a significant number of cases.  Self-harm amongst farmed animals does seem to happen, but it never seems  to directly lead to death, at most it leads to infections that kill  them later (e.g. due to the excessive grooming behavior that most  animals exhibit in captivity).</p><p>The obvious conclusion from this ought to be that animals in  captivity are on the whole &quot;happy&quot;, even those in factory farms. Or at  least, not suffering so much as to think their condition to be worst  than death.</p><p>You may retort that the kind of animals we farm aren&apos;t able of the  reasoning needed to conclude &quot;My present condition is worst than not  being at all&quot;. But then, why assume the concepts of joy and suffering as  we understand them to apply to them at all? If they aren&apos;t agentic  enough to reason about their condition in that objective sense, then the  obvious model seems one where they lack our partially objective  concepts of &quot;good&quot; and &quot;bad&quot; entirely.</p><p>To summarize:</p><p>The assumption that factory-farmed animals lead a life of  &quot;suffering&quot;, that is to say, they get &quot;negative&quot; joy out of life and  they&apos;d be better off being dead, seem shacky.</p><p>Suffering and happiness are human concepts, and we can in part attest  there are forms of suffering worst than death by looking at our choice  to commit suicide (i.e. chose death over suffering).</p><p>This behavior seems to be exhibited by some animals of presumably  similar intelligence (cetaceans and monkeys), but not by the animals we  farm.</p><p>Thus, based on our best interpretation of the hard subject of  consciousness and feeling in different species, it seems reasonable to  assume that:</p><p>a) The animals we farm &quot;prefer&quot; living in their current state to dying.</p><p>b) The animals we farm lack any concepts of suffering and joy similar to ours.</p><p>The utilitarian argument for not farming animals would fall over in  both of these situations. Even worst, if the problem falls into the case  a), then as a utilitarian, you&apos;d have a duty to eat as many animals as  possible, thus ensuring the birth and happiness-positive lives of as  many farm animals as you can. Being a vegetarian would not only fail to  prevent any suffering but might actually diminish the amount of  happiness in the world.</p><p>The normative argument for not farming animals might still stand if  it involves religious reasons (e.g. the insistence upon not killing in  certain branches of Buddhism and Hinduism). But the version that is  based on the normative value assigned to &quot;happiness&quot; and &quot;suffering&quot;  would be invalid in this paradigm.</p><p>This is not to say that we can certainly conclude that animals being  farmed don&apos;t actually dislike life more than they enjoy it. This could  certainly be the case, and they might just lack the reasoning to commit  suicide. But this is an arbitrary anthropomorphic trait we decide to  assign upon them and it could equally well be assigned to mosquitos, or  waps, or mycelia.</p><p>Thus I fail to see a strong ethical argument against the eating of  animals from this perspective. Although there&apos;s a completely unrelated  environmental perspective against farming animals which this doesn&apos;t  address.</p><p>It seems that we should at most &quot;shelf&quot; this problem for later when  there is enough time to actually address the fundamental question of  whether or not these animals would or could prefer inexistence to their  current state.</p><p>Until then, the sanest choice would seem to be that of focusing our  suffering-diminishing potential onto the beings that can most certainly  suffer so much as to make their condition seem worst than death.</p>",George3d6,george3d6,George3d6,
pa6GWazpBuJcAE9oq,Reading Discussion Group,reading-discussion-group,https://www.lesswrong.com/events/pa6GWazpBuJcAE9oq/reading-discussion-group,2020-09-06T18:49:15.044Z,8,2,2,False,False,,"<p>We will be discussing an article that you all suggest.
Suggest articles at <a href=""https://forms.gle/SEV5P8gH6H77DCNP8"">https://forms.gle/SEV5P8gH6H77DCNP8</a> by September 8, then vote by the 13th (voting link to be posted). Articles that were suggested last time will be included in the poll, so no need to suggest them again.</p>
",AspiringRationalist,nosignalnonoise,NoSignalNoNoise,
nJihRcJkjHBYR4XJt,Ice,ice,https://www.lesswrong.com/posts/nJihRcJkjHBYR4XJt/ice,2020-09-06T18:30:07.677Z,42,14,1,False,False,,"<figure><img src=""https://hivewired.files.wordpress.com/2020/09/dry_falls-1.jpg?w=1024"" srcset=""https://hivewired.files.wordpress.com/2020/09/dry_falls-1.jpg?w=1024 1024w, https://hivewired.files.wordpress.com/2020/09/dry_falls-1.jpg?w=2048 2048w, https://hivewired.files.wordpress.com/2020/09/dry_falls-1.jpg?w=150 150w, https://hivewired.files.wordpress.com/2020/09/dry_falls-1.jpg?w=300 300w, https://hivewired.files.wordpress.com/2020/09/dry_falls-1.jpg?w=768 768w"" /></figure>



<p>What you’re looking at is a geological formation called Dry Falls, in the Sun Lakes-Dry Falls state park in my home state of Washington. The Dry Falls are a series of escarpments and cliffs near Grand Coulee, deep in Eastern Washington’s channelled scablands region. These are four hundred foot high cliffs in the middle of the desert, how did they get here? What secrets does this terrain hold? What can the strange rock formations and alien landscapes of eastern Washington state tell us about the future of our planet?</p>



<p>During the end of the last ice age, a massive amount of glacial ice in continental Europe and North America melted away. During the period from 25,000 to 10,000 years ago, the Laurentide, Cordilleran, and Fennoscandian ice sheets completely melted, leading to a 120 meter rise in the global sea level. The rise in sea levels from this melting is estimated to have averaged in at roughly one meter per century while being augmented by two intense periods of melting between 15,000 and 13,000 years ago, and between 11,000 and 9,000 years ago.</p>



<p>While the current consensus among paleoclimatologists is that this melting was relatively gradual and steady, occurring at a linear rate over the course of 15,000 years, there is some evidence beginning to surface both in our current ice sheets and in the geologic records on the last one, that a gradual and linear melting rate is not what we should expect to see going forward. </p>



<p>In this post, I’ll look at recent melting trends in Antarctica and Greenland as well as at paleoclimate data from ice and seabed cores to propose a model of continental ice sheet collapses as rapid and potentially cataclysmic historical events which we should be aware of as potentially civilization destabilizing. Most of our current population, our largest cities, and most of our power and industry facilities, are all located in low lying areas susceptible to coastal flooding. If the water levels rise at a rate faster than can be mitigated by a slow withdrawal from the coastline over the course of many decades and centuries, it could cripple human civilization and bring an end to our current way of life. </p>



<p>The first piece of evidence to note here is that the geologic record of the last ice age is littered with superfloods and seemingly cataclysmic sea level rise events. Water topped over earthen berms and flooded into lowlying areas, Doggerland and Sundaland vanished beneath the waves and the Bering Strait cut Asia and North America apart. These events have left scars on the surface of the Earth which you can see from space, you just need to know what to look for. </p>



<figure><img src=""https://hivewired.files.wordpress.com/2020/09/2020-09-06-2.png?w=879"" srcset=""https://hivewired.files.wordpress.com/2020/09/2020-09-06-2.png 879w, https://hivewired.files.wordpress.com/2020/09/2020-09-06-2.png?w=150 150w, https://hivewired.files.wordpress.com/2020/09/2020-09-06-2.png?w=300 300w, https://hivewired.files.wordpress.com/2020/09/2020-09-06-2.png?w=768 768w"" /></figure>



<p>This is the North Fork of the Toutle River as it flows across the soft dried mud and ash of the Mount Saint Helens lahar zone, I provide this image just to given an example of stream braiding, the lahar zone gives a nice canvas on which you can really see how the water carves all these winding channels through the surface material. This happens in rivers around the world though, there are dozens of examples of this sort of river braiding I could show you. The important thing to note here though is the scale of this landform. The lahar zone is less than a kilometer across, and we can see roads and trees and houses at this level of zoom. </p>



<p>So now lets zoom out and look east across the Cascade range. </p>



<figure><img src=""https://hivewired.files.wordpress.com/2020/09/2020-09-06-3.png?w=1024"" srcset=""https://hivewired.files.wordpress.com/2020/09/2020-09-06-3.png?w=1024 1024w, https://hivewired.files.wordpress.com/2020/09/2020-09-06-3.png?w=150 150w, https://hivewired.files.wordpress.com/2020/09/2020-09-06-3.png?w=300 300w, https://hivewired.files.wordpress.com/2020/09/2020-09-06-3.png?w=768 768w, https://hivewired.files.wordpress.com/2020/09/2020-09-06-3.png 1081w"" /></figure>



<p>This is the channelled scablands from far above. At the height which satellites orbit, the mass scouring of hundreds of square kilometers can clearly be seen. braids tens of kilometers across and hundreds of kilometers long draw tracks across all of eastern Washington before spilling into the Columbia River Valley to flow onward toward the Pacific. This event, or events, geologists aren’t sure, is referred to as the Missoula Megafloods, and was the source of the Dry Falls pictured at the beginning of this post. At their peak flow, the Dry Falls were twice the height of Niagara Falls and five times the width. So much water poured into the Columbia River that it backfilled and flooded most of the Willamette Valley.</p>



<p>According to current consensus, these massive floods were caused when a proglacial lake formed in what is now Missoula, Montana. The leading theory is that a fifty mile long ice dam formed across the Clark Fork River which caused the waters of the receding Cordilleran Ice sheet to back up and pool around Missoula. This presents the first problem with the current consensus and is where a rather peculiar group of individuals become involved. </p>



<p>There are a group of slightly kooky geologists and historians who call themselves the Catastrophists. They hold that a moderately advanced civilization in North America was destroyed during the Younger Dryas period around 12,000 years ago and have found all sorts of interesting things to lend credence to their theory. </p>



<p>The Catastrophists looked at the story of the Missoula Megafloods and said, “That doesn’t work.” They pointed out that an ice dam the size of the one proposed cannot possibly have held back the amount of water under the head pressure that Glacial Lake Missoula was under, long enough for the lake to each its maximum historical depth of over 600 meters. Glacial Lake Missoula is estimated at having held 2,500 cubic kilometers of water, and the catastrophists say that there’s no way that could have happened with an ice dam triggered outburst flood, the ice would give before that much water could build up.</p>



<p>Instead, the Catastrophists propose that glacial lake missoula wasn’t a long term lake, but formed temporarily as a result of water flowing in from further north pooling and backfilling around Missoula as it interacted with the chokepoint in its flow along the Clark Fork River Valley. </p>



<p>The Catastrophists also have other evidence of rapid melting which they have found from seabed cores. Most of the seabed is composed mostly of decaying organic material, crushed up tiny organisms that rain down to the bottom in an ever present snow. However, there are notable strata lines within seabed cores, which contain mostly rocks, pebbles, sand, and other inorganic debris. These layers are called Heinrich Events, and it is believed that they are caused by large masses of icebergs breaking off, carrying rocks and sediment with them, and then dropping these bits of rock and sediment as they melt away. All of these things come together, according to the catastrophists, to seemingly support their theory of a cataclysmic event during the Younger Dryas period, 12,900 years ago. </p>



<p>So the Catastrophists look at all the data for speed of melting, heating from sunlight, atmospheric C02 levels, and conclude that the melting just happens too fast to be explained without an outside source. They claim there simply wasn’t enough energy available for the math to work out unless you added a bunch of extra energy from somewhere outside the climactic system. </p>



<p>The solution to this problem, they say, is that around 12,900 years ago, a comet or asteroid struck the top of the Laurentide Ice Sheet, triggering a massive pulse in melting which we observe in the form of megafloods and Meltwater Pulses and Heinrich Events. The evidence for this is shaky, but I sincerely hope they end up being correct. And they might actually be, late last year a 19 kilometer wide impact crater was discovered under the Hiawatha Crater in greenland. This impactor, if it occured at the right time period, might actually be the catastrophists smoking gun. </p>



<p>However, I am not particularly confident that they are correct. Because it’s under a glacier, we don’t yet know how old the crater at Hiawatha actually is. It could be significantly older than 12,000 years, and if it is, than we’re once again left with too much melting to fit our model and no discernible cause. The currently dominant theory is that a combination of increased insolation on the glaciers and high C02 levels at the time caused their final retreat and collapse. However, the effect seems to have exceeded the cause and the extremity of the events, especially the large pulses of meltwater, seem to imply some other mechanism was present. </p>



<p>Without invoking some outside event like a volcanic eruption or an extraterrestrial impact, the only explanation we’re left with is the ill-understood climate feedback mechanisms which we are currently engaged in setting off en-masse.</p>



<p>The impact theory is in some senses comforting. We have big telescopes, we can see into space now, in theory, if we knew an impact event was coming, we could prevent it. If it takes an impact to cause a catastrophic melting and sea level rise event, then we’re mostly safe from it happening. If the melting was caused by an impact, then it means our current climate models which estimate around a meter of sea level rise by the year 2100, are largely accurate. </p>



<p>But if these melting spikes were not caused by an impact, then it means something on earth which we currently do not understand triggered them. Something caused the ice sheets to suddenly and rapidly destabilize and release a large quantity of meltwater over a relatively brief period. If such an event were to occur today, the effects would be globally catastrophic. If an event caused a one-meter sea level rise over the course of a few years, it would render many of the world’s coastal cities uninhabitable. </p>



<p>Scientists have posited that the West Antarctic Ice sheet, which is sitting on bedrock below sea level, could potentially experience a catastrophic collapse event if sea water was able to access the roots of this glacier. Although computer models have been unable to construct the timeline of events in detail, the possibility remains that the entire ice sheet could collapse over a period as short as a few years, which, if the entire thing went, would lead to 6 to 9 meters of sea level rise, enough to submerge a large number of urban cores around the world and utterly remake coastlines. </p>



<p>The possibility of this catastrophic melting event is often left out of the climate change conversation, with the assumption being that melting will be a nuisance and force the eventual abandonment of low-lying areas or construction of new seawalls, but is not an existential threat to civilization by itself.</p>



<p>If the entire West Antarctic ice sheet was to collapse over a five year period, it would lead to a global crisis as populations were forced to relocate and cities were rendered unlivable. In many ways, the predictions that the ice sheets will last for centuries more and take a thousand years to melt away are overly optimistic and based on older and less accurate models of past climate events. A recent paper has provided evidence that melting may not be linear in nature but exponential, and if recent trends in accelerating melting are extrapolated out, we could see multimeter sea level rise within the next fifty years.</p>



<p>This would not by itself be an X-Risk, but would represent a major case of cranking up the pressure that humanity is put under, and make other X-Risks such as nuclear wars and pandemics more likely. It is my opinion that the possibility of catastrophic ice sheet collapse should be carefully considered and studied as a real possibility. It’s unlikely we could prevent such a collapse from occurring, but by anticipating such an event we may be able to save many lives and livelihoods. </p>
			<div></div>
			
			",Hivewired,hivewired,Slimepriestess,
ZdtFBCtixqay5aoWF,Design thoughts for building a better kind of social space with many webs of trust,design-thoughts-for-building-a-better-kind-of-social-space,https://www.lesswrong.com/posts/ZdtFBCtixqay5aoWF/design-thoughts-for-building-a-better-kind-of-social-space,2020-09-06T02:08:54.766Z,54,19,41,False,False,https://makopool.com/better_space_with_wots.html,"<h1>Principles for building a better kind of social network with webs of trust</h1>
<h2>Abstract</h2>
<p>Discussion of the unrealized potential of webs of trust as a decentralized institution for appointing moderators and curators.</p>
<p>Webs of Trust seem to be applicable to improving almost every social network process, including comment moderation, content sorting, determining the visibility of backlinks and edits in a globally editable wikis, but the article focuses primarily on tagging systems: The appointment of curators of categories of content.</p>
<h2>Why webs of trust work</h2>
<p>A web of trust is a network of users endorsing other users.</p>
<p>If you tell a web of trust who you trust, you can then travel along and find out who they trust, and so on as far as you wish to go, and that will give you a large set of people who you can trust to some extent, by transitivity.</p>
<p>Webs of trust can scale up at an exponential rate, as each new user can immediately add more users (better, they can start issuing endorsements in their own network segment even before they're added). Wonderfully, despite that, webs of trust can also be pruned and weeded fairly easily: If a few bad endorsements do get made, and the newly empowered bad users start adding more bad users, we will be able to trace the source of badness back through the endorsement relations to the root causes, and pruning them away will also prune away everyone that came through them, directly or indirectly. Crucially, the pruning and weeding does not need to be carried out by a central authority. Every user is the center of their own web, they make their own moderation decisions, they can bring in anyone they like.</p>
<p>Webs of trust essentially are the delegation or sharing of moderation work.</p>
<p>Webs of trust are useful whenever we wish to track a quality that people can have, which comes with the ability to recognize the presence of that quality in others. Most personal qualities are self-recognizing, in this way, to some extent. A person who has it, faced with another person, can usually figure out whether they have it too.</p>
<p>Examples of such qualities include <code>good taste</code>, <code>responsibility</code>, or <code>not a spambot</code>.</p>
<p>Some non-examples would be <code>is a spambot</code> (spambots are mainly about spamming and are not very interested in identifying each other), or <code>is a fool</code>. A web of trust would not help with keeping track of these qualities, but, again, most qualities people talk about aren't like these. If you do find yourself in need of a web that tracks a non-selfrecognizing quality, consider just making a web that tracks its negation. <code>not a spambot</code>, or <code>no fool</code> would work pretty well.</p>
<p>You might notice that some of the given examples of selfrecognizing qualities have rather subjective meanings. Not everyone will agree about what <code>good taste</code> or <code>responsibility</code> means. It's often useful for a word to be this way. The neat thing about webs of trust is that they will often still work pretty well in those cases! If people disagree about the nuances of a quality, they will often end up organizing into separate webs of trust that agree within themselves. <strong>Webs of trust are compatible with subjectivity</strong>.</p>
<p>That makes webs of trust suitable for moderating a truly global platform. At no point does a central authority have to decide for everyone else what any of the webs are about. If two groups disagree about what sorts of things should be posted in a fundamental tag like <code>respectful discourse</code> or <code>safe content</code>, they don't have to interact! The web of trust is so powerful as a moderation technology that they can wholly split their webs and keep using the same tags in completely different ways without stepping on each other.</p>
<h3>Some noteworthy systems that use webs of trust</h3>
<p>The prototypical example of webs of trust seems to have been the process of establishing of real identity in PGP signature networks.</p>
<p>A friend, <a href=""https://www.cblgh.org"">Alexander Cobleigh</a>, is implementing a subjective moderation system for the P2P chat protocol Cabal, which you can read some things about <a href=""https://cblgh.org/articles/trustnet.html"">here</a></p>
<p>Webs of Trust are being used to measure social adjacency in various distributed systems, for instance</p>
<ul>
<li>
<p><a href=""https://blog.radicalxchange.org/blog/posts/2019-10-24-uh78r5/"">Intersectional Social Data</a>, a system for sharing personal information with only the right groups of people, uses <a href=""https://academic.oup.com/qje/article-abstract/124/3/1307/1905159"">Staked Collateral</a>, where trust links are represented with commitments to lend money.</p>
<ul>
<li>Additionally, I believe I've heard other RadicalXChange rumblings about <em>absence</em> of trust being potentially useful for evidence that a set of parties aren't very likely to be able to collude, which may be an important component in mechanisms for preventing vote trading in quadratic voting systems or guaranteeing competition in global auditing process.</li>
</ul>
</li>
<li>
<p><a href=""https://duniter.org/en/introduction/"">Duniter</a> requires that ""Every member is strongly identified through a Web of Trust mechanism to prevent any one individual from receiving multiple Universal Dividends by using more than one identity""</p>
</li>
</ul>
<h2>Core principle: Users should not be asked to reduce themselves to a single brand</h2>
<p>A web of trust can be used to exclude spammers, ban-evaders, annoying people, rude people, bad people, or people with bad taste. However, if one web of trust were used to cover all of those meanings and purposes at once, I imagine the results will be pretty inhumane; people would commit chilling, cowardly omissions of self to avoid any risk of being perceived as rude lest the web put them in the same icy hell as spammers.</p>
<p>Twitter kind of is like that, and I think it exhibits a lot of the problems we should expect that to have. On twitter, you have one face, you get one tube, the people who follow you have to be alright with everything you put in that tube. If you ever want to to post a type of content that some of your followers explicitly want to never see, you have nowhere to put it. Brand is totalizing. Everyone has to compress themselves down to one legible brand before the network can thrive.</p>
<p>Users should be encouraged to have more than one side to them. The situation could be helped if twitter were more encouraging of the use of alt accounts. In a way, the system I'm about to propose is a way of streamlining that sort of mode of use.</p>
<p>As it currently stands, we can conceptualize Twitter as a kind of thick slow web of trust for the overly broad content category of <code>good tweet</code>. This web's quality is not truly <em>self-recognizing</em>; the endorsements do not represent a transitive relation, they do not conduct very far, if you travel just a few steps along through your follows of follows you will find mostly people you wouldn't want to follow. Only shitposts and the most general interest news propagates well, everything else propagates depressingly incompletely, there is no strong agreement in most networks about what is good to post, and where there is no strong agreement there is no truth about what is good to post. Nothing is good to post. We must simply log off.</p>
<p>What if, instead, we had many webs of trust that discuss and define the many different dimensions of interest that people can have, which users could choose to participate in or not. Most of these webs may have specific enough meanings that content could be automatically propagated fairly far through them with confidence that everyone in them would be interested in most of it. Some of these webs might be nebulous or subjective in meaning, which would have lower recommended automatic propagation constants, and those would work too.</p>
<p>It is important that users are not required to present as a category of information, and it is important that categories of information can grow larger than any one curator. A person should not be a brand, and a brand should not be a person.</p>
<h2>The Tasteweb Concept</h2>
<p>Tasteweb would consist mostly of these four types of thing:</p>
<ul>
<li>
<p><strong>article</strong>: Content, posts, replies, records of actions and declarations.</p>
</li>
<li>
<p><strong>tag</strong>: A property articles can be declared by presences to have (or, equivalently, can also be thought of as a set that articles can be put into.)</p>
</li>
<li>
<p><strong>presence</strong>: A presence of a user in the web. A user can (and usually should) have many presences in different webs.</p>
</li>
<li>
<p><strong>web</strong>: A network of endorsements over presences, generally about a type of quality that presences can have that tells us which sorts of tags they are assured to use agreeably.</p>
</li>
</ul>
<p><strong>presences</strong> apply <strong>tags</strong> to <strong>articles</strong> to organize them, filter them, and to alert interested parties of them. The <strong>webs</strong> of trust in which <strong>presences</strong> are organized speed and shape the propagation of updates about what articles have been tagged recently, and guide queries over the presences most worth visiting.</p>
<p>That's pretty much it. The rest of the document will give you a clearer picture of how many things those primitives will enable.</p>
<p>Some tags will have simple, objective meanings. <code>music</code>, for instance. A tag like this would be affiliated with the <code>uses basic tags correctly</code> web (meaning that basically everyone would be able to use it), it would be useful for confirming that an article is music, but it might not be an especially useful tag to most people for finding or promoting attentionworthy examples of music. Here's where things get interesting:</p>
<p>Consider a tag called <code>good music</code>. Its meaning would, of course, need to be subjective, and webs of trust can support that! You would find a <code>good music taste</code> web, Find someone you align with and trust them along the <code>good music taste</code> dimension, and you'll get their recommendations, and if they haven't recommended anything today you'll see the recommendations of the people they trust, and so on, and it will immediately function as a music recommendation system that you and the musicfriends have complete control over. You would wake up every morning and have your client essentially run a query like ""time:today tag:<code>good music</code> from:my(<code>good music taste</code>) min_similarity:0.04"" and it would all be great stuff, or if it's not, you can rearrange your endorsements and move towards a web where it is.</p>
<p>The crucial advantage this has over other recommender systems that use user similarity, is that it is fully transparent, accountable, and controllable. You can see how it works, you know where the recommendations are coming from, and you can fix it yourself when there is too much bad or not enough good being recommended. It is not a black box algorithm. You can trust it for a lot more, because it consists of people, who you can see.</p>
<h3>A taxonomy of very good webs of trust that should arise under a healthy culture of usage</h3>
<ul>
<li>
<p>There are a few basic webs that every human should be in. In most cases, they will be bundled together and reciprocated through a <strong>friend endorsement</strong>.</p>
<p>Very silly abusive actions can sometimes result in people getting cut out of some of these. Since they're so important, we must try to make sure the punishment is suitable and proportionate to the crime and not a complete disenfranchisement from the entire system, so that's why we have to separate them into different specific categories when we can.</p>
<ul>
<li>
<p><code>Isn't a spambot</code></p>
</li>
<li>
<p><code>Isn't a propagandist from a troll farm</code>. Will, inevitably, be occasionally partially infiltrated by propagandists from troll farms, but it would still be a good thing to have for fighting them.</p>
</li>
<li>
<p><code>uses basic tags correctly</code>. Lets people use any tag with truly simple, objective criteria, stuff like <code>is a picture of a dog</code>, <code>is a book review</code>.</p>
</li>
<li>
<p><code>suggests respectfully</code> allows them to issue requests that an article be lifted up by widely endorsed tag curators who are open to suggestions.</p>
</li>
</ul>
</li>
<li>
<p>Meta topics</p>
<ul>
<li>
<p><code>curator curator curators</code>: metametacurators</p>
</li>
<li>
<p><code>good webs</code>: An intentionally vague and subjective tag for discovering a wide variety of places</p>
</li>
<li>
<p>Fancifully; it would be neat to have a web that maintains a light read that explains how people are supposed to use the system, <code>proposals for alterations to the text of culture of common usage</code>.</p>
</li>
</ul>
</li>
<li>
<p><code>scholarly consensus</code>: Wikipedian type people (or the type of people wikipedians ought to be) who are honest and informed about what reasonable people should be able to agree on. Trust only good scholars and you will be a good scholar too.</p>
<p>Wikipedia generally works because no portion of the accountholding userbase brigades articles outside of their expertise. This will be a useful mechanism for maintaining that sort of condition.</p>
<ul>
<li>
<p>Most users aren't going to want to post like this. The tags that <code>scholarly consensus</code> curate are no use for speculation or for anything subjective. Upholding the norms of <code>scholarly consensus</code> requires you to post very conservatively. That is usually boring, a lot of the time it's not even intellectually productive.</p>
</li>
<li>
<p>Many users won't have the types of research skills or the network epistemic sense to post like this. Maybe they will aspire to learn.</p>
</li>
<li>
<p>Regardless, this web will perform many important functions that require soberly reporting widely accepted truths.</p>
</li>
<li>
<p>There would be controversies and web splits sometimes, of course, but unlike with most webs they should always be considered to be potentially avoidable tragedies. Wherever <code>scholarly consensus</code> gets into commenting about things that might turn out to be deeply importantly wrong, well that never had any place in <code>scholarly consensus</code> in the first place.</p>
</li>
</ul>
</li>
<li>
<p><code>good talk</code>: Replies well, converses well. Use it to decide who you want to see first in replies and who you want to be able to receive reply notifications from.</p>
<ul>
<li>The first thing is sort of a stricter requirement than the second thing. There should probably also be <code>replies okay</code> a web that means ""I want to be told when these people are saying something to me, even if I don't necessarily want to see their replies first when they're replying to something else"". Many people, though not all, are totally up to having the level of responsibility to be allowed to begin dialogue with a random stranger online. It would be a much broader web.</li>
</ul>
</li>
<li>
<p>a general class: networks of -<code>taste</code>. When a post or comment is tagged with, for instance, <code>good music</code>, we will only see that post or comment in our queries if it was tagged by someone who is near to us in our <code>good music taste</code> network. For many types of tag, having curators turns out to be very important. Tagging/categorization systems currently mainly only work reliably for very plain objective qualities that everyone can pretty much agree on the meaning of and which few people have an incentive to lie about, and such categories tend to be boring. For almost any interesting category, things like <code>important news</code>, <code>honest summary</code>, <code>interesting game</code>, <code>scientific breakthrough</code>, there can be no widely agreeable shared objective understanding of which articles belong in it. In practice, such categories, when opened to the most general possible audience, will tend to become pretty mediocre. Many will leave and the ones who remain will do so grudgingly. Given a subjective inclusion though, networks of -<code>taste</code> could work wonderfully.</p>
<ul>
<li>
<p>Real world examples of tag systems that suffer from being uncurated</p>
<ul>
<li>
<p>reddit's subreddits, which provide no way of holding voters accountable for upvoting reposts, upvoting inane bullshit, or downvoting interesting or challenging things, so they tend to end up either needing extensive moderation, to an extent that almost obviates the usefulness of reddit's vote sorting.</p>
</li>
<li>
<p>twitter's hashtags, which the people who get the most out of twitter never seem to use in earnest. I have seen more hashtags being brigaded than I have seen hashtags being used fruitfully. If I have seen any hashtags used fruitfully, I don't recall, they must not have been for me.</p>
</li>
</ul>
</li>
<li>
<p>I can't wait to see if <code>taste in humor</code> would work. Shitposting twitter tends to be pretty powerful so it'd probably work really well.</p>
</li>
</ul>
</li>
<li>
<p>I think it might turn out to be impossible to engage in nation-sized online political discourses without some kind of <code>actually listens</code> web.</p>
<ul>
<li>Since the entire point of it would be to facilitate civil political dialogue between people who do not already agree about everything I think this probably wouldn't result in totalizing echo chambers, especially if you factor in users changing their behaviour in response to knowing that this specific moderation pressure to be respectful and accessible are hanging over them.</li>
</ul>
</li>
</ul>
<h3>UX</h3>
<p>I wish I could present a clear and complete design, but that will take some consideration. For now I'll just throw some thoughts out</p>
<ul>
<li>
<p>A lot of it would just look like a feed</p>
</li>
<li>
<p>Some quick ways of responding to any articles</p>
<ul>
<li>
<p><code>bookmark</code>, for when something is important but large and should be considered again later. All platforms should have this!</p>
</li>
<li>
<p><code>relevant</code>, a signal that means ""I do agree that this belongs in these tags in this web"". Should probably cause your presence to confirm the tag? But I dunno, I kind of want there to be a lower stakes ""I appreciate this"" signal that just goes to the article submitter.</p>
</li>
<li>
<p><code>irrelevant</code>, a signal that means ""This should not be in these tags or this web"". May be used to prevent an article from propagating upstream to your endorsers? but also just makes a note locally for the client to help you to identify bad curators. If enough <code>bad</code> signals turn out to stem from a specific curator that you have trusted (with an inadequate propotion of <code>relevant</code> signals to balance them out), the client will ask if you'd like to unendorse that curator.</p>
</li>
</ul>
</li>
<li>
<p>The suggestion queue.</p>
<ul>
<li>
<p>If a user is in the basic <code>suggests respectfully</code> web, they can put things before their favored curators, who may then tag the article themselves. Curators may, if they choose, view those suggestions in their suggestion queue. And they may well choose to, because it helps them to find new things to post.</p>
</li>
<li>
<p>There should probably be some automated system that notices when an outside presence has submitted multiple successful articles, and recommends bringing them inside. This would have a lot of subtleties though. Hm. I guess that would be another governance process that curators would have to opt into, similar to the suggestion queue.</p>
</li>
</ul>
</li>
<li>
<p>A way of viewing the feed of each presence the user owns (the articles tagged with the relevant tags by the presences that presence trusts).</p>
</li>
<li>
<p>Maybe allow seeing the view of presences they don't own, but I'm unsure. They could get an equivalent experience by creating a presence of their own that just trusts that other presence. This would have two advantages.</p>
<ul>
<li>
<p>Encourage them to get engaged with building their own presence in the web, instead of relying on the submission process or not contributing articles at all</p>
</li>
<li>
<p>Lower the barrier to making a view that consists of multiple curators by trusting additional curators. If all a person knows how to do is select one curator, they wont get as much benefit from the software. We should encourage them to use it in a different way.</p>
</li>
</ul>
</li>
<li>
<p>To consume information in a healthy way, the user needs to be able to control what they'll be seeing on any given session. Sometimes we don't want to face the notifications of one of our accounts. Sometimes we want to make sure we will be able to engage in a particular news-processing task or research project without being distracted. For such cases, users will be encouraged to define modes, or use profiles, which they can switch between.</p>
<ul>
<li>Modes would mostly consist of a combination of the views of some of the user's presences, I think? Maybe it should support hand-written queries, in some cases.</li>
</ul>
</li>
<li>
<p>Please can we not make it a web browser app. All web apps consume infinite memory and become slow and the layout language of the web is actually pretty bad. The answer is no, though. There aren't any great alternatives to the web for multiplatform UI development, afaik, and the site will need to be able to render a lot of things for the web anyway, for the sake of making it widely accessible. I really wish we didn't have to. We'll just have to work really hard at doing things in the simplest, fastest, lightest way.</p>
</li>
</ul>
<h2>Getting this made</h2>
<p>This is not going to happen at all if you leave it to me alone.</p>
<p>If you come to believe that a system like this would be good, <a href=""https://aboutmako.makopool.com"">reach out</a>, and we'll get organized, and then maybe it will happen.</p>
<p>A good thing must fight in any way it can to grow faster than the bad things that are growing now.</p>
<hr>
<p><a href=""https://roamresearch.com/#/app/Test-database/page/0nfYFRK4u"">Roam document: the engineering problems of tasteweb</a>, principles, questions and solutions.</p>
",MakoYass,mako-yass,mako yass,
oYbztGevs9iT6LLMW,Petrov Day celebration,petrov-day-celebration-1,https://www.lesswrong.com/events/oYbztGevs9iT6LLMW/petrov-day-celebration-1,2020-09-06T01:09:34.718Z,4,2,1,False,False,,"<p>Come join us as we celebrate the world <a href=""https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day"">not having been destroyed</a>, and raise a glass of Vodka to Stanislav Petrov.</p><p>We have chosen an outdoor venue for pandemic concerns, and will have a potluck for anyone willing to break bread during these troubled times. Masks are encouraged if you are not eating or drinking, and social distancing is recommended for anyone not already spending time together (some attendees have created quarantine circles, so don&apos;t be surprised if you see people ignoring typical precautions).</p><p>Please do register at the following Google Form so we can get an estimated headcount and provide enough food:</p><p><a href=""https://bit.ly/2R1WgyP"">https://bit.ly/2R1WgyP</a></p>",Senarin,senarin,Bae's Theorem,
FY6bZAxHTcMi5nhyb,Stop pressing the Try Harder button,stop-pressing-the-try-harder-button,https://www.lesswrong.com/posts/FY6bZAxHTcMi5nhyb/stop-pressing-the-try-harder-button,2020-09-05T09:10:05.964Z,45,25,5,False,False,https://www.neelnanda.io/blog/mini-blog-post-6-stop-pressing-the-try-harder-button,"<p> <em>(This is a post from a daily blogging experiment I did at</em> <em><a href=""http://neelnanda.io/"">neelnanda.io</a>, which I thought might also fit the tastes of LessWrong. This is very much in the spirit of <a href=""https://www.lesswrong.com/posts/WLJwTJ7uGPA5Qphbp/trying-to-try"">Trying to Try</a>)</em> </p><p>I recently had a productivity coaching session, and at the end we agreed on a few actions points that I&#x2019;d do by the next session. But, come the next session, these had completely slipped my mind. These suggestions were good ideas, and I had no issue with implementing them, the problem was just that they completely slipped my mind! (We then spent the second session debugging my ability to actually follow action points, and this was pretty successful!) </p><p>I think the error I made there is a <em>really </em>common one when planning, and one I observe often in myself and others. Often I&#x2019;ll hear a cool book recommendation, offer to meet up with someone some time, hear about a new productivity technique, notice an example sheet deadline looming. But I consistently fail to action upon this. So this post is about what <em>exactly </em>went wrong, and the main solution I&#x2019;ve found to this problem!</p><p>Planning, as I define it, is about <strong>ensuring that the future goes the way I currently want it to</strong>. And the error I made was that, implicitly, I was <em>trying </em>to make the future go the way I currently wanted it to. That by committing to do things, and wanting to them, and just applying effort, things would happen. And the end result of this was that I totally forgot about it. Or sometimes, that I vaguely remembered the commitment or idea, and felt some guilt about it, but it never felt urgent or my highest priority. And every time I thought about the task, I resolved to Try Harder, and felt a stronger sense of motivation, but this never translated into action. I call this error <strong>Pressing the Try Harder button</strong>, and it&#x2019;s characterised by feelings of guilt, obligation, motivation and optimism.</p><p>This is a classic case of failing to Be Deliberate. It feels <em>good </em>to try hard at something, it feels important and virtuous, and it&#x2019;s easy to think that trying hard is what matters. But ultimately, trying hard is just a means to an end - my goal is to ensure that the task happens. If I can get it done in half the effort, or get somebody else to do it, that&#x2019;s awesome! Because my true goal is the <em>result</em>. And pressing the Try Harder button is <em>not </em>an effective way of achieving the goal - you can tell, because it so often fails!</p><p>A good litmus test for whether you&#x2019;re pressing the Try Harder button: Imagine it&#x2019;s 2 weeks from now, and you never got round to doing the task. Are you surprised that this happened? Often my intuitions are well-calibrated when I phrase the question like this - on some level I <em>know </em>that I procrastinate on things and forget them all the time. </p><p>But just noticing yourself pushing the Try Harder button isn&#x2019;t enough - you need to do something stronger to change this. You need to <em>find strategies that actually work</em>. This is pretty personal, and much easier said than done! But it can be done. Look for common trends, strategies that have worked for you in the past, and things that you can repurpose. </p><p>Strategies that work for me:</p><ul><li>Scaffold systems - meta-systems that I check regularly</li><ul><li>Calendars</li><li>Trello (my to-do list) - especially future reminders that result in an email</li><li>Getting friends to check in with me</li></ul><li>Do it <em>now</em>, not later. Set a 5 minute timer, and see if you can finish the task. Or at least make a start!</li><ul><li>You can get a surprising amount done in 5 minutes! (Say, writing a third of a blog post)</li><li>Often the bottleneck is that getting <em>started </em>takes a bit of energy. Doing something for 5 minutes can take much less energy, and I find that timers help me focus a lot</li></ul><li>Make things <em>concrete </em>- often tasks feel overwhelming and fuzzy, so you put them off. Can you break it down into a concrete next action? Something that can be done in under 5 minutes?</li><li>Schedule time for it - often the bottleneck is that it doesn&#x2019;t feel <em>urgent </em>- I care about the task getting done, but I always have something seemingly-higher-priority to do</li><ul><li>This is terrible, because in the long-run I <em>always </em>have something that seems short-term higher priority, and I never make time for my long-term goals</li><li>So if I make time in my calendar for it, and make it feel <em>important </em>that I stick to these, that&#x2019;s valuable</li><li>I find focusmate.com valuable for carving out an hour for a specific task</li></ul><li>Add it to a queue</li><ul><li>Ie a to-do list</li><li>I find Trello great for this - on Desktop I can add a new card from anywhere with CTRL+ALT+SPACE, it makes it really low friction</li><li><strong>Important: </strong>It&#x2019;s not enough to just add it to a list - the other half of a good to-do list system is having a regular time to <em>process </em>the list!</li><ul><li>Having an eg weekly routine for this is important - a routine doesn&#x2019;t involve decisions, it can just happen automatically. While if I just say &#x201C;I&#x2019;ll make time for it&#x201D;, that&#x2019;s pushing the Try Harder button!</li></ul></ul><li>Accountability</li><ul><li>Message a friend saying that I commit to this task</li><li>Extreme: Give a friend some money, and tell them to only give it back when the task is done</li></ul></ul><p>These are just the strategies that work for me - I&#x2019;d love to hear what works for others, and expect it vary a lot between people. The message I want you to take from this post is just to notice when you next push the Try Harder button. And ask yourself: &#x201C;am I just being virtuous and trying? Or am I trying to <em>change what my future self actually does</em>?&#x201D;</p>",neel-nanda-1,neel-nanda-1,Neel Nanda,
F7LnYfSms2ydpqLF8,Li and Vitanyi's bad scholarship,li-and-vitanyi-s-bad-scholarship,https://www.lesswrong.com/posts/F7LnYfSms2ydpqLF8/li-and-vitanyi-s-bad-scholarship,2020-09-05T08:07:02.851Z,-1,10,11,False,False,,"<p>In <a href=""https://www.amazon.co.uk/Introduction-Kolmogorov-Complexity-Applications-Computer/dp/3030112977"">An Introduction to Kolmogorov Complexity and Its Applications</a> 4th edition Li and Vitanyi claim that Solomonoff Induction solves the problem of induction. In Section 5.1.4 they write:<br></p><blockquote>The philosopher D. Hume (1711&#x2013;1776) argued that true induction is impossible because we can reach conclusions only by using known data and methods. Therefore, the conclusion is logically already contained in the start configuration. Consequently, the only form of induction possible is deduction. Philosophers have tried to find a way out of this deterministic conundrum by appealing to probabilistic reasoning such as using Bayes&#x2019;s rule. One problem with this is where the prior probability one uses has to come from. Unsatisfactory solutions have been proposed by philosophers such as R. Carnap (1891&#x2013;1970) and K.R. Popper.</blockquote><p>What Hume <a href=""http://web.lemoyne.edu/courseinformation/Magee/PHL201/Hume%20-%20Enquiry%20Concerning%20Human%20Understanding.pdf"">actually wrote</a> about induction was (Section IV, Part II, pp. 16-17):<br></p><blockquote>Should it be said that, from a number of uniform experiments, we infer a connexion between the sensible qualities and the secret powers; this, I must confess, seems the same difficulty, couched in different terms. The question still recurs, on what process of argument this inference is founded? Where is the medium, the interposing ideas, which join propositions so very wide of each other? It is confessed that the colour, consistence, and other sensible qualities of bread appear not, of themselves, to have any connexion with the secret powers of nourishment and support. For otherwise we could infer these secret powers from the first appearance of these sensible qualities, without the aid of experience; contrary to the sentiment of all philosophers, and contrary to plain matter of fact. Here, then, is our natural state of ignorance with regard to the powers and influence of all objects. How is this remedied by experience? It only shows us a number of uniform effects, resulting from certain objects, and teaches us that those particular objects, at that particular time, were endowed with such powers and forces. When a new object, endowed with similar sensible qualities, is produced, we expect similar powers and forces, and look for a like effect. From a body of like colour and consistence with bread we expect like nourishment and support. But this surely is a step or progress of the mind, which wants to be explained. When a man says, I have found, in all past instances, such sensible qualities conjoined with such secret powers: And when he says, Similar sensible qualities will always be conjoined with similar secret powers, he is not guilty of a tautology, nor are these propositions in any respect the same. You say that the one proposition is an inference from the other. But you must confess that the inference is not intuitive; neither is it demonstrative: Of what nature is it, then? To say it is experimental, is begging the question. For all inferences from experience suppose, as their foundation, that the future will resemble the past, and that similar powers will be conjoined with similar sensible qualities. If there be any suspicion that the course of nature may change, and that the past may be no rule for the future, all experience becomes useless, and can give rise to no inference or conclusion. It is impossible, therefore, that any arguments from experience can prove this resemblance of the past to the future; since all these arguments are founded on the supposition of that resemblance. Let the course of things be allowed hitherto ever so regular; that alone, without some new argument or inference, proves not that, for the future, it will continue so. In vain do you pretend to have learned the nature of bodies from your past experience. Their secret nature, and consequently all their effects and influence, may change, without any change in their sensible qualities. This happens sometimes, and with regard to some objects: Why may it not happen always, and with regard to all objects? What logic, what process or argument secures you against this supposition? My practice, you say, refutes my doubts. But you mistake the purport of my question. As an agent, I am quite satisfied in the point; but as a philosopher, who has some share of curiosity, I will not say scepticism, I want to learn the foundation of this inference. No reading, no enquiry has yet been able to remove my difficulty, or give me satisfaction in a matter of such importance. Can I do better than propose the difficulty to the public, even though, perhaps, I have small hopes of obtaining a solution? We shall at least, by this means, be sensible of our ignorance, if we do not augment our knowledge.<br></blockquote><p>What Hume wrote isn&#x2019;t that we can only use known data and methods. Rather, he said that no argument can prove that the future will resemble the past. So drawing conclusions about what will happen in the future from past data is illogical. He didn&#x2019;t say that the only possible form of induction is deduction.</p><p>In addition, the future always resembles the past in some respects and not others, so saying the future resembles the past is irrelevant to creating and assessing ideas.<br></p><p>Popper wasn&apos;t trying to solve the problem of how to make Bayesian induction work. He claimed that induction was impossible, not that he had a way of making it work by finding the right prior (<a href=""https://www.amazon.co.uk/Realism-Aim-Science-Postscript-Scientific/dp/0415084008"">Realism and the Aim of Science</a>, Chapter I, Section 3, I):<br></p><blockquote>It seems that almost everybody believes in induction; believes, that is, that we learn by the repetition of observations. Even Hume, in spite of his great discovery that a natural law can neither be established nor made &#x2018;probable&#x2019; by induction, continued to believe firmly that animals and men do learn through repetition: through repeated observations as well as through the formation of habits, or the strengthening of habits, by repetition. And he upheld the theory that induction, though rationally indefensible and resulting in nothing better than unreasoned belief, was nevertheless reliable in the main&#x2014;more reliable and useful at any rate than reason and the processes of reasoning; and that &#x2018;experience&#x2019; was thus the unreasoned result of a (more or less passive) accumulation of observations.<br><br>As against all this, I happen to believe that in fact we never draw inductive inferences, or make use of what are now called &#x2018;inductive procedures&#x2019;. Rather, we always discover regularities by the essentially different method of trial and error, of conjecture and refutation, or of learning from our mistakes; a method which makes the discovery of regularities much more interesting than Hume thought. </blockquote><p>Li and Vitanyi want us to think they can solve the problem of induction, but they can&#x2019;t even summarise the arguments against their position accurately.</p>",alanf,alanf,alanf,
HHnAJi3ZucuLNSg2Z,MOOCS That teach this stuff?,moocs-that-teach-this-stuff,https://www.lesswrong.com/posts/HHnAJi3ZucuLNSg2Z/moocs-that-teach-this-stuff,2020-09-05T05:36:38.289Z,3,4,0,False,True,,"<p>Related Q, </p><p>Are there MOOCS that teach this that anyone would recommend me? </p>",Periergo,periergo,Periergo,
jkvAYpzjk8DF5tszD,"Conflict, the Rules of Engagement, and Professionalism",conflict-the-rules-of-engagement-and-professionalism,https://www.lesswrong.com/posts/jkvAYpzjk8DF5tszD/conflict-the-rules-of-engagement-and-professionalism,2020-09-05T05:04:16.081Z,47,16,3,False,False,,"<p><i>(Talk given at </i><a href=""https://www.lesswrong.com/posts/g9QdXySpydd6p8tcN/sunday-august-16-12pm-pdt-talks-by-ozzie-gooen-habryka-ben""><i>an event on Sunday 16th of August</i></a><i>. habryka is responsible for the talk, Justis Mills edited the transcript.&nbsp;</i></p><p><i>If you're a curated author and interested in giving a 5-min talk, which will then be transcribed and edited, sign up </i><a href=""https://forms.gle/iwFatbhys9muPmQA7""><i>here</i></a><i>.)&nbsp;</i></p><p><i>(This was all very off-the-cuff about stuff that I am very confused about, so take all of this with a grain of salt. There are at least a few things in here I disagree with myself, that I wouldn't have put into written form directly.)</i></p><p><strong>habryka:</strong> I’m going to talk about three frames I have involving relationships and sociology. I’ll present the frames, some short justifications for why I believe them, and discuss how they connect to each other.</p><p>1. I think most relationships go better if you lean into conflict.</p><p>2. Most conflicts are hierarchically embedded within different rules, and maintaining the integrity of those rules is quite important.</p><p>3. Professionalism is really interesting. I like thinking about it, and I've gotten a bunch of value from thinking about it, because I didn't realize how much of my life has been shaped by professionalism.</p><h2>Leaning into conflict</h2><p>One of the things that has been pretty useful for me in life, is a general heuristic of realizing that conflict in relationships is usually net positive. (It depends a bit on the exact type of conflict, but works as a very rough heuristic.) I find it pretty valuable too, if I'm in a relationship, whether it's a working relationship, a romantic relationship, or a friendship, to pay a good amount of attention to where conflicts<i> could</i> happen in that relationship. And generally, I choose to steer towards those conflicts, to talk about them and seize them as substantial opportunities.</p><p>I think there are two reasons for this.&nbsp;</p><p>First, if startups should <a href=""https://whatis.techtarget.com/definition/fail-fast""><u>fail fast</u></a>, so should relationships. The number of people you could have relationships with is much greater than the number of people that you will have relationships with. So there is a selection problem here, and in order to get as much data as you want, I think going through relationships quickly and figuring out whether they will break or not is quite valuable.</p><p>Second, I've found that having past successful conflicts in a relationship is a very strong predictor for that relationship going well more generally, and for my ability to commit to the relationship and get things done within it.&nbsp; In fact, I find it a better predictor of my capacity to coordinate with that person than the length of the relationship, the degree to which we even enjoy spending time with each other, or other obvious indicators. I have some models of why successful conflict is such a good predictor, which brings me to my second frame.</p><h2>Honoring the rules of engagement</h2><p>I have this frame of thinking about rules of conflict and hierarchically embedded conflict in a lot of different situations. The basic situation that happens relatively frequently is that I have been interfacing publicly with some organization or person on the internet, and they take some small action that hurts me in some way. They might write an angry comment, for instance, or they might try to insult me.&nbsp;</p><p>A thing that I find useful to think about with these conflicts is: do we have any rules of engagement that we could rely on? That me and whoever I'm in conflict with, that we could together coordinate on, in order to prevent that conflict from spreading over into other parts of our life? With online commenting, for example, there's a generally widely accepted rule that we will not let our conflict leak into other platforms. If we're in a pseudonymous context, you won’t seek me out in a non-pseudonymous context and try to continue the conflict there.</p><p>Similarly, let's say LessWrong is in conflict with some other organization. There is a general expectation that if we have some kind of debate on the internet and there's a bunch of conflict happening there, we won't take that conflict to a broader venue like Twitter and then spark an angry mob to attack the other organization. The reason for that is pretty straightforward: the collateral damage of high levels of conflict tends to be quite large. I've often, in situations of conflict, got a lot of value out of asking myself, ""What are the lowest collateral damage rules that I could embed and act on here in order to make this conflict go well?""</p><p>This leads me to frame three.&nbsp;</p><h2>Professionalism</h2><p>I argue that this frame is the culmination of everything I’ve said thus far.&nbsp;</p><p>I’d like to talk about professionalism as a culture. Professionalism requires navigating conflict in lots of situations successfully, and it often encourages having important conflicts early on. Furthermore, it’s a really important component of people's lives.</p><p>I've found it useful to think about professionalism, or “presenting consistent APIs to other people”, as a safeguard against serious conflict. These APIs enable very narrow interfaces, which also enables very narrow potential for collateral damage. If I try to get a plumber, it's pretty easy. I don't have to think exactly about whether the plumber will like me or not, or other broader concerns about them as a person. Instead, I can pick a plumber based only on their fulfilling that specific role.</p><p>And also, another key component that I find really interesting to think about is the degree to which professional culture is optimized for replacing any individual within a given system. So that, for example, if you do professional writing, I would say that one key component is the ability to switch out writers in the middle of the writing process, which is false for 99% of other situations involving writing.&nbsp;</p><p>More broadly, I find professionalism is a modern religion whose details and culture it can be quite valuable to analyze.</p><p>&nbsp;</p><h2>Questions</h2><p><strong>jacobjacob:</strong> Regarding the first point: Nassim Taleb, who sure likes conflict, has a metaphor of <a href=""https://quotecatalog.com/quote/nassim-nicholas-taleb-variations-also-1Mo6nVp""><u>forest fires</u></a>. The basic idea is that if you prevent the small fires that come up naturally, it leads to an accumulation of excess flammable material, such that the next big fire is exceptionally bad. Does this resonate with your model?&nbsp;</p><p><strong>habryka: </strong>Sorry, my first reaction was just that Taleb is my key example of failing at point two, and escalating way too much. There's this thing where you disagree with Taleb on some topic and suddenly he calls you deeply disingenuous and sends an internet mob after you. That was my first reaction -- but that is mostly about Taleb as a person.</p><p>I do think that the idea of not having conflict explode violently later on is pretty important here, but that actually most of the time we manage to avoid such explosions, and that’s interesting. Humans are pretty good at predicting when big conflict will happen and actually, most of the time, we manage to prevent it. I think it’s true that small fires help guard against big ones, but I also think that big fires are rare in any case because people are good at predicting and averting them.</p><p>---</p><p><strong>mingyuan:</strong> I have previously heard you talk about professionalism in a more negative sense, and I'm wondering if you've changed your perspective over the past couple months?</p><p><strong>habryka:</strong> My current relationship to professionalism is very similar to my relationship with Christianity, which is that it’s really damn important to understand how Christianity has influenced the West, because it's everywhere. There are obviously central concerns about Christianity. The dominant one being that it is horribly wrong. But my current relationship to professionalism is: ""Oh god, it is doing massive amounts of damage in many different contexts."" It's also quite useful in other contexts, in the same way that Christianity is.</p><p><strong>mingyuan:</strong> So you're not a practicing professional?</p><p><strong>habryka:</strong> I am in some dimensions. I definitely recognize that in certain situations it's really important to present a consistent API that's consistent with professional norms. I also try to avoid situations where that's truly necessary, because the constraints that come from professionalism are quite substantial.</p><p>---</p><p><strong>Ozzie:</strong> For relationships, I was curious if you’ve thought about “anti-dates” or purposely difficult situations that you could put yourself in, to generate helpful conflict. Like taking care of a few kids that are difficult for a day or organizing a friend's wedding. Things that maybe should be common practice for people dating each other.</p><p><strong>habryka: </strong>I mean, I don't know. The first thing that came to mind, I think it was someone in our community who wrote the “questions to ask if you wanted to end your relationship”.&nbsp;</p><p>Maybe it's a good idea for everyone to ask themselves those questions early on. It’s well understood that a lot of typical relationship advice is actually about vulnerability, about finding places where there might be a potential source of conflict, and stuff like that, so I think all this is relatively entangled here.</p><p>It’d be hard to think of specific tasks that would expose potential conflicts across all relationships, though, since the situations that produce conflict are very different per relationship. If I want to work with someone at an organization, the dimensions in which I need to understand how future conflict will go differs a lot from if I want to have a Dungeons &amp; Dragons group with someone. So it's very hard for me to come up with one thing that you would want to do in both, but it doesn't feel very hard to come up with stuff I'd want to do in each individual situation to stress-test.</p><p>---</p><p><strong>jacobjacob: </strong>I'm pretty confused about the claim that professionalism does conflict fast. My impression is that many professional environments are incredibly risk averse, and they’re characterized by things like “putting your emotions to the side and just showing up to do the work”. This is an important norm for some situations, but it doesn't enable conflict.&nbsp;</p><p>The outlier environment that most have embraced conflict seems to be Bridgewater, a large hedge fund centered around something like “having the most crucial conversations as early as possible”. (If people know what a <a href=""https://www.meetup.com/South-Bay-Effective-Altruism/events/241761949/""><u>Hamming Circle</u></a>, or even a Doom Circle, is, my impression is that at Bridgewater they basically do that all the time.)&nbsp;</p><p>So could you say a little more about professionalism’s relationship to conflict?</p><p><strong>habryka:</strong> I think the key thing here is to distinguish professional norms within and between organizations. Professionalism has very different rules for these two cases. One of the core components of professionalism between organizations is contractualism. All parties agree on a contract, and negotiations on that contract frontload a bunch of conflict. All parties&nbsp; figure out what they will do in the relevant different scenarios very early on. Compared to other norms, contractualism is really costly. If I hang out with a friend and we already have an existing trust relationship we usually don't need a contract, because we trust that we will later figure out how to negotiate if something weird happens or one of us falls through on an implied obligation.</p><p>In professional relationships, if you were to rely only on norms and ad hoc negotiations, it would often end very badly. So what we do is put a lot of the negotiation and conflict very early on, where we sign a contract and the contract is very well specified. All parties negotiate the rules of the contract, bring in some lawyers, and they figure out all the ways in which the contract could go wrong. Then,&nbsp; once negotiations are done, the contract codifies a working relationship. Without that frontloaded conflict, a professional relationship would go very badly. Contractualism is really powerful here.</p><p>---</p><p><strong>Anonymous:</strong> Where do professional hierarchies start and end? Does professionalism have some sort of consistent variation with power or scale? And is there a predictable way it changes as you go up a power hierarchy?</p><p><strong>habryka:</strong> Let me restate the question to make sure I get it: “Let's say we have conflicts that are hierarchically embedded. What does that hierarchy actually look like?”</p><p>The top of the&nbsp; hierarchy would be something like outright war. You’re in conflict with each other and you're both just out to kill each other. And then, even above that, you're out to blackmail each other using threats to directly harm the other person's values. In the superintelligence example, this could look like torturing trillions and trillions of simulated beings that you really like. On the very lower level, it's very minor conflict, like we're playing Scrabble and/or we are playing a video game and killing each other's units. Even though there's some kind of damage we're causing each other, it's an extremely limited type of damage.</p><p>I feel like there’s a component of professionalism here that boils down to a bunch of very well-negotiated rules of conflict. These rules work pretty well when you're talking about tens of thousands to millions of dollars. I think it starts working substantially less well when you start talking about billions and trillions of dollars. Wars are not fought, for instance, within the professional culture, nor are other things with stakes in the trillions of dollars. There are still rules of conflict in these cases, but they are fought with different sets of norms.</p>",habryka4,habryka4,habryka,
kTzADPE26xh3dyTEu,Multivariate estimation & the Squiggly language,multivariate-estimation-and-the-squiggly-language,https://www.lesswrong.com/posts/kTzADPE26xh3dyTEu/multivariate-estimation-and-the-squiggly-language,2020-09-05T04:35:01.206Z,44,16,5,False,False,,"<p><i>(Talk given at </i><a href=""https://www.lesswrong.com/posts/g9QdXySpydd6p8tcN/sunday-august-16-12pm-pdt-talks-by-ozzie-gooen-habryka-ben""><i>an event on Sunday 16th of August</i></a><i>. Ozzie Gooen is responsible for the talk, Justis Mills edited the transcript.&nbsp;</i></p><p><i>If you're a curated author and interested in giving a 5-min talk, which will then be transcribed and edited, sign up </i><a href=""https://forms.gle/iwFatbhys9muPmQA7""><i>here</i></a><i>.)&nbsp;</i></p><figure class=""image image_resized"" style=""width:624px""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/ap1dunvqw2hvzon9y9dg""></figure><p><strong>Ozzie:</strong> This image is my <a href=""https://en.wikipedia.org/wiki/Wikipedia:Too_long;_didn%27t_read""><u>TLDR</u></a> on probability distributions:&nbsp;</p><figure class=""image image_resized"" style=""width:624px""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/xallv3f890yqk3xwl40l""></figure><p>Basically, distributions are kind of old school. People are used to estimating and predicting them. We don't want that. We want functions that return distributions -- those are way cooler. The future is functions, not distributions.</p><p>What do I mean by this?&nbsp;</p><p>For an example, let's look at some of the existing COVID models. This is one of them, from the IHME:</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/w4ycwhtif1kejnhnhhmk"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/gs5cyfl3jens8sx7p4xp 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/u20glctnzc56hwkycnkv 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/njzz0hrxsmeuwtugbme8 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/bzp8u04rzppylvy5kjhp 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/b4mv27kiq1oxs3vya9oi 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/sdboqnxw8wfwcfrrqmsu 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/a2dm6ulgviisnyujfrem 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/dd3xjhszaxypogfdufr6 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/chaohv29tahb8hdcdpl3 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/isnujeb2ecfrj7pzpwrz 1332w""></figure><p>You can see that it made projections for total deaths, daily deaths, and a bunch of other variables. And for each indicator, you could choose a country or a location, and it gives you a forecast of what that indicator may look like.&nbsp;</p><p>So basically there's some function that for any parameter, which could be deaths or daily deaths or time or&nbsp; whatever, outputs a probability density. That's the core thing that's happening.</p><figure class=""image image_resized"" style=""width:100%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/cdjsynvvqemhmsdtrsrf""></figure><p>So if you were able to parameterize the model in that way, and format it in these terms, you could basically wrap the function in some encoding. And then do the same forecast, but now using a centralized encoding.&nbsp;</p><p>So right now, basically for people to make something like the COVID dashboard from before, they have to use this intense output and write some custom GUI. It's a whole custom process. Moreover, it's very difficult to write<i> your own</i> function that calls their underlying model.</p><figure class=""image image_resized"" style=""width:80.51%""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/ahgsxcwwct1m9rroqni7"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/zfugppzhtfvrrrtgsp6r 86w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/jewwy8jdzppzbsyzvqxg 166w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/eie6dywmvum3rnnc2t9w 246w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/xocesrmvv2blzhq4bhvh 326w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/n2umziyoynoabi9z3jkz 406w""></figure><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/hbpbknopqrqpxajnsroh"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/cpoyntukwmjblj5jdbjm 101w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/wxwf44fyva64ihmapwsj 181w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/ofbvq0qqkn1zkpyk2qd8 261w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/iia8zcbzn8q1x3sd0bpt 341w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/gvsdfdkynyiyd56qwuzp 421w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/mdpjoc4enuvslfedrxwa 501w""></figure><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/w4ycwhtif1kejnhnhhmk"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/gs5cyfl3jens8sx7p4xp 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/u20glctnzc56hwkycnkv 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/njzz0hrxsmeuwtugbme8 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/bzp8u04rzppylvy5kjhp 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/b4mv27kiq1oxs3vya9oi 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/sdboqnxw8wfwcfrrqmsu 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/a2dm6ulgviisnyujfrem 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/dd3xjhszaxypogfdufr6 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/chaohv29tahb8hdcdpl3 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/isnujeb2ecfrj7pzpwrz 1332w""></figure><p>But, hypothetically, if we had an encoding layer between the model and the output, these forecasters could basically write the results of their model into one function, or into one big file. Then that file could be interpreted and run on demand. That would be a much nicer format.&nbsp;</p><p>Let’s take a look at Metaculus, which is about the best forecasting platform we have right now.</p><figure class=""image""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/klh1pzudoltci6yiz9zf"" srcset=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/ouaxmnht08uj0g6s6q9f 146w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/jqws0fdjkpzjppkssaqb 226w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/vq3aqj9kq8d8hlfg8pji 306w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/sni2koohjutqwpamrzff 386w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/kxxvkikq5j20o098q03s 466w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/ykzhkjccnkjirbzkukc7 546w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/chvpn9v0uzejottajroz 626w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/podym1ge1ysoh3hzrsez 706w""></figure><p>On Metaculus, everything is a point estimate, which is limiting. In general, it's great that we have good point estimates, but most people don't want to look at this. They’d rather look at the pretty dashboard from before, right?</p><p>So we need to figure out ways of getting our predictors to work together to make things that look more like the pretty graphs. And one of those questions is: how do we get predictors to write functions that return distributions?&nbsp;</p><p>Ultimately, I think this is something that we obviously want. But it is kind of tricky to get there.&nbsp;</p><figure class=""image image_resized"" style=""width:624px""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/hvcvtbdx3nrzegac36cr""></figure><p>So in Estimation Utopia, as I call it, we’d allow for people to take the results of their data science models and convert them into a unified format. But also, humans could just intuitively go ahead and write in the unified format directly. And if we have unified formats that are portable and could be run in different areas with different programming languages, then it would be very easy to autogenerate GUIs for them, including aggregates which combined multiple models at the same time. We could also do scoring, which is something that is obvious that we want, as well as compose models together.</p><p>So that's why I've been working on the Squiggly language.&nbsp;</p><p>Let’s look at some quick examples!&nbsp;</p><figure class=""image image_resized"" style=""width:624px""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/qtxze55e8x1ljdgpfoyi""></figure><p>This is a classic normal distribution, but once you have this, some of the challenge is making it as easy as possible to make functions that return distributions.&nbsp;</p><p>Here's a case for any <i>t</i>:</p><figure class=""image image_resized"" style=""width:624px""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/uaa9tvx3jxmswo0ol2m9""></figure><p>We're going to give you a normal, with <i>t</i> as a mean and the standard deviation of 3. This is a plot where it's basically showing bars at each one of the deciles. It gets a bit wider at the end. It's very easy once you have this to just create it for any specific combination of values.</p><p>It's also cool, because once you have it in this format, it's very easy to combine multiple models. For instance, here’s a lognormal.&nbsp;</p><figure class=""image image_resized"" style=""width:624px""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/chzdatg1fr47xxem9rp2""></figure><p>For example, if I have an estimate and my friend Jacob has an estimate, then we could write a function that for every time <i>t</i>, basically queries each one of our estimates and gives that as a combined result.&nbsp;</p><p>This kind of shows you a problem with fan charts, that they don't show the fact that all the probability amasses on the very top and the very bottom. That's an issue that we'll get over soon. Here’s what it looks like if I aggregate my model with Jacob’s.&nbsp;</p><figure class=""image image_resized"" style=""width:624px""><img src=""https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kTzADPE26xh3dyTEu/tz2me9biuryirjnr9h0o""></figure><h2>&nbsp;</h2><h2>Questions</h2><p><strong>Raemon:</strong> I had a little bit of excitement, and then fear, and then excitement again, when you talked about a unified format. The excitement was like, ""Ah, a unified format, that sounds nice."" Then I had an image of all of the giant coordination problems that result from failed attempts to create a new unified format, where the attempted unified format becomes <a href=""https://xkcd.com/927/""><u>yet another distinct format</u></a> among all the preexisting options.</p><p>Then I got kind of excited again because to a first approximation, as far as I can tell, in the grand scheme of things currently, approximately zero people use prediction markets. You might actually be able to figure out the right format and get it right the first time. You also might run into the same problems that all the other people that tried to come up with unified formats did, which was that it was hard to figure that out right at the beginning. Maybe now I am scared again. Do you have any thoughts on this?</p><p><strong>Ozzie:</strong> Yeah, I'd say in this case, I think there's no format that does this type of thing yet. This is a pretty unexplored space. Of course, writing the first format in a space is kind of scary, right? Maybe I should spend a huge amount of time making it great, because maybe it'll lock in. Maybe I should just iterate. I'm not too sure what to do there.</p><p>And there are also a few different ways that the format could go. I don't know who it's going to be the most useful for, which will be important. But right now, I'm just experimenting and seeing what's good for small communities. Well, specifically what’s good for me.</p><p><strong>Raemon:</strong> Yeah, you can build the thing that seems good for you. That seems good. If you get to a point where you want to scale it up, making sure that whatever you're scaling up is reasonably flexible or something might be nice. I don't know.</p><p><strong>Ozzie:</strong> Yeah. Right now, I'm aiming for something that's good at a bunch of things but not that great at any one of them. I'm also very curious to get outside opinions. Hopefully people could start playing with this, and I can get their thoughts.</p><p>---</p><p><strong>habryka:</strong> This feels very similar to <a href=""https://www.getguesstimate.com/""><u>Guesstimate</u></a>, which you also built, just in programming language as opposed to visual language. How does this project differ?</p><p><strong>Ozzie: </strong>Basically, you could kind of think about this as “Guesstimate: The Language”. But it does come with a lot of advantages. The main one is that you could write functions. With Guesstimate you couldn't write functions. That was a gigantic limitation!</p><p>Really, a lot of Squiggly is me trying to remake for my sins with Guesstimate. With Guesstimate, if one person makes a model of how the damage from bicycling, like the micromorts that they're taking when they bike, that model only works for them. If you wanted to go and configure it to match your situation, you’d have to go in and modify it manually. It's actually very difficult to port these models. If one person writes a good model, it's hard for somebody else to copy and paste it, hopefully into another programming tool. It's not very portable.</p><p>So I think these new features are pretty fundamental. I think that this is a pretty big step in the right direction. In general text-based solutions have a lot of benefits when you can use them, but it is kind of tricky to use them.</p><p>---</p><p><strong>Johnswentworth: </strong>I'm getting sort of mixed vibes about what exactly the use case here is. If we're thinking of this as a sort of standard for representing models, then I should be able to convert models in other formats, right?&nbsp; Like, if I have a model in Excel or I have a model in <a href=""https://pyro.ai/""><u>Pyro</u></a>, then there should be some easy way to turn it into this standard format?</p><p>On the other hand, if we're trying to create a language in which people write models, then that's a whole different use case where being a standard isn't really part of it at all (instead it looks more like the actual UI you showed us).&nbsp;</p><p>So I'm sort of not sure what the picture is in your head for how someone is actually going to use this and what it's going to do for them, or what the value add is compared to Excel or Pyro.</p><p><strong>Ozzie:</strong> Yeah, great question. So I would say that I’d ideally have both data scientists and judgemental forecasters trying to use it, and those are two very distinct types of use cases, as you mentioned. It's very possible that they both want their own ideal format, and it doesn't make sense to have one format for the two of them. I’m excited for users who don't have any way of making these methods intuitively at the moment.</p><p>Suppose, for example, that you’re trying to forecast the GDP of US for each year in the coming decades.&nbsp;</p><p>Step one is making sure that, basically, people on Metaculus or existing other forecasting platforms, could basically be writing functions using this language and then submitting those instead of just submitting point forecasts. So you’d be able to say “given as input a specific year, and some other parameters, output this distribution” -- instead of having to make a new and separate forecast for each and every year. Then having&nbsp; the whole rest of the forecasting pipeline work with that (e.g. scoring, visualisations, and so forth).&nbsp;</p><p>When you do that, though, it is pretty easy to take some results from other, more advanced tools, and put them into probably very simple functions. So, for instance, if there is a distribution over time (as in the GDP example), that may be something they could interpolate with a few different points. There could be some very simple setups where you take your different Pyro model or something that actually did some intense equations, and then basically put them into this very simple function that just interpolates based on that and then uses this new format.</p><p><strong>Johnswentworth:</strong> What would be the advantage of that?</p><p><strong>Ozzie:</strong> It’s complicated. If you made your model in Pyro and you wanted to then export it and allow someone to play with it, that could be a tricky thing, because your Pyro model might be computationally expensive to run. As opposed to trying to export a representation that is basically a combination of a CSV and a light wrapper function. And then people run that, which is more convenient and facilitates more collaboration.</p><p><strong>Johnswentworth:</strong> Why would people run that though? Why do people want that compressed model?</p><p><strong>Ozzie:</strong> I mean, a lot of the COVID models are like that, where basically the <i>running</i> of the simulation was very time intensive and required one person's whole PC. But it would still be nice to be able to export the <i>results </i>of that and then make those interactable, right?</p><p><strong>Johnswentworth: </strong>Oh, I see. Okay, I buy that.</p><p><strong>Ozzie: </strong>I also don't want to have to write all of the work to do all of the Pyro stuff in this language. It's way too much.</p><p><strong>Johnswentworth: </strong>Usually, when I'm thinking about this sort of thing, and I look at someone's model, I really want to know what the underlying gears were behind it. Which is exactly the opposite of what you're talking about. So it's just a use case that I'm not used to thinking through. But I agree, it does make sense.</p><p>---</p><p><strong>habryka: </strong>Why call the language Squiggly? There were a surprising lack of squiggles in the language. I was like, ""Ah, it makes sense, you just use the squiggles as the primary abstraction"" -- but then you showed me your code editor and there were no squiggles, and I was very disappointed.</p><p><strong>Ozzie:</strong></p><p>&nbsp;Yeah, so I haven't written my own parser yet. I've been using the one from math.js. When I write my own, it's possible I'll add it. I also am just really unsure about the name.</p>",ozziegooen,ozziegooen,ozziegooen,
DqkD3pFSiGWJJ5wBt,When should I be concerned about my Oura measurements indicating COVID-19?,when-should-i-be-concerned-about-my-oura-measurements,https://www.lesswrong.com/posts/DqkD3pFSiGWJJ5wBt/when-should-i-be-concerned-about-my-oura-measurements,2020-09-04T22:15:54.577Z,11,3,2,False,True,,"<p>The <a href=""https://wvumedicine.org/rni/"">Rockefeller Neuroscience Institute </a>seems to have developed an app that can detect COVID-19 3 days before symptoms based on Oura data but the app isn't publically available. How should a user that doesn't have access to the app interpret his Oura measurements to know when they should self-isolate or get tested?</p>",ChristianKl,christiankl,ChristianKl,
R9QkdCeLkeqW34TSz,"Hello ordinary folks, I'm the Chosen One",hello-ordinary-folks-i-m-the-chosen-one-1,https://www.lesswrong.com/posts/R9QkdCeLkeqW34TSz/hello-ordinary-folks-i-m-the-chosen-one-1,2020-09-04T19:59:10.799Z,31,22,3,False,False,,"<p><em>This is an informal argument against <a href=""https://plato.stanford.edu/entries/fine-tuning/#toc"">the fine-tuned universe</a>. The complete argument can be found on <a href=""https://www.sleepingbeautyproblem.com/about-fine-tuned-universe/"">my website</a>. The purpose is to show the importance of perspectives in reasoning, especially in anthropic topics. See my <a href=""https://www.lesswrong.com/posts/zjNRPgZAx8cgHgH9n/anthropic-reasoning-and-perspective-based-arguments"">previous post</a> for a summary.</em></p>
<h1>Why I am the Chosen One</h1>
<p>I know this sounds ridiculous. But bear with me for a minute. I will prove it to you. The clue is in the history.</p>
<p>I have 2 parents, 4 grandparents, and 8 great-grandparents. This number keeps multiplying. 50 generations back, the theoretical upper bound for the number of my ancestors is about 10^15, way above the total number of humans ever lived. Of course, there would be significant overlaps in this mega family tree. At this scale, calling it a family web would no longer be a joke. Nonetheless, even with overlapping, it still means going back far enough, say around 1000AD, my direct ancestors would cover a significant portion of the world population.</p>
<p>My existence is the result of all those people’s reproductive success. That means any historical event that affected the lives of a moderate number of people must have unfolded exactly the way it did for the creation of me. If Alexsander lost the battle of Gaugamela, or Genghis Khan failed to unite the Mongol tribes, or Columbus did not reach the New World in 1492, I would not be here.</p>
<p>Why stop there? It can go even further and be more disturbing. For my existence, it is not enough that all my ancestors successfully produced offsprings, they had to produce the exact offsprings. Meaning in each case the exact sperm had to fertilize the exact egg. Couple this with the exponential growth of my family tree the chance of my existence is unfathomably small. Yet, here I am.</p>
<p>This is either a statistical miracle or, an unknown force has guided every aspect of the past to ensure my existence. The odd is too obvious. The history must be fine-tuned for me.</p>
<h1>The Fine-Tuned Universe</h1>
<p>I can imagine how people would react if I tell them the above. They will say I am unbelievably egocentric and narcissistic. Why would history care if you are produced or not? Something could very well happen differently causing you not to be born. Big deal? More importantly, if the past is fine-tuned for you, am I just a by-product of that fine-tuning? I can use the same argument from my perspective and say history is fine-tuned for me instead of you. Safe to say it is not going to be well received.</p>
<p>Foreseeing these criticisms I decide to modify that argument a bit. I need more allies on this. So instead of focusing on the immediate “me”, extend it to “my kind”. Depending on how inclusive I want to be, that could mean humans, or life, or conscious beings, or complex physical systems, or maybe something even more general. On the other hand, to keep the probability low, extend the history further back. Consider the entire universe: all of its past up to the initial conditions and how the fundamental parameters came to be.</p>
<p>Now we have the argument known as the fine-tuned universe. It looks at the fundamental parameter of our universe and made an amazing discovery. They are all compatible with life's existence. Given the odds, ""life"" must be in some way significant to the universe.
It is still the same egocentric and narcissistic argument. But this time, it is less obvious. Because everybody discussing it is""life"". We are all on the same side.</p>
<h1>Perspective is the key</h1>
<p>Both arguments take a first-person perspective while presenting their evidence. I analyzed the historical events basing on their eventual effect on ""my"" existence. The fine-tuning argument analyzed all fundamental parameters according to their compatibility with life (our kind). There is nothing inherently wrong with doing so. However, we must realize this focus on oneself is perspective dependent. E.g. If another person analyzes historical events the same way I just did, he would analyze them based on their effects on him rather than on me.</p>
<p>From here, if I am to ask ""why are all past events compatible with my existence?"" it would be a perspective-dependent question. So it must accept a perspective-based answer. The answer is clearly ""because I would always find myself exist"". Reasoning from any perspective, it would always conclude the existence of its perspective center. That tautology is the Weak Anthropic Principle (WAP) response to fine-tuning.</p>
<p>However, proponents of fine-tuning argue that response is not a causal explanation of the fundamental parameters' value. Others criticize it for being unscientific. To that, I would say of course it is not scientific or causal. Because it is not answering a scientific or causal question. For those answers, the question should be objective/impartial. It should not be focusing on ""me"", or be perspective-dependent.</p>
<p>There are multiple ways to be impartial, but perhaps the easiest is to take a god's eye perspective, to reason with ""a view from nowhere"". I personally think that is not the best approach but that is a topic for another day. The important thing here is to recognize ""why are the fundamental parameters compatible with life?"" and ""why the parameters have these values"" are two different questions. They are formulated from different perspectives. The WAP response answers the previous question. While an impartial/scientific explanation is needed for the latter.</p>
<p>Fine-tuning presents the first-person question. Yet, it demands an impartial explanation. It does not reason from a consistent perspective. It effectively assumes we are significant not just to ourselves, but also to the universe. That is why it usually ends up with teleological conclusions. E.g. ""I"" am the chosen one or, the universe is designed to support life.</p>
",dadadarren,dadadarren,dadadarren,
9AWoAAA59hN9PEwT7,Why would code/English or low-abstraction/high-abstraction simplicity or brevity correspond?,why-would-code-english-or-low-abstraction-high-abstraction,https://www.lesswrong.com/posts/9AWoAAA59hN9PEwT7/why-would-code-english-or-low-abstraction-high-abstraction,2020-09-04T19:46:29.174Z,2,4,15,False,True,,"<p>Solomonoff Induction (SI) focuses on short code. What’s short in English is not necessarily short in code, and vice versa. Most of our intuition in favor of short, simple explanations is from our experience with English, not code. Is there literature arguing that code and English brevity usually or always correspond to each other? If not, then most of our reasons for accepting Occam’s Razor wouldn’t apply to SI.</p>
<p>Another way to think of the issue may be that coding is a low level or reductionist way to deal with an issue, while English is a high level approach that uses high level tools like explanations. Ideas can be represented in many ways, including at different levels of abstraction. It’s unclear that length or simplicity is consistent for the same idea across different levels of abstraction. That is, if you have two ideas X and Y, and X is simpler and shorter when you compare both ideas at a one level of abstraction, it may be unsafe to assume that X will also be simpler and shorter than Y when you compare them at a different level of abstraction. Is there any literature which addresses this?</p>
",curi,curi,curi,
PkpuvsFYr6yuYnppy,Open & Welcome Thread - September 2020,open-and-welcome-thread-september-2020,https://www.lesswrong.com/posts/PkpuvsFYr6yuYnppy/open-and-welcome-thread-september-2020,2020-09-04T18:14:17.056Z,12,3,73,False,False,,"<p>If it’s worth saying, but not worth its own post, here's a place to put it. (You can also make a <a href=""http://www.lesslong.com/"">shortform post</a>)</p><p>And, if you are new to LessWrong, here's the place to introduce yourself. Personal stories, anecdotes, or just general comments on how you found us and what you hope to get from the site and community are welcome.</p><p>If you want to explore the community more, I recommend <a href=""https://www.lesswrong.com/library"">reading the Library,</a> <a href=""https://www.lesswrong.com/?view=curated"">checking recent Curated posts</a>, <a href=""https://www.lesswrong.com/community"">seeing if there are any meetups in your area</a>, and checking out the <a href=""https://www.lessestwrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1#Getting_Started"">Getting Started</a> section of the <a href=""https://www.lessestwrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq-1"">LessWrong FAQ</a>. If you want to orient to the content on the site, you can also check out the new <a href=""https://www.lesswrong.com/tags/all"">Concepts section</a>.</p><p>The Open Thread tag is <a href=""https://www.lessestwrong.com/tag/open-threads?sortedBy=new"">here</a>.</p>",habryka4,habryka4,habryka,
67yzoZE88Qd3EjQ8d,Estimating the ROI of Insulation,estimating-the-roi-of-insulation,https://www.lesswrong.com/posts/67yzoZE88Qd3EjQ8d/estimating-the-roi-of-insulation,2020-09-03T20:30:08.886Z,13,5,0,False,False,,"<p><span>

A few months ago I wrote about </span>

<a href=""https://www.jefftk.com/p/outbuilding-thoughts"">trying to decide what to do with our
shed</a>.  It turns out that not only is our property too small for a
tiny house (""backyard cottage""), the only reason we're able to have
the shed where it is it's because it is grandfathered in.  The shed
turned out to have solid footings, so we decided to have it repaired.



</p><p>

I hired a mason to redo the cinderblock and a carpenter to replace the
roof.  The mason has finished, and I'm waiting for the carpenter to
start:

</p>

<p>

<a href=""https://www.jefftk.com/shed-blocks-rework-big.jpg""><img src=""https://www.jefftk.com/shed-blocks-rework.jpg"" srcset=""https://www.jefftk.com/shed-blocks-rework.jpg 550w, https://www.jefftk.com/shed-blocks-rework-2x.jpg 1100w"" /></a>

</p>

<p>

<a href=""https://www.jefftk.com/shed-mason-finished-big.jpg""><img src=""https://www.jefftk.com/shed-mason-finished.jpg"" srcset=""https://www.jefftk.com/shed-mason-finished.jpg 550w, https://www.jefftk.com/shed-mason-finished-2x.jpg 1100w"" /></a>

</p>

<p>

When the carpenter is done, we'll have a weathertight structure, but
we still need to figure out what we want to do with the inside.  One
thing I'm trying to figure out is how much insulation makes sense.  As
a detached building with electricity, we would probably use resistive
heat, which is relatively expensive (<a href=""https://www.jefftk.com/p/rough-utility-costs"">~$0.21/kWh</a>).

</p>

<p>

The structure is:

</p>

<table>
<tr>
<td>Walls</td>
<td>335 sqft
</td>
</tr>
<tr>
<td>Roof</td>
<td>145 sqft
</td>
</tr>
<tr>
<td>Floor</td>
<td>145 sqft
</td>
</tr>
<tr>
<td>Door</td>
<td>18 sqft
</td>
</tr>
<tr>
<td>Side windows</td>
<td>12 sqft
</td>
</tr>
<tr>
<td>End window</td>
<td>8 sqft
</td>
</tr>
</table>



<p>

Most of the structure is exposed to air, which is generally much
colder than the ground, so I'm going to ignore the floor.

</p>

<p>

Heat loss is proportional to area and temperature difference, and also
varies by material.  This per-material factor is called the U-factor
(heat / area-degree).  When dealing with conductive heat loss, which
is the main kind of loss I expect us to see here, people generally use
its reciprocal, the R-value.  Attempts at estimating the R-values of
the shell components, in F*sqft*hr/BTU:

</p>

<p>

</p>

<table>
<tr>
<td>Walls</td>
<td>8"" cinderblock</td>
<td>1.11
</td>
</tr>
<tr>
<td>Roof</td>
<td>3/4 plywood sheathing, asphault shingles</td>
<td>0.94 + 0.44
</td>
</tr>
<tr>
<td>Door</td>
<td>solid wood</td>
<td>2.17
</td>
</tr>
<tr>
<td>Side windows</td>
<td>single pane</td>
<td>0.91
</td>
</tr>
<tr>
<td>End windows</td>
<td>double pane</td>
<td>3.4
</td>
</tr>
</table>



<p>

This gives me:

</p>

<pre>
  335 sqft / 1.11 F*sqft*hr/BTU +
  145 sqft / 1.38 F*sqft*hr/BTU +
  18 sqft / 2.17 F*sqft*hr/BTU +
  12 sqft / 0.91 F*sqft*hr/BTU +
  8 sqft / 3.4 F*sqft*hr/BTU
  = 430 BTU / F*hr
</pre>



<p>

How many heating degree hours (F*h) should we count?  If it is 50F
outside for one hour and I want it to be 70F inside, that's 20F*h.
What is it over the course of a year?

</p>

<p>

The standard approach is to look up heating degree days for your
location.  This is a standard number computed for many places, and
assumes you heat your building to 65F.  Per <a href=""https://www9.nationalgridus.com/niagaramohawk/energy_supplier/gas_hdd.asp"">NationalGrid</a>,
over the last 30 years Boston has averaged 6,521 heating degree-days.
We're not planning to live in the shed, though, and instead we would
probably use it as a home office.  So we only care about heating
degree hours that fall during working hours.  There aren't standard
tables for this, as far as I can find, but we can calculate them from
NOAA's <a href=""https://www.ncdc.noaa.gov/cdo-web/datatools/lcd"">Local
Climatological Data</a> summaries.

</p>

<p>

LCD offers hourly temperatures for each weather station as CSV, and
then you can do your own processing.  I wrote a script (<a href=""https://github.com/jeffkaufman/heating-degrees/blob/master/process.py"">github</a>)
that assumes we heat it to 68F, turning on the heat at 7am (since it
cooled off at night and needs time to warm up) until 5pm.

</p>

<p>

It does tend to be warmer during the day than overall, though it's
more of a difference in summer:

</p>

<p>

<a href=""https://www.jefftk.com/boston-hourly-temperatures-2010-2020-big.png""><img src=""https://www.jefftk.com/boston-hourly-temperatures-2010-2020.png"" srcset=""https://www.jefftk.com/boston-hourly-temperatures-2010-2020.png 550w, https://www.jefftk.com/boston-hourly-temperatures-2010-2020-2x.png 1100w"" /></a>

</p>

<p>

During working hours, 30% of the time we need no heat at all.  The
rest of the time we typically need a modest amount of heat, and
occasionally it is very cold and we need a lot:

</p>

<p>

<a href=""https://www.jefftk.com/boston-heating-degrees-during-working-hours-big.png""><img src=""https://www.jefftk.com/boston-heating-degrees-during-working-hours.png"" srcset=""https://www.jefftk.com/boston-heating-degrees-during-working-hours.png 550w, https://www.jefftk.com/boston-heating-degrees-during-working-hours-2x.png 1100w"" /></a>

</p>

<p>

On average, over all the working hours including the ones when the
heat is off, we need 7.09 heating degrees.  If someone used it full
time (9hr/d x 250d/y) that's 2,250hr/y and 16,000 heating degree hours.

</p>

<p>

Going back to our calculation earlier, we can now estimate how much
heat we need:

</p>

<p>

</p>

<pre>
  430 BTU / F*hr *
  16,000 F*hr *
  0.00029 kWh / BTU
  = 1990 kWh
</pre>



<p>

At $0.21/kWh, this is $420/y in heating.  What would it be with
insulation?

</p>

<p>

Let's imagine we use R-15 insulation for the walls and R-30 for the
ceiling, plus 1/2"" drywall (R-0.45).  Where does that leave us?

</p>

<pre>
  335 sqft / (1.11+15+0.45) F*sqft*hr/BTU +
  145 sqft / (1.38+30+0.45) F*sqft*hr/BTU +
  18 sqft / 2.17 F*sqft*hr/BTU +
  12 sqft / 0.91 F*sqft*hr/BTU +
  8 sqft / 3.4 F*sqft*hr/BTU
  = 49 BTU / F*hr

  49 BTU / F*hr *
  16,000 F*hr *
  0.00029 kWh / BTU *
  $0.21 / kWh
  = $48/y
</pre>



<p>

It looks like it would save about $375/y, and bring heating costs down
to a very reasonable $48/y.  This isn't perfect because it ignores the
effect of thermal mass (so too high) and heat loss through the floor
(so too low), but I think it's probably in the right range.

</p>

<p>

How much would that much insulation cost?  Fiberglass R-15 is about
$0.70/sqft and R-30 is about $0.91/sqft, add $0.37/sqft for the
drywall and another $0.37/sqft for the wall studs, and ignore my
installation time (since this is something I would enjoy doing), and
this is ~$668.  Payback of 1.5y for something that should last
decades, and would make the space much nicer than having concrete
walls.  Seems worth it!

  </p>",jkaufman,jkaufman,jefftk,
zXfqftW8y69YzoXLj,Using GPT-N to Solve Interpretability of Neural Networks: A Research Agenda,using-gpt-n-to-solve-interpretability-of-neural-networks-a,https://www.lesswrong.com/posts/zXfqftW8y69YzoXLj/using-gpt-n-to-solve-interpretability-of-neural-networks-a,2020-09-03T18:27:05.860Z,68,22,11,False,False,,"<p>Tl;dr We are attempting to make neural networks (NN) modular, have GPT-N interpret each module for us, in order to catch mesa-alignment and inner-alignment failures.</p><h1>Completed Project</h1><p>Train a neural net with an added loss term that enforces the sort of modularity that we see in well-designed software projects. To use <a href=""https://arxiv.org/pdf/2003.04881.pdf"">this paper&apos;s</a> informal definition of modularity</p><blockquote>a network is modular to the extent that it can be partitioned into sets of neurons where each set is strongly internally connected, but only weakly connected to other sets.</blockquote><br><span><figure><img src=""https://imgur.com/wnVjGYs.png"" class=""draft-image "" style=""width:89%""></figure></span><br><p><em>Example of a &#x201C;Modular&#x201D; GPT. Each module should be densely connected w/ relatively larger weights. Interfaces between modules should be sparsely connected w/ relatively smaller weights.</em></p><p>Once we have a Modular NN (for example, a GPT), we will use a normal GPT to map each module into a natural language description. Notice that there are two different GPT&#x2019;s at work here. </p><br><span><figure><img src=""https://imgur.com/MfOuXs0.png"" class=""draft-image "" style=""width:100%""></figure></span><br><p><em>GPT-N reads in each &#x201C;Module&#x201D; of the &#x201C;Modular GPT&#x201D;, outputting a natural language description for each module. </em></p><p>If successful, we could use GPT-N to interpret any modular NN in natural language. Not only should this help our understanding of what the model is doing, but it should also catch mesa-alignment and inner-alignment failures. </p><h1>Cruxes</h1><p>There are a few intuitions we have that go counter to other&#x2019;s intuitions. Below is an elaboration of our thoughts and why we think this project could work. </p><h2>Finding a Loss function that Induces Modularity</h2><p>We currently think a Gomory-Hu Tree  (GH Tree) captures the relevant information. We will initially convert a NN to a GH Tree to calculate the new loss function. This conversion will be computationally costly, though more progress can be made to calculate the loss function directly from the NN. See Appendix A for more details</p><h2>Small NN&#x2019;s are Human Interpretable</h2><p>We&#x2019;re assuming humans can interpret small NN&#x2019;s, given enough time. A &#x201C;Modular&#x201D; NN is just a collection of small NN&#x2019;s connected by sparse weights. If humans could interpret each module in theory, then GPT-N could too. If humans can interpret the interfaces between each, then GPT-N could too.</p><span><figure><img src=""https://imgur.com/7p2xLzn.png"" class=""draft-image "" style=""width:100%""></figure></span><p>Examples from <u><a href=""https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.77377&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false"">NN Playground</a></u> are readily interpretable (such as the above example).</p><p>GPT-3 can already<a href=""https://player.vimeo.com/video/427943407/""> turn comments into code</a>. We don&apos;t expect the reverse case to be fundamentally harder, and neural nets can be interpreted as just another programming language.</p><p>Microscope AI has had some success in interpreting large NN&#x2019;s. These are NN&#x2019;s that should be much harder to interpret than modular NN&#x2019;s that we would be interpreting. </p><h1>Technical Questions:</h1><p>First question: Capabilities will likely be lost by adding a modularity loss term. Can we spot-check capability of GPT by looking at the loss of the original loss terms? Or would we need to run it through NLP metrics (like Winograd Schema Challenge questions)?</p><p>To create a modular GPT, we have two paths, but I&apos;m unsure of which is better.</p><ol><li>Train from scratch with modified loss</li><li>Train OpenAI&#x2019;s gpt-2 on more data, but with added loss term. The intuition here is that it&#x2019;s already capable, so optimizing for modularity starting here will preserve capabilities. </li></ol><h1>Help Wanted  </h1><p>If you are interested in the interpretability of GPT (even unrelated to our project), I can add you to a discord server full of GPT enthusiasts (just DM me). If you&apos;re interested in helping out our project specifically, DM me and we&apos;ll figure out a way to divvy up tasks.</p><h1>Appendix A</h1><h2>Gomory-Hu Tree Contains Relevant Information on Modularity</h2><p>Some readily accessible insights:</p><ol><li>The size of the <u><a href=""https://en.wikipedia.org/wiki/Minimum_cut"">minimum cut</a></u> between two neurons can be used to measure the size of the interface between their modules.</li><li>Call two graphs G and G&#x2019; on the same vertices equivalent if for every two u,v, the sizes of their minimum cuts are the same in G and G&#x2019;. It turns out that there always exists a G&#x2019; which is a tree! (The <u><a href=""https://en.wikipedia.org/wiki/Gomory%E2%80%93Hu_tree"">Gomory-Hu tree</a></u>.)</li><li>It turns out that the minimum cut between two neurons within a module never needs to expose the innards of another module.</li></ol><p>Therefore, the Gomory-Hu tree probably contains all the information needed to calculate the loss term and the hierarchy of software modules.</p><h2></h2>",elriggs,elriggs,Logan Riggs,
TtBik82RQLBCG3h8j,Emotional valence vs RL reward: a video game analogy,emotional-valence-vs-rl-reward-a-video-game-analogy,https://www.lesswrong.com/posts/TtBik82RQLBCG3h8j/emotional-valence-vs-rl-reward-a-video-game-analogy,2020-09-03T15:28:08.013Z,12,9,6,False,False,,"<p><i>(Update Sept 2021: I no longer believe that we make decisions that maximize the expected sum of future rewards—see discussion of TD learning </i><a href=""https://www.lesswrong.com/posts/jrewt3rLFiKWrKuyZ/big-picture-of-phasic-dopamine""><i>here</i></a><i>. It's more like ""maximizing expected reward next step"" (but ""next step"" can entail making a long-term plan). So this post isn't quite right in some of its specifics. That said, I don't think it's wildly wrong and I think it would just need a couple tweaks. Anyway, it's just a brainstorming post, don't take it too literally.)</i></p><p>I recently read a <a href=""https://www.amazon.com/How-Emotions-Made-Lisa-Barrett/dp/1328915433/ref=tmm_pap_swatch_0?_encoding=UTF8&amp;qid=&amp;sr="">book about emotions and neuroscience</a> (brief review <a href=""https://www.lesswrong.com/posts/DWFx2Cmsvd4uCKkZ4/inner-alignment-in-the-brain"">here</a>) that talked about ""valence and arousal"" as two key ingredients of our <a href=""https://en.wikipedia.org/wiki/Interoception"">interoception</a>. Of these, arousal seems pretty comprehensible—the brain senses the body's cortisol level, heart rate, etc. But the valence of an emotion—what is that? What does it correspond to in the brain and body? My brief literature search didn't turn up anything that made sense to me, but after thinking about it a bit, here is what I came up with (with the usual caveat that it may be wrong or obvious). But first,</p><p><strong>Definition of ""the valence of an emotional state"" (at least as I'm using the term)</strong></p><p>Here's how I want to define the valence of an emotional state:</p><ul><li>When I'm proud, that's a nice feeling, I like having that feeling, and I want that feeling to continue. That's positive valence.</li><li>When I have a feeling of guilt and dread, that's a bad feeling, I don't like having that feeling, and I want that feeling to end as soon as possible. That's negative valence.</li></ul><p>There's a chance that I'm misusing the term; the psychological literature itself seems all over the place. For example, some people say anger is negative valence, but when I feel righteous anger, I like having that feeling, and I want that feeling to continue. (I don't <i>want to want</i> that feeling to continue, but I do <i>want</i> that feeling to continue!) So by my definition, righteous anger is positive valence!</p><p>There are some seemingly-paradoxical aspects of how valence does or doesn't drive behavior:</p><ul><li>Sometimes I have an urge to snack, or to procrastinate, but doing so doesn't make me happy or put me in a positive-valence state; it makes my mood worse, and I know it's going to make my mood worse, but I do it anyway.</li><li>Conversely, sometimes it occurs to me that I should go meditate, and I know it will make me happy, but I feel an irresistible urge not to, and I don't.</li><li>...and yet these are exceptions. I <i>do</i> tend to usually take actions that lead to more positive-valence states and fewer negative-valence states. For example, I personally go <i>way </i>out of my way to try to avoid future feelings of guilt.</li></ul><p>(See also: <a href=""https://www.lesswrong.com/posts/yDRX2fdkm3HqfTpav/approving-reinforces-low-effort-behaviors"">Scott Alexander on Wanting vs Liking vs Approving</a>)</p><p><strong>How is emotional valence implemented computationally? A video game analogy</strong></p><figure class=""image""><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_120 120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_360 360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_720 720w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_840 840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_960 960w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_1080 1080w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5437d143d4692d827129b7b4494300f1f337db1e787e602.png/w_1164 1164w""><figcaption>In Doom II (1994 ... I guess I'm showing my age), you could lose a bunch of health points all at once by getting hit by an enemy (left), or you could go running on lava and you'll lose a few health points every second until you get off the lava (right). By analogy, when I eat junk food, I get a big transient positive reward (a.k.a. ""dopamine hit""); when I feel positive-valence emotions (happy, proud, righteous indignation, etc.) I claim that I'm getting a constant stream of positive reward as long as I'm in that state; and conversely when I feel negative-valence emotions (guilt, suffering, etc.), I claim that I'm getting a constant stream of negative reward as long as I'm in that state. &nbsp;</figcaption></figure><p>Here's a simple picture I kinda like, based on an analogy to action-type video games. (Ha, I knew it, playing all those video games in middle school wasn't a waste of time after all!)</p><p>In many video games you control a character with a ""health"" level. It starts at 100 (or whatever), and if it ever gets to 0, you die. There are two ways to gain or lose health:</p><ul><li><i>Event-based health changes:</i> When you get hit by an enemy, you lose health points. When you fall from a great distance and hit the ground, you lose health points. When you pick up a health kit, you gain health points. Etc.</li><li><i>State-based health changes:</i> In certain situations, you lose or gain a certain number of health points every second.<ul><li>For example, maybe you can walk across lava, but if you don't have the <a href=""https://zelda.gamepedia.com/Goron_Tunic"">appropriate protective gear</a>, you keep losing health points at a fixed rate, for as long as you're in the lava. So you run across the lava as fast as you can, and with luck, you can make it to the other side before you die.</li></ul></li></ul><p>In the brain, I've come around to the reinforcement-learning-type view that <a href=""https://www.lesswrong.com/posts/cfvBm2kBtFTgxBB7s/predictive-coding-rl-sl-bayes-mpc"">the neocortex tries to maximize a ""reward"" signal (among other things)</a>. So in the above, replace ""gain health points"" with ""get positive reward"", replace ""lose health points"" with ""get negative reward"", then the ""state-based"" situation corresponds to my current working theory of what valence is. Pretty simple, right?</p><p>To be explicit:</p><ul><li>If negative reward keeps flowing as long as you're in a state, you perceive that state as having negative valence. As a reward-maximizing system, you will feel an urge to get out of that state as quickly as possible, you will describe the state as aversive, and you will try to avoid it in the future (other things equal).</li><li>If positive reward keeps flowing as long as you're in a state, you perceive that state as having positive valence. As a reward-maximizing system, you will feel an urge to stay in that state as long as possible, you will describe the state as attractive, and you will try to get back into that state in the future (other things equal).</li></ul><p><strong>Worked examples</strong></p><ul><li><i>Other things equal, I seek out positive-valence emotional states and try to avoid negative-valence emotional states</i>. Easy: I choose actions based <a href=""https://www.lesswrong.com/posts/cfvBm2kBtFTgxBB7s/predictive-coding-rl-sl-bayes-mpc"">in part</a> on the predicted future rewards, and the rewards associated with the valence of my emotional state is one contributor to that total reward.</li><li><i>I have an urge to have a snack, even though I know eating it will make me unhappy:</i> I predict that as I eat the snack, I'll get a bunch of positive reward right while I eat it. I also predict that the negative-valence feeling after the snack will dole out slightly negative rewards for a while afterwards. So I feel a bit torn. But if the positive reward is sufficiently large and soon, and the negative-valence feeling afterwards is sufficiently mild and short-lived, then this is appealing on net, so I eat the snack. In the video game analogy, it's a bit like jumping down onto a platform with a giant restorative health kit ... but then you need to run through lava for a while to get back to where you were. Well, if the health gain from the health kit is large enough to outweigh the health loss from needing to run through lava afterwards, then OK, maybe that's worth doing.</li><li><i>I have an urge to NOT meditate, even though I know meditating will make me happy:</i> Just the opposite. Starting meditating involves stopping other things I'm doing, or turning down the opportunity to do other more-immeditely-appealing things, and that gives me a bunch of negative reward all at once. That outweighs the steady drip of positive reward that I get from time spent being happy, in my brain's unconscious calculation.</li></ul>",steve2152,steve2152,Steven Byrnes,
RF5HjYYLY5FWgtA9v,Covid 9/3: Meet the New CDC,covid-9-3-meet-the-new-cdc,https://www.lesswrong.com/posts/RF5HjYYLY5FWgtA9v/covid-9-3-meet-the-new-cdc,2020-09-03T13:40:01.019Z,49,17,9,False,False,,"<p>This week’s news all centers around policy decisions. The new data contains few important surprises, so attention shifts to what actions will be taken and how that will affect the path we follow going forward. The CDC’s fall and transformation into an arm of the White House reelection campaign is now complete. Others continue to come up with, suggest and criticize various policies. </p>



<p>Before we get to all that, let’s run the numbers.</p>



<h3>Positive Test Counts</h3>



<figure><table><tbody><tr><td>Date</td><td>WEST</td><td>MIDWEST</td><td>SOUTH</td><td>NORTHEAST</td></tr><tr><td>July 9-July 15</td><td>108395</td><td>53229</td><td>250072</td><td>20276</td></tr><tr><td>July 16-July 22</td><td>117506</td><td>57797</td><td>265221</td><td>20917</td></tr><tr><td>July 23-July 29</td><td>110219</td><td>67903</td><td>240667</td><td>26008</td></tr><tr><td>July 30-Aug 5</td><td>91002</td><td>64462</td><td>212945</td><td>23784</td></tr><tr><td>Aug 6-Aug 12</td><td>93042</td><td>61931</td><td>188486</td><td>21569</td></tr><tr><td>Aug 13-Aug 19</td><td>80887</td><td>63384</td><td>156998</td><td>20857</td></tr><tr><td>Aug 20-Aug 26</td><td>67545</td><td>66540</td><td>132322</td><td>18707</td></tr><tr><td>Aug 7-Sep 2</td><td>55000</td><td>75401</td><td>127414</td><td>21056</td></tr></tbody></table></figure>



<figure><img src=""https://lh5.googleusercontent.com/-rQRDxnc-HxSHd-WzN4C4Kr-v24vLRax8gD_5j9xkzb1Axll5eZLC0U3_qKdWK0PTbZ1JynJDRk8AoqlDiPNo1xix0KmVOlhW8Wf3oVe5lT9aF8x6csQXUAmWozIaDET50X2ZDTH"" /></figure>



<p>Only the West’s number here is reassuring. The South’s number here is disappointing but reflects a rebound in the number of tests after a steep decline last week. The Midwest situation continues to get worse. The Northeast has some reason to worry, but the increase is <em>mostly </em>explained by increased testing.</p>



<h3>Deaths</h3>



<figure><table><tbody><tr><td>Date</td><td>WEST</td><td>MIDWEST</td><td>SOUTH</td><td>NORTHEAST</td></tr><tr><td>June 25-July 1</td><td>858</td><td>658</td><td>1285</td><td>818</td></tr><tr><td>July 2-July 8</td><td>894</td><td>559</td><td>1503</td><td>761</td></tr><tr><td>July 9-July 15</td><td>1380</td><td>539</td><td>2278</td><td>650</td></tr><tr><td>July 16-July 22</td><td>1469</td><td>674</td><td>3106</td><td>524</td></tr><tr><td>July 23-July 29</td><td>1707</td><td>700</td><td>4443</td><td>568</td></tr><tr><td>July 30-Aug 5</td><td>1831</td><td>719</td><td>4379</td><td>365</td></tr><tr><td>Aug 6-Aug 12</td><td>1738</td><td>663</td><td>4554</td><td>453</td></tr><tr><td>Aug 13-Aug 19</td><td>1576</td><td>850</td><td>4264</td><td>422</td></tr><tr><td>Aug 20-Aug 26</td><td>1503</td><td>745</td><td>3876</td><td>375</td></tr><tr><td>Aug 27-Sep 2</td><td>1245</td><td>759</td><td>3631</td><td>334</td></tr></tbody></table></figure>



<figure><img src=""https://lh3.googleusercontent.com/Tjfx5_JdVbVAEyiyAyIXe0_LDhy7dgFRStc-h0YTGXX4Hs3R2FLzIUfZeTSrZMalepmYFYmRCF4ldlyiIb6Gwlcar2LQ7fBa5aflSx4btHLbY7ikivb06OusoPzB0nVOwSx6gyA_"" /></figure>



<p>The Midwest number is bad news, the West and Northeast numbers are excellent news. The South’s is an improvement, but less of an improvement than expected, so it counts as bad news. Deaths are on a clear downward trend in general and that should continue for at least several weeks, as the overall situation continues to improve right now. </p>



<h3>Positive Test Percentages by Region</h3>



<p>The Covid Tracking Project’s data has a very strange and very negative number of positive tests from Massachusetts this week, which I’ve corrected to a reasonable number. </p>



<figure><table><tbody><tr><td>Percentages</td><td>Northeast</td><td>Midwest</td><td>South</td><td>West</td></tr><tr><td>7/16 to 7/22</td><td>2.49%</td><td>5.13%</td><td>13.29%</td><td>8.56%</td></tr><tr><td>7/23 to 7/29</td><td>2.54%</td><td>5.51%</td><td>12.32%</td><td>7.99%</td></tr><tr><td>7/30 to 8/5</td><td>2.58%</td><td>7.26%</td><td>12.35%</td><td>6.68%</td></tr><tr><td>8/6 to 8/13</td><td>2.30%</td><td>5.67%</td><td>14.67%</td><td>6.98%</td></tr><tr><td>8/13 to 8/20</td><td>2.06%</td><td>5.62%</td><td>9.41%</td><td>6.47%</td></tr><tr><td>8/20 to 8/26</td><td>1.86%</td><td>5.78%</td><td>9.93%</td><td>5.88%</td></tr><tr><td>8/27 to 9/2</td><td>1.87%</td><td>6.37%</td><td>9.38%</td><td>4.78%</td></tr></tbody></table></figure>



<figure><img src=""https://lh6.googleusercontent.com/GjcDxxnqg_G-Esp5_GeFpKuwaa1C4OD7Mv6oLE1KqFJ9kcnZ1zjaw8KZBEWBfzmC5Fx62DV9gHJX6Qs5tnfaxObYzY_8rHfgmc7N_TgHc_UGwit65lEhRl_LtsAcStjzAyrMu_wa"" /></figure>



<p>This makes it clear the Midwest is getting worse and not merely testing more, and the West is rapidly improving. The South’s situation remains ambiguous, but looking at the individual states makes it looks like things are indeed improving slowly. </p>



<h3>Test Counts</h3>



<figure><table><tbody><tr><td>Date</td><td>USA tests</td><td>Positive %</td><td>NY tests</td><td>Positive %</td><td>Cumulative Positives</td></tr><tr><td>June 25-July 1</td><td>4,352,981</td><td>7.1%</td><td>419,696</td><td>1.2%</td><td>0.82%</td></tr><tr><td>July 2-July 8</td><td>4,468,850</td><td>8.2%</td><td>429,804</td><td>1.1%</td><td>0.93%</td></tr><tr><td>July 9-July 15</td><td>5,209,243</td><td>8.4%</td><td>447,073</td><td>1.1%</td><td>1.06%</td></tr><tr><td>July 16-July 22</td><td>5,456,168</td><td>8.6%</td><td>450,115</td><td>1.1%</td><td>1.20%</td></tr><tr><td>July 17-July 29</td><td>5,746,056</td><td>7.9%</td><td>448,182</td><td>1.1%</td><td>1.34%</td></tr><tr><td>July 30-Aug 5</td><td>5,107,739</td><td>7.8%</td><td>479,613</td><td>1.0%</td><td>1.46%</td></tr><tr><td>Aug 6-Aug 12</td><td>5,121,011</td><td>7.3%</td><td>502,046</td><td>0.9%</td><td>1.58%</td></tr><tr><td>Aug 13-Aug 19</td><td>5,293,536</td><td>6.2%</td><td>543,922</td><td>0.8%</td><td>1.68%</td></tr><tr><td>Aug 20-Aug 26</td><td>4,785,056</td><td>6.0%</td><td>549,232</td><td>0.8%</td><td>1.77%</td></tr><tr><td>Aug 27-Sep 2</td><td>5,042,113</td><td>5.5%</td><td>606,842</td><td>0.8%</td><td>1.85%</td></tr></tbody></table></figure>



<p>New York’s positive percentage creeped up substantially this week while the test count continued to rise, especially in the last few days. I am definitely worried that something has gone wrong and we are no longer on a slowly but steadily improving path. If things are suddenly getting worse here now, presumably it is a school problem, and that does not at all bode well.</p>



<p>The national picture here however is quite good. Our test numbers creeped back up a bit and the positive percentage fell substantially. (Recorded) hospitalizations are down as well. Yesterday was the first day in a long time they didn’t decline day over day, but for now I’m treating that as a mere blip.</p>



<h3>Center For Disease Control Sorta Partially Walks Back Its Opposition To Disease Control</h3>



<p>After taking a pounding from all sides for several days, director Robert Redfield (who, alas, probably can’t be played by the newly retired Robert Redford in the inevitable HBO movie version, but I’m hoping he’ll make an exception because come on) <a href=""https://thehill.com/policy/healthcare/public-global-health/513946-cdc-director-walks-back-change-in-coronavirus-testing"">‘clarified’ the new guidelines</a> that led to <a href=""https://thezvi.wordpress.com/2020/08/27/covid-8-27-the-fall-of-the-cdc/"">last week’s headline</a>.</p>



<p>In a statement, Director Robert Redfield said those who come into contact with confirmed or probable COVID-19 patients could be tested themselves, even if they do not show symptoms of the virus.</p>



<p>“Testing is meant to drive actions and achieve specific public health objectives. Everyone who needs a COVID-19 test, can get a test. Everyone who wants a test does not necessarily need a test; the key is to engage the needed public health community in the decision with the appropriate follow-up action,” Redfield said.</p>



<p>So he allows for the possibility that people who come into contact with confirmed cases <em>could </em>be tested, in theory, I mean it’s a thing that happens from time to time. Very generous of him. And it’s great to hear that everyone who “needs” a test can get a test, especially considering the numerous reports that this is not the case for any meaningful value of getting a test, <em>and the fact that this is not the case is the only good reason to revise the guidelines.</em></p>



<p>So… Do you feel clarified now? </p>



<p>Me neither. This does not feel like a walk back to me. It feels like they’re doubling down.</p>



<p>Instead, it seems their strategy is to assert control over… <a href=""https://www.npr.org/sections/coronavirus-live-updates/2020/09/01/908581048/sweeping-new-eviction-ban-from-trump-administration"">evictions</a>? </p>



<p>I don’t want to get too deep into the economics of this move. I won’t discuss whether it is completely and totally insane, or how much it will permanently drive up rental costs since renting means the government might decide to seize your property outright and pay you nothing in return, while you maintain it under penalty of law at your own expense in the hopes that the government will one day give it back. </p>



<p>I will instead say that this is completely and utterly unconstitutional and illegal and in no way something the CDC has any authority whatsoever to do. You are the Centers For Disease Control, not the Centers for Rent Control. </p>



<p>So you know what? Fine. You did it. Congratulations. Burn it to the ground. CDC Delenda Est. </p>



<h3>Centers For Disease Control Advocates Disease Control</h3>



<p>This just in: <a href=""https://www.cnn.com/2020/09/02/health/cdc-covid-19-vaccine-distribution/index.html"">The CDC has also informed states</a> that they should be ready to distribute one of two vaccine candidates by November 1.</p>



<p>Under normal circumstances this would be both the correct action and great news. It would mean that the two vaccine candidates have a substantial probability of being far enough along to be worth deploying soon, potentially heralding a swift end to the pandemic. Given “medical ethics” and the general overwhelming paranoia about deploying a vaccine by all Very Serious People, I have an extremely strong prior that any deployment would be too late rather than too early.</p>



<p>It’s certainly good news, even in these times, that they have the good sense to tell states to <em>get ready to distribute </em>whether or not there is any intent to actually distribute. We should get ready to distribute long before we expect to need distribution. Things will inevitably go wrong and cause delays, which we can address now <em>before </em>those delays cost lives.</p>



<p>Unfortunately on so many levels, these are not normal times. We have the president we have, who is facing a presidential election… on November 3, <em>two days </em>after the target date. That does not in any way feel like a coincidence.</p>



<p>I would be very surprised if this CDC announcement is not being made under, at a bare minimum, extreme pressure from the White House. This was a political decision, and together with other CDC news, it seems safe to respond as if the CDC is completely captured by the White House and is acting under its direct orders to serve the President’s political interests and whims, rather than as a center for the control of disease.</p>



<p>If we take as given that Trump is planning a big October Surprise, I’ll take ‘issues an order to distribute the vaccine early’ over every other alternative I can come up with, except for the possibility that it might actually work and win him the election.</p>



<p>The thing is, <em>he’s right.</em></p>



<p>He’s not right <em>for the right reasons. </em>He’s not understanding the situation and doing the Bayesian calculus and realizing that early distribution of a known-to-be-safe vaccine is a huge net benefit to America and the world, and we should follow in the footsteps of China and Russia and get on that. Of course not. That’s not how he thinks. </p>



<p>He will issue the order, <em>if he issues it,</em> because he thinks it will help him get reelected, full stop, without caring about whether it is a good idea.</p>



<p>That doesn’t make him wrong. If you think he’s wrong, as Tyler Cowen says, <a href=""http://feedproxy.google.com/~r/marginalrevolution/feed/~3/DgW_NvGat1g/show-your-work-people-advice-on-vaccines.html"">show your work</a>.</p>



<p>And if and when he does issue that order, if you are Biden, how do you respond?</p>



<p>If Biden says ‘yes, that was the right thing to do’ then obviously it’s a huge Trump win (and also a win for the world, but in context neither side cares about that).</p>



<p>If Biden says ‘no, that’s not a responsible thing to do’ then Trump is the one who is doing the only action that matters to get us out of that, and Biden is the one not doing it because “medical ethics.” </p>



<p>Thus, it would be a great play even if there were risks that made it a bad idea – it’s not like those risks could be properly communicated to the public. Nor could a <em>lack </em>of such risks be communicated to the public, especially over the objections of the Very Serious People, but also even with their full support. A huge percentage of Americans don’t want the vaccine, sight unseen, even under the best conditions. </p>



<p>I wonder why the public has such distrust for public health authorities and doesn’t want to inject strange things into their bodies on such authorities’ say so. It’s not like they are constantly lying to us about pretty much everything.    </p>



<h3>Health Experts Warn of Dangers of Ignoring Health Experts</h3>



<p>What’s new with those vaccines in Russia and China? I can’t find any news on whether they’re working, but we do have news that the <a href=""https://www.reuters.com/article/us-health-coronavirus-vaccines-adenoviru/scientists-see-downsides-to-top-covid-19-vaccines-from-russia-china-idUSKBN25R19H"">Very Serious People are Very Concerned</a>. </p>



<p>Whenever people who will always have objections object to something, it’s important to remember that you should not expect to update your beliefs in any particular direction. Health experts will warn about the dangers of doing the thing their ‘ethics’ say not to do, with whatever case they think is the strongest, whether or not they have a good case. So when you see them make their case, you should update based on whether their case is stronger or weaker than expected. If they make terrible arguments that are worse than you expect, you should update in favor of there not being good objections.</p>



<p>In this case, it seems there are two concerns.</p>



<p>The first concern is that the vaccine is based on the common cold. Therefore, those who have had the wrong common cold will already have an immune response ready, and the vaccine won’t work on those people. This might reduce how often the vaccine is effective. </p>



<p>That’s a reasonably good objection. It’s a great objection if you’re choosing what approach to use. As an objection to deploying the vaccine versus doing nothing, though, it’s rather weak. If often the vaccine does nothing, then the calculus on whether the vaccine is a net benefit is unlikely to change much. Every extra immune person helps, and the costs of deployment are trivial relative to that benefit. What you’re looking for is active downsides, not reduced frequency of upside.</p>



<p>The second objection is that a previous HIV vaccine that used some similar characteristics in its delivery ended up making people more vulnerable to HIV, so they warn that this too could make people more vulnerable to HIV.</p>



<p>I know complete and utter BS when I see it. The previous HIV vaccine put people at risk for HIV because it was trying to be an HIV vaccine and messed up. Not because it so generically forked with the immune system that it happened to make HIV worse. This vaccine is trying to be a Covid-19 vaccine. It could plausibly make <em>Covid-19 </em>worse. But if Very Serious People are talking about <em>HIV </em>risk here, it means they have no cards to play. Update accordingly.</p>



<h3>Arizona University Kind of Solves Covid-19</h3>



<p>Seriously, <a href=""https://threadreaderapp.com/thread/1299049476288544768.html"">it kind of did</a>. Check this out.</p>



<p>It turns out, if you actually care about solving the problem, you can test waste water from each building, and then test everyone in the building when the water tests positive, thus catching cases before they have much chance to spread. Do that consistently, using the quick tests that are actually easy and dirt cheap, and it’s over. That doesn’t mean the University of Arizona is in the clear, because <em>no one else </em>is doing it and they therefore have to constantly worry about reintroduction. But if we all followed this procedure? It would all be over in a month.</p>



<p>This has been your periodic reminder of The Kinds of Things a Functional Civilization Would Do.</p>



<p>As opposed to, say, <a href=""https://www.insidehighered.com/news/2020/08/31/colleges-want-professors-stay-mum-student-covid-19-cases?utm_source=Inside+Higher+Ed&amp;utm_campaign=0871a6bb25-DNU_2020_COPY_02&amp;utm_medium=email&amp;utm_term=0_1fcbc04421-0871a6bb25-197603613&amp;mc_cid=0871a6bb25&amp;mc_eid=495c6bd417"">not telling people when a classmate tests positive</a>. </p>



<h2>What About Those Reinfection Cases?</h2>



<p><a href=""https://www.businessinsider.com/four-coronavirus-reinfection-cases-reported-in-the-netherlands-2020-8"">This week’s periodic panic</a> about lack of immunity was unique because it had actual bad news to consider. Normally people don’t need actual bad news, and mumble something about how we can’t be sure how long things will last in order to sound serious. In the past, this has somehow kept happening while there were actual zero reports of reinfections.</p>



<p>Now there are a non-zero number of reports of reinfections, which led to a <em>moderately </em>larger amount of panic and fear mongering. It turns out that its frequency and intensity does respond <em>somewhat </em>to actual news. So how worried should we be about these new reports?</p>



<p>As usual, the news article starts out with the scariest take it’s willing to dish out, with bullet points like “These reinfection cases demonstrate how immunity to the novel coronavirus is somewhat transient, especially with mild infections.” But overall, I’m actually very happy with the lack of mongering going on here from Business Insider, so positive reinforcement to them. </p>



<p>They get to the right answer here, which is definitely ‘not very worried.’ </p>



<p>What these cases show is not that immunity is short lived. They show that a very small number of people don’t get complete immunity when they are infected. </p>



<p>But that is neither surprising nor particularly impactful. A system of containment doesn’t care much about a 1% failure rate given how this virus works. With a total of 6 known cases worldwide and large incentives to find them, there’s no way the number of people who don’t regain full immunity is enough to be worth worrying about. It shouldn’t impact how anyone lives their life <em>at least </em>until after they have symptoms again. And in most of these cases, the secondary infections were mild anyway. </p>



<p>What this definitely <em>doesn’t </em>mean is that we now have to suddenly worry about immunity fading quickly. In these cases, the second infection happened quickly, often within a month or so. We know for sure that immunity almost always lasts far longer than that. So this isn’t people who got immunity and then lost it, it’s people part of the small group who were never immune in the first place. Which we’d prefer didn’t happen, sure, but isn’t impactful. </p>



<p>If we suddenly had six new cases, <em>all of which had their first infection in February or March and their second one in August, </em>then I’d be much more worried that five or six months was enough to start to meaningfully degrade immunity. That’s not what we saw, so six months is insufficient to do this. We can assume that for practical purposes immunity lasts a minimum of seven months, and then <a href=""https://en.wikipedia.org/wiki/Lindy_effect"">apply Lindy</a>, and assume that the end of that is where things <em>begin </em>to be a problem. Which should be enough time to get the vaccine online. Excellent.</p>



<p>This was worse immunity news than I expected this week. But overall, does this week make us think immunity is shorter (because we found some reinfection cases) or longer (because almost everyone stayed immune one more week)? I don’t think that is clear.</p>



<h2>Physical World Does Not Think Six Feet Is a Magic Distance</h2>



<p>People claiming with presumably straight faces to be ‘researchers’ used that authority to get into the paper that <a href=""https://www.sfgate.com/science/article/The-6-foot-rule-is-outdated-Researchers-devise-15520286.php"">perhaps the six foot rule could use a bit of nuance</a>. That it matters how long you’re there for, indoors or outdoors, poorly or well ventilated, silent versus spoken versus shouting or singing, dense versus sparse crowd. If I had to choose three additional considerations when measuring risk and deciding how far to keep away and whether to require masks, then those are probably the correct variables to consider. And all their directional assessments seem right. So, good job, I guess. As far as it goes.</p>



<p>If it makes people actually think about their physical situations a bit and optimize somewhat, that would be great. Hopefully the nuance is net helpful. </p>



<p>If you want a <em>lot </em>of nuance on what to be doing and how to measure risk, <a href=""https://www.microcovid.org/"">the microCOVID project</a> is one option. I had the chance to comment on their document and models a bit. They didn’t take every suggestion I made, but they are definitely <em>trying </em>to come up with reasonable answers and provide practical help. If that seems interesting or valuable, check it out for another opinion. </p>



<p>A note for those who try the microCOVID project is that their basic system of ‘use a budget to allocate risk’ originates in the need to find a policy that roommates can all live with and follow, without anyone feeling cheated or causing anything too perverse. If you have different binding constraints, different strategies will make sense for you.</p>



<h2>Important Things Are More Important</h2>



<p>Periodically we see <a href=""https://spectator.us/vmas-new-york-covid-rule-hypocrisy/"">outrage like this</a> about the hypocrisy of letting Very Important People like celebrities or the rich <em>get away with </em>doing things that the rest of us are told not to do. It seems that while mostly not allowing concerts, New York allowed the Video Music Awards to completely break a lot of the rules. </p>



<p>Good. </p>



<p>If anything, the report shows a decided <em>shortage </em>of such hypocrisy. The event had to be spread out throughout the city, extensive precautions were taken for spots that lasted only a few minutes. I am guessing that everyone involved was tested in advance, probably multiple times. And that was then shown to <em>millions </em>of people. Not my thing, but the same way that sports must go on, other things that bring joy to millions in exchange for the exposure of dozens or hundreds is obviously a trade-off that we want to make.</p>



<p>People are so against doing things that make sense, and so unwilling to deal with ‘hypocrisy’ or ‘inequality’ that they think that you not being allowed to have a private dance party means the VMAs should stop. That we shouldn’t look at the value of an activity in dollars or happiness, and compare it to the risks involved, when deciding what to do, to maybe help make this lockdown liveable for all and helping the economy survive. </p>



<p>Or that we shouldn’t give extraordinary flexibility to those willing to take extraordinary precautions. If you have the time and money to test everyone and make something safe, I don’t care if it otherwise violates guidelines.</p>



<p>The key is that this needs consensus that the exception is a reasonable exception. That it involves minimal risk given the benefits involved, that precautions were taken, that it is an <em>efficient </em>allocation of risk with a solid story attached. Otherwise, even if it’s a good idea, it decays people’s willingness to follow the rules.</p>



<p>I would hope that the ‘it’s being broadcast to millions of people who want to see it’ rule together with the ‘it’s worth enough to spend what it takes to get everyone tested beforehand and take all the precautions’ rule would cover the right times to make an exception pretty well. </p>



<p>If both of those apply, do it. If they don’t both apply, respect the rules.</p>



<p>Or, if there’s something you think is too important and has to be done anyway, understand that not doing so will undermine the rules themselves and decide whether it is worth it.</p>



<p>Contrast this with, say, Nancy Pelosi going to a hair salon and not taking precautions. There is zero excuse for that. The outrage is completely justified.</p>",Zvi,zvi,Zvi,
ybRaZMr3MKdxa89qg,"""How to Talk About Books You Haven't Read"", by Pierre Bayard",how-to-talk-about-books-you-haven-t-read-by-pierre-bayard,https://www.lesswrong.com/posts/ybRaZMr3MKdxa89qg/how-to-talk-about-books-you-haven-t-read-by-pierre-bayard,2020-09-03T13:05:53.698Z,11,6,2,False,False,,"<p><em><strong>Salticidae Philosophiae</strong></em> <em>is</em> <em>a series of abstracts, commentaries, and reviews on philosophical articles and books.</em> </p><p>Somewhere out there is a universe where my first post here was&#xA0;<em>How to Talk About Books You Haven&apos;t Read</em>, and ours is flawed by comparison. Still, I&apos;ve gotten to it at last, and here we are, with everything you need to know in order to talk about&#xA0;<em>How to Talk About Books You Haven&apos;t Read</em>, without having read it.</p><p>I can only hope that Pierre Bayard gets an inexplicable warm feeling in his chest at the moment that I publish this post.</p><h1>Highlights</h1><ul><li>We do not have access to, or an unfiltered &quot;true&quot; understanding of, any text.</li><li>The first reason for this is that our experience of any text, and our understanding of that text, is filtered by factors like our experiences with other books, our preconceptions, etc.</li><li>The second reason is that, even as we are reading a book, we fail to have a perfect recollection of what we have read, transforming it into a &quot;book we have (partly) forgotten.&quot;</li><li>More important than having read a book is being able to understand its content, its relation to other books, and so on, which are all theoretically possible without even picking up the book.</li><li>Do not be afraid to talk about a book that you have not personally read.</li><li>Do, however, be upfront about the degree to which you are familiar with it, and in what ways.</li></ul><h1>Chapter-by-Chapter</h1><h2>Preface</h2><p>The preface is worth noting for this passage:</p><blockquote>As I will reveal through my own case, authors often refer to books of which we have only scanty knowledge, and so I will attempt to break with the misrepresentation of reading by specifying exactly why I know of each book.</blockquote><p>The four abbreviations which Bayard uses are:</p><ol><li>UB, or books unknown to me.</li><li>SB, or books I have skimmed.</li><li>HB, or books I have heard of.</li><li>FB, or books I have forgotten.</li></ol><p>Bayard also uses the symbols - -, - , +, and ++ to denote various degrees of positive and negative opinion. Together with the previous abbreviations (which will be elaborated on in the next section), Bayard would like to see this system be more widely adopted.</p><ul><li>Bayard could have been clearer (here or in the upcoming chapters) about the demarcations between each category, however. It&apos;s unclear to me where the dividing line should be drawn between UB and HB, or (to a lesser extent) SB and FB.</li></ul><h2>Ways of Not Knowing</h2><p>The primary problem is that we have practical access only to a certain number of books (and the internet ultimately makes this problem worse, not better, because we can access more books than ever before but they also exist in a far greater number). Reading is also non-reading: every decision to pick up a book is also a decision to not pick up every other book.</p><p>Sometimes what we are talking about is not even the book itself but a fantasy of a book. We can exchange comments about a book and build beliefs (accurate or otherwise) about a book without having read it, or even build new beliefs about a book we&#xA0;<em>have</em> read in response to the comments of other people.</p><p>The collective library is the cultural discourse and context in which books exist. A book may be <em>UB, or unknown to me</em>, but I may still be able to place it in the collective library or understand its relevance. For example, I have never read&#xA0;<em>Paradise Lost</em>, but I know what&apos;s going on when someone says that it is better to reign in Hell than to serve in Heaven. I can, furthermore, not just understand references to it but make valid references of my own to that text. Another example might be the Arthurian mythos: Very few of us have read&#xA0;<em>The Once and Future King</em>, let alone any other Arthurian texts, but most of us can answer various questions about King Arthur.</p><p><em>SB, or skimmed books</em>, are just that, but Bayard believes that this can be valuable, and sometimes even more valuable than reading the book more closely. We can skim linearly or circuitously, and someone who has skimmed may still get the essential facts. For example, one doesn&apos;t need to study the&#xA0;<em>Meditations</em> of Marcus Aurelius very long to understand that the central message is this: We need to focus on controlling ourselves rather than worrying about outside circumstances which we cannot control, and also Marcus Aurelius would really prefer to be dead.</p><p>Next are <em>HB, or books which one has heard of</em>. It is possible to get enough information about a book to meaningfully engage on it. Returning to a previous example, I am not only culturally literate with regard to&#xA0;<em>Paradise Lost</em> but could talk for a fair while on its plot, characters, themes, and so on, between what I have heard about the book in particular and what I know of its author, John Milton, and the ideas which would have appealed to him, etc etc. I can follow a conversation on Paradise Lost, and even start one.</p><p>Last of all there are the&#xA0;<em>FB, or forgotten books.&#xA0;</em>Only a few people have perfect memories, so for the majority of us, every book we have read is a book which we have, to one degree or another, forgotten. For the reasons described in this book, Bayard prefers to not refer to &quot;books which I have read,&quot; or which he has &quot;not read,&quot; but&#xA0;<em>FB</em> may be the abbreviation which most closely approximates the first.</p><p>It is worth noting that, after UB, the most common reference is to SB&#xA0;<em>and</em> HB, together, and that there are are also books which are SB, HB, and FB. Also, it is absolutely emblematic of this book that Bayard feels no shame in assigning negativity or positivity to UB in exactly the same manner as the other markings.</p><ul><li>&quot;For a true reader, one who cares about being able to reflect on literature, it is not any specific book that counts, but the totality of all books.&quot; pg 30 para 4</li><li>Though the object of this post is to let you get away without reading the book, I believe that it is worth reading just for the pages on Michel de Montaigne, who writes, &quot;To compensate a little for the treachery and weakness of my memory, so extreme that it has happened to me more than once to pick up again, as recent and unknown to me, books which I had read carefully a few years before and scribbled over with my notes, I have adopted the habit for some time now of adding at the end of each book (I mean of those I intend to use only once) the time I finished reading it and the judgment I have derived of it as a whole, so that this may represent to me at least the sense and general idea I had conceived of the author in reading it.&quot;</li></ul><h3>Literary Confrontations</h3><p>If we have both skimmed a book, then we might be talking about different books, really. It&apos;s possible for two people to skim different books and come away with the same general idea (consider how derivative and generic many fantasy novels are, for example), or to skim the same book but come away with different enough ideas about that book that, if not for the title, they might not realize in later conversation that they had skimmed the same book.&#xA0;</p><p>The &quot;inner library&quot; is the set of books around which <em>you</em> in particular are constructed. Each of us is the sum of our own inner library. The inner book is a &quot;fragmentary and reconstituted object&quot; which is not the book itself, as an objective text existing outside yourself, but the book as you understood it: Roger Ebert and I may both go to the theater together (or we might have done, before he died) but we will, in this sense, be watching very different films.</p><p>The &quot;inner book&quot; is personal to us. It is the filter which encounters every new text and determines which elements we consciously perceive, and how we interpret those elements. The reason that Ebert and I will have watched different films is that we have different inner books. Writing is the act of bringing, in one form or another, our inner book into the world, but because our hands are imperfect and because everyone has their own inner book, this is usually unsuccessful.</p><ul><li>One chapter is mostly a retelling of &quot;Shakespeare in the Bush,&quot; which you can read in its complete form <a href=""https://hdo.utexas.edu/wp-content/uploads/2014/11/Shakespeare-in-the-Bush.pdf"">here</a>.</li></ul><h3>Ways of Behaving</h3><p>Do not be ashamed over a failure to have read a book. We need to be honest with ourselves and others about the degree to which we have not read things.</p><p>Talking about books is, of course, not reading, but the&#xA0;<em>virtual library</em> is the space in which we discuss books, and in which our inner books meet (or try to meet). Because it is a space of discussion, and none of us actually has direct access to the text itself (mediated, as the act of reading is, by our personal filters), the virtual library does not contain any &quot;objectively existing&quot; books but only a plethora of subjective experiences of books.&#xA0;There is currently a great resistance to the idea of (to use my own wording)&#xA0;<em>calcifying</em> the virtual library and acknowledging this fact.</p><p><em>The book is not the thing</em>. The text can be changed by the conversation. The discussion surrounding the book is also part of the book. Books are reinvented in the reading.&#xA0;There are &quot;Phantom books&quot; based on mistaken recollections.</p><ul><li>Sometimes reading is harmful. Oscar Wilde had three categories: books to read, books to reread, and books to convince people to not read.</li><li>Creators are critics and critics are creators.&#xA0;Talking about things is an act of creation.</li><li>&quot;If it is true that he hasn&apos;t &apos;read&apos;&#xA0;<em>Hamlet</em>, Ringbaum certainly has at his disposal a great deal of information about it and, in addition to Laurence Olivier&apos;s movie adaptation, is familiar with other plays by Shakespeare. Even without having had access to its contents, he is perfectly well equipped to gauge its position within the collective library.&quot; pg 124-125 para 2.</li></ul><h1>Favorite passage</h1><blockquote>This encounter with the infinity of available books offers a certain encouragement not to read at all. Faced with a quantity of books so vast that nearly all of them must remain unknown, how can we escape the conclusion that even a lifetime of reading is utterly in vain? [pg 6 | para 2]</blockquote><h1>Additional comments</h1><p>It might be worth talking about the Bible (or rather, our ways of reading and talking about the Bible) in order to give further examples of what Bayard means.</p><p>First of all, any such discussion &quot;of what Bayard means&quot; is already running into forgotten books and virtual libraries and so forth. I&apos;m only making this post several years after I originally purchased Bayard&apos;s book and read it for the first time, and I think I read it a couple more times after that, highlighting it and adding marginalia at least once in all those readings (and on another occasion I read bits and pieces, making it, at that point, a Skimmed Book). Then I began to read it again, almost two years ago, and this time took more complete notes until, two-thirds of the way through, something distracted me and I set the book aside till recently, when I read finished the last part of the book at last and...waited several more weeks before I returned to my notes and created this post as you see it now.</p><p><em>How to Talk About Books You Haven&apos;t Read</em> is very definitely a Forgotten Book for me, and in some ways it is also a Heard Book: I&apos;ve only read <a href=""https://www.brainpickings.org/2012/06/15/how-to-talk-about-books-you-havent-read/"">one other blog post</a> on this book, but all these notes which &quot;I&quot; made are from people who are, to varying degrees, arguably not myself. To what extent do they fill the same role as totally separate persons who are merely telling me things which they recall, and which, at this point, I no longer do?</p><p>This is a book which I have skimmed, heard about, and forgotten. It is a book which was at some point unknown to you, and which you have now heard about. When I talk about &quot;what Bayard means,&quot; what you are getting is a filtered conception of&#xA0;<em>my</em> filtered conception of what Bayard means, and at every step of the way there has been a transformation of information, from the point that Bayard put pen to paper (or finger to keyboard) to the point that you are reading and interpreting these words.</p><p>Now, the Bible. Most English-speaking people are familiar with various passages and references. We know about swords turning to plowshares and lions laying down with lambs, what is meant when someone reminds you that some politician or priest does not &quot;walk on water,&quot; and when someone refers to the Book of Genesis they are talking about one of the Bible&apos;s constituent parts. This is the Bible in the context of the cultural library.</p><p>We all have a personal interpretation of the Bible, to whatever extent we are familiar with. Slaveholders and abolitionists both referred to the Bible in support of their respective positions, and we can say that there was outright willful misrepresentation to one extent or another, in some number of cases, but that leaves out the role of motivated reasoning in the production of a genuinely-held interpretation. At least <em>some</em> slaveholders truly believed that there was a Biblical mandate for that institution, even if they only came to that belief to support their position, rather than their position as a natural consequence of this interpretation. This is the &quot;inner Bible&quot; as it existed in each person&apos;s inner library, and something that many literalists simply do not understand or refuse to accept. Even a &quot;literal&quot; interpretation of the Bible in each of its original languages is going to result in multiple inner Bibles.</p><p>This discourse about &quot;the Bible,&quot; when each of is speaking according to our inner Bibles, then produces virtual Bibles, which may be very far removed from the objectively-existing Bible, which probably exists and may even be reachable, but which, if we did, we could not&#xA0;<em>know</em> we had reached, and which we still could not transmit to others. Our conversation about the Bible, and the virtual Bible which that conversation alters by its very existence, may, in fact, be more important than any objectively-existing Bible could be.</p><h1><a href=""https://www.bloomsbury.com/author/pierre-bayard/"">Author biography</a></h1><p>Pierre Bayard&#xA0;is a professor of French literature at the University of Paris VIII and a psychoanalyst. He is the author of&#xA0;<em>Who Killed Roger Ackroyd?</em>, and many other books.</p>",Callmesalticidae,callmesalticidae,Callmesalticidae,
SCuTkCfoKbf9ShyaN,What's the best overview of common Micromorts?,what-s-the-best-overview-of-common-micromorts,https://www.lesswrong.com/posts/SCuTkCfoKbf9ShyaN/what-s-the-best-overview-of-common-micromorts,2020-09-03T02:39:42.002Z,37,17,8,False,True,,"<p>I want to get generally oriented on how various common risks compare against each other. I've seen some of this come up in recent Covid discussion, but I'm interested in a good article that's like ""Here's all the most dangerous stuff it's likely that you do, and here's how it breaks down for various sub-activities.""</p><p>This question triggered by ""the first few google results not being that good.""</p>",Raemon,raemon,Raemon,
RrHTmnLqcGg9Cfb2r,"Sunday September 6, 12pm (PT) — Casual hanging out with the LessWrong community",sunday-september-6-12pm-pt-casual-hanging-out-with-the,https://www.lesswrong.com/posts/RrHTmnLqcGg9Cfb2r/sunday-september-6-12pm-pt-casual-hanging-out-with-the,2020-09-03T02:08:25.687Z,34,11,10,False,False,,"<p>This Sunday at 12pm (PDT), we will have another one of our weekly online meetups. This time we will simply hangout without much of an introduction. In the future we will be back with our curated talks, but my guess is having a more casual hangout meetup is actually something I would be more excited about this week.</p><p><i>If you're a curated author and interested in giving a 5-min talk at a future event, which will then be transcribed and edited, sign up </i><a href=""https://forms.gle/iwFatbhys9muPmQA7""><i>here</i></a><i>.</i></p><h2>Gather.town</h2><p>We will be meeting in <a href=""https://gather.town/hwEYAKZHNxIG2Hgo/lesswrong"">gather.town</a>. We will also have a backup Zoom room here, in case Gather Town breaks for anyone, which Charlie Steiner has conveniently volunteered to moderate (thank you!): <a href=""https://illinois.zoom.us/j/94331893306?pwd=SitHMW93ZlJpd01oOHdjTUtYVWQydz09"">https://illinois.zoom.us/j/94331893306?pwd=SitHMW93ZlJpd01oOHdjTUtYVWQydz09</a></p><h2>Details</h2><p><strong>When?</strong> Sunday September 6, 12pm (PT)</p><p><strong>Where? </strong><a href=""https://gather.town/hwEYAKZHNxIG2Hgo/lesswrong?fbclid=IwAR2Fi3KqKSgQxp5Po_34bVNkNLC9mDk4ESHWGMRYC5tsms8JmsHZrTqprfw"">https://gather.town/hwEYAKZHNxIG2Hgo/lesswrong</a></p><p><strong>Backup Zoom Room: </strong><a href=""https://illinois.zoom.us/j/94331893306?pwd=SitHMW93ZlJpd01oOHdjTUtYVWQydz09"">https://illinois.zoom.us/j/94331893306?pwd=SitHMW93ZlJpd01oOHdjTUtYVWQydz09</a></p>",habryka4,habryka4,habryka,
639swnxzb8YLGbcoR,Study Group for Progress – 50% off for LessWrongers,study-group-for-progress-50-off-for-lesswrongers,https://www.lesswrong.com/posts/639swnxzb8YLGbcoR/study-group-for-progress-50-off-for-lesswrongers,2020-09-03T00:17:10.030Z,14,4,2,False,False,,"<p>Recently I <a href=""https://www.lesswrong.com/posts/RbK3GqT3uGxdghRfR/announcing-the-study-group-for-progress"">announced the Study Group for Progress</a>: a weekly discussion/Q&amp;A on the history, economics and philosophy of progress, with featured guests including Robert Gordon (<i>Rise &amp; Fall of American Growth), </i>Margaret Jacob, and Richard Nelson.</p><p>LessWrong members can now <a href=""https://highergroundeducation.formstack.com/forms/progress_studies_enrollment_adult_student_50_percent_discount"">get a 50% discount on registration by using this link</a>. Register by Monday, September 7.</p>",jasoncrawford,jasoncrawford,jasoncrawford,
bevquxoYwkMx3NK6L,[AN #115]: AI safety research problems in the AI-GA framework,an-115-ai-safety-research-problems-in-the-ai-ga-framework,https://www.lesswrong.com/posts/bevquxoYwkMx3NK6L/an-115-ai-safety-research-problems-in-the-ai-ga-framework,2020-09-02T17:10:04.434Z,19,6,16,False,False,,"<p>Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world</p><p><strong>Newsletter #115</strong></p><p>Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter <strong><a href=""http://rohinshah.com/alignment-newsletter/"">resources here</a></strong>. In particular, you can look through <strong><a href=""https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing"">this spreadsheet</a></strong> of all summaries that have ever been in the newsletter.</p><p><em>Audio version <strong><a href=""http://alignment-newsletter.libsyn.com/alignment-newsletter-115"">here</a></strong> (may not be up yet). 			  </em></p><h1>SECTIONS</h1><p><strong><a href=""about:blank#HIGHLIGHTS"">HIGHLIGHTS</a></strong></p><p><strong><a href=""about:blank#TECHNICAL_AI_ALIGNMENT"">TECHNICAL AI ALIGNMENT</a></strong></p><p><strong><a href=""about:blank#PROBLEMS"">PROBLEMS</a></strong></p><p><strong><a href=""about:blank#FORECASTING"">FORECASTING</a></strong></p><p><strong><a href=""about:blank#MISCELLANEOUS_(ALIGNMENT)"">MISCELLANEOUS (ALIGNMENT)</a></strong></p><p><strong><a href=""about:blank#AI_STRATEGY_AND_POLICY"">AI STRATEGY AND POLICY</a></strong></p><p><strong><a href=""about:blank#OTHER_PROGRESS_IN_AI"">OTHER PROGRESS IN AI</a></strong></p><p><strong><a href=""about:blank#REINFORCEMENT_LEARNING"">REINFORCEMENT LEARNING</a></strong></p><p><strong><a href=""about:blank#NEWS"">NEWS</a></strong></p><h1>HIGHLIGHTS </h1><p><strong><a href=""https://arxiv.org/abs/2006.07495"">Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity</a></strong> <em>(Adrien Ecoffet et al)</em> (summarized by Rohin): One potential pathway to powerful AI is through <em>open-ended search</em>, in which we use search algorithms to search for good architectures, learning algorithms, environments, etc. in addition to using them to find parameters for a particular architecture. See the <strong><a href=""https://arxiv.org/abs/1905.10985"">AI-GA paradigm</a></strong> (<strong><a href=""https://mailchi.mp/533c646a4b21/an-63how-architecture-search-meta-learning-and-environment-design-could-lead-to-general-intelligence"">AN #63</a></strong>) for more details. What do AI safety issues look like in such a paradigm?</p><p>Building on <strong><a href=""https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1"">DeepMind&#x2019;s framework</a></strong> (<strong><a href=""https://mailchi.mp/1ecd1b775703/alignment-newsletter-26"">AN #26</a></strong>), the paper considers three levels of objectives: the ideal objective (what the designer intends), the explicit incentives (what the designer writes down), and the agent incentives (what the agent actually optimizes for). Safety issues can arise through differences between any of these levels.</p><p>The main difference that arises when considering open-ended search is that it&#x2019;s much less clear to what extent we can control the result of an open-ended search, even if we knew what result we wanted. We can get evidence about this from existing complex systems, though unfortunately there are not any straightforward conclusions: several instances of convergent evolution might suggest that the results of the open-ended search run by evolution were predictable, but on the other hand, the effects of <em>intervening</em> on complex ecosystems are notoriously hard to predict.</p><p>Besides learning from existing complex systems, we can also empirically study the properties of open-ended search algorithms that we implement in computers. For example, we could run search for some time, and then fork the search into independent replicate runs with different random seeds, and see to what extent the results converge. We might also try to improve controllability by using meta learning to infer what learning algorithms, environments, or explicit incentives help induce controllability of the search.</p><p>The remaining suggestions will be familiar to most readers: they suggest work on interpretability (that now has to work with <em>learned</em> architectures), better benchmarks, human-in-the-loop search, safe exploration, and sim-to-real transfer.</p><p><strong>Rohin&apos;s opinion:</strong> I&#x2019;m glad that people are paying attention to safety in this AGI paradigm, and the problems they outline seem like reasonable problems to work on. I actually expect that the work needed for the open-ended search paradigm will end up looking very similar to the work needed by the &#x201C;AGI via deep RL&#x201D; paradigm: the differences I see are differences in difficulty, not differences in what problems qualitatively need to be solved. I&#x2019;m particularly excited by the suggestion of studying how particular environments can help control the result of the open-ended search: it seems like even with deep RL based AGI, we would like to know how properties of the environment can influence properties of agents trained in that environment. For example, what property must an environment satisfy in order for agents trained in that environment to be risk-averse?</p><h1>TECHNICAL AI ALIGNMENT </h1><h2>PROBLEMS </h2><p><strong><a href=""https://www.alignmentforum.org/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1"">Model splintering: moving from one imperfect model to another</a></strong> <em>(Stuart Armstrong)</em> (summarized by Rohin): This post introduces the concept of <em>model splintering</em>, which seems to be an overarching problem underlying many other problems in AI safety. This is one way of more formally looking at the out-of-distribution problem in machine learning: instead of simply saying that we are out of distribution, we look at the model that the AI previously had, and see what model it transitions to in the new distribution, and analyze this transition.</p><p>Model splintering in particular refers to the phenomenon where a coarse-grained model is &#x201C;splintered&#x201D; into a more fine-grained model, with a one-to-many mapping between the environments that the coarse-grained model can distinguish between and the environments that the fine-grained model can distinguish between (this is what it means to be more fine-grained). For example, we may initially model all gases as ideal gases, defined by their pressure, volume and temperature. However, as we learn more, we may transition to the van der Waal&#x2019;s equations, which apply differently to different types of gases, and so an environment like &#x201C;1 liter of gas at standard temperature and pressure (STP)&#x201D; now splinters into &#x201C;1 liter of nitrogen at STP&#x201D;, &#x201C;1 liter of oxygen at STP&#x201D;, etc.</p><p>Model splintering can also apply to reward functions: for example, in the past people might have had a reward function with a term for &#x201C;honor&#x201D;, but at this point the &#x201C;honor&#x201D; concept has splintered into several more specific ideas, and it is not clear how a reward for &#x201C;honor&#x201D; should generalize to these new concepts.</p><p>The hope is that by analyzing splintering and detecting when it happens, we can solve a whole host of problems. For example, we can use this as a way to detect if we are out of distribution. The full post lists several other examples.</p><p><strong>Rohin&apos;s opinion:</strong> I think that the problems of generalization and ambiguity out of distribution are extremely important and fundamental to AI alignment, so I&#x2019;m glad to see work on them. It seems like model splintering could be a fruitful approach for those looking to take a more formal approach to these problems.</p><p><strong><a href=""https://www.garymcgraw.com/wp-content/uploads/2020/02/BIML-ARA.pdf"">An Architectural Risk Analysis of Machine Learning Systems: Towards More Secure Machine Learning</a></strong> <em>(Gary McGraw et al)</em> (summarized by Rohin) (H/T Catherine Olsson): One systematic way of identifying potential issues in a system is to perform an <em>architectural risk analysis</em>, in which you draw an architecture diagram showing the various components of the system and how they interact, and then think about each component and interaction and how it could go wrong. (<strong><a href=""https://www.tomeveritt.se/papers/alignment.pdf"">Last week&#x2019;s highlight</a></strong> (AN #114) did this for Bayesian history-based RL agents.) This paper performs an architectural risk analysis for a generic ML system, resulting in a systematic list of potential problems that could occur.</p><p><strong>Rohin&apos;s opinion:</strong> As far as I could tell, the problems identified were ones that we had seen before, but I&#x2019;m glad someone has gone through the more systematic exercise, and the resulting list is more organized and easier to understand than previous lists.</p><h2>FORECASTING </h2><p><strong><a href=""https://www.alignmentforum.org/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines"">Forecasting Thread: AI Timelines</a></strong> <em>(Amanda Ngo et al)</em> (summarized by Rohin): This post collects forecasts of timelines until human-level AGI, and (at the time of this writing) has twelve such forecasts.</p><p><strong><a href=""http://dmip.webs.upv.es/EPAI2020/papers/EPAI_2020_paper_11.pdf?fbclid=IwAR15Z0CMX4rBBUJEHhn6NdcMK2ZCF07pPpkcmfD36_oEI9WhV310bRkbaiQ"">Roadmap to a Roadmap: How Could We Tell When AGI is a &#x2018;Manhattan Project&#x2019; Away? </a></strong> <em>(John-Clark Levin et al)</em> (summarized by Rohin): The key hypothesis of this paper is that once there is a clear &#x201C;roadmap&#x201D; or &#x201C;runway&#x201D; to AGI, it is likely that state actors could invest a large number of resources into achieving it, comparably to the Manhattan project. The fact that we do not see signs of such investment now does not imply that it won&#x2019;t happen in the future: currently, there is so little &#x201C;surface area&#x201D; on the problem of AGI that throwing vast amounts of money at the problem is unlikely to help much.</p><p>If this were true, then once such a runway is visible, incentives could change quite sharply: in particular, the current norms of openness may quickly change to norms of secrecy, as nations compete (or perceive themselves to be competing) with other nations to build AGI first. As a result, it would be good to have a good measure of whether we have reached the point where such a runway exists.</p><p><strong>Read more:</strong> <strong><a href=""https://jack-clark.net/2020/08/31/import-ai-212-robots-are-getting-smart-humansmachines-trouble-says-dhs-a-10k-product-dataset/"">Import AI summary</a></strong></p><h2>MISCELLANEOUS (ALIGNMENT) </h2><p><strong><a href=""https://montrealethics.ai/wp-content/uploads/2020/06/State-of-AI-Ethics-June-2020-report.pdf"">State of AI Ethics</a></strong> <em>(Abhishek Gupta et al)</em> (summarized by Rohin): This report from the Montreal AI Ethics Institute has a wide variety of summaries on many different topics in AI ethics, quite similarly to this newsletter in fact.</p><h1>AI STRATEGY AND POLICY </h1><p><strong><a href=""https://cltc.berkeley.edu/wp-content/uploads/2020/05/Decision_Points_AI_Governance.pdf"">Decision Points in AI Governance</a></strong> <em>(Jessica Cussins Newman)</em> (summarized by Rohin): While the last couple of years have seen a proliferation of &#x201C;principles&#x201D; for the implementation of AI systems in the real world, we are only now getting to the stage in which we turn these principles into practice. During this period, <em>decision points</em> are concrete actions taken by some AI stakeholder with the goal of shaping the development and use of AI. (These actions should not be predetermined by existing law and practice.) Decision points are the actions that will have a disproportionately large influence on the field, and thus are important to analyze. This paper analyzes three case studies of decision points, and draws lessons for future decision points.</p><p>First, we have the Microsoft AETHER committee. Like many other companies, Microsoft has established a committee to help the company make responsible choices about its use of AI. Unlike e.g. <strong><a href=""https://www.vox.com/future-perfect/2019/4/4/18295933/google-cancels-ai-ethics-board"">Google&#x2019;s AI ethics board</a></strong>, this committee has actually had an impact on Microsoft&#x2019;s decisions, and has published several papers on AI governance along the way. The committee attributes its success in part to executive-level support, regular opportunities for employee and expert engagement, and integration with the company&#x2019;s legal team.</p><p>Second, we have the <strong><a href=""https://blog.openai.com/better-language-models/"">GPT-2</a></strong> (<strong><a href=""https://mailchi.mp/c48f996a5db5/alignment-newsletter-46"">AN #46</a></strong>) staged release process. We&#x2019;ve <strong><a href=""https://medium.com/@NPCollapse/the-hacker-learns-to-trust-62f3c1490f51"">covered</a></strong> (<strong><a href=""https://mailchi.mp/92b3a9458c2d/an-58-mesa-optimization-what-it-is-and-why-we-should-care"">AN #58</a></strong>) <strong><a href=""https://www.partnershiponai.org/when-is-it-appropriate-to-publish-high-stakes-ai-research/"">this</a></strong> (<strong><a href=""https://mailchi.mp/405e29e1f1cd/an-55-regulatory-markets-and-international-standards-as-a-means-of-ensuring-beneficial-ai"">AN #55</a></strong>) <strong><a href=""https://rowanzellers.com/grover/"">before</a></strong> (<strong><a href=""https://mailchi.mp/92b3a9458c2d/an-58-mesa-optimization-what-it-is-and-why-we-should-care"">AN #58</a></strong>), so I won&#x2019;t retell the story here. However, this shows how a deviation from the norm (of always publishing) can lead to a large discussion about what publication norms are actually appropriate, leading to large changes in the field as a whole.</p><p>Finally, we have the OECD AI Policy Observatory, a resource that has been established to help countries implement the OECD AI principles. The author emphasizes that it was quite impressive for the AI principles to even get the support that they did, given the rhetoric about countries competing on AI. Now, as the AI principles have to be put into practice, the observatory provides several resources for countries that should help in ensuring that implementation actually happens.</p><p><strong>Read more:</strong> <strong><a href=""https://montrealethics.ai/research-summary-decision-points-in-ai-governance/"">MAIEI summary</a></strong></p><h1>OTHER PROGRESS IN AI </h1><br><h2>REINFORCEMENT LEARNING </h2><p><strong><a href=""https://arxiv.org/abs/2007.13544"">Combining Deep Reinforcement Learning and Search for Imperfect-Information Games</a></strong> <em>(Noam Brown, Anton Bakhtin et al)</em> (summarized by Rohin): <strong><a href=""https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/"">AlphaZero</a></strong> (<strong><a href=""https://mailchi.mp/6751e45fbb48/alignment-newsletter-36"">AN #36</a></strong>) and its predecessors have achieved impressive results in zero-sum two-player perfect-information games, by using a combination of search (MCTS) and RL. This paper provides the first combination of search and deep RL for <em>imperfect-information</em> games like poker. (Prior work like <strong><a href=""https://science.sciencemag.org/content/early/2019/07/10/science.aay2400.full"">Pluribus</a></strong> (<strong><a href=""https://mailchi.mp/49c956f84771/an-74separating-beneficial-ai-into-competence-alignment-and-coping-with-impacts"">AN #74</a></strong>) did use search, but didn&#x2019;t combine it with deep RL, instead relying on significant expert information about poker.)</p><p>The key idea that makes AlphaZero work is that we can estimate the value of a state independently of other states without any interaction effects. For any given state s, we can simulate possible future rollouts of the game, and propagate the values of the resulting new states back up to s. In contrast, for imperfect information games, this approach does not work since you cannot estimate the value of a state independently of the policy you used to get to that state. The solution is to instead estimate values for <em>public belief states</em>, which capture the public common knowledge that all players have. Once this is done, it is possible to once again use the strategy of backing up values from simulated future states to the current state, and to train a value network and policy network based on this.</p><h1>NEWS </h1><p><strong><a href=""https://www.fhi.ox.ac.uk/ai-governance-project-manager/"">AI Governance Project Manager</a></strong> <em>(Markus Anderljung)</em> (summarized by Rohin): The Centre for the Governance of AI is hiring for a project manager role. The deadline to apply is September 30.</p><h4><strong>FEEDBACK</strong></h4><p> I&apos;m always happy to hear feedback; you can send it to me, <strong><a href=""https://rohinshah.com/"">Rohin Shah</a></strong>, by <strong>replying to this email</strong>.                         </p><h4><strong>PODCAST</strong></h4><p>An audio podcast version of the <strong>Alignment Newsletter</strong> is available. This podcast is an audio version of the newsletter, recorded by <strong><a href=""http://robertskmiles.com/"">Robert Miles</a></strong>.</p>",rohinmshah,rohinmshah,Rohin Shah,
GAuFkfib3HirgTbEF,"If there were an interactive software teaching Yudkowskian rationality, what concepts would you want to see it teach?",if-there-were-an-interactive-software-teaching-yudkowskian,https://www.lesswrong.com/posts/GAuFkfib3HirgTbEF/if-there-were-an-interactive-software-teaching-yudkowskian,2020-09-02T05:37:08.758Z,25,18,20,False,True,,"<p>I've been noticing some complaints (such as <a href=""https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/ricraz-s-shortform?commentId=75gg3Nzn6KMst8eNt"">this post by Richard Ngo</a>) lately about the quality of the modern LW community's contribution to the big picture of humanity's knowledge.</p>
<p>Ideally, if it were the case that reading something automatically made you internalize deeply everything it said, then just by having a group of people who have read The Sequences, you'd have a superteam of intellectuals. And while I do think LW is a pretty cool group of smart thinkers, that isn't fully the case- just reading The Sequences isn't enough. To really internalize the lessons that one must learn, one must apply the principles, push against the problem, see where their understanding needs improvement, and where they are good enough.</p>
<p>The simplest form of this is having a high-quality Anki deck that tests users on the principles, both by testing recall of the stated principle itself, and even more importantly, giving them test cases where they can apply the principles (in the same vein as Ankifying medium-difficulty multiplication problems).  I have seen some rationality-themed Anki decks, but many of the cards are poorly formatted (both esthetically and in terms of learnability), and are also poorly curated. Ideally, if there were to be an Anki deck, it would be well formatted, and the cards would be carefully chosen to maximize quality of information.</p>
<p>Another idea that I've been thinking about is making explorables, a la <a href=""https://ncase.me/"">Nicky Case</a>, that would introduce important rationality concepts. This would have the advantage of providing more flexibility in experience than Anki, but also would sacrifice the benefits of having already implemented SRS.</p>
<p>My question is: if there were to be either an Anki deck or an explorable teaching concepts from The Sequences, targeted primarily as an aide for current LW users, but also as an introduction aimed at the public at large, what concepts from The Sequences would you most want to see covered?</p>
",mikkel-wilson,mikkel-wilson,MikkW,
8NSKMMDXS8gjFHfQa,The Box Spread Trick: Get rich slightly faster,the-box-spread-trick-get-rich-slightly-faster,https://www.lesswrong.com/posts/8NSKMMDXS8gjFHfQa/the-box-spread-trick-get-rich-slightly-faster,2020-09-01T21:41:50.143Z,43,23,44,False,False,,"<p><em>Disclaimer: This is not financial advice. Also, following these steps slightly wrong could result in you losing 100% of your savings. I have listed what I think are the most likely pitfalls, but still, beware.</em></p>
<p><em>Thanks to gilch and Mark Xu for helping edit this post, and Wei Dai for the original idea.</em></p>
<h2>Summary</h2>
<p>If you have a non-IRA brokerage account, you can borrow money on the open market at 0.55% and put it in a certificate of deposit (CD) yielding at least 1.25%. After taxes and commissions, this increases annual returns on your entire portfolio by 0.2-0.5%, nearly risk-free. This opportunity could continue for several years.</p>
<p>As this is $1000-2500 a year on a $500,000 portfolio, it's probably worth your time to set up if you're mid- to late-career, especially if you already do things like <a href=""https://reddit.com/r/churning"">credit card churning</a>. The administrative work includes making one trade every 1-3 years and a small amount of extra tax work.</p>
<h2>Requirements</h2>
<p>Before actually performing the trick, there are a few things you need:</p>
<ul>
<li>A taxable (not IRA/401k) investment portfolio worth more than about $100k.</li>
<li>A brokerage account with portfolio margin and options approval. Portfolio margin is not strictly necessary but increases the return. I suggest using Interactive Brokers or TD Ameritrade.<sup class=""footnote-ref""><a href=""#fn-Z4jfJQXEn4od5Auc6-1"" id=""fnref-Z4jfJQXEn4od5Auc6-1"">[1]</a></sup></li>
<li>An account at an FDIC-insured bank (or NCUA-insured credit union) with a high-yield CD or savings account. You can find a list of such banks at <a href=""https://www.depositaccounts.com/cd/3-year-cd-rates.html"">depositaccounts.com</a>, and approval is fairly quick. FDIC/NCUA insurance is limited to $250,000 per individual per bank, so if your portolio is over $500k you might need multiple banks.<sup class=""footnote-ref""><a href=""#fn-Z4jfJQXEn4od5Auc6-2"" id=""fnref-Z4jfJQXEn4od5Auc6-2"">[2]</a></sup></li>
</ul>
<p>In total, the setup took me about 50 hours, but this involved lots of trial and error and double-checking; it can be done in 10 hours.</p>
<h2>The Box Spread Trick</h2>
<p>As of September 2020, some banks offer a 3-year CD yield of 1.25%, while the Treasury yield is only 0.2%. If you could borrow at the same rate as the US Treasury, you could make 1.05% per year risk-free. Sadly, you can't do this conventionally: mortgage rates are upwards of 2%, and most brokers charge upwards of 3% on margin loans. But you <em>should</em> be able to, because you have collateral in the form of a stock portfolio. This is where the box spread comes in.</p>
<p>A box spread is a combination of four options which cancel each other out so there is no risk from market movements.<sup class=""footnote-ref""><a href=""#fn-Z4jfJQXEn4od5Auc6-3"" id=""fnref-Z4jfJQXEn4od5Auc6-3"">[3]</a></sup> This allows institutions to lend each other large sums of money: one party sells the box spread for a premium, and pays the loan back on the exercise date up to 3 years later. As a retail investor, you can't withdraw the cash from this loan, but you can take out a margin loan from your broker which is financed by this box spread. By using box spread financing, you pay market rates (typically treasury yields plus ~0.3%) rather than the broker's 3-10% margin rates.</p>
<p>To perform the trick, sell SPX box spreads in a total amount that's 30-65% of the value of your account depending on how frequently you want to monitor it. The exercise date should be as far in the future as possible, currently 2-3 years.<sup class=""footnote-ref""><a href=""#fn-Z4jfJQXEn4od5Auc6-4"" id=""fnref-Z4jfJQXEn4od5Auc6-4"">[4]</a></sup> Each box spread is worth 100x the spread width; i.e. $10,000 for a 2900/3000 box. The legs should be 100-300 points apart to minimize commissions cost, and the upper strike price should be near-the-money (near the current value of the SPX) to maximize liquidity.<sup class=""footnote-ref""><a href=""#fn-Z4jfJQXEn4od5Auc6-5"" id=""fnref-Z4jfJQXEn4od5Auc6-5"">[5]</a></sup> In current market conditions, the trade looks like this for a $250,000 portfolio; it's a loan with $150,000 due in December 2023.</p>
<p><code>SELL 5 DEC 15 2023 3000/3300 SPX BOX</code></p>
<p>Look up the current 2- or 3-year Treasury yield <a href=""https://www.treasury.gov/resource-center/data-chart-center/interest-rates/pages/textview.aspx?data=yield"">here</a>, add 0.29%, and calculate the price, e.g. using this <a href=""https://docs.google.com/spreadsheets/d/14DL5Onscnwz0W1Ns-mQBMAZOMnuV3mMwHuFK-2lawxw/edit?usp=sharing"">spreadsheet</a>. Enter a limit order for that price.</p>
<p>WARNING: Don't trust a spreadsheet someone on the internet made. Double-check the math yourself. Also, make sure that:</p>
<ul>
<li>You're using a LIMIT order (not market). Bid-ask spreads are absurdly wide.</li>
<li>You typed in the price correctly.</li>
<li>You're selling European-style options, e.g. SPX. This prevents early exercise risk.</li>
<li>You're selling the box, not buying it. (An order to sell a 3000/3300 box is equivalent to buying a 3300/3000 box; this is fine. The preview price should be negative, or say CREDIT).</li>
<li>You're doing a single trade, not four separate trades.</li>
<li>When previewing the order, the profit graph is a perfectly flat horizontal line, indicating that the options cancel out.</li>
</ul>
<p>Execute the trade, and wait for it to be filled. If it isn't filled within a day or two, slowly walk up the price until it is. (I was filled at 0.32% above the treasury yield.) You may get a slightly better price or fill time by direct-routing to an exchange rather than using your broker's ""smart routine"". Once the order is filled, deposit the money into your bank account/CD. Your brokerage account should now have a cash balance near zero.</p>
<p>If the value of your investments declines enough during the 3-year period, you could be subject to a margin call and have your investments liquidated. Before this happens, withdraw some money from the CD (paying a small penalty, typically the last six months' interest), and redeposit it into your brokerage account.</p>
<h2>Example</h2>
<p>Let's say you have $350,000 in a Vanguard brokerage account, which is invested in mostly broad-based indices with some narrow-based indices, say mostly VT with some VWOB.<sup class=""footnote-ref""><a href=""#fn-Z4jfJQXEn4od5Auc6-6"" id=""fnref-Z4jfJQXEn4od5Auc6-6"">[6]</a></sup> You get an Interactive Brokers account with options trading and portfolio margin, and pay a $50 fee to transfer your assets.</p>
<p><code>SELL 10 DEC 15 2023 3200/3400 SPX BOX</code></p>
<p>Say the Treasury yield on September 16, 2020 is 0.17%; you target a rate of 0.46%, and calculate the price. You make the trade above for $197,000 (0.471% APY); after a $26 commission, you get a $196,974 credit. The loan is due in 3 years for $200,000. You have already set up an account in a credit union with a 1.25% Jumbo 3 Year CD rate, so you withdraw the $196,974 from IB and use it to create a CD maturing in December 2023.</p>
<p>The liquidation value of your IB account is now $150,000 and your exposure is $350,000, a 43% ratio, comfortably above the 15% requirement. If the market declines by 28%, you could be subject to liquidation, so you set up a phone alert on the IB app.</p>
<p>In December 2023, the options are about to expire, so you buy them back or let them be exercised for $200,000. By then the CD has accumulated $8,164 in interest, which is $6,182 after-tax (your tax bracket is 24%) while the box spread has lost $3,026; this is $3,134 in profit. You then sell another box spread expiring in 2026, repeating the process.</p>
<h2>FAQ</h2>
<p><em>Is this really risk-free?</em></p>
<p>I'm moderately confident that the only major risks are (a) doing the setup wrong, and (b) having your options partially liquidated if the market plummets by 30%+ before you notice. Having part of a box spread liquidated is bad for two reasons: first, bid-ask spreads are wide, so you'll be liquidated at unfavorable prices, and second, holding part of a box spread is extremely risky, often equivalent to &gt;20x leverage. You can limit this risk by limiting the size of the loan or using Interactive Brokers' ""liquidate last"" feature. You can eliminate it entirely by buying put options, though these are insurance and cut into your profit.</p>
<p><em>Why does such an arbitrage opportunity exist?</em></p>
<p>In short, people who put their money in savings accounts or CDs are less likely to withdraw it. Banks therefore give you higher rates. Wei Dai, who posted a comment outlining the trick in April, <a href=""https://www.lesswrong.com/posts/RWna48W4bPr2ChePz/tips-tricks-notes-on-optimizing-investments?commentId=8P95Sd7q3HBAftMcz"">wrote</a> that this effectively amounts to an FDIC subsidy. It's not quite free though, since you are giving up liquidity.</p>
<p>CD rates are currently 1% higher than treasury yields. This is <a href=""https://fred.stlouisfed.org/graph/?g=uYO7"">higher than historical averages</a>, but the gap previously stayed around 1% between 2009 and 2013. Because FDIC insurance on CDs is limited to $250,000 per bank per individual, a 1% gap probably can't be exploited at scale by institutions. The excess return could continue until market forces cause the gap to shrink, or until a large percentage of the money in high-yield CDs and savings accounts is from people using the box spread trick.</p>
<p><em>What if I pick stocks/own crypto/use <a href=""https://www.wealthfront.com/investing"">Wealthfront</a>/don't have portfolio margin?</em></p>
<p>The portfolio margin requirement for narrow indices and individual stocks is 15% rather than 10%. If you don't have portfolio margin, the Reg-T margin requirement is 25%. For cryptocurrency, no margin whatsoever is allowed. Actual margin requirements are often higher in volatile market conditions; sometimes brokers also have higher ""house"" margin requirements. Use your broker's tool to adjust the loan amount according to your margin requirement.</p>
<p>Passive services like Wealthfront don't support trading options, and so don't support the box spread trick.</p>
<p><em>Tax consequences?</em></p>
<p>I'm not a tax professional, but I think LEAPS (the long-term index options we're using) are <a href=""https://greentradertax.com/trader-tax-center/tax-treatment/section-1256-contracts/"">Section 1256 contracts</a> taxed as 60% long-term and 40% short-term capital losses. Interest from a savings account or CD is taxed as ordinary income, at up to 37%. The value of the box spread trick should decrease in higher tax brackets.</p>
<p><em>What if I'm outside the US?</em></p>
<p>The US has one of the strongest <a href=""https://en.wikipedia.org/wiki/Deposit_insurance"">deposit insurance</a> systems in the world. I haven't seen non-US banks with similarly strong insurance and high CD/savings yields.</p>
<hr class=""footnotes-sep"">
<section class=""footnotes"">
<ol class=""footnotes-list"">
<li id=""fn-Z4jfJQXEn4od5Auc6-1"" class=""footnote-item""><p>Interactive Brokers gives you options and PM approval on any account above $125k by checking some boxes. Rather than give a margin call, they <a href=""https://www.interactivebrokers.com/en/index.php?f=1232"">immediately liquidate</a> on a margin violation, but they offer a ""liquidate last"" feature so your options are not liquidated. They also offer lower margin rates, but this is not impactful because you don't use the margin loan. However, their software is outdated and less user-friendly. TD Ameritrade has better software and (as of 2020) also has a bonus for transferring your assets there, but requires that you pass a written test on options. Other brokers might work (leave a comment if they do), but I've heard that Robinhood doesn't let you trade box spreads. <a href=""#fnref-Z4jfJQXEn4od5Auc6-1"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-Z4jfJQXEn4od5Auc6-2"" class=""footnote-item""><p>NCUA and FDIC are both backed by the full faith and credit of the US government, so the risk of bank failures is small. If you have a spouse, you can get one account for each of you, plus a $500,000 joint account; this lets you have $1M of total FDIC/NCUA insurance at just one bank. <a href=""#fnref-Z4jfJQXEn4od5Auc6-2"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-Z4jfJQXEn4od5Auc6-3"" class=""footnote-item""><p>The box spread is explained further at the links in <a href=""https://www.lesswrong.com/posts/RWna48W4bPr2ChePz/tips-tricks-notes-on-optimizing-investments?commentId=aEcusRn6aGtYcfoyn"">this comment</a>. <a href=""#fnref-Z4jfJQXEn4od5Auc6-3"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-Z4jfJQXEn4od5Auc6-4"" class=""footnote-item""><p>December options start trading in the September 3 years prior. <a href=""#fnref-Z4jfJQXEn4od5Auc6-4"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-Z4jfJQXEn4od5Auc6-5"" class=""footnote-item""><p>I have heard that institutions use a 1000/2000 box, but these can only be exchanged in increments of $100,000 and so could be a bit unwieldy. <a href=""#fnref-Z4jfJQXEn4od5Auc6-5"" class=""footnote-backref"">↩︎</a></p>
</li>
<li id=""fn-Z4jfJQXEn4od5Auc6-6"" class=""footnote-item""><p>This is a reasonable asset allocation if you mostly believe in EMH and want to approximate the global market portfolio but already own real estate and think Treasury yields are unreasonably low. <a href=""#fnref-Z4jfJQXEn4od5Auc6-6"" class=""footnote-backref"">↩︎</a></p>
</li>
</ol>
</section>
",thomas-kwa,thomas-kwa,Thomas Kwa,
Jrt6LZBR6i5vRkdBY,August 2020 gwern.net newsletter,august-2020-gwern-net-newsletter,https://www.lesswrong.com/posts/Jrt6LZBR6i5vRkdBY/august-2020-gwern-net-newsletter,2020-09-01T21:04:58.299Z,25,7,4,False,False,https://www.gwern.net/newsletter/2020/08,,gwern,gwern,gwern,
4wJMEau327sMFGCE3,Russian x-risks newsletter Summer 2020,russian-x-risks-newsletter-summer-2020,https://www.lesswrong.com/posts/4wJMEau327sMFGCE3/russian-x-risks-newsletter-summer-2020,2020-09-01T14:06:30.196Z,22,10,6,False,False,,"<p>Russian opposition leader Navalny was poisoned. I read <a href=""https://twitter.com/tanilbar/status/1297921336292016128"">rumours</a> that he was doing an investigation about the origins of coronavirus pandemic and its possible relation to the explosion in the Vector biolab Novosibirsk last fall. He <a href=""https://ngs.ru/text/politics/2020/08/15/69423568/"">visited</a> Novosibirsk 5 day before the poisoning. But others <a href=""https://www.rbc.ru/politics/31/08/2020/5f4d09fb9a79476892bd59b7?utm_source=yxnews&amp;utm_medium=desktop"">said</a> that he investigated a case of corruption there.</p><p>Meanwhile, there is an <a href=""https://edition.cnn.com/2020/08/16/europe/putin-lukashenko-belarus-analysis-intl/index.html"">uprising</a> against Belarussian leader Lukashenko. Putin can&#x2019;t afford to lose Belarussia for military and prestige reasons and he said he prepared a group of the military to help preserve order in Belarussia. Long story short, this may escalate in conflict with Western countries, which may not like Russian occupation of Belarus. </p><p>Russia was first in the world who <a href=""https://www.bloomberg.com/news/articles/2020-08-12/russia-defends-first-covid-19-vaccine-as-safe-amid-skepticism"">declared</a> that its coronavirus vaccine is registered. But actual production is very small and it will still be tested in phase 3 until 2021. Many people said that they don&#x2019;t believe it as its tested may be rigged and the results are not published. Anyway, the vaccine consists of two adenoviruses with fragments of coronavirus&#x2019; nucleotide. However, it was <a href=""https://www.bloomberg.com/news/articles/2020-07-20/russian-elite-got-experimental-covid-19-vaccine-from-april"">reported</a> that some rich people had access to the vaccine as early as in April. Almost no deaths from coronavirus between establishment except one person, former governor, who started act against Putin - files a legal case against his resignation. </p><p>Meanwhile, coronavirus pandemic smoothly declines in Russia. Maybe even too smoothly which makes the data statistically improbable. For example, Moscow has between 650 and 700 new cases a day for a long time. However, the numbers started to grow again at the end of August and people believe rumours that the next lockdown will be from 20 of September. </p><p>A nuclear submarine capable to carry six Doomsday weapons torpedoes &#x201C;Poseidon&#x201D; has been <a href=""https://www.gazeta.ru/army/2020/06/10/13113697.shtml"">launched</a>. </p><p>Russian EMP-guns can <a href=""https://iz.ru/1031696/2020-07-05/dalnost-boia-rossiiskikh-elektromagnitnykh-pushek-uvelichili-do-10-km"">now</a> fire 10 km.</p><p>There was a new <a href=""https://www.forbes.com/sites/hisutton/2020/07/01/russias-new-super-weapons-may-be-cause-of-radiation-leak/?fbclid=IwAR1TwaLpy8i9xoRmeT1V6IajXeyJY9Ml5i3yYo-oCt8lL_FjeDUrbQrzR6w#7e0382295f8c"">leak</a> of radioactivity somewhere, may be connected with tests of nuclear-powered cruise missile. </p><p>In summer, a state-connected Russian AI company started searching specialists for in NLP and transformers, probably inspired by GPT-3. </p><p>Extreme heat <a href=""https://lenta.ru/news/2020/06/08/hottest/"">hit</a> in Siberia and affected permafrost. Permafrost in Russia and Canada contains a lot of organics which is equal into 1.5 trillion tons of CO2  But permafrost could also emit methane which is an even stronger greenhouse gas. It still not <a href=""https://www.popsci.com/story/environment/permafrost-release-methane-debate/"">clear</a>, which gas will be produced as it depends on the activity of microorganisms. Popsci: &#x201C;Studies estimate that climate warming could release <a href=""https://theconversation.com/will-the-arctic-shift-from-a-carbon-sink-to-a-carbon-source-47826"">up to 15 percent</a> of the carbon stored in permafrost this century, so understanding this risk is important.&#x201D; </p><p>RAND <a href=""https://www.rand.org/pubs/research_reports/RR3063.html"">wrote</a> about Russian containment strategy.</p>",avturchin,avturchin,avturchin,
zjNRPgZAx8cgHgH9n,Anthropic Reasoning and Perspective-Based Arguments ,anthropic-reasoning-and-perspective-based-arguments,https://www.lesswrong.com/posts/zjNRPgZAx8cgHgH9n/anthropic-reasoning-and-perspective-based-arguments,2020-09-01T12:36:41.444Z,28,11,59,False,False,,"<p>This pandemic has allowed me some time to finish up my anthropic arguments. The complete argument is now accessible on my website, <a href=""https://www.sleepingbeautyproblem.com"">https://www.sleepingbeautyproblem.com</a></p>
<p>My approach is a Perspective-Based Argument (PBA). I think anthropic problems often end up paradoxical because a critical aspect of reasoning has been consistently overlooked. I propose perspectives are integral parts of reasoning. If an argument seems to be purely objective and does not appear to originate from any particular agent, that typically means it is formulated from a god's eye perspective, i.e. a view from nowhere.</p>
<p>PBA can be expressed as follows:</p>
<ol>
<li>
<p>Perspective and its center are primitively identified. It cannot be derived. Nor is it the outcome of any event. E.g. If Caesar ponders upon his perspective center and asks ""Why am I Caesar?"" Then there is no logical explanation to that question other than ""it just feels so"". The perspective center is a reasoning starting point. Very much like an axiom or a postulate.</p>
</li>
<li>
<p>Indexicals, such as ""I"", ""now"", and ""here"", are references to the perspective center. Each of them points to a different aspect of it. ""I"" refers to the agent at the perspective center; ""now"" the time; and ""here"" the location.</p>
</li>
<li>
<p>Due to their relationship with the perspective center, indexicals are logically unique. E.g. the concept of ""I"" and the concepts of other physical persons are incomparable. In plain language, it just means ""to me, I am inherently special.""</p>
</li>
<li>
<p>Indexicals' uniqueness is perspective-bounded. So the person referred to as ""I"" from my perspective is not inherently unique from your viewpoint. If we reason as an impartial outsider, i.e. with a god's eye view, then no particular person/time would be unique. Due to this indifference, an explanation is needed when attention is given to a particular person/time. That explanation would be, conceptually speaking, a sampling process.</p>
</li>
<li>
<p>Because perspectives are reasoning starting points, logics from different perspectives must not be mixed. It's like propositions from different axiomatic systems cannot be used together. A valid argument must be formulated from one consistent perspective.</p>
</li>
<li>
<p>Anthropic paradoxes treat all observers/moments with indifference, yet their arguments focus on ""I"" or ""now"" without any justification. The indifference is valid from a god's eye view while the special focus is valid from the first-person view. They are conflicting since they are based on different perspectives.</p>
</li>
<li>
<p>It might be tempting to try to validate this conflict. The easiest way would be to regard indexical such as ""I"" and ""now"" as the outcome of some fictional sampling process. That would lead to common approaches such as SSA and SIA (and FNC in a less obvious manner). However, that is unjustified as perspective centers are primitively identified. It is not the outcome of some event.</p>
</li>
<li>
<p>There is no valid reference class for indexicals. Subsequently, self-locating probabilities (the probabilities of indexicals being a particular member of some proposed reference class) are also invalid concepts. Examples include ""the probability that ""today"" is Monday/Tuesday"" in the sleeping beauty problem; ""the prior probability distribution of ""my"" birth rank among all humans "" in the doomsday argument, etc.</p>
</li>
<li>
<p>Perspective disagreement over probability while sharing all information can exist in anthropic problems. It happens when the probability's underlying meaning depends on the answerer's perspective. Just like the question of ""Am I a man?"". Its answer depends on who the responder is.</p>
</li>
</ol>
<p>Base on the above, PBA leads to the following conclusions:</p>
<ol>
<li>Double-halving in the Sleeping Beauty Problem without being unBayesian</li>
<li>The Doomsday Argument is false.</li>
<li>The presumptuous philosopher is wrong</li>
<li>In the Simulation Argument, the probability of me being simulated is an invalid notion</li>
<li>The idea of the fine-tuned universe is invalid.</li>
</ol>
<p>Obviously I am biased. However, I genuinely believe PBA starts with plausible postulates then explains all paradoxes in this field without ever being ad hoc. If anything, I hope people would pay more attention to other anthropic arguments besides common approaches as SSA, SIA, and FNC.</p>
",dadadarren,dadadarren,dadadarren,
hRsRgRcRk3zHLPpqm,Forecasting Newsletter: August 2020.,forecasting-newsletter-august-2020,https://www.lesswrong.com/posts/hRsRgRcRk3zHLPpqm/forecasting-newsletter-august-2020,2020-09-01T11:38:45.564Z,16,7,1,False,False,,"<h2>Highlights</h2>
<p>538 releases <a href=""https://projects.fivethirtyeight.com/2020-election-forecast/"">model</a> of the US elections; Trump predicted to win ~30% of the time.</p>
<p><a href=""https://link.springer.com/article/10.1007%2Fs10654-020-00669-6"">Study</a> offers instructive comparison of New York covid models, finds that for the IHME model, reported death counts fell inside the 95% prediction intervals only 53% of the time.</p>
<p>Biggest decentralized trial <a href=""https://blog.kleros.io/kleros-community-update-july-2020/#case-302-the-largest-decentralized-trial-of-all-time"">to date</a>, with 511 jurors asked to adjudicate a case coming from the Omen prediction market: ""Will there be a day with at least 1000 reported corona deaths in the US in the first 14 days of July?.""</p>
<h2>Index</h2>
<ul>
<li>Highlights</li>
<li>Prediction Markets &amp; Forecasting Platforms</li>
<li>In The News</li>
<li>Hard To Categorize</li>
<li>Long Content</li>
</ul>
<p>Sign up <a href=""https://forecasting.substack.com/"">here</a> or browse past newsletters <a href=""https://forum.effectivealtruism.org/s/HXtZvHqsKwtAYP6Y7"">here</a>.</p>
<h2>Prediction Markets &amp; Forecasting Platforms</h2>
<p>On <a href=""htps://predictit.org/"">PredictIt</a>, presidential election prices are close to <a href=""https://www.predictit.org/markets/detail/3698"">even odds</a>, with Biden at 55, and Trump at 48.</p>
<p>Good Judgement Inc. continues providing their <a href=""https://goodjudgment.io/covid-recovery/"">dashboard</a>, and the difference between the probability assigned by superforecasters to a Biden win (~75%), and those offered by <a href=""https://www.betfair.com/sport/politics"">betfair</a> (~55%) was enough to make it worth for me to place a small bet. At some point, Good Judgement Inc. and Cultivate Labs started a new platform on the domain <a href=""https://www.covidimpacts.com"">covidimpacts.com</a>, but forecasts there seem weaker than on Good Judgement Open.</p>
<p><a href=""https://www.replicationmarkets.com/"">Replication Markets</a> started their COVID-19 round, and created a page with COVID-19 <a href=""https://www.replicationmarkets.com/index.php/frequently-asked-questions/resources-for-forecasters/"">resources for forecasters</a>.</p>
<p>Nothing much to say about <a href=""https://www.metaculus.com/questions/"">Metaculus</a> this month, but I appreciated their previously existing list of <a href=""https://www.metaculus.com/help/prediction-resources/"">prediction resources</a>.</p>
<p><a href=""https://www.cset-foretell.com"">Foretell</a> has a <a href=""https://www.cset-foretell.com/blog"">blog</a>, and hosted a forecasting forum which discussed</p>
<ul>
<li>metrizicing the grand. That is, decomposing and operationalizing big picture questions into smaller ones, which can then be forecasted.</li>
<li>operationalizing these big picture questions might also help identify disagreements, which might then either be about the indicators, proxies or subquestions chosen, or about the probabilities given to the subquestions.</li>
<li>sometimes we can't measure what we care about, or we don't care about what we can measure.</li>
<li>one might be interested in questions about the future 3 to 7 years from now, but questions which ask about events 3 to 15 months in the future (which forecasting tournaments can predict better) can still provide useful signposts.</li>
</ul>
<p>Meanwhile, ethereum-based prediction markets such as Omen or Augur are experiencing difficulties because of the rise of decentralized finance (DeFi) and speculation and excitement about it. That speculation and excitement has increased the gas price (fees), such that making a casual prediction is for now too costly.</p>
<h2>In The News</h2>
<p><a href=""https://www.fastcompany.com/90532945/forecasting-the-future-of-philanthropy"">Forecasting the future of philanthropy</a>. The <a href=""https://en.wikipedia.org/wiki/American_Lebanese_Syrian_Associated_Charities"">American Lebanese Syrian Associated Charities</a>, the largest healthcare related charity in the United States, whose mission is to fund the <a href=""https://en.wikipedia.org/wiki/St._Jude_Children%27s_Research_Hospital"">St. Jude Children's Research Hospital</a>. To do this, they employ aggressive fundraising tactics, which have undergone modifications throughout the current pandemic.</p>
<p><a href=""https://blog.kleros.io/kleros-community-update-july-2020/#case-302-the-largest-decentralized-trial-of-all-time"">Case 302: the Largest Decentralized Trial of All Time</a>. Kleros is a decentralized dispute resolution platform. ""In July, Kleros had its largest trial ever where 511 jurors were drawn in the General Court to adjudicate a case coming from the Omen prediction market: Will there be a day with at least 1000 reported Corona death in the US in the first 14 days of July?."" <a href=""https://court.kleros.io/cases/302"">Link to the case</a></p>
<p><a href=""https://www.naturalgasintel.com/exxonmobil-slashing-permian-rig-count-forecasting-global-oil-glut-extending-well-into-2021/"">ExxonMobil Slashing Permian Rig Count, Forecasting Global Oil Glut Extending ‘Well into 2021’</a>. My own interpretation is that the gargantuan multinational's decision is an honest signal of an expected extended economic downturn.</p>
<blockquote>
<p>Supply is expected to exceed demand for months, “and we anticipate it will be well into 2021 before the overhang is cleared and we returned to pre-pandemic levels,” Senior Vice President Neil Chapman said Friday during a conference call.</p>
</blockquote>
<blockquote>
<p>“Simply put, the demand destruction in the second quarter was unprecedented in the history of modern oil markets. To put it in context, absolute demand fell to levels we haven’t seen in nearly 20 years. We’ve never seen a decline with this magnitude and pace before, even relative to the historic periods of demand volatility following the global financial crisis and as far back as the 1970s oil and energy crisis.”</p>
</blockquote>
<blockquote>
<p>Even so, ExxonMobil’s Permian rig count is to be sharply lower than it was a year ago. The company had more than 50 rigs running across its Texas-New Mexico stronghold as of last fall. At the end of June it was down to 30, “and we expect to cut that number by at least half again by the end of this year,” Chapman said.</p>
</blockquote>
<p><a href=""https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-is-releasing-the-covid-19-public-forecasts"">Google Cloud AI and Harvard Global Health Institute Collaborate on new COVID-19 forecasting model</a>.</p>
<p><a href=""https://smarkets.com/event/40554343/politics/uk/brexit/trade-deals"">Betting markets</a> put <a href=""https://sports.yahoo.com/betting-odds-put-ukeu-trade-deal-in-2020-at-66-095009521.html"">UK-EU trade deal in 2020 at 66%</a> (now 44%).</p>
<p><a href=""https://www.hindustantimes.com/mumbai-news/flood-forecasting-system-didn-t-help/story-mJanM39kxJPOvFma6TeqUM.html"">Experimental flood forecasting system didn’t help</a> in Mumbai. The system was to provide a three day advance warning, but didn't.</p>
<p>FiveThirtyEight covers various facets of the USA elections: <a href=""https://fivethirtyeight.com/features/biden-is-polling-better-than-clinton-at-her-peak/"">Biden Is Polling Better Than Clinton At Her Peak</a>, and releases <a href=""https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/"">their model</a>, along with some <a href=""https://fivethirtyeight.com/features/our-election-forecast-didnt-say-what-i-thought-it-would/"">comments about it</a></p>
<p>In other news, this newsletter reached 200 subscribers last week.</p>
<h2>Hard to Categorize</h2>
<p><a href=""https://en.wikipedia.org/wiki/Groundhog_Day"">Groundhog day</a> is a tradition in which American crowds pretend to believe that a small rat has oracular powers.</p>
<p><a href=""https://politicalpredictionmarkets.com/blog/"">Tips</a> for forecasting on PredictIt. These include betting against Trump voters who arrive at PredictIt from Breitbart.</p>
<p>Linch Zhang asks <a href=""https://forum.effectivealtruism.org/posts/SBbwzovWbghLJixPn/what-are-some-low-information-priors-that-you-find"">What are some low-information priors that you find practically useful for thinking about the world?</a></p>
<p><a href=""https://careers.astrazeneca.com/job/wilmington/forecasting-director-us-renal/7684/16951921"">AstraZeneca looking for a Forecasting Director</a> (US-based).</p>
<p><a href=""https://www.drivendata.org/competitions/63/genetic-engineering-attribution/"">Genetic Engineering Attribution Challenge</a>.</p>
<p>NSF-funded tournament looking to compare human forecasters with a random forest ML model from Johns Hopkins in terms of forecasting the success probability of cancer drug trials. More info <a href=""https://www.fandm.edu/magazine/magazine-issues/spring-summer-2020/spring-summer-2020-articles/2020/06/10/is-there-a-better-way-to-predict-the-future"">here</a>, and one can sign-up <a href=""https://www.pytho.io/human-forest"">here</a>. I've heard rewards are generous, but they don't seem to be specified on the webpage.  Kudos to Joshua Monrad.</p>
<p>Results of an <a href=""https://twitter.com/juan_cambeiro/status/1291153289879392257"">expert forecasting session</a> on covid, presented by expert forecaster Juan Cambeiro.</p>
<p>A playlist of <a href=""https://open.spotify.com/playlist/4LKES4QcFNozmwImjHWrBX?si=twuBPF-fSxejbpMwUToatg"">podcasts related to forecasting</a>. Kudos to Michał Dubrawski.</p>
<h2>Long Content</h2>
<p><a href=""https://link.springer.com/article/10.1007%2Fs10654-020-00669-6"">A case study in model failure? COVID-19 daily deaths and ICU bed utilization predictions in New York state</a> and commentary: <a href=""https://link.springer.com/article/10.1007/s10654-020-00667-8"">Individual model forecasts can be misleading, but together they are useful</a>.</p>
<blockquote>
<p>In this issue, Chin et al. compare the accuracy of four high profile models that, early during the outbreak in the US, aimed to make quantitative predictions about deaths and Intensive Care Unit (ICU) bed utilization in New York. They find that all four models, though different in approach, failed not only to accurately predict the number of deaths and ICU utilization but also to describe uncertainty appropriately, particularly during the critical early phase of the epidemic. While overcoming these methodological challenges is key, Chin et al. also call for systemic advances including improving data quality, evaluating forecasts in real-time before policy use, and developing multi-model approaches.</p>
</blockquote>
<blockquote>
<p>But what the model comparison by Chin et al. highlights is an important principle that many in the research community have understood for some time: that no single model should be used by policy makers to respond to a rapidly changing, highly uncertain epidemic, regardless of the institution or modeling group from which it comes. Due to the multiple uncertainties described above, even models using the same underlying data often have results that diverge because they have made different but reasonable assumptions about highly uncertain epidemiological parameters, and/or they use different methods</p>
</blockquote>
<blockquote>
<p>.. the rapid deployment of this approach requires pre-existing infrastructure and evaluation systems now and for improved response to future epidemics. Many models that are built to forecast on a scale useful for local decision making are complex, and can take considerable time to build and calibrate</p>
</blockquote>
<blockquote>
<p>a group with a history of successful influenza forecasting in the US (Los Alamos National Lab (4)) was able to produce early COVID-19 forecasts and had the best coverage of uncertainty in the Chin et al. analysis (80-100% of observations fell within the 95% prediction interval for most forecasts). In contrast, the new Institute for Health Metrics and Evaluation statistical approach had low reliability; after the latest analyzed revision only 53% of reported death counts fell with the 95% prediction intervals.</p>
</blockquote>
<blockquote>
<p>The original IHME model underestimates uncertainty and 45.7% of the predictions (over 1- to 14-step-ahead predictions) made over the period March 24 to March 31 are outside the 95% PIs. In the revised model, for forecasts from of April 3 to May 3 the uncertainty bounds are enlarged, and most predictions (74.0%) are within the 95% PIs, which is not surprising given the PIs are in the order of 300 to 2000 daily deaths. Yet, even with this major revision, the claimed nominal coverage of 95% well exceeds the actual coverage. On May 4, the IHME model undergoes another major revision, and the uncertainty is again dramatically reduced with the result that 47.4% of the actual daily deaths fall outside the 95% PIs—well beyond the claimed 5% nominal value.</p>
</blockquote>
<blockquote>
<p>the  LANL  model  was  the  only  model  that was found to approach the 95% nominal coverage, but unfortunately this model was unavailable at the time Governor Cuomo needed to make major policy decisions in late March 2020.</p>
</blockquote>
<blockquote>
<p>Models that are consistently poorly performing should carry less weight in shaping policy considerations. Models may be revised in the process, trying to improve performance. However, improvement of performance against retrospective data offers no guarantee for continued improvement in future predictions. Failed and recast models should not be given much weight in decision making until they have achieved a prospective track record that can instill some trust for their accuracy. Even then, real time evaluation should continue, since a model that performed well for a given period of time may fail to keep up under new circumstances.</p>
</blockquote>
<p><a href=""https://academic.oup.com/ej/article-abstract/123/568/491/5079498"">Do Prediction Markets Produce Well‐Calibrated Probability Forecasts?</a>.</p>
<blockquote>
<p>Abstract: This article presents new theoretical and empirical evidence on the forecasting ability of prediction markets. We develop a model that predicts that the time until expiration of a prediction market should negatively affect the accuracy of prices as a forecasting tool in the direction of a ‘favourite/longshot bias’. That is, high‐likelihood events are underpriced, and low‐likelihood events are over‐priced. We confirm this result using a large data set of prediction market transaction prices. Prediction markets are reasonably well calibrated when time to expiration is relatively short, but prices are significantly biased for events farther in the future. When time value of money is considered, the miscalibration can be exploited to earn excess returns only when the trader has a relatively low discount rate.</p>
</blockquote>
<blockquote>
<p>We confirm this prediction using a data set of actual prediction markets prices from1,787 market representing a total of more than 500,000 transactions</p>
</blockquote>
<p>Paul Christiano on <a href=""https://ai-alignment.com/learning-the-prior-48f61b445c04"">learning the Prior</a> and on <a href=""https://ai-alignment.com/better-priors-as-a-safety-problem-24aa1c300710"">better priors as a safety problem</a>.</p>
<p>A presentation of <a href=""https://www.lesswrong.com/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1"">radical probabilism</a>; a theory of probability which relaxes some assumptions in classical Bayesian reasoning.</p>
<p><a href=""https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines"">Forecasting Thread: AI timelines</a>, which asks for (quantitative) forecasts until human-machine parity. Some of the answers seem insane or suspicious, in that they have very narrow tails, sharp spikes, and don't really update on the fact that other people disagree with them.</p>
<hr>
<p>Note to the future: All links are added automatically to the Internet Archive. In case of link rot, go <a href=""https://archive.org/"">there</a> and input the dead link.</p>
<hr>
<blockquote>
<p><em>We hope that people will pressure each other into operationalizing their [big picture outlooks]. If we have no way of proving you wrong, we have no way of proving you right. We need falsifiable forecasts.</em></p>
</blockquote>
<blockquote>
<p>Source: Foretell Forecasting Forum. Inexact quote.</p>
</blockquote>
<hr>
",Radamantis,nunosempere,NunoSempere,
ipT9SupH4s6xXYD29,SlateStarCodex Online Meetup: Connor Leahy on GPT-N,slatestarcodex-online-meetup-connor-leahy-on-gpt-n,https://www.lesswrong.com/posts/ipT9SupH4s6xXYD29/slatestarcodex-online-meetup-connor-leahy-on-gpt-n,2020-09-01T06:49:29.267Z,12,4,1,False,False,,"<p><strong>Title of talk</strong>:&#xA0;<em>We Got Our AGI Firealarm, Now What?</em></p><p><strong>Summary</strong>: There has been much disagreement over how big of a deal GPT3 is in the larger context of progress towards strong AGI. While most pundits will claim it&apos;s not &quot;really&quot; &quot;intelligent&quot; or &quot;conscious&quot; or that it doesn&apos;t &quot;understand&quot; words, I want to make the case that GPT3 (or its near term successors) may be our final warning shot before we see real AGI emerge. And as a bonus, I will show you how to build your very own AGI in three easy steps! Why wait for Deepmind, you can build your very own paperclip maximizer!</p><p>September 13th at  10:30 PDT, 17:30 UTC, 20:30 IDT</p><p>Please sign up here by an hour before the event, and we will send you an invitation. <a href=""https://forms.gle/4eeL6DX6jjBGykMJ6"">https://forms.gle/4eeL6DX6jjBGykMJ6</a></p>",JoshuaFox,joshuafox,JoshuaFox,
3uxX2cCH9oxANzpk3,Why is Bayesianism important for rationality?,why-is-bayesianism-important-for-rationality,https://www.lesswrong.com/posts/3uxX2cCH9oxANzpk3/why-is-bayesianism-important-for-rationality,2020-09-01T04:24:49.099Z,37,13,24,False,True,,"<p>My impression from the Sequences seems to be that Eliezer considers Bayesianism to be a core element of rationality. Some people have even referred to the community as Bayesian Rationalists. I&apos;ve always found it curious, like it seemed like more of a technicality most of the time. Why is Bayesianism important or why did Eliezer consider it important?</p>",Chris_Leong,chris_leong,Chris_Leong,
6LJjzTo5xEBui8PqE,Reflections on AI Timelines Forecasting Thread,reflections-on-ai-timelines-forecasting-thread,https://www.lesswrong.com/posts/6LJjzTo5xEBui8PqE/reflections-on-ai-timelines-forecasting-thread,2020-09-01T01:42:40.349Z,53,26,7,False,False,,"<p>It’s been exciting to see people engage with the <a href=""https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines""><u>AI forecasting thread</u></a> that Ben, Daniel, and I set up! The thread was inspired by Alex Irpan’s AGI timeline update, and our hypothesis that visualizing and comparing AGI timelines could <a href=""https://www.lesswrong.com/posts/WFx8iPDS4WaaHyCtL/alex-irpan-my-ai-timelines-have-sped-up?commentId=cprCAKQBssXtbNHLq""><u>generate better predictions</u></a>. Ought has been working on the probability distribution tool, <a href=""http://elicit.ought.org/""><u>Elicit</u></a>, and it was awesome to see it in action.</p><p>14 users shared their AGI timelines. Below are a number of their forecasts overlaid, and an aggregation of their forecasts.</p><p><a href=""https://elicit.ought.org/builder/sSKTCHGFf""><u>Comparison of 6 top-voted forecasts</u></a></p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_140 140w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_280 280w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_420 420w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_560 560w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_840 840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_980 980w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_1120 1120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_1260 1260w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7251df05b0c43256cb5dde20744ae1bda042c5bfde0b32e9.png/w_1321 1321w""></figure><p>&nbsp;</p><p><a href=""https://elicit.ought.org/builder/cRksRmQEg""><u>Aggregation, weighted by votes</u></a></p><figure><img src=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png"" srcset=""https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_130 130w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_260 260w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_390 390w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_520 520w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_650 650w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_780 780w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_910 910w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_1040 1040w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_1170 1170w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/247c643fc777db318527d4fc6d07e92d0f81d007e30f17b7.png/w_1278 1278w""></figure><p>The thread generated some interesting learnings about AGI timelines and forecasting. Here I’ll discuss my thoughts on the following:</p><ul><li>The object level discussion of AGI timelines</li><li>How much people changed their minds and why</li><li>Learnings about forecasting</li><li>Open questions and next steps</li></ul><p>&nbsp;</p><h2>AGI timelines</h2><p><strong>Summary of beliefs</strong></p><p>We calculated an aggregation of the 14 forecasts weighted by the number of votes each comment with a forecast received. The question wasn’t precisely specified (people forecasted based on slightly different interpretations) so I’m sharing these numbers mostly for curiosity’s sake, rather than to make a specific claim about AGI timelines.</p><ul><li>Aggregated median date:<strong> June 20, 2047</strong></li><li>Aggregated most likely date: <strong>November 2, 2033</strong></li><li>Earliest median date of any forecast: <strong>June 25, 2030</strong></li><li>Latest median date of any forecast: <strong>After 2100</strong></li></ul><p>&nbsp;</p><p><strong>Emergence of categories</strong></p><p>I was pleasantly surprised by the emergence of categorizations of assumptions. Here are some themes in the way people structured their reasoning:</p><ul><li><strong>AGI from current paradigm (2023 –&nbsp;2033)</strong><ul><li>GPT-N gets us to AGI</li><li>GPT-N + improvements within existing paradigm gets us to AGI</li></ul></li><li><strong>AGI from paradigm shift (2035 –&nbsp;2060)</strong><ul><li>We need fundamental technical breakthroughs<ul><li>Quantum computing</li><li>Other new paradigms</li></ul></li></ul></li><li><strong>AGI after 2100, or never (2100 +)</strong><ul><li>We decide not to build AGI<ul><li>We decide to build tool AI / CAIS instead</li><li>We move into a stable state</li></ul></li><li>It’s harder than we expect<ul><li>It’s hard to get the right insights</li><li>We won’t have enough compute by 2100</li></ul></li><li>We can’t built AGI<ul><li>There’s a catastrophe that stops us from being able to build AGI</li></ul></li></ul></li><li><strong>Outside view reasoning</strong><ul><li>With 50% probability, things will last twice as long as they already have</li><li>We can extrapolate from rate of reaching past AI milestones</li></ul></li></ul><p><i>When sharing their forecasts, people associated these assumptions with a corresponding date interval for when we would see AGI. I took the median lower bound and median upper bound for each assumption to give a sense of what people are expecting if each assumption is true. </i><a href=""https://docs.google.com/spreadsheets/d/1l8mCkBaKuzW5ciy8U3pJYjsFxZ1NRBEx80OSvg9fiAQ/edit?usp=sharing""><i><u>Here’s</u></i></a><i> a spreadsheet with all of the assumptions. Feel free to make a copy of the spreadsheet if you want to play around and make edits.</i></p><p>&nbsp;</p><h2>Did this thread change people’s minds?</h2><p>One of the goals of making public forecasts is to help people identify disagreements and resolve cruxes. The number of people who updated is one measure of how well this format achieves this goal.</p><p>There were two updates in comments on the thread (<a href=""https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines?commentId=dfYuv646CG4vj5vT9#PRihXuMyvosoPu6eu""><u>Ben Pace</u></a> and <a href=""https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines?commentId=jFZc6GTCtmHhEjTLg#PRihXuMyvosoPu6eu""><u>Ethan Perez</u></a>), and several others not explicitly on the thread. Here are some characteristics of the thread that caused people to update (based on conversations and inference from comments):</p><ul><li><strong>It was easy to notice surprising probabilities.</strong> In most forecasts, Elicit’s bin interface meant probabilities were linked to specific assumptions. For example, it was easy to disagree with Ben Pace’s specific belief that with 30% probability, we’d reach a stable state and therefore wouldn’t get AGI before 2100. Seeing a visual image of people’s distributions also made surprising beliefs (like sharp peaks) easy to spot.</li><li><strong>Visual comparison provided a sense check.</strong> It was easy to verify whether you had too little or too much uncertainty compared to others.</li><li><strong>Seeing many people’s beliefs provides new information. </strong>Separate from the information provided by people’s reasoning, there’s information in how many people support certain viewpoints.<strong> </strong>For example, multiple people placed a non-trivial probability mass on the possibility that we could get AGI from scaling GPT-3.</li><li><strong>The thread catalyzed conversations outside of LessWrong</strong></li></ul><p>&nbsp;</p><h2>Learnings about forecasting</h2><p><strong>Vaguely defining the question worked surprisingly well</strong></p><p>The question in this thread (“Timeline until human-level AGI”) was defined much less precisely than similar Metaculus questions. This meant people were able to forecast using their preferred interpretation, which provided more information about the range of possible interpretations and sources of disagreements at the interpretation level. For example:</p><ul><li><a href=""https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines?commentId=TZeRXfvffp6fq3xfH""><u>tim_dettmers’ forecast</u></a> defined AGI as not making ‘any ""silly"" mistakes,’ which generated a substantially different distribution</li><li><a href=""https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines?commentId=PRihXuMyvosoPu6eu"">datscilly’s forecast</a> used the criteria from <a href=""https://www.metaculus.com/questions/3479/when-will-the-first-artificial-general-intelligence-system-be-devised-tested-and-publicly-known-of/""><u>this Metaculus question</u></a> and <a href=""https://www.metaculus.com/questions/384/human-machine-intelligence-parity-by-2040/""><u>this Metaculus question</u></a>, including, for example: “Able to reliably pass a Turing test of the type that would win the Loebner Silver Prize.”</li><li><a href=""https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines?commentId=GCGFPsrgW8TRKc2aL""><u>Rohin Shah</u></a> predicted timelines for transformative AI</li></ul><p>A good next step would be to create more consensus on the most productive interpretation for AGI timeline predictions.</p><p>&nbsp;</p><p><strong>Value of a template for predictions</strong></p><p>When people make informal predictions on AGI, they often define their own intervals and ways of specifying probabilities (e.g. ‘30% probability by 2035’, or ‘highly likely by 2100’). For example, <a href=""https://docs.google.com/spreadsheets/d/19edstyZBkWu26PoB5LpmZR3iVKCrFENcjruTj7zCe5k/edit?usp=sharing""><u>this list of predictions </u></a>shows how vague a lot of timeline predictions are.</p><p>Having a standard template for predictions forces people to have numerical beliefs across an entire range. This makes it easier to compare predictions and compute disagreements across any range (e.g. <a href=""https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines?commentId=FEjmACAoxEbWgjHPu#yrSq7yYy7H7rbApw6""><u>this bet suggestion</u></a> based on finding the earliest range with substantial disagreement). I’m curious how much more information we can capture over time by encouraging standardized predictions.</p><p>&nbsp;</p><p><strong>Creating AGI forecasting frameworks</strong></p><p>Ought’s mission is to apply ML to complex reasoning. A key first step is making reasoning about the future explicit (for example, by decomposing the components of a forecast, isolating assumptions, and putting numbers to beliefs) so that we can then automate parts of the process. We’ll share more about this in a blog post that’s coming soon!</p><p>In this thread, it seemed like a lot of people built their own forecasting structure from scratch. I’m excited about leveraging this work to create structured frameworks that people can start with when making AGI forecasts. This has the benefits of:</p><ul><li>Avoiding replication of cognitive work</li><li>Clearly isolating the assumptions that people disagree with</li><li>Generating more rigorous reasoning by encouraging people to examine the links between different components of a forecast and make them explicit</li><li>Providing data that helps us automate the reasoning process</li></ul><p>&nbsp;</p><p>Here are some ideas for what this might look like:</p><ul><li><strong>Decomposing the question more comprehensively based on the categories outlined above</strong><ul><li>For example, creating your overall distribution by calculating: P(Scaling hypothesis is true) * Distribution for when we will get AGI | Scaling hypothesis is true + P(Need paradigm shift) * Distribution for when we will get AGI | Need paradigm shift + P(Something stops us) * Distribution for when we will get AGI | Something stops us</li></ul></li><li><strong>Decomposing AGI timelines into the factors that will influence it</strong><ul><li>For example, compute or investment</li></ul></li><li><strong>Inferring distributions from easy questions&nbsp;</strong><ul><li>For example, asking questions like: “If the scaling hypothesis is true, what’s the mean year we get AGI?” and use the answers to infer people’s distributions</li></ul></li></ul><p>&nbsp;</p><h2>What’s next? Some open questions</h2><p>I’d be really interested in hearing other people’s reflections on this thread.&nbsp;</p><p>&nbsp;</p><p><strong>Questions I'm curious about</strong></p><ul><li>How was the experience for other people who participated?</li><li>What do people who didn’t participate but read the thread think?</li><li>What updates did people make?</li><li>What other questions would be good to make forecasting threads on?</li><li>What else can we learn from information in this thread, to capture the work people did?</li><li>How can Elicit be more helpful for these kinds of predictions?</li><li>How else do you want to build on the conversation started in the forecasting thread?</li></ul><p>&nbsp;</p><p><strong>Ideas we have for next steps</strong></p><ul><li><strong>Running more forecasting threads on other x-risk / catastrophic risks.</strong><i> </i>For example:<ul><li>When will humanity go extinct from global catastrophic biological risks?</li><li>How many people will die from nuclear war before 2200?</li><li>When will humanity go extinct from asteroids?</li><li>By 2100, how many people will die for reasons that would not have occurred if we solved climate change by 2030?</li></ul></li><li><strong>More decomposition and framework creation for AGI timeline predictions</strong><ul><li>We’re working on making Elicit as useful as we can for this!</li></ul></li></ul>",Amandango,amandango,Amandango,
