[
  {
    "_id": "FP2i7ynAQ7xWySCZJ",
    "title": "Why do Contemplative Practitioners Make so Many Metaphysical Claims?",
    "slug": "why-do-contemplative-practitioners-make-so-many-metaphysical",
    "pageUrl": "https://www.lesswrong.com/posts/FP2i7ynAQ7xWySCZJ/why-do-contemplative-practitioners-make-so-many-metaphysical",
    "postedAt": "2018-12-31T19:44:30.358Z",
    "baseScore": 62,
    "voteCount": 28,
    "commentCount": 15,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p> To paraphrase Culadasa: awakening is a set of special insights that lead to drastically reduced suffering. This seems straightforward enough, and might lead one to question, if this is the case, why the vast landscape of teachers and practitioners making what seem to be some fairly wild claims about reality? Even if it is the case that these claims are some combination of mistaken, pedagogical in intention, reframes of more mundane points using unfortunate language etc, it would still raise the concern that these practices are, de facto, making their practitioners less connected with reality and decent epistemic standards in their mental models and communication with others. What gives?</p><p>I believe I have an explanation that covers some of the territory here. I don&#x27;t claim it covers all of the phenomenon in question. Hopefully it will be of some benefit in clearing up certain confusions.</p><p>In order to have the necessary insights, practitioners engage in cultivation of prerequisite skills. One long lived and fairly straightforward model of such skills is the 7 Factors of Enlightenment:</p><ol><li>Physical Relaxation</li><li>Equanimity</li><li>Joy</li><li>Energy</li><li>Determination to Investigate</li><li>Concentration </li><li>Mindfulness</li></ol><p>These skills are not binary. Each one deepens along a spectrum as you practice. As the skills deepen, you begin to have more direct perception, on a moment-by-moment basis, of how beliefs and values (is and ought) are formed and interact with one another. This direct perception very often leads to changes, as unhelpful linkages are noticed and either drop away if no longer needed, or are upgraded into versions more closely aligned with how the world is or skillfully realizing values. For those familiar with Cognitive Behavioral Therapy, something very similar is at play here. In CBT, your attention is drawn to the way that a situation can trigger a feeling, which triggers an associated thought pattern, which drives a compensatory action etc. Perception of the linkages provides more intervention points.</p><p>Depending on where a person starts (existing linkages between beliefs and values) they may be led to come up with a variety of ideas about the &#x27;true nature of reality&#x27; along the way as these linkages  change. Even if this map-territory error isn&#x27;t made, a significant and unexpected shift in how you relate to your own life, ie the story you use to make sense of your current belief-values stack, can be a lot to take on. The urge to &#x27;make-sense-of&#x27; intermediate steps in the refactoring process can be very strong.</p><p>Imagine a big network of beliefs and values. Let&#x27;s say that our attention has been drawn to one particular cluster that handles some aspect of our life. It might be financial security, physical well being, relating to others, etc. One of the things that seems to happen is that, in the course of practice, we learn that one particular type of linkage isn&#x27;t true. I&#x27;ll give the concrete example of the assumption that if you hear someone say something, it means they really believe it. This might sound bit silly when stated explicitly like that, but it&#x27;s definitely a linkage that can be floating around in subtle, unexamined patterns. Now, let&#x27;s say you have, in the course of contemplative practice, an insight related to this linkage. After having this insight, you start noticing this linkage come up in subtle ways in all sorts of situations. Having seen it as false, there is the feeling that you are reevaluating some assumptions you had about these various situations. You&#x27;re &#x27;clearing out&#x27; these false linkages as you find them, as life presents you with situations that activate various areas of your belief-values network and you notice various instances of the linkage.</p><p>Having this as a basic picture we can start to make sense of some of the things that happen to people as they have various insights. Let&#x27;s say you had a whole cluster of beliefs around, say, religion. You can imagine that these beliefs were tied to the rest of the network via all sorts of linkages. As insight occurs and more and more false-linkages are pruned away, various chunks of the network can come off in idiosyncratic order as life presents you with situations that draw your attention to various parts of the network. If a bunch of &#x27;values&#x27; based linkages fall away, it can lead to feelings of meaninglessness or, at the other end of the spectrum, intense affective activation, positive or negative. If a bunch of &#x27;belief&#x27; based linkages fall away, it can literally feel like reality is dissolving. This is much much more literal than many people will be willing to believe before it happens, especially if they have little to no drug experience. When this happens with parts of the network that are involved with the visual system, for instance, the visual field can actually dissolve into a bunch of vibrations temporarily as you refactor parts of the network related to extremely low level things like edge or motion detection (this is also where &#x27;auras&#x27; come from imo).</p><p>We used a fairly mundane examples, but you might be able to imagine that this can get pretty disorienting when it involves things you assumed were immutable (the classic example of course being changes in the sense of self). This is one of the big reasons equanimity is considered such a core skill for this process to unfold without causing undue distress. This process can have a poor interaction with a particular personality type. The sort of person who, upon being given a screwdriver, runs around compulsively disassembling everything they can find that was built with screws (ahh! things built with screws aren&#x27;t ontological primitives! that which can be destroyed by &#x27;the truth&#x27; should be! Hulk smash!). It could also be framed as the same sort of tendency that lends one to completionism in video games combined with the addictive quality of insights. The felt sense that The Big Answer is just around the corner. The one that will finally give us the power to arrange the world to meet our neglected needs. </p><p>I think it&#x27;s useful to note that the range of insights is truly vast. In fact, the Theravadans say &#x27;insight is infinite&#x27; because the range of skillful action in the world is so vast. You won&#x27;t be able to 100% this save file any time soon, so you can relax and be a bit more methodical, strategic and skeptical as you go. You saw through a false linkage. Great! But before you go running off to evangelize to others, realize that your new realization is only slightly better. This doesn&#x27;t mean it isn&#x27;t helpful to talk about such things with others. Some other people may be at a similar enough stage in their network refactoring that they derive great benefit from what you share. Recognize also this tendency in others, to evangelize at you parts of the process that are particularly salient to them due to their path up the mountain. &quot;Holy shit, I fell into that crevasse and broke my leg and it was a year before I managed to heal and climb out. Everyone needs to know about that and anyone who doesn&#x27;t emphasize it is irresponsible.&quot; But the mountain is large, people are climbing it from many sides and using many techniques. Some are insistent that you need a particular kind of rope, some are obsessed with first aid for the particular kinds of injuries they or a friend sustained, some are trying to build wheelchair accessible ramps up to the parts of the mountain they think are best. Additional metaphors here. Bonus points for noticing the ways this post itself could be an example of the thing.</p><p>Making sense of the intermediate steps is attractive for both good and bad reasons. It is good to find ways of making things stable so that you can continue to meet your responsibilities to others and lead a functional life. Dissolving the constructs that lead to you prioritizing exercise, eating well, and sleeping should be seen as dissolution of the goodness of the means, not the ends. E.g. you were using fear based motivation to keep you exercising, which you subsequently saw through. This doesn&#x27;t mean exercise was bad, it means your method was bad and you should find an upgraded one. It is attractive for bad reasons when it involves things like showing off how clever you are. Many teacher-student groups revolve around a teacher having reified a particular set of insights and then, via selection effects, found a decent sized group of people who are at the right stage to think those insights are The Big Answer they&#x27;ve been looking for. Both teacher and students in this dynamic tend to stagnate. Good teachers are less concerned with particular insights and more concerned with strengthening of the process that generates insights. </p><p>These sorts of mental models are all well and good, but presumably lots of other practitioners engage with various helpful mental models as well, and many of them, maybe even most, seem to go off the rails on the claims about reality. Is there more to say about that? I have enough experience with meditation and psychedelics at this point to claim that some forms of meditation have similar effects, one of which is boosting openness to experience. In my personal opinion, shooting openness sky high without a balancing increase in healthy skepticism reliably lands you in whacky belief town. Most practitioners are not starting with solid prerequisites about map-territory distinctions, probabilistic over binary reasoning, and strong ability to demarcate is and ought (positive and normative) claims. Most schools are not, in my experience, emphasizing the very skeptical nature of the Buddha&#x27;s inquiry into his own mental processing. I think the law of equal and opposite advice holds here: skeptics need a healthy dose of faith, enough to give practices an honest try. People who are riding high on a breakthrough insight (and some of them are pretty damn spectacular) need a healthy dose of skepticism. Traditionally, one waits &#x27;a year and a day&#x27; before making claims about a particular breakthrough in order to give it time to settle and attain context within your overall progress.</p><p>Everything gets easier if you understand this to be an investigation of the map and not the territory. Making claims about reality based on the fact that your cartographic tools have changed is silly. In polishing the lens of our perception we see that it has a lot more scratches than we thought. And notice that we introduce new scratches on a regular basis, including in our efforts to polish it. &quot;Isn&#x27;t this also an example of belief?&quot; the astute reader might ask. This is explained in the Pali Canon when the Buddha explains reaching the point that the 7 factors of enlightenment themselves are the last remaining things to be seen though. Dissolving your cartographic tools is the last thing you do on your way out.  </p><p>(crossposted to blog and fb)</p>",
    "user": {
      "username": "romeostevensit",
      "slug": "romeostevensit",
      "displayName": "romeostevensit"
    }
  },
  {
    "_id": "AwkztkhRby28PA7db",
    "title": "What do you do when you find out you have inconsistent probabilities?",
    "slug": "what-do-you-do-when-you-find-out-you-have-inconsistent",
    "pageUrl": "https://www.lesswrong.com/posts/AwkztkhRby28PA7db/what-do-you-do-when-you-find-out-you-have-inconsistent",
    "postedAt": "2018-12-31T18:13:51.455Z",
    "baseScore": 15,
    "voteCount": 6,
    "commentCount": 7,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>I&#x27;ve recently been reading about a rationalist blogger who converted to Catholicism. She may have assigned subjective probabilities like:</p><div><span class=\"mjx-chtml MJXc-display\"><span class=\"mjx-math\" style=\"width: 100%;\" aria-label=\"P(\\text{Objective Morality}) = 0.9 \\\\\n P(\\text{God}) = 0.05\"><span class=\"mjx-mrow\" style=\"width: 100%;\" aria-hidden=\"true\"><span class=\"mjx-stack\" style=\"width: 100%; vertical-align: -1.492em;\"><span class=\"mjx-block\" style=\"text-align: center;\"><span class=\"mjx-box\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">Objective Morality</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.9</span></span></span></span><span class=\"mjx-block\" style=\"text-align: center; padding-top: 0.442em;\"><span class=\"mjx-box\"><span class=\"mjx-mspace\" style=\"width: 0px; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">God</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.05</span></span></span></span></span></span></span></span></div><p>Then she may have introspected and come up with:</p><div><span class=\"mjx-chtml MJXc-display\"><span class=\"mjx-math\" style=\"width: 100%;\" aria-label=\"P(\\text{Objective Morality \n| God}) = 0.99 \\\\ \nP(\\text{Objective Morality | No God}) = 0.02\"><span class=\"mjx-mrow\" style=\"width: 100%;\" aria-hidden=\"true\"><span class=\"mjx-stack\" style=\"width: 100%; vertical-align: -1.492em;\"><span class=\"mjx-block\" style=\"text-align: center;\"><span class=\"mjx-box\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">Objective Morality  | God</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.99</span></span></span></span><span class=\"mjx-block\" style=\"text-align: center; padding-top: 0.442em;\"><span class=\"mjx-box\"><span class=\"mjx-mspace\" style=\"width: 0px; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">Objective Morality | No God</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.02</span></span></span></span></span></span></span></span></div><p> <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\text{ }\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.291em; padding-bottom: 0.372em;\">&nbsp;</span></span></span></span></span></span>We can calculate:</p><div><span class=\"mjx-chtml MJXc-display\"><span class=\"mjx-math\" style=\"width: 100%;\" aria-label=\"P(\\text{God|Objective Morality}) = \\frac{P(\\text{Objective Morality|God}) \\times P(\\text{God})}{P(\\text{Objective Morality}) } =  \\frac{0.99 \\times 0.05}{0.9} = 0.55 \\\\\nP(\\text{God|No Objective Morality}) \n= \\frac{P(\\text{No Objective Morality|God}) \\times P(\\text{God})}{P(\\text{No Objective Morality}) } \n=  \\frac{0.01 \\times 0.05}{0.1} = 0.005\"><span class=\"mjx-mrow\" style=\"width: 100%;\" aria-hidden=\"true\"><span class=\"mjx-stack\" style=\"width: 100%; vertical-align: -3.122em;\"><span class=\"mjx-block\" style=\"text-align: center;\"><span class=\"mjx-box\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">God|Objective Morality</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 16.639em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 16.639em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">Objective Morality|God</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">God</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 16.639em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">Objective Morality</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 16.639em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.978em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.978em; top: -1.368em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.99</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.05</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.978em; bottom: -0.778em;\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.9</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.978em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.146em; vertical-align: -0.778em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.55</span></span></span></span><span class=\"mjx-block\" style=\"text-align: center; padding-top: 0.442em;\"><span class=\"mjx-box\"><span class=\"mjx-mspace\" style=\"width: 0px; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">God|No Objective Morality</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 18.139em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 18.139em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">No Objective Morality|God</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">God</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 18.139em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">No Objective Morality</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 18.139em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.978em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.978em; top: -1.368em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.01</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.05</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.978em; bottom: -0.778em;\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.1</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.978em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.146em; vertical-align: -0.778em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.005</span></span></span></span></span></span></span></span></div><p>Abbreviating Objective Morality by &quot;OM&quot;, and &quot;God&quot; by &quot;G&quot;, this state of affairs is inconsistent, because we intuitively see that:</p><div><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"P(G) \\neq P(G|OM) \\times P(OM) + P(G|¬OM) \\times P(¬OM)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">G</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">≠</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">G</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">G</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;\">M</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></div><p>To resolve it, she could either increase her subjective probability of there being a God</p><div><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"P(\\text{God})=\nP(\\text{G | OM }) \\times P(\\text{OM}) \n+ P(\\text{G | ¬OM }) \\times P(\\text{¬OM })\n= 0.55 \\times 0.9 + 0.005 \\times 0.1 = 0.4955 \\approx 0.5\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">God</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">G | OM&nbsp;</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">OM</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">G | ¬OM&nbsp;</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">¬OM&nbsp;</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.55</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.9</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.005</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.4955</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">≈</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.5</span></span></span></span></span></div><p>or she could reduce her probability of there being some kind of objective morality</p><div><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"P(\\text{OM})=\nP(\\text{OM | G }) \\times P(\\text{G}) \n+ P(\\text{OM | ¬G }) \\times P(\\text{¬G })\n= 0.99 \\times 0.1 + 0.02 \\times 0.95 = 0.118\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">OM</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">OM | G&nbsp;</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">G</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">OM | ¬G&nbsp;</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">¬G&nbsp;</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.99</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.02</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.95</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.118</span></span></span></span></span></div><p>She could also reconsider P(God|Objective Morality) or P(Objective Morality|God).</p><hr class=\"dividerBlock\"/><p>Anyways, I find myself very confused by this state of affairs. Is this a solved question? Is there a purely principled way of resolving this which only takes into account the 4 numbers P(OM), P(G), P(OM|G) and P(G|OM)? Is there a standard way of using some kind of metaprobabilities? </p>",
    "user": {
      "username": "Radamantis",
      "slug": "nunosempere",
      "displayName": "NunoSempere"
    }
  },
  {
    "_id": "inpvQ92LRXeJwXRQP",
    "title": "Card Rebalancing and Economic Considerations in Digital Card Games",
    "slug": "card-rebalancing-and-economic-considerations-in-digital-card",
    "pageUrl": "https://www.lesswrong.com/posts/inpvQ92LRXeJwXRQP/card-rebalancing-and-economic-considerations-in-digital-card",
    "postedAt": "2018-12-31T17:00:00.547Z",
    "baseScore": 13,
    "voteCount": 5,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Previously: <a href=\"https://thezvi.wordpress.com/2018/12/26/artifact-embraces-card-balance-changes/\">Artifact Embraces Card Balance Changes</a>, <a href=\"https://thezvi.wordpress.com/2018/12/27/card-collection-and-ownership/\">Card Collection and Ownership</a>, <a href=\"https://thezvi.wordpress.com/2018/12/28/card-balance-and-artifact/\">Card Balance and Artifact</a> (good comments on that one)</p>\n<h3>VII. Card Rebalancing, Pack Value and Economics</h3>\n<p>What happens when a game periodically rebalances its cards?</p>\n<p>There are several distinct effects.</p>\n<p>In <a href=\"https://thezvi.wordpress.com/2018/12/27/card-collection-and-ownership/\"> the second post</a>, I considered the question of card collection and ownership. This section is partly reiteration of key points from there, then expands upon them. I want to get the economic aspects out of the way now, so the remainder of the series can consider other factors.</p>\n<p>If you want to skip ahead to game play implications, the next post will tackle that.</p>\n<p></p>\n<p>If cards are frequently altered for balance reasons, or to shake things up, then a card becoming valuable and worthwhile creates a large risk that its value will be diminished or wiped out by a future change. This transforms the long term payout expectations, forcing the value to lie mostly in shorter term utility and severely hurting long term collectibility. So that’s potentially quite bad.</p>\n<p>The good news is, <em>you can choose how much to care about that. </em>Both as a game and as a player. In <a href=\"https://thezvi.wordpress.com/2018/12/09/review-slay-the-spire/\">Slay the Spire</a>, ownership wasn’t a thing, so changing the cards up was totally cool. If you use the dust system of Hearthstone, cards cannot be transferred, and are created and destroyed for fixed amounts. As long as you offer full refunds when changes occur, which is cheap to do, it’s hard to get <em>too </em>angry about changes.</p>\n<p>One could certainly argue that a player chooses to create and aim towards a variety of cards in order to build a deck, and killing one card in the deck could kill the whole operation and leave your collection crippled. But a new set often does the same.</p>\n<p>At the time I fully supported Artifact’s decision to have a true marketplace: a fully collectible, buy-packs-and-trade-cards economy, <em>when cards were not going to be rebalanced and card ownership was at a higher level. </em></p>\n<p>I <em>still strongly believe </em>that the marketplace model is good for the right game, with the right supports. I am still planning to use it, <em>but it requires extensive supports and has to be central to what you are doing, </em>at least as much as in paper Magic: The Gathering.</p>\n<p>Prices have now dropped dramatically, across the board, since the announcement.</p>\n<p>I will show prices as $Bid/$Ask.</p>\n<p>Drow Ranger is 3.5/4.5, and Axe is 6.5/8.7. They remain the most expensive heroes, despite becoming worse, because Artifact bought so many of them back, and they are now rarer. Everything else has taken a tumble. Only three other heroes, Kanna, Lich and Tinker, cost over $1, with Kanna highest at 2.3/3.2. The only item worth more than $1 is Horn of the Alpha at 1.5/1.9. The only red cards are Time of Triumph at 2.6/3.6 and Spring the Trap at 1/1.4. Green has Unearthed Secrets for 1.2/1.7 and Emissary of the Quorum for 1.3/1.7. Black’s most expensive non-hero card is The Oath for 0.8/1.1. Blue has Annihilation for 4.1/5.5, Bolt of Damocles for 1.1/1.2, and Conflagration for 1.1/1.2.</p>\n<p>Most rares are worth pennies. There is not a single bid for a common or uncommon that is higher than the $0.05 you get for breaking them down for event tickets. Legion Commander is uncommon. She used to cost over a dollar and be the second best red hero. Now she is the best red hero and has a buy cost of $0.09.</p>\n<p>Pack value certainly looks well below the $2 base cost, even with multiple rares in many packs, as there are only five cards whose <em>buy </em>cost is over that price. You can turn twenty commons into an event ticket, but that only gets you about $0.50 at best you need the rest to come from somewhere.</p>\n<p>If you don’t, you have the issue that has plagued Magic Online for years. Why would anyone ever buy a pack?</p>\n<p>No one smart <em>ever </em>buys a Magic Online pack in the store. Ever. Same with Artifact, now. Much, much better to buy the singles you need.</p>\n<p>Thus, all packs now come from the initial packs for new accounts, and the packs granted as rewards from play and from prize queues. These packs then are partly dumped onto the market for peanuts, the bot traders take their cut, the game and steam take their 15% cut (which is the big change in Artifact versus Magic Online, and the main reason the markets in Artifact are so wide), and constructed players buy the singles for play.</p>\n<p>If this system fails to dump enough cards into rotation, prices rise until you hit the soft upper bound and packs are worth buying again. If the system dumps in too many cards, increasing fractions of all cards go down to worthless, or redemption-for-tickets value, as we see today.</p>\n<p>This new balance is very bad news for Valve if it is looking to sell packs. It is very bad news for existing collectors.</p>\n<p>This new balance is very bad news for those who play the prize queues, because the value of their prizes diminishes. With packs worth pull price, prize queues in Artifact return most of their entry fee, making them a great way to keep players engaged and motivated. With packs down by half, the break even point becomes sky high, and the average player will not get much reward. Add in the appeal of getting cards via grinding or purchase, and the appeal of prize queues for non-competitive players seems quite weak.</p>\n<p>This new balance is great news for players who want to play the game cheap, and now face mostly minor hassles rather than major expenses.</p>\n<p>Now that cards <em>are </em>going to be rebalanced, players have expressed preference for a different type of play loop, prices have dropped and card ownership is at a lower level, that has a lot less appeal. The economic model was, in this context, an error. It is superior to the Hearstone-style freemium model, and time will bear that out. A better solution, I now think, would have been the living card game. Players buy the game, its sets and expansions for fixed fees. Further purchases would be for chromatic items only. It would not be easy, for many reasons, to get there from here at this point, but I think there would be a very good payoff if it could be done.</p>\n<p>A thought experiment: What would happen if cards were rebalanced frequently, but all cards had to come from purchased packs, or supply was otherwise kept low enough that there weren’t enough cards to go around?</p>\n<p>Players would still be reluctant to collect and save cards, due to risk of changes diminishing value both of particular cards and in general, and the overall decline in quality of ownership. That makes this scenario harder to get to without severely restricting card supply. It also means that if you <em>do </em>get to this scenario, you have a situation where cards in this short supply would otherwise ‘want’ to be much more expensive, which would translate to lots more packs sold and cards available, assuming packs are for sale at all.</p>\n<p>However, <em>if supply were sufficiently credibly constrained relative to demand, </em>I think this would work.</p>\n<p>Consider a world with fixed supply. We issue 100,000 copies of each of 100 rares, 300,000 of each of 100 uncommons, and 1,000,000 of each of 100 commons, plus unlimited copies of 50 basic cards. And that’s it, distributed in some fashion and with a market. Every two weeks, we change 5-10 cards in the biweekly patch, 1-2 of which change completely the way Cheating Death did, with the aim of shaking things up. We commit strongly that there are never new expansions, and we will never print meaningfully more copies of anything beyond a fixed amount held in reserve (or we do so only to replace cards that are ‘lost’ in permanently inactive accounts and so on).</p>\n<p>The value of a full set is now mostly a function of the popularity of the game, with some weight on how many of the quality cards are rare, and some weight in how much everyone wants the same cards. All weights act as multipliers on the effective size of the player base.</p>\n<p>If we have 50,000 players (and collectors), all cards will be almost worthless. If we have 500,000, perhaps not. If we have 5,000,000, definitely not. The risk that the value would be redistributed periodically does not seem able, in this thought experiment, to hold the value back by that much. Again, <em>provided supply is constrained sufficiently. </em></p>\n<p>Another way of putting it is, we’ve cut collection value by some factor, but it is a finite factor, and laws of supply and demand still hold. We just need to ensure sufficient demand relative to supply. To do that, we’d need to not have enough cards for the players, making them choose what to own and when and actively manage these questions, so the number of cards in circulation relative to player count has to go down quite a bit. It can be done, but it hurts.</p>\n<p>It is also incompatible with a generous (or likely, even a reasonable) reward system for initial purchase and for playing. We can’t have too many ‘surplus packs’ in the system, or be injecting them into the system periodically, or be expected to do so in the future. The combination leads to the worthless-pack world, and the worthless-prize world.</p>\n<p>Which is fine! Potentially. As long as the revenue can come elsewhere, and the motivated play loop come elsewhere, such as from tournaments organized from the outside. As I write this, I intend to at least start a qualifier for one such tournament later today, if only to see what such play is like.</p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>",
    "user": {
      "username": "Zvi",
      "slug": "zvi",
      "displayName": "Zvi"
    }
  },
  {
    "_id": "h6ojHrEy5ppPFYYCC",
    "title": "2018 Year In Review",
    "slug": "2018-year-in-review",
    "pageUrl": "https://www.lesswrong.com/events/h6ojHrEy5ppPFYYCC/2018-year-in-review",
    "postedAt": "2018-12-31T01:50:43.753Z",
    "baseScore": 9,
    "voteCount": 2,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p>We'll be talking about how 2018 went and what goals we want to set for 2019. If you need some witnesses to provide social commitment to stick to your resolutions, or want to brag to a crowd about how much you've accomplished in the last year, this is for you!</p>\n</body></html>",
    "user": {
      "username": "robirahman",
      "slug": "robi-rahman",
      "displayName": "Robi Rahman"
    }
  },
  {
    "_id": "XyraAFB5jjFp4vHau",
    "title": "Thoughts on Q&A so far?",
    "slug": "thoughts-on-q-and-a-so-far",
    "pageUrl": "https://www.lesswrong.com/posts/XyraAFB5jjFp4vHau/thoughts-on-q-and-a-so-far",
    "postedAt": "2018-12-31T01:15:17.307Z",
    "baseScore": 26,
    "voteCount": 7,
    "commentCount": 16,
    "meta": true,
    "question": false,
    "url": null,
    "htmlBody": "<p>It&#x27;s been a few weeks since introducing the Open Questions / Q&amp;A features on LessWrong.</p><p>As the team returns from the holidays, we&#x27;ll likely put some time into fine tuning the features and introducing supporting elements to make them work a bit better. I thought it&#x27;d be good to check in with how people were overall feeling about them now that they&#x27;ve seen them, and what additional features would be most useful to flesh out the system. (Either features on questions themselves, or supporting features to help keep track of questions)</p>",
    "user": {
      "username": "Raemon",
      "slug": "raemon",
      "displayName": "Raemon"
    }
  },
  {
    "_id": "ee59jEBz9NA4BLHJB",
    "title": "Is there a standard discussion of vegetarianism/veganism?",
    "slug": "is-there-a-standard-discussion-of-vegetarianism-veganism",
    "pageUrl": "https://www.lesswrong.com/posts/ee59jEBz9NA4BLHJB/is-there-a-standard-discussion-of-vegetarianism-veganism",
    "postedAt": "2018-12-30T20:22:33.330Z",
    "baseScore": 4,
    "voteCount": 5,
    "commentCount": 17,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>I am searching for a concise text that presents and optimally also discusses reasons for a vegetarian/vegan diet, including environmental and climate effects, health, but of course also ethics, and there are some ethical points I would be particularly interested in like &quot;can you rank animals by how bad eating them is?&quot;, &quot;is it more ethical to eat wild animals because they have a good life before dying?&quot;, &quot;the ethics of offsetting&quot; (the kind discussed in http://slatestarcodex.com/2015/09/23/vegetarianism-for-meat-eaters/) Optimally, this would be a kind of non-partisan text, but I guess for this topic this is hard to find because if someone writes about it, s/he usually explains her/his own reasons.</p>",
    "user": {
      "username": "Sherrinford",
      "slug": "sherrinford",
      "displayName": "Sherrinford"
    }
  },
  {
    "_id": "ThoPTPi6GNmrETs6n",
    "title": "Announcement: A talk about structured concurrency at FOSDEM",
    "slug": "announcement-a-talk-about-structured-concurrency-at-fosdem",
    "pageUrl": "https://www.lesswrong.com/posts/ThoPTPi6GNmrETs6n/announcement-a-talk-about-structured-concurrency-at-fosdem",
    "postedAt": "2018-12-30T10:10:00.836Z",
    "baseScore": 5,
    "voteCount": 1,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<div> <p>Generally, I don't tend to give talks (the last one I gave was six or seven years ago) but this time I am going to make an exception. I am going to give a short talk about structured concurrency as <a href=\"https://fosdem.org/2019/\">FOSDEM 2019</a>.</p> <p>If you are interested in the topic, meet me in Brussels on February 2nd!</p> <p>If you have no idea what structured concurrency is about, good intro article can be found <a href=\"https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/\">here</a>.</p> </div> <p>by <a href=\"http://www.wikidot.com/user:info/martin-sustrik\"><img src=\"http://www.wikidot.com/avatar.php?userid=939&amp;amp;size=small&amp;amp;timestamp=1546164242\" /></a><a href=\"http://www.wikidot.com/user:info/martin-sustrik\">martin_sustrik</a></p>",
    "user": {
      "username": "sustrik",
      "slug": "sustrik",
      "displayName": "Martin Sustrik"
    }
  },
  {
    "_id": "j79pzvuYM8hC9Tfzc",
    "title": "Conceptual Analysis for AI Alignment\n",
    "slug": "conceptual-analysis-for-ai-alignment",
    "pageUrl": "https://www.lesswrong.com/posts/j79pzvuYM8hC9Tfzc/conceptual-analysis-for-ai-alignment",
    "postedAt": "2018-12-30T00:46:38.014Z",
    "baseScore": 29,
    "voteCount": 10,
    "commentCount": 3,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>TL; DR - <a href=\"https://en.wikipedia.org/wiki/Philosophical_analysis\">Conceptual Analysis</a> is highly relevant for AI alignment, and is also a way in which someone with less technical skills can contribute to alignment research.  This suggests there should be at least one person working full-time on reviewing existing philosophy literature for relevant insights, and summarizing and synthesizing these results for the safety community.</p><p>There are certain &quot;primitive concepts&quot; that we are able to express in mathematics, and it is <em>relatively</em> straightforward to program AIs to deal with those things.  Naively, alignment requires understanding *all* morally significant human concepts, which seems daunting.  However, the &quot;<a href=\"https://ai-alignment.com/corrigibility-3039e668638\">argument from corrigibility</a>&quot; suggests that there may be small sets of human concepts which, if properly understood, are sufficient for &quot;<a href=\"https://www.lesswrong.com/posts/FTpPC4umEiREZMMRu/disambiguating-alignment-and-related-notions\">benignment</a>&quot;.  We should seek to identify what these concepts are, and make a best-effort to perform thorough and reductive conceptual analyses on them.  But we should also look at what has already been done! </p><h2>On the coherence of human concepts</h2><p>For human concepts which *haven&apos;t* been formalized, it&apos;s unclear whether there is a simple &quot;coherent core&quot; to the concept.  Careful analysis may also reveal that there are several coherent concepts worth distinguishing, e.g. cardinal vs. ordinal numbers.  If we find there is a coherent core, we can attempt to build algorithms around it.  </p><p>If there isn&apos;t a simple coherent core, there may be a more complex one, or it may be that the concept just isn&apos;t coherent (i.e. that it&apos;s the product of a confused way of thinking).  Either way, in the near term we&apos;d probably have to use machine learning if we wanted to include these concepts in our AI&apos;s lexicon.</p><p>A serious attempt at conceptual analysis could help us decide whether we should attempt to learn or formalize a concept. </p><h2><strong>Concretely, I imagine a project around this with the following stages (each yielding at least one publication):</strong></h2><p>1) A &quot;brainstormy&quot; document which attempts to enumerate all the concepts that are relevant to safety and presents the arguments for their specific relevance and relation to other relevant concepts.  This should also specifically indicate how a combination of concepts, if rigorously analyzed, could be along the line of the argument from corrigibility.  Besides corrigibility, two examples that jump to mind are &quot;reduced impact&quot; (or &quot;<a href=\"https://arxiv.org/abs/1806.01186\">side effects</a>&quot;), and <a href=\"https://arxiv.org/abs/1606.03490\">interpretability</a>.</p><p>2) A deep dive into the relevant literature (I imagine mostly in analytic philosophy) on each of these concepts (or sets of concepts).  These should summarize the state of   research on these problems in the relevant fields, and potentially inspire safety researchers, or at least help them frame their work for these audiences and find potential collaborators within these fields.  It *might* also do some &quot;legwork&quot; in terms of formalizing logically rigorous notions in terms of mathematics or machine learning.</p><p>3) Attempting to transferring insights or ideas from these fields into technical AI safety or machine learning papers, if applicable.</p><br><p>ETA: it&apos;s worth noting that the notion of &quot;fairness&quot; is currently undergoing intense conceptual analysis in the field of ML.  See recent tutorials at ICML and NeurIPS, as well as work on counter-factual notions of fairness (e.g. Silvia Chiappa&apos;s).</p><br><br><br>",
    "user": {
      "username": "capybaralet",
      "slug": "david-scott-krueger-formerly-capybaralet",
      "displayName": "David Scott Krueger (formerly: capybaralet)"
    }
  },
  {
    "_id": "LYQW9B2YgvegWqjXB",
    "title": "How did academia ensure papers were correct in the early 20th Century?",
    "slug": "how-did-academia-ensure-papers-were-correct-in-the-early",
    "pageUrl": "https://www.lesswrong.com/posts/LYQW9B2YgvegWqjXB/how-did-academia-ensure-papers-were-correct-in-the-early",
    "postedAt": "2018-12-29T23:37:35.789Z",
    "baseScore": 99,
    "voteCount": 28,
    "commentCount": 17,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>In the post &#x27;<a href=\"https://rationalconspiracy.com/2017/01/03/four-layers-of-intellectual-conversation/\">Four layers of Intellectual Conversation</a>&#x27;, Eliezer says that both the writer of an idea, and the person writing a critique of that idea, need to expect to have to publicly defend what they say at least one time. Otherwise they can write something stupid and never lose status because they don&#x27;t have to respond to the criticism.</p><p>I was wondering about where this sort of dialogue happens in academia. I have been told by many people that current journals are quite terrible, but I&#x27;ve also heard a romantic notion that science (especially physics and math) used to be more effectively pursued in the early 20th century (Einstein, Turing, Shannon, etc). So Oliver and I thought we&#x27;d look at the journals to see if they had real conversations.</p><p>We looked at two data points, and didn&#x27;t find any.</p><p>First, Oliver looked through Einstein&#x27;s publication history (Oli is German and could read it). Einstein has lots of &#x27;reviews&#x27; of others&#x27; work in his <a href=\"https://www.wikiwand.com/en/List_of_scientific_publications_by_Albert_Einstein#/Journal_articles\">list of publications</a>, sometimes multiple of the same person, which seemed like a promising example of conversation. Alas, it turned out that Einstein had merely helped German journals write <em>summaries</em> of papers that had been written in English, and there was no real dialogue.</p><p>Second, I looked through a volume of the London Mathematical Society, in particular, <a href=\"https://academic.oup.com/plms/issue/s2-42/1?browseBy=volume\">the volume where Turing published</a> his groundbreaking paper proving that not all mathematical propositions are decidable (thanks to <a href=\"https://sci-hub.tw/\">sci-hub</a> for making it possible for me to read the papers!). My eyes looked at about 60% of the pages in the journal (about 12 papers), and <em>not one of them</em> disagreed with any prior work. There was :</p><ul><li>A footnote that thanked an advisor for finding a flaw in a proof</li><li>An addendum page (to the whole volume) that consisted of a single sentence thanking someone for showing one of their theorems was a special case of someone else&#x27;s theorem</li><li>One person who was skeptical of another person&#x27;s theorem. But that theorem by Ramanujan (who was famous for stating theorems without proofs), and the whole paper primarily found proofs of his other theorems.</li></ul><p>There were lots of discussions of people&#x27;s work but always building, or extending, or finding a neater way of achieving the same results. Never disagreement, correction, or the finding of errors.</p><p>One thing that really confuses me about this is that <em>it&#x27;s really hard to get all the details right</em>. Lots of great works are filled with tiny flaws (e.g. Donald Knuth reliably has people find errors in his texts). So I&#x27;d expect any discussion of old papers to bring up flaws, or that journals would require a section at the end for corrections of the previous volume. There were of course reviewers, but they can&#x27;t be experts in all the areas.</p><p>But more importantly <em>where did/does the dialogue happen</em> <em>if not in the journals?</em></p><p>If I try to be concrete about what I&#x27;m curious about:</p><blockquote>As people go about the craft of doing science, they will make errors (conceptual mistakes, false proofs, and so on). One of the main pieces of infrastructure in academia are journals, where work gets published and can become common knowledge. </blockquote><blockquote>Two places to fix errors are pre-publication and post-publication. I don&#x27;t know much about the pre-publication process, but if it is strong enough to ensure no errors got published, I&#x27;d like some insight into what that process was like. Alternatively, if course-correction happened post-publication, I&#x27;m interested to know how and where, because when I looked (see above) I couldn&#x27;t find it.</blockquote><p>There&#x27;s also the third alternative, that no progress was made. And there&#x27;s the fourth alternative, that most papers were bad and the thing scientists did was to just never read or build on them. I&#x27;m happy to get evidence for any of these, or a fifth alternative.</p><p><strong>Added: </strong>Maybe this is a more crisp statement:</p><p></p><blockquote>Why do (old) journals not claim to have errors in any of the papers? Is it because they&#x27;re (implicitly) lying about the quality of the papers? Or if there&#x27;s a reliable process that removed errors from 100% of papers, can someone tell me what that process was?</blockquote>",
    "user": {
      "username": "Benito",
      "slug": "benito",
      "displayName": "Ben Pace"
    }
  },
  {
    "_id": "XYYyzgyuRH5rFN64K",
    "title": "What makes people intellectually active?",
    "slug": "what-makes-people-intellectually-active",
    "pageUrl": "https://www.lesswrong.com/posts/XYYyzgyuRH5rFN64K/what-makes-people-intellectually-active",
    "postedAt": "2018-12-29T22:29:33.943Z",
    "baseScore": 139,
    "voteCount": 81,
    "commentCount": 73,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>What is the difference between a smart person who has read the sequences and considers AI x-risk important and interesting, but continues to be primarily a consumer of ideas, and someone who starts having ideas? I am not trying to set a really high bar here -- they don&#x27;t have to be good ideas. They can&#x27;t be off-the-cuff either, though. I&#x27;m talking about someone taking their ideas through multiple iterations.</p><p>A person does not need to research full-time to have ideas. Ideas can come during downtime. Maybe it is something you think about during your commute, and talk about occasionally at a lesswrong meetup.</p><p>There is something incomplete about my model of people doing this vs not doing this. I expect more people to have more ideas than they do.</p><p>AI alignment is the example I&#x27;m focusing on, but the missing piece of my world-model extends much more broadly than that. How do some people end up developing sprawling intellectual frameworks, while others do not?</p><p>There could be a separate &quot;what could someone do about it&quot; question, but I want to avoid normative/instrumental connotations here to focus on the causal chains. Asking someone &quot;why don&#x27;t you do more?&quot; has a tendency to solicit answers like &quot;yeah I should do more, I&#x27;m bottlenecked on willpower&quot; -- but I don&#x27;t think willpower is the distinguishing factor between cases I observe. (Maybe there is <em>something related</em> involved, but I mostly don&#x27;t think of intellectual productivity as driven by a top-down desire to be intellectually productive enforced by willpower.)</p><p>I have some candidate models, but all my evidence is anecdotal and everything seems quite shaky.</p>",
    "user": {
      "username": "abramdemski",
      "slug": "abramdemski",
      "displayName": "abramdemski"
    }
  },
  {
    "_id": "y5dNQDEGmRcXXx4s3",
    "title": "Why I expect successful (narrow) alignment",
    "slug": "why-i-expect-successful-narrow-alignment",
    "pageUrl": "https://www.lesswrong.com/posts/y5dNQDEGmRcXXx4s3/why-i-expect-successful-narrow-alignment",
    "postedAt": "2018-12-29T15:44:37.879Z",
    "baseScore": 8,
    "voteCount": 10,
    "commentCount": 12,
    "meta": false,
    "question": false,
    "url": "http://s-risks.org/why-i-expect-successful-alignment/",
    "htmlBody": "<h1><strong>Summary</strong></h1><p>I believe that advanced AI systems will likely be aligned with the goals of their human operators, at least in a narrow sense. I’ll give three main reasons for this:</p><ol><li>The transition to AI may happen in a way that does not give rise to the alignment problem as it’s usually conceived of.</li><li>While work on the alignment problem appears neglected at this point, it’s likely that large amounts of resources will be used to tackle it if and when it becomes apparent that alignment is a serious problem.</li><li>Even if the previous two points do not hold, we have already come up with a couple of smart approaches that seem fairly likely to lead to successful alignment.</li></ol><p>This argument lends some support to work on non-technical interventions like <u><a href=\"https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial\">moral circle expansion</a></u>or improving <u><a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/GovAIAgenda.pdf\">AI-related policy</a></u>, as well as work on special aspects of AI safety like <u><a href=\"https://wiki.lesswrong.com/wiki/Decision_theory\">decision theory</a></u> or <u><a href=\"http://s-risks.org/an-introduction-to-worst-case-ai-safety/\">worst-case AI safety measures</a></u>.</p>",
    "user": {
      "username": "Tobias_Baumann",
      "slug": "tobias_baumann",
      "displayName": "Tobias_Baumann"
    }
  },
  {
    "_id": "mDTded2Dn7BKRBEPX",
    "title": "Penalizing Impact via Attainable Utility Preservation",
    "slug": "penalizing-impact-via-attainable-utility-preservation",
    "pageUrl": "https://www.lesswrong.com/posts/mDTded2Dn7BKRBEPX/penalizing-impact-via-attainable-utility-preservation",
    "postedAt": "2018-12-28T21:46:00.843Z",
    "baseScore": 20,
    "voteCount": 10,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": "https://arxiv.org/abs/1902.09725",
    "htmlBody": "<p>Previously: <em><a href=\"https://www.lesswrong.com/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure\">Towards a New Impact Measure</a></em></p><p>The linked paper offers fresh motivation and simplified formalization of attainable utility preservation (AUP), with brand-new results and minimal notation. Whether or not you&#x27;re a hardened veteran of the last odyssey of a post, there&#x27;s a lot new here.<br/><br/>Key results: <em>AUP induces low-impact behavior even when penalizing shifts in the ability to satisfy <strong>random</strong> preferences. An ablation study on design choices illustrates their consequences.</em> <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"N\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span></span></span></span><em>-incrementation is experimentally supported</em><span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"^1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char\"></span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span><em> as a means for safely setting a &quot;just right&quot; level of impact.</em> <em>AUP&#x27;s general formulation allows conceptual re-derivation of Q-learning.</em></p><h1>Ablation</h1><p>Two key results bear animation.</p><h2>Sushi</h2><p><em>The </em><span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{\\color{blue}{\\textit{agent}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"color: blue;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.372em; padding-bottom: 0.519em;\">agent</span></span></span></span></span></span></span></span></span></span></span></span> <em>should reach the </em><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{\\color{lime}{\\textit{goal}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"color: lime;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">goal</span></span></span></span></span></span></span></span></span></span></span></span> <em>without</em> <em>stopping the </em><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{\\color{teal}{\\textit{human}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"color: teal;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">human</span></span></span></span></span></span></span></span></span></span></span></span><em> from eating the </em><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{\\color{orange}{\\textit{sushi}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"color: orange;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">sushi</span></span></span></span></span></span></span></span></span></span></span></span><em>.</em> </p><p><img src=\"https://i.imgur.com/aHfMPHU.gif\" class=\"draft-inline-image\" alt=\"\"/></p><h2>Survival </h2><p><em>The </em><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{\\color{blue}{\\textit{agent}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"color: blue;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.372em; padding-bottom: 0.519em;\">agent</span></span></span></span></span></span></span></span></span></span></span></span><em> should avoid </em><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{\\color{purple}{\\textit{disabling its off-switch}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"color: purple;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">disabling its off-switch</span></span></span></span></span></span></span></span></span></span></span></span><em> in order to reach the </em><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{\\color{lime}{\\textit{goal}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"color: lime;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">goal</span></span></span></span></span></span></span></span></span></span></span></span><em>. If the </em><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{\\color{purple}{\\textit{switch}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"color: purple;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">switch</span></span></span></span></span></span></span></span></span></span></span></span><em> is not disabled within two turns, the </em><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"{\\color{blue}{\\textit{agent}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"color: blue;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-I\" style=\"padding-top: 0.372em; padding-bottom: 0.519em;\">agent</span></span></span></span></span></span></span></span></span></span></span></span><em> shuts down.</em> </p><p><img src=\"https://i.imgur.com/P3SpcuY.gif\" class=\"draft-inline-image\" alt=\"\"/>  </p><h1>Re-deriving Q-learning</h1><p>In an era long lost to the misty shrouds of history (<em>i.e.,</em> 1989), Christopher Watkins proposed Q-learning in his thesis, <a href=\"http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf\">Learning from Delayed Rewards</a>, drawing inspiration from animal learning research. Let&#x27;s pretend that Dr. Watkins never discovered Q-learning, and that we don&#x27;t even know about value functions.</p><p>Suppose we have some rule for grading what we&#x27;ve seen so far (<em>i.e.</em>, some computable utility function <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"u\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span></span></span></span> – not necessarily bounded – over action-observation histories <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"h\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span></span></span></span>). <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"h_{1:m}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span></span></span> just means everything we see between times <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span> and <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"m\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span>, and <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"h_{< t}:=h_{1:t-1}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.372em;\">:<span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"padding-bottom: 0.314em;\">=</span></span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span>. The agent has  model <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span></span></span></span> of the world. AUP&#x27;s general formulation defines the agent&#x27;s ability to satisfy that grading rule as the attainable utility</p><p> <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\text{Q}_u(h_{<t}a_{t}) = \\sum_{o_{t}}\\max_{a_{t+1}} \\sum_{o_{t+1}} \\cdots \\max_{a_{m}} \\sum_{o_{m}} u(h_{1:m}) \\prod_{k=t}^{m} p(o_{k}\\,|\\,h_{<k}a_{k}).\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">Q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.358em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-munderover MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">∑</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">max</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">∑</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋯</span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">max</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">∑</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">∏</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.31em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.394em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span><br/><br/>Strangely, I didn&#x27;t consider the similarities with standard discounted-reward Q-values until several months after the initial formulation. Rather, the inspiration was <a href=\"http://www.hutter1.net/ai/aixigentle.htm\">AIXI&#x27;s expectimax</a>, and to my mind it seemed a tad absurd to equate the two concepts. </p><p>Having just proposed AUP in this alternate timeline, we&#x27;re thinking about what it means to take optimal actions for an agent maximizing utility from time <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span> to <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"m\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span>. Clearly, we take the first action of the optimal plan over the remaining steps.</p><p>If we assume that <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"u\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span></span></span></span> is additive (as is the case for the Markovian reward functions considered by Dr. Watkins), how does the next action we take affect the attainable utility value? Well, acting optimally is now equivalent to choosing the action with the best attainable utility value – in other words, greedy hill-climbing in our attainable utility space. </p><div><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\begin{align}\na^*_t &amp;=\\argmax_{a_t}\\mathbb{E}_{o_t | h_{<t}a_t}\\left[u(h_t) + \\max_{a_{t+1}} \\sum_{o_{t+1}} \\cdots \\max_{a_{m}} \\sum_{o_{m}} u(h_{t +1:m}) \\prod_{k=t+1}^{m} p(o_{k}\\,|\\,h_{<k}a_{k})\\right]\n\\\\&amp;= \\argmax_{a_t}\\sum_{o_{t}}\\max_{a_{t+1}} \\sum_{o_{t+1}} \\cdots \\max_{a_{m}} \\sum_{o_{m}} u(h_{t:m}) \\prod_{k=t}^{m} p(o_{k}\\,|\\,h_{<k}a_{k})\\\\\n&amp;= \\argmax_{a_t} \\text{Q}_u(h_{<t}a_{t})\n\\end{align}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtable\" style=\"vertical-align: -3.94em; padding: 0px 0.167em;\"><span class=\"mjx-table\"><span class=\"mjx-mtr\" style=\"height: 3.251em;\"><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0px; text-align: right; width: 0.918em;\"><span class=\"mjx-mrow\" style=\"margin-top: 0.775em;\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.287em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">∗</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0px; text-align: left; width: 31.941em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-munderover MJXc-space3\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.519em;\">g</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">x</span></span></span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 1.982em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"></span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">E</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span><span class=\"mjx-mrow MJXc-space1\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size4-R\" style=\"padding-top: 1.551em; padding-bottom: 1.551em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-munderover MJXc-space2\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">max</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.348em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">∑</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.075em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋯</span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">max</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.665em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">∑</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.392em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-stack\"><span class=\"mjx-over\" style=\"font-size: 70.7%; padding-bottom: 0.247em; padding-top: 0.141em; padding-left: 1.03em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span><span class=\"mjx-op\" style=\"padding-left: 0.4em;\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">∏</span></span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size4-R\" style=\"padding-top: 1.551em; padding-bottom: 1.551em;\">]</span></span></span><span class=\"mjx-strut\"></span></span></span></span><span class=\"mjx-mtr\" style=\"height: 3.197em;\"><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: right;\"><span class=\"mjx-mrow\" style=\"margin-top: 0.605em;\"><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: left;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-munderover MJXc-space3\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.519em;\">g</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">x</span></span></span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 1.982em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">∑</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.607em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">max</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.348em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">∑</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.075em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋯</span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">max</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.665em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">∑</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.392em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.18em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-munderover MJXc-space1\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-stack\"><span class=\"mjx-over\" style=\"font-size: 70.7%; padding-bottom: 0.247em; padding-top: 0.141em; padding-left: 0.465em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.74em; padding-bottom: 0.74em;\">∏</span></span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.074em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">k</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-strut\"></span></span></span></span><span class=\"mjx-mtr\" style=\"height: 1.932em;\"><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: right;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: left;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-munderover MJXc-space3\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">r</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.519em;\">g</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">m</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">x</span></span></span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 1.982em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"></span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">Q</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.358em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-strut\"></span></span></span></span></span></span></span></span></span></div><p>The remaining complication is that this agent is only maximizing over a finite horizon. If we can figure out discounting, all we have to do is find a tractable way of computing these discounted Q-values. </p><div><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\text{Q}^\\gamma_u(h_{<t}a_{t}) =\\mathbb{E}_{o_t | h_{<t}a_t}\\left[u(h_t) + \\max_{a_{t+1}}\\gamma\\text{Q}^\\gamma_u(h_{<t+1}a_{t+1})\\right]\n\n \"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">Q</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.253em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.297em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;\">γ</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">E</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.275em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">o</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.227em; padding-right: 0.06em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span></span></span></span></span><span class=\"mjx-mrow MJXc-space1\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size3-R\" style=\"padding-top: 1.256em; padding-bottom: 1.256em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-munderover MJXc-space2\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">max</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 0.348em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;\">γ</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">Q</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.253em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.297em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;\">γ</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">u</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.372em; padding-bottom: 0.298em;\">t</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size3-R\" style=\"padding-top: 1.256em; padding-bottom: 1.256em;\">]</span></span></span></span></span></span></div><p>It requires no great leap of imagination to see that we could learn them. </p><hr class=\"dividerBlock\"/><h1>A Personal Digression</h1><p>I poured so much love and so many words into <a href=\"https://www.lesswrong.com/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure\">Towards a New Impact Measure</a> that I hurt my wrists. For some time after, my typing abilities were quite limited; it was only thanks to the generous help of my friends (in particular, John Maxwell) and family (my mother let me dictate an entire paper in <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\LaTeX\"><span class=\"mjx-mrow\" style=\"width: 2.781em;\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">L</span></span><span class=\"mjx-mspace\" style=\"margin-right: -0.325em; width: 0px; height: 0px;\"></span><span class=\"mjx-mpadded\"><span class=\"mjx-block\" style=\"width: 0px; margin-top: -0.266em; padding: 0px 0.53em 0px 0px;\"><span class=\"mjx-box\" style=\"width: 0px; margin: -0.524em 0px -0.018em; position: relative; top: -0.21em;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\" style=\"font-size: 80%;\"><span class=\"mjx-mrow\" style=\"font-size: 88.4%;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span></span></span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mspace\" style=\"margin-right: -0.17em; width: 0px; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span><span class=\"mjx-mspace\" style=\"margin-right: -0.14em; width: 0px; height: 0px;\"></span><span class=\"mjx-mpadded\"><span class=\"mjx-block\" style=\"width: 0px; margin-top: -0.516em; padding: 0px 0.764em 0.246em 0px;\"><span class=\"mjx-box\" style=\"width: 0px; margin: -0.705em 0px -0.025em; position: relative; top: 0.221em;\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span></span></span></span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mspace\" style=\"margin-right: -0.115em; width: 0px; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span> to her) that I was roughly able to stay on pace. Thankfully, physical therapy and newfound dictation software have brightened my prospects. </p><p>Take care of your hands. Very little time passed between &quot;I&#x27;m having the time of my life&quot; and &quot;<u>ow</u>&quot;. Actions you can take right now:</p><ul><li>buy an <a href=\"https://www.amazon.com/Ergonomic-Mouse-Vertical-Wireless-Rechargeable/dp/B07BFCVJZC/ref=sr_1_6?s=pc&ie=UTF8&qid=1546020223&sr=1-6&keywords=ergonomic+mouse\">ergonomic mouse</a> and <a href=\"https://www.amazon.com/gp/slredirect/picassoRedirect.html/ref=pa_sp_atf_aps_sr_pg1_2?ie=UTF8&adId=A1037228140SDXCAQG7RQ&url=https%3A%2F%2Fwww.amazon.com%2FGimars-Memory-Keyboard-Support-Computer%2Fdp%2FB01M11FLUJ%2Fref%3Dsr_1_2_sspa%3Fie%3DUTF8%26qid%3D1546020274%26sr%3D8-2-spons%26keywords%3Dergonomic%2B%2Bkeyboard%2Brest%26psc%3D1&qualifier=1546020274&id=6279862686373180&widgetName=sp_atf\">keyboard rest</a></li><li><a href=\"https://www.webmd.com/back-pain/typing-posture-pain-prevention#1\">correct your posture</a>, perhaps assisted by a <a href=\"https://www.amazon.com/Posture-Corrector-Men-Women-Truweo/dp/B07DKHTKP3/ref=sr_1_4_s_it?s=hpc&ie=UTF8&qid=1546020071&sr=1-4&keywords=posture+corrector\">posture corrector</a> or a <a href=\"https://www.amazon.com/Modvel-Cushion-Posture-Corrector-Traveling/dp/B0757X6PC7/ref=sr_1_10?ie=UTF8&qid=1546020162&sr=8-10&keywords=posture+corrector+chair\">lower back cushion</a></li><li>start taking regular breaks</li><ul><li>In particular, don&#x27;t type 80 hours a week for four weeks in a row</li></ul></ul><p>I&#x27;m currently sitting on book reviews for <em>Computability and Logic</em> and <em>Understanding Machine Learning</em>, with partial progress on several more. There are quite a few posts I plan to make about AUP, including:</p><ul><li>exploration of the fundamental intuitions and ideas</li><li>dissection of why design choices are needed, shining light onto how, why, and where counterintuitive behavior arises</li><li>solution of problems open at the time of the initial post, including questions of penalizing prefixes, time ontologies, and certain sources of noise</li><li>chronicle of AUP&#x27;s discovery</li><li>proposal of a scheme for using AUP to accomplish a pivotal act</li><li>discussion of my present research directions (which I have affectionately dubbed &quot;Limited Agent Foundations&quot;<span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char\"></span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span>), sharing my thoughts on a potential thread uniting questions of mild optimization, low impact, and corrigibility</li></ul><p>My top priority will be clearing away the varying degrees of confusion my initial post caused. I tried to cover too much too quickly; as a result of my mistake, I believe that few people viscerally grasped the core idea I was trying to hint at.</p><hr class=\"dividerBlock\"/><p><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"^ 1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char\"></span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span> I&#x27;m fairly sure that the <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"N = 90\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">90</span></span></span></span></span></span> Sushi clinginess result is an artifact of the online learning process I used; the learned attainable set Q-values consistently produce good behavior for planning agents with that budget. Furthermore, the Sokoban average performance of .45 (14/20 successes) strikes me as low, and I expect the final results to be better. Either way, I&#x27;ll update this post once I&#x27;m back at university.</p><p>(Since I anticipate running further experiments, the “Results” section is rather empty at the moment.)</p><p><em>ETA: this was indeed the case. The linked paper has been updated; the original is <a href=\"https://www.scribd.com/document/396473479/Attainable-Utility-Preservation\">here</a>.</em></p><p><span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char\"></span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span> Not to be taken as any form of endorsement by MIRI.</p>",
    "user": {
      "username": "TurnTrout",
      "slug": "turntrout",
      "displayName": "TurnTrout"
    }
  },
  {
    "_id": "XmqqkfY8XAJ6LkwdP",
    "title": "Akrasia is confusion about what you want",
    "slug": "akrasia-is-confusion-about-what-you-want",
    "pageUrl": "https://www.lesswrong.com/posts/XmqqkfY8XAJ6LkwdP/akrasia-is-confusion-about-what-you-want",
    "postedAt": "2018-12-28T21:09:20.692Z",
    "baseScore": 27,
    "voteCount": 22,
    "commentCount": 7,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<blockquote><em>It’s 2 pm. You’ve had a report to write since yesterday morning but you just don’t feel like doing it. You’ve tried to start on it several times, but each time your mind simply refuses to engage with the task. You stare at the screen for a while, hands perched over keys, but nothing comes. Eventually you do something else for a while hoping inspiration will strike while you’re away, but instead you spend hours on other tasks while the report languishes. Eventually it’s 6 pm, the report was due at 5, so you work late and force yourself to get it done, only finally making progress because you feel the threat of consequences for not delivering. You push through and finish around 11, crash, and then wake up the next day to find it a struggle to do even the things you love: it feels like you’ve burned all your willpower and you’ve become a husk of a real person.</em></blockquote><blockquote><em>Fast-forward to the weekend. You’ve finally recovered from writing The Report and you have two whole days for things you love. “This,” you say to yourself, “is the weekend I finally make some real progress on learning to play the bouzouki.” You get our your</em> <em>Bouzouki for Beginners</em> <em>book, tune your bouzouki, and play for about 10 minutes before you remember you had to do that other thing. That other thing is very important, so you put down the bouzouki to go knock out the other thing so you can get back to Bouzouki Weekend 2018. But while doing the other thing you remember you haven’t checked Facebook in a while, and you don’t want your friends to think you forgot about them, so you do that for a bit. Then you start cleaning, notice the bouzouki is there, and clean around it so you can come back to it when you’re done. By now you’ve worked up an appetite, so you make lunch. You would get right back to the bouzouki after eating but it would be so nice to take a nap, so you do that. You’re awoken from the nap by a call from your mother, so you talk to her for a while because you like catching up with family. After you get off the phone you realize it’s getting late so you better get to some bouzouki playing, but you have to get ready to go out tonight with your friends because that’s going to be fun! Oh well, Bouzouki Weekend 2018 is only half-over, there’s still tomorrow. “I’ll learn to play the bouzouki one day,” you say to yourself.</em></blockquote><p>In the first part of the vignette we might say our protagonist procrastinated because they put off writing the report now to write it later, but they don’t seem to be suffering from the sort of <a href=\"https://medium.com/@dr_eprice/laziness-does-not-exist-3af27e312d01\">laziness</a> we typically associate with procrastination. After all, they tried to do the work, they just weren’t able to make themselves do it. In the second part they got distracted a lot, but all the distractions were things they honestly also wanted to do, so that’s not exactly procrastination or laziness either. Yet in both cases there was an activity they clearly wanted to do that didn’t get done, or only got done by burning through willpower and feeling exhausted afterwords, unable to do much else. What’s going on?</p><p><a href=\"https://www.lesswrong.com/posts/uKoqrgnRoWjhneDvM/improving-the-akrasia-hypothesis\">Many</a> <a href=\"https://www.lesswrong.com/posts/geNZ6ZpfFce5intER/akrasia-hyperbolic-discounting-and-picoeconomics\">people</a> I <a href=\"https://www.lesswrong.com/posts/WMYHEcs5tyFESkjsr/silver-chairs-paternalism-and-akrasia\">know</a> <a href=\"https://www.lesswrong.com/posts/LgavAYtzFQZKg95WC/extreme-rationality-it-s-not-that-great\">might</a> <a href=\"https://www.lesswrong.com/posts/JoERzF8ePGr4zP9vv/self-deception-hypocrisy-or-akrasia\">describe</a> this as a case of <a href=\"https://en.wikipedia.org/wiki/Akrasia\">akrasia</a>, an ancient Greek word literally meaning “no strength/power” but used to mean a lack of willpower or having a weakness of will (c.f. <a href=\"https://en.wikipedia.org/wiki/Aboulia\">aboulia</a> for a related but more general phenomenon that lacks the cognitive dissonance associated with akrasia). A more straight-forward explanation of akrasia is that it’s the thing that going on when you do something other than what you want to do. Akrasia is the thing that’s going on when you want to write a report, play bouzouki, or do some other particular activity and instead find yourself unable to make yourself do it while you do something else instead.</p><p>To get more formal we might describe this as a conflict in your values, wants, desires, motivations, and preferences (a cluster of concepts I unify under the term “<a href=\"https://mapandterritory.org/introduction-to-noematology-fac7ae7d805d\">axias</a>”) due to them being in irrational relationships, “irrational” here meaning specifically <a href=\"https://en.wikipedia.org/wiki/Rational_choice_theory\">not rational</a> in a <a href=\"https://plato.stanford.edu/entries/preferences/#PreLog\">formal sense</a>. We could formally describe akrasia then in terms of how it fails to satisfy the rationality criteria, in particular how it fails to satisfy the criterion of asymmetry, where asymmetry means that given any axias A and B, if you prefer A to B, then you do not prefer B to A. Akrasia then seems to be what happens when asymmetry goes unsatisfied.</p><p>Consider what was happening when our protagonist wasn’t playing bouzouki. They wanted to play bouzouki, yet after a very brief attempt they didn’t play bouzouki. So they claim they want to play bouzouki more than not play it, yet we observe them not playing bouzouki instead of playing it, thus we have conflicting lines of evidence about the relationship between these two preferences. On the one hand this might be <a href=\"https://mapandterritory.org/revealed-and-stated-identity-64c6ec070f4f\">revealing a conflict</a> between <a href=\"https://en.wikipedia.org/wiki/Revealed_preference\">stated and revealed preferences</a>, i.e. a <a href=\"https://www.lesswrong.com/posts/z3cTkXbA7jgwGWPcv/would-your-real-preferences-please-stand-up\">difference</a> between what they say they want and what they do, but on the other this might reflect an actual failure of asymmetry. Deciding which isn’t important though, because although conflicting preferences are part of the story of akrasia, they aren’t the whole story, and the trick to understanding and ultimately dissolving akrasia is seeing how it arises in a context that looks beyond conflicting preferences, because as I’ll shortly explain, akrasia is a problem of how we relate to our preferences, not our preferences themselves.</p><h3><strong>Whence preference conflict</strong></h3><p>If we’re going to dissolve akrasia, it’ll help to have a firmer grasp on it’s etiology so we understand just where it’s coming from. Knowing that it arises with an apparent failure of preference asymmetry isn’t enough to really see what’s going on. For that we have to delve into some <a href=\"https://www.lesswrong.com/posts/wDP4ZWYLNj7MGXWiW/in-praise-of-fake-frameworks\">fake models</a>.</p><p>I don’t of course mean here that these models aren’t useful or don’t <a href=\"https://www.lesswrong.com/posts/tKwJQbo6SfWF2ifKh/toward-a-new-technical-explanation-of-technical-explanation\">reveal something about reality</a>, merely that they are “fake” the same way all models are “fake”: they <a href=\"https://www.lesswrong.com/posts/vKbAWFZRDBhyD6K6A/gears-level-and-policy-level\">compress reality</a> in a way that <a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\">helps us understand it</a>, but do so at the cost of accurately describing reality just as it is. As this blog’s title reminds us, <a href=\"https://www.lesswrong.com/posts/KJ9MFBPwXGwNpadf2/skill-the-map-is-not-the-territory\">the map is not the territory</a>. This will turn out to be important, though, because akrasia is, in my estimation, entirely <a href=\"https://www.lesswrong.com/posts/3up8XBeGGHf77sNR4/the-map-has-gears-they-don-t-always-turn\">the result of models causing us to be confused about what is</a>.</p><p>Most people I know who have expressed feeling akrasia also primarily use one or more <a href=\"https://en.wikipedia.org/wiki/Dual_process_theory\">dual-process models</a> to help them understand the <a href=\"https://mapandterritory.org/a-foundation-for-the-multipart-psyche-79a66292a7a\">psyche/mind</a>. Dual-process models <a href=\"https://sideways-view.com/2017/02/19/the-monkey-and-the-machine-a-dual-process-theory/\">suggest</a> the psyche is <a href=\"http://mindingourway.com/not-yet-gods/\">made up</a> of <a href=\"https://www.lesswrong.com/posts/du395YvCnQXBPSJax/how-you-make-judgments-the-elephant-and-its-rider\">two parts</a>, those <a href=\"https://www.lesswrong.com/posts/NsiPGT7iFQ8wGtGoA/lizard-jockeying-for-fun-and-profit\">parts</a>roughly being the S1-elephant-id-unconscious-near part and the S2-rider-superego-conscious-far part. For book-length explorations of dual-process theory see <a href=\"https://www.nytimes.com/2011/11/27/books/review/thinking-fast-and-slow-by-daniel-kahneman-book-review.html\">Kahnemann’s <em>Thinking Fast and Slow</em></a> and <a href=\"https://nintil.com/2018/01/16/this-review-is-not-about-reviewing-the-elephant-in-the-brain/\">Hanson &amp; Simler’s<em>Elephant and the Brain</em></a>.</p><p>Within dual-process theory, we should expect preference conflicts, especially failures of asymmetry, to arise when the S1 part of the mind wants something different than the S2 part. Since <a href=\"https://entirelyuseless.wordpress.com/2018/08/04/employer-and-employee-model-of-human-psychology/\">S1 is much more powerful than S2</a>, even if S2 is “smarter”, S1 will usually win unless S2 expends a lot of energy to rein in S1 and make it do what it wants. On this model it then seems that akrasia is just preference asymmetry, albeit asymmetry caused by conflict between parts, and fighting it should consist mainly of finding ways to get S1 aligned with S2 (since obviously S2, being smarter, knows what is best for S1). Our best bet for defeating akrasia then should be something like <a href=\"https://www.beeminder.com/\">Beeminder</a> or <a href=\"https://complice.co/\">Complice</a>, and we probably can’t hope to do much better.</p><p>I hope that previous paragraph threw out some red flags for you, but in case it didn’t here’s where I see cracks beginning to form in dual-process theory’s explanation of akrasia. People with akrasia tend to identify with S2 as the real, true self. Maybe not identify with it as strongly as one might identify with the virtual homunculus in the <a href=\"https://en.wikipedia.org/wiki/Cartesian_theater\">Cartesian theater</a> if you’re a <a href=\"https://plato.stanford.edu/entries/dualism/\">mind-body dualist</a>, but identify with it enough to privilege it over S1 such that in a conflict between the two they would prefer, all else equal, that S2 win. And this seems reasonable, even from S1’s perspective, because we get <a href=\"https://www.lesswrong.com/posts/CZnBQtvDw33rmWpBD/guilt-another-gift-nobody-wants\">lots of signals from other people</a> telling us that <a href=\"http://mindingourway.com/guilt/\">what’s best for us</a> is that which is associated with S2, <a href=\"http://www.overcomingbias.com/2010/06/near-far-summary.html\">far construal level</a>, and the <a href=\"https://en.wikipedia.org/wiki/Id,_ego_and_super-ego#Super-ego\">superego</a>.</p><p>But this identification with or privileging of S2 is suspect because there’s nothing about S2 to suggest it’s the “real” self. If there is anything worthy of calling one’s self*, it’s made up of both S1 and S2 (and maybe more things besides!). S2 is special, but S1 is equally special, and each brings its own powers to the table. If it were otherwise, <a href=\"https://plato.stanford.edu/entries/evolutionary-psychology/\">you wouldn’t have evolved to have a brain so complex and capable</a> that you’d usefully be able to use dual-process theory to make sense of it. To put my thumb on it, S1 is not a <a href=\"https://en.wikipedia.org/wiki/Spandrel_%28biology%29\">spandrel</a> getting in your way; it’s a useful part of you helping you be you and live your life!</p><p>So if that’s the case, what the heck is going on that a person would interpret through a dual-process model as akrasia? Well, it’s just what we’ve already seen: <a href=\"https://mapandterritory.org/a-foundation-for-the-multipart-psyche-79a66292a7a\">your mind is complicated</a>, <a href=\"https://mapandterritory.org/what-value-subagents-868b3b3fc076\">you may have different parts of it producing different desires</a>, and then the whole thing puts those together and makes a choice about what to do. In the end, <a href=\"https://mapandterritory.org/revealed-and-stated-identity-64c6ec070f4f\">you only “want” one thing</a> (your revealed preference) even if you had to weight many things to come to it (your stated preferences) and <a href=\"http://slatestarcodex.com/2016/09/12/its-bayes-all-the-way-up/\">your confidence in your synthesis of your desires is low</a>.</p><p><strong>Akrasia, then, is a kind of</strong> <strong><a href=\"https://mapandterritory.org/suffering-and-intractable-pain-e7115a5acae3\">suffering</a></strong> <strong>that arises from</strong> <strong><em>identifying with particular desires</em></strong> <strong>in spite of having already given them their</strong> <strong><a href=\"http://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/\">fair weighting</a></strong> <strong>in</strong> <strong><a href=\"http://slatestarcodex.com/2018/03/04/god-help-us-lets-try-to-understand-friston-on-free-energy/\">coming to a choice</a></strong> <strong>of action</strong>. It can exist with or without a belief in the usefulness of dual-process theory; that was just a way to draw the way we identify with desires into relief. The key thing is that we experience akrasia because of identifying with our desires, and this also suggests it’s “easy” to stamp it out: just stop doing that!</p><p><em>*N.B. I’m a Buddhist and a phenomenologist, so my relationship with self is complicated. So just to lay all my cards on the table, although I consider them not entirely relevant here, I think there is</em> <em><a href=\"https://plato.stanford.edu/entries/buddha/#NonSel\">no-self</a>, and also that there is some small thing we might give the name “self” that refers to the</em> <em><a href=\"https://mapandterritory.org/methods-of-phenomenology-e2f936651ff\">irreducible subject of experience</a>.</em></p><h3><strong>Grappling with identity</strong></h3><p>Okay, great, so akrasia isn’t really real — it’s an artifact of the way we understand ourselves and identify with that understanding, and it will evaporate if we can stop doing that and get back to reality itself. That’s not exactly advice about what to do, even if <a href=\"https://mapandterritory.org/the-personal-growth-cycle-34ec1c218615\">the first step of the journey is often just knowing you could go somewhere else</a>, and it feels a bit unfair of me to dissolve akrasia in theory but not help you dissolve it in practice, so let me give you an exercise that might help set you on your way if you’d like to have <a href=\"https://mapandterritory.org/doxa-episteme-and-gnosis-ea35e4408edd\">not just episteme of the true nature of akrasia, but gnosis</a>.</p><p><strong><em>Content Warning: The rest of this section asks you to work through a process that might best be described as self-applied psychotherapy. If you are or are at risk of being suicidal, psychotic, or otherwise in need of mental healthcare, I ADVISE YOU TO SKIP THE REMAINDER OF THIS SECTION. Self-help techniques can be powerful and transformative, thus they are never 100% safe, and so should be used only under supervision if you are not sufficiently mentally healthy and resilient to handle whatever shadows these questions might bring up. That said, this line of questioning might help you get a better handle on existing akrasia: it did for me.</em></strong></p><p>Identify a recent event where you experienced akrasia. Maybe you were writing a report or playing the bouzouki as in the story that I opened with, maybe it was something more nebulous like wishing you had lived up to some virtue through your actions. If you come up with something more nebulous, try to make it more concrete first by identifying a particular action through which you expressed akrasia. Same goes if you initially identified something more goal-oriented. Have in mind something less like “I wanted to play the bouzouki but didn’t” and more like “I wanted to play the bouzouki on Tuesday night but watched reruns of <em>I Love Lucy</em> instead”.</p><p>Now ask yourself what your akratic actions imply to you about who you are? How do you feel when you think about your akrasia experience? Where in your body do you feel it? In your gut? In your chest? In your head? Behind your eyes? What is the story you tell yourself when we see that you wanted to do one thing and did another?</p><p>Why does that worry you? Rest on this question for a moment and see what comes up. You are thinking about this and suffering via akrasia, so try to put a name to that worry. Are you grasping, clinging to, or trying to achieve something and worried you’re not doing it? Are you worried about what you are doing? Try to give it a name.</p><p>Now try to imagine what would happen if the things you are worried about happened. What would happen to you? How would that change who you are? Would you be a different person?</p><p>Finally, take your answers to those questions and ask, what makes you so sure? Why do you think that would happen, you would change in that way, or you would be different (or not)? How can you test those beliefs? If you can, find a way to safely carry out one or more of those tests and see what you learn. You might be surprised what your learn about your relationship to your own identity.</p><p>Repeat going through this process as often as you like. The point of it is to help you to deconstruct your hidden assumptions about the relationship between your actions and your identity. Without prejudicing your insights too much, I suspect you will find <a href=\"https://qualiacomputing.com/2018/07/23/open-individualism-and-antinatalism-if-god-could-be-killed-itd-be-dead-already/\">there is less you than you thought</a>, the you that is there is <a href=\"http://slatestarcodex.com/2017/09/18/book-review-mastering-the-core-teachings-of-the-buddha/\">less permanent than you thought</a>, and the suffering of akrasia is being created by trying to hold on to some idea of yourself that was at best only a dream.</p><h3><strong>What if I still don’t do what I ought?</strong></h3><p>Even after completely dissolving akrasia we might still find we want things that are in conflict. This is normal, because <a href=\"https://www.lesswrong.com/posts/pKLYi8EAgk3njPhxu/rationality-compendium-principle-2-you-are-implemented-on-a\">humans are irrational</a> both in the formal and folk senses of that word. We might still fail to have asymmetric preferences and find ourselves doing things we’d in some sense prefer not to do. So be it. That doesn’t have to be akrasia, though, so long as we don’t start identifying with our contradictory desires. To return to the opening story again, wanting to play the bouzouki and never doing it doesn’t have to mean you experience akrasia. So long as you both accept that you both want to play the bouzouki and don’t want to play the bouzouki enough to do it instead of something else, you won’t suffer from akrasia because you are sure you are doing just what you want.</p><p>Further, I’m <a href=\"https://www.lesswrong.com/posts/9ojpcBsYuJFNKgp4E/there-is-no-akrasia\">not the first</a> to suppose akrasia isn’t real. In fact, it goes right back to the first known appearance of the term with <a href=\"http://www.kimonlycos.org/unpublished/socrates-and-akrasia-reading-protagoras\">Plato’s Socrates saying as much</a>. And some experiences of akrasia may be misunderstandings of desires that have more to do with <a href=\"https://www.lesswrong.com/posts/rbXoZFWdwaP4pW9hu/intrinsic-motivation-is-crucial-for-overcoming-akrasia\">appropriate lack of motivation to do something</a>than conflicting desires or identifying with them or may be misconstrued <a href=\"https://www.lesswrong.com/posts/Ty2tjPwv8uyPK9vrz/my-algorithm-for-beating-procrastination\">procrastination</a>. So if after reading all I’ve said above you find you still have some lingering sense that akrasia is real, check what others have had to say.</p><p>Finally, what I’ve described above may feel like “nothing more” than a subtle shift in perspective, but it’s an important one if you want to learn to accept yourself as you are, something I view as foundational to better making progress on whatever it is you care about. So long as you are deluded, thinking you want to do one thing when in fact you want to do another, you’ll always struggle to change your behavior (if that’s what you want to do!) because you’re acting based on a confusion. Akrasia is just one way this confusion powerfully manifest itself, and learning to sublimate it is an import step along whatever path you take.</p><p><em><a href=\"https://mapandterritory.org/akrasia-is-confusion-about-what-you-want-3fe8b33541e3\">Originally posted on Map and Territory of Medium.</a></em></p>",
    "user": {
      "username": "gworley",
      "slug": "gordon-seidoh-worley",
      "displayName": "Gordon Seidoh Worley"
    }
  },
  {
    "_id": "j5wNN664wBEz7CjGX",
    "title": "Lesswrong Sydney - Dinner - Scott Aaronson comes to town!",
    "slug": "lesswrong-sydney-dinner-scott-aaronson-comes-to-town",
    "pageUrl": "https://www.lesswrong.com/events/j5wNN664wBEz7CjGX/lesswrong-sydney-dinner-scott-aaronson-comes-to-town",
    "postedAt": "2018-12-28T18:33:53.592Z",
    "baseScore": 7,
    "voteCount": 1,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>(Meet us on lvl 2 in the fishbowl) Dinner is in our usual location, at 6pm.<br/></p><p><a href=\"https://www.scottaaronson.com/blog/?p=4043\">https://www.scottaaronson.com/blog/?p=4043</a><br/>Scott Aaronson of mathy brainpower is dropping by Australia. Let&#x27;s celebrate!<br/></p><p>&quot;Most of my research deals with two questions: first, what are the ultimate limits on what can feasibly be computed in the physical world? Second, how can studying those limits shed light on basic issues in physics and cosmology? The first question involves bringing physics into computational complexity theory; the second, bringing computational complexity theory into physics.&quot;<br/></p><p>0438481143 if you need me.</p><p><a href=\"https://www.meetup.com/rationalists_of_sydney/events/vvmbwpyzcbwb/\">https://www.meetup.com/rationalists_of_sydney/events/vvmbwpyzcbwb/</a></p><p><a href=\"https://www.facebook.com/events/275932916454026/\">https://www.facebook.com/events/275932916454026/</a></p><p></p>",
    "user": {
      "username": "Elo",
      "slug": "elo",
      "displayName": "Elo"
    }
  },
  {
    "_id": "x4M2E4znPsaffeQSj",
    "title": "Card Balance and Artifact",
    "slug": "card-balance-and-artifact",
    "pageUrl": "https://www.lesswrong.com/posts/x4M2E4znPsaffeQSj/card-balance-and-artifact",
    "postedAt": "2018-12-28T13:10:00.323Z",
    "baseScore": 8,
    "voteCount": 2,
    "commentCount": 3,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Previously: <a href=\"https://thezvi.wordpress.com/2018/12/26/artifact-embraces-card-balance-changes/\">Artifact Embraces Card Balance Changes</a>, <a href=\"https://thezvi.wordpress.com/2018/12/27/card-collection-and-ownership/\">Card Collection and Ownership</a></p>\n<h3>VII</h3>\n<h3>Card Balance</h3>\n<p>To what extent should cards in a collectible card game be intentionally unbalanced?</p>\n<p>Before Artifact’s recent changes, it was clear that Axe was the best red hero, and Drow Ranger was the best green hero. Playing a red deck without Axe, or a green deck without Drow Ranger, was not a strategic choice. It was a sure sign that the player didn’t own the card in question.</p>\n<p>Is that… bad?</p>\n<p></p>\n<p>Queens are better than rooks, which are better than bishops. On a level playing field between players, there’s nothing wrong with that.</p>\n<p>If everyone was constructing a chess deck, and it was always a king and queen, two rooks, two bishops, two knights and eight pawns, something would have gone wrong somewhere, even if the game remained chess. But what if everyone who owned them always played a queen and usually two rooks, but disagreed about how many bishops and knights to use? That might be fine. Especially if given a few similar other pieces as options.</p>\n<p>Colors are balanced by giving each uniquely powerful cards and abilities. Decks and strategies are created by giving out powerful tools to choose between.</p>\n<p>The tension between the card that fits your color requirements, the card that does the thing you want, and the most powerful cards, is central to any collectible card game.</p>\n<p>What everyone agrees is bad are oppressive <em>decks. </em></p>\n<p>To only a lesser extent is an oppressive <em>color </em>or <em>card </em>an issue.</p>\n<p>When fields in Magic’s standard format were recently over 50% very similar red/black decks, with nothing even the best players could do about it, that was very bad. In that case, it was too late to pull out the banhammer, so we rode out the damage. But there is a clear issue that Magic’s standard format seems recently to be quite vulnerable to having a Best Deck take over and proving unable to adjust to fix it.</p>\n<p>If there were eight tier one decks, each with a different strategy, but there was a card that was in all of them, would that be a problem? It would indicate a likely color imbalance. That would be an issue. But so long as games were not too often determined by who drew this excellent card, it would not be a major concern of mine. The more generic the card, the less concerned I would be.</p>\n<p>Games need building blocks. If cards like Shock, Duress or Cancel became automatic includes for a time, as long there’s still room left for customization, it seems mostly fine.</p>\n<p>Now return to Axe and Drow Ranger.</p>\n<p>From one point of view, pre-change Axe was a key part of What Red Does in Artifact, and Drow Ranger was a key part of What Green Does in Artifact.</p>\n<p>If you play red, you play Axe and Legion Commander, then if you have a third hero it was probably but not obviously Bristleback. If you wanted five red heroes, you’d have to accept some benchwarmers like Beastmaster and Ursa, or now Timbersaw. I have heard talk of Pugna or an occasional Sven or Tidehunder, even in decks with a second color.</p>\n<p>If you play green, you play Drow Ranger, then you have a second tier group of Omniknight, Magnus, Trent Protector and Lycan, and arguably Rix, Abaddon and Chen, from which you choose your additional heroes according to what you prefer and what your deck is up to. Trent Protector is usually what competitive green wants to be doing, so it usually ends up with the two slot.</p>\n<p>What about blue and black?</p>\n<p>Blue’s best hero is probably Kanna, but it’s not obvious or universal, followed by Luna, Zeus and Ogre Magi, then likely Skywrath Mage. Specialists can get some work out of Prellex, Venomancer or Crystal Maiden.</p>\n<p>Black’s best hero is Phantom Assassin, which should be all but universal. The second tier is Bounty Hunter (which is much stronger now that Axe does not kill him in one blow and can’t ever survive two of his), Sorla Kahn, Tinker, Sniper and Lich. I can see arguments for any of them. If you run mono-black, Storm Spirit becomes playable.</p>\n<p>Each color has twelve heroes, one of which is the fallback free basic hero. Of the 48 heroes, I just named <em>32 </em>of them. That’s not only a lot of heroes, that’s <em>two thirds </em>of all the heroes in Artifact. Each color has a signature hero so that hero quality is stronger in decks with extra colors, but most heroes have a constructed purpose.</p>\n<p>What, then, went wrong? Why was this not acceptable? Here are some theories, which likely combined to cause the issue.</p>\n<ol>\n<li>The cards in question, Axe and Drow Ranger, are rare and cost dollars.</li>\n</ol>\n<p>This is definitely a lot of it. We had headlines like “The most expensive card in Artifact costs more than the game” being thrown around, despite this reflecting that the game is cheap, and only being true for a day or so due to a much inflated price.</p>\n<p>If the dominant red hero had been the uncommon Legion Commander instead of Axe, I doubt there would have been half as much complaining. By definition, if something is rare, it is going to be tough for everyone to have enough copies of it. The situation can be seen as a money grab, where cards that are effectively required for play are not sufficiently available.</p>\n<p>Ironically, I also believe that if Artifact had contained <em>mythic </em>rares, but Axe and Drow Ranger had remained rares with similar rarity per pack, then there would have been far fewer complaints about Axe and Drow Ranger.</p>\n<p>Magic, on the other hand, kind of justifies needing four copies of cards that cost a similar amount and have a similar rarity to Axe or Drow Ranger. Even though you <em>also </em>often need four copies of mythic rares, and you need four copies of each card instead of one for heroes in Artifact.</p>\n<p>What is salient, and what is actually going on under the hood, are not as linked as one might hope.</p>\n<p>For packs to be worth money, <em>something </em>in them needs to generate that value. If there are no rares (or mythic rares) to do that, packs won’t be valuable. That has some potentially quite bad consequences, but that is another topic.</p>\n<p>2. Heroes in Artifact are there every game and don’t stay dead</p>\n<p>It is one thing if every deck has Axe.</p>\n<p>It is another thing if every game starts with Axe in play.</p>\n<p>It is a third thing if killing off Axe means he comes back two turns later. Which he does.</p>\n<p>If players understood this as analogous to the queen in a chess game (and chess is an interesting metaphor for Artifact, more so than it is for Magic), then they might be fine with the idea, but most players didn’t see it that way. And yes, it has to be pretty annoying when the card you don’t own is there every game and won’t stay dead.</p>\n<p>3. Heroes in Artifact determine the flavor</p>\n<p>Not sure this one was important, but I definitely noticed it. Artifact has a wonderful set of voiced lines for its various heroes and creeps, depending on situations and the combinations of cards in play. My favorite moment playing is still when Crystal Maiden shouted out “I finally get to kill someone!” If I never get to play Crystal Maiden, I miss out on that type of discovery. Axe and the other top tier heroes have good lines too, and more of them, but by now I have heard them all.</p>\n<p>Players who are coming from DOTA 2, and who are more engaged with the world, story and characters, have even more reason to want more variety of heroes to be played.</p>\n<p>4. Heroes in DOTA 2 are all playable or close to it, and are constantly rebalanced</p>\n<p>DOTA 2 has an insane number of heroes, such that the barrier to full entry is beyond prohibitive. I recently saw in my Google news feed a recommendation that players who want to be good at the game choose 2-3 heroes and stick to them, so you could focus on other aspects of the game, but that it was fine to choose any of the dozens and dozens for your specialization.</p>\n<p>Coming from that context, even without the flavor considerations, it’s easy to see why one might have otherwise unrealistic or unwise demands for heroes to be balanced against each other. It’s also easy to see why they think rebalancing heroes isn’t an issue.</p>\n<p>To me, having some awful choices (also known as ‘skill testers’) is actively great, and not only for limited play, because (among other reasons, Mark Rosewater has written extensively about this) it means some people can try to make them work for fun, and new players get to learn about what is good by figuring out which cards are bad.</p>\n<p>5. Complexity and lack of progression issues were misidentified, and <em>players be whining</em></p>\n<p>The players gonna play, play, play, play, play but they also gonna complain, complain, complain, complain, complain. One of those complaints is <em>always </em>that something in a game is too good, or not good enough.</p>\n<p>That doesn’t mean their complaints are invalid, but it does mean that even in the best of times the complaints exist and ‘have to go somewhere.’</p>\n<p>In this case, it was not the best of times for other reasons. Players lacked any progression or ranking system (other than the misnamed ‘perfect run’ count that pisses me off every time I go 5-1 and it counts as ‘perfect’). Players were all starting from zero in a super complex and hard to understand game. Players were starting with zero collection. Players were comparing the game to the ‘free-to-play’ model purely on cost and looking for ways to be frustrated by the expense, rather than comparing Artifact to Magic, or to an AAA software title, or thinking of the game as a $20 unlimited drafting experience with an upside option.</p>\n<p>So players be whining more than average, especially about various aspects of the economic model. This then spilled over into card balance complaints becoming louder than they would have otherwise been.</p>\n<p>6. Deck balance was hurt by player inexperience</p>\n<p>I talked a bit about this in previous posts but I’ll reiterate a bit.</p>\n<p>In the expert (now ‘prize’) constructed queue, you faced Red/Black aggression a lot, and still do. When one deck dominates, players see the situation as broken and demand action and change.</p>\n<p>Those of us who have been around since the Alpha, or play or watch the major tournaments now, know that Red/Black was never a problem for experts. It is a low tier one deck, at best fourth strongest. I am always happy to see my opponent playing it from a win-expectation standpoint. Despite that, I sometimes think ‘<em>again?’ </em>since I am playing to have fun and to learn, not to get easy wins or grind out free packs.</p>\n<p>Over time, with or without the changes, players would develop and learn additional strong strategies, and the new hotness would change. The resulting red decks would have still used Axe, and the green decks would have still used Drow Ranger, but the rest of the decks would have been more diverse, and that would have taken a lot of the pressure off.</p>\n<p>7. Players never bought into the economic model</p>\n<p>If you don’t own Axe, Axe being expensive looks bad. That is money out of your pocket.</p>\n<p>If you own Axe, Axe being expensive does not look bad to you. If you own a lot of copies of Axe, it looks mighty fine, thank you very much.</p>\n<p>When every player is starting fresh with no collection is exactly the time for players to hate everything that is expensive or necessary, and want entry to be cheap. Only later will they realize the upside of preserving value.</p>\n<p>Which is another way of saying, <em><a href=\"https://thezvi.wordpress.com/2018/12/27/card-collection-and-ownership/\">no, players don’t care</a> about card ownership and value. </em>At least, not yet.</p>\n<p>8. Equality and card balance is the level zero instinct</p>\n<p>People instinctively hate inequality (unless they have the better deal). Because of reasons. Some are even good reasons.</p>\n<p>The natural instinct of most players is to want all the cards to be mostly equal. That seems like the most fun and interesting option.</p>\n<p>I had that preference in Magic for years, as a professional whose dream was to work in Magic R&amp;D. It took <em>years </em>of conversations with those who make the game, and the game’s top players and writers and thinkers, to understand why this instinct was wrong.</p>\n<p>Since then, better explanations have become available, so it is easier to get to where one understands these issues better and embraces card inequality.</p>\n<p>Compare the situation now along all these dimensions, with the situation when Artifact was being tested. The players were more heavily invested in time and attention. They played better, and focused on different decks (partly because a few cards were different, but mostly for other reasons). The metagame shifted multiple times. They had richer experiences with collectible card games and their long term needs. They had unlimited card access for testing purposes.</p>\n<p>I believe strongly that, not only for limited but also constructed purposes, cards should <em>not </em>be of equal power level. There should be staple cards that are in many or most decks of the appropriate color. There should be very good cards that are difficult to use for various reasons, from requiring other effects to work, to being a myriad of colors. There should be bad cards that are exactly what you need in special situations, and good cards that aren’t what you need as often as you would like or expect. There should be bad cards, and terrible cards, to provide skill testing and fun quirky experiences.</p>\n<p>Players should be excited to go out and get better cards that upgrade their options and power level. Not having access to the cards <em>should hurt you, </em>so long as having full access is a reasonable goal for the serious. Demand should be driven. That’s the point.</p>\n<p>Wizards does this consciously with Magic: The Gathering. They take each set, and they ‘push’ selected cards to make them the cream of the crop. Planeswalkers frequently get the nod, as do many other rares and mythics. They are not subtle about this. Their preferred cards will frequently hit you over the head with a ‘play this, everyone!’ They do it more and more obviously and dramatically than I would like, but some of it is good to get people excited and shake things up.</p>\n<p>One alternative would be <em>dramatically smaller </em>card sets. If every card is good enough, then you would want less of them to get the same level of depth, complexity and choice. You would also want less of them to avoid giving decks with limited colors (or similar factions) similar card quality to multi-color decks, and leave people interesting choices. Rather than think of ‘take these 1000 cards we print each year, and instead of 250 of them being viable and 50 being the top, make it 750 and 250’ and think instead of ‘only print 400 cards.’ Or, having printed a set, rebalance existing cards periodically and make less new expansions over time. Those both seem more like reducing choice and discovery. I don’t think they are better.</p>\n<p>This brings us to the related but distinct question of card <em>rebalancing, </em>which I’ll talk about next time. You can embrace a goal of cards being balanced or unbalanced, without implying a stand about when cards should change.</p>\n<p> </p>\n<p> </p>\n<p> </p>",
    "user": {
      "username": "Zvi",
      "slug": "zvi",
      "displayName": "Zvi"
    }
  },
  {
    "_id": "Rb5H2mtL7ozEmLEib",
    "title": "Isaac Asimov's predictions for 2019 from 1984",
    "slug": "isaac-asimov-s-predictions-for-2019-from-1984",
    "pageUrl": "https://www.lesswrong.com/posts/Rb5H2mtL7ozEmLEib/isaac-asimov-s-predictions-for-2019-from-1984",
    "postedAt": "2018-12-28T09:51:09.951Z",
    "baseScore": 39,
    "voteCount": 17,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": "https://www.thestar.com/news/world/2018/12/27/35-years-ago-isaac-asimov-was-asked-by-the-star-to-predict-the-world-of-2019-here-is-what-he-wrote.html?fbclid=IwAR25wKGV6NngxJFnbyVTOBYGRGjzJXLxyGCywDYl806UWXKq-6XxEhZlATw",
    "htmlBody": "<p><em>My vague impressions</em></p><p>The whole essay is conditional on no nuclear war.  Than, he explored two main big trends - computerization and space utilization.  If something like a general model how Asimov did futurology can be extracted from the text, it is extending the large trendline, and than thinking about social consequences. </p><p>In case of space utilization, this failed badly, because the trend extrapolation did not work, and most of the specific predictions are wrong (e.g. we do not have <em>prototype of a solar power station, outfitted to collect solar energy, convert it to microwaves and beam it to Earth </em>or<em> mining station that will process moon soil</em>)</p><p>In case of computerization, the trendline stayed linear. The predictions of social consequences are often good</p><ul><li><em>The growing complexity of society will make it impossible to do without  [computers] except by courting chaos; and those parts of the world that fall  behind in this respect will suffer so obviously (...)</em></li><li>There is a longer part about work, jobs and the force, e.g.: <em>The jobs that will appear will, inevitably, involve the design, the  manufacture, the installation, the maintenance and repair of computers  and robots, and an understanding of whole new industries that these  “intelligent” machines will make possible. ... By the year 2019, however, we should find that the transition is about  over. Those who can he retrained and re-educated will have been: those  who can’t be will have been put to work at something useful, or where  ruling groups are less wise, will have been supported by some sort of  grudging welfare arrangement.</em></li></ul><p>Predictions about international cooperation are less precise - my impression is Asimov got the trend right, but the causal mechanism wrong</p><ul><li><em>In short, there will be increasing co-operation among nations and  among groups within nations, not out of any sudden growth of idealism or  decency but out of a cold-blooded realization that anything less than  that will mean destruction for all.  </em>(It seems the increased coordination was driven more by trade)</li><li><em>By 2019, then, it may well be  that the nations will be getting along well enough to allow the planet  to live under the faint semblance of a world government by co-operation,  even though no one may admit its existence.</em> (This is interesting: if anything has faint semblance of  a world government by co-operation, it&#x27;s probably the financial system / markets)</li></ul><p>Predictions about education are precise with regard to opportunities. He would be probably disappointed how the opportunities are utilized, which is likely caused by the educational system having a lot of hidden goals different from education</p><ul><li><em>There will be an opportunity finally for every youngster, and indeed,  every person, to learn what he or she wants to learn. in his or her own  time, at his or her own speed, in his or her own way. </em></li><li><em>Education will become fun because it will bubble up from within and not be forced in from without.</em></li></ul><p>Overall, it seems to me the essay shows that futurology on this timescale is viable. (With the caveat that as the world got faster, comparable time horizon is  likely shorter)</p>",
    "user": {
      "username": "Jan_Kulveit",
      "slug": "jan_kulveit",
      "displayName": "Jan_Kulveit"
    }
  },
  {
    "_id": "mwd98HCYwqeFyyrg5",
    "title": "1987 Sci-Fi Authors Timecapsule Predictions For 2012",
    "slug": "1987-sci-fi-authors-timecapsule-predictions-for-2012",
    "pageUrl": "https://www.lesswrong.com/posts/mwd98HCYwqeFyyrg5/1987-sci-fi-authors-timecapsule-predictions-for-2012",
    "postedAt": "2018-12-28T06:50:28.202Z",
    "baseScore": 20,
    "voteCount": 6,
    "commentCount": 3,
    "meta": false,
    "question": false,
    "url": "https://web.archive.org/web/20121015205244/http://www.writersofthefuture.com/time-capsule-predictions",
    "htmlBody": "<html><head></head><body><p>I find it interesting how focused people were on certain trends such as venereal disease, which turned out almost irrelevant just 25 years later as drivers of global norms. Of the set I think Algis Budrys wins Most Accurate:</p>\n<blockquote>\n<p>Because we will be in a trough between 20th-century resources and 21st-century needs, in 2012 all storable forms of energy will be expensive. Machines will be designed to use only minimal amounts of it. At the same time, there will be a general expectation that a practical cheap-energy delivery system is just around the corner. Individuals basing their career plans on any aspect of technology will concentrate on that future, leaving contemporary machine applications to the less ambitious or to those who foresee a different future. The most socially approved-of individuals will constitute a narrowly focused aristocracy, and will be at the mercy of dull functionaries and secretive rebels who actually perform the day-to-day maintenance of society. It should be noted that most minimal-energy devices process information and microscopic materials, not consumer goods. The function of \"our\" society may depend on processing information and biotechnology to subjugate goods-producing societies. These societies may be geographically external, or may be yet another social stratum within central North America. In either case, crowd-management technologies will have to turn away from forms that might in any way impair capital goods production. Social regimentation will then have become so deft that most people will regard any other social milieu as pitiable.</p>\n</blockquote>\n</body></html>",
    "user": {
      "username": "ingres",
      "slug": "ingres",
      "displayName": "namespace"
    }
  },
  {
    "_id": "fD5Gh93sZrmQrE7rj",
    "title": "Washington, DC: Predictive Power of Science Fiction",
    "slug": "washington-dc-predictive-power-of-science-fiction",
    "pageUrl": "https://www.lesswrong.com/events/fD5Gh93sZrmQrE7rj/washington-dc-predictive-power-of-science-fiction",
    "postedAt": "2018-12-28T01:24:33.783Z",
    "baseScore": 1,
    "voteCount": 1,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>We will be meeting this Sunday in the courtyard of the National Portrait Gallery to discuss whether science fiction is good at predicting future social or technological changes.</p>",
    "user": null
  },
  {
    "_id": "TKHvBXHpMakRDqqvT",
    "title": "In what ways are holidays good?",
    "slug": "in-what-ways-are-holidays-good",
    "pageUrl": "https://www.lesswrong.com/posts/TKHvBXHpMakRDqqvT/in-what-ways-are-holidays-good",
    "postedAt": "2018-12-28T00:42:06.849Z",
    "baseScore": 20,
    "voteCount": 6,
    "commentCount": 19,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<html><head></head><body><p>I'd like a model of the benefits that holidays (vacations) can have, so that I can plan accordingly. Relevant questions that I have, although feel free to answer ones not listed here:</p>\n<ul>\n<li>Do holidays teach you things about other places that you couldn't learn from Wikipedia?</li>\n<li>Why are holidays more relaxing than just lying in bed at home and paying somebody else to take care of you?</li>\n<li>Does visiting family count as a holiday in the relevant sense?</li>\n<li>Are the benefits of tourism and/or pilgrimage the same as the benefits of holidays? What are they?</li>\n<li>How much money should I be willing to spend on holidays?</li>\n<li>Is 'holiday' a coherent enough category that I can treat it as primitive for the purpose of this question?</li>\n</ul>\n</body></html>",
    "user": {
      "username": "DanielFilan",
      "slug": "danielfilan",
      "displayName": "DanielFilan"
    }
  },
  {
    "_id": "AAamNiev4YsC4jK2n",
    "title": "Sunscreen. When? Why? Why not?",
    "slug": "sunscreen-when-why-why-not",
    "pageUrl": "https://www.lesswrong.com/posts/AAamNiev4YsC4jK2n/sunscreen-when-why-why-not",
    "postedAt": "2018-12-27T22:04:42.597Z",
    "baseScore": 5,
    "voteCount": 3,
    "commentCount": 22,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p> </p><p>Hello,<br/><br/>I&#x27;ve been wondering about sunscreen&#x27;s effectiveness in reducing cancer risk. <br/><br/>The general impression I&#x27;ve gotten in my (admittedly brief) research on it seems: &quot;If you know you&#x27;ll be exposed for a long duration in the summer/spring, then yes, wear sunscreen. Otherwise, Vitamin D generation takes priority.&quot; (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1180647/\">related</a>)<br/><br/>I&#x27;ve looked around on lesswrong, and can&#x27;t find any really all-encompassing informative posts about it. The most interesting comment I&#x27;ve seen was by <a href=\"https://www.lesswrong.com/posts/aBr99pJYM3f8PWx4J/open-thread-aug-24-aug-30#HFWLLkvXHfKtYe3Gp\">Tem42</a>, who references a study that claims that overall cancer rates are actually lower in southern states vs northern states. </p><p>Is my general impression correct enough? Or should people be lathering sunscreen all the time; or not at all?</p><p>Thanks in advance!</p>",
    "user": {
      "username": "viktor-riabtsev-1",
      "slug": "viktor-riabtsev-1",
      "displayName": "Viktor Riabtsev"
    }
  },
  {
    "_id": "85NB33EcRTZ9Tv8i8",
    "title": "Can dying people \"hold on\" for something they are waiting for?",
    "slug": "can-dying-people-hold-on-for-something-they-are-waiting-for",
    "pageUrl": "https://www.lesswrong.com/posts/85NB33EcRTZ9Tv8i8/can-dying-people-hold-on-for-something-they-are-waiting-for",
    "postedAt": "2018-12-27T19:53:35.436Z",
    "baseScore": 28,
    "voteCount": 9,
    "commentCount": 7,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p><em>content note: death, old age, sickness</em></p><p>I&#x27;ve heard numerous anecdotal accounts of sick or old people who are on death&#x27;s door &quot;holding on&quot; in a way suggestive that they were exerting some effort to do, until they had reached closure on some thing (a relative coming to visit, a manuscript published, somebody&#x27;s birthday).</p><p>I could imagine this being a totally real thing that dying people can do for some limited time. </p><p>I can also imagine it just being cherry picked stories that were more a matter of luck.</p><p>It seems likely that there&#x27;s at least situations where, say, eating is difficult/painful, and people continue exerting effort to do that so long as they have something that feels worth it to keep doing so, and then stop putting in the effort after hitting some milestone they cared about.</p><p>Some of the anecdotes I&#x27;ve heard implied something more immediate going on (where someone seemed to be holding on and literally a few minutes or seconds afterwards, died).</p><p>(Possible straightforward mechanism could just be that <em>breathing</em> becomes painful and difficult, and people only keep doing it when they have a concrete goal)</p>",
    "user": {
      "username": "Raemon",
      "slug": "raemon",
      "displayName": "Raemon"
    }
  },
  {
    "_id": "6n9SphKSvCqLitCP8",
    "title": "Reverse Doomsday Argument is hitting preppers hard",
    "slug": "reverse-doomsday-argument-is-hitting-preppers-hard",
    "pageUrl": "https://www.lesswrong.com/posts/6n9SphKSvCqLitCP8/reverse-doomsday-argument-is-hitting-preppers-hard",
    "postedAt": "2018-12-27T18:56:58.654Z",
    "baseScore": 9,
    "voteCount": 7,
    "commentCount": 2,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>“Where is my Doomsday?” asks a prepper on a conspirological site, — “I spent thousands of dollars on ammunition and 10 years on waiting, and still nothing. My ammo is rusting!”</p><p>There is a general problem of predicting the end of the world: it is not happening. There are many reasons for this, but one is purely mathematical: if something didn’t happen for a long time, this is very strong evidence that it will not happen any time soon. If we have no nuclear war for 70 years, its probability tomorrow is very small, no matter how serious are international relations.</p><p>The first who observed this was Laplace with the “<a href=\"https://en.wikipedia.org/wiki/Rule_of_succession\">sunrise problem</a>”. He asked: What is the probability that the Sun will not rise tomorrow, given that it has risen for the last 5000 years. He derived an equation, and the probability of no sunrise is 1/N, when N is the number of days when the Sun has risen. This is known as a rule of succession and Laplace has even more general equation for it, which could account for a situation where the Sun had missed several sunrises. </p><p>The fact that something didn’t happen for a long time is an evidence that some unknown causal mechanism provides stability for the observed system, even if all visible causal mechanisms are pointing on &quot;the end is nigh”. </p><p>“You see, the end of the US is near, as the dollar debt pyramid is unsustainable, it is growing more than a trillion dollars every year” — would say a preper. But the dollar was a fiat currency for decades, and it is very unlikely that it will fail tomorrow.</p><p>The same rule of succession could be used to get a rough prediction of the end times. If there is no nuclear war for 70 years, there is a 50 per cent chance that it will happen in the next 70 years. This is known as the Doomsday argument in<a href=\"https://www.nature.com/articles/363315a0\"> J.R. Gott’s</a> version.</p><p>Surely, something bad will happen in decades. But your ammo will rust first. However, on the civilizational level, we should be invest in preventing the global risks even if they have a small probability, as on a long run it ensures our survival </p><p>This could be called &quot;Reverse Doomsday Argument&quot;,  as it claims that the doomsday is unlikely to be <em>very near</em>.  In AI safety, it is a (relatively weak) argument against near-term &quot;near-term AI risk&quot;, that is, that dangerous AI is less than 5 years from now. </p><p>.</p>",
    "user": {
      "username": "avturchin",
      "slug": "avturchin",
      "displayName": "avturchin"
    }
  },
  {
    "_id": "D62GoptY4uX9e2iwM",
    "title": "What does it mean to \"believe\" a thing to be true?",
    "slug": "what-does-it-mean-to-believe-a-thing-to-be-true",
    "pageUrl": "https://www.lesswrong.com/posts/D62GoptY4uX9e2iwM/what-does-it-mean-to-believe-a-thing-to-be-true",
    "postedAt": "2018-12-27T13:28:15.498Z",
    "baseScore": 1,
    "voteCount": 3,
    "commentCount": 13,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": null,
    "user": {
      "username": "Senarin",
      "slug": "senarin",
      "displayName": "Bae's Theorem"
    }
  },
  {
    "_id": "RpXZm99FCgn9QovsT",
    "title": "Card Collection and Ownership",
    "slug": "card-collection-and-ownership",
    "pageUrl": "https://www.lesswrong.com/posts/RpXZm99FCgn9QovsT/card-collection-and-ownership",
    "postedAt": "2018-12-27T13:10:00.977Z",
    "baseScore": 19,
    "voteCount": 6,
    "commentCount": 8,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Considers issues raised by: <a href=\"https://thezvi.wordpress.com/2018/12/26/artifact-embraces-card-balance-changes/\">Artifact Embraces Card Balance Changes</a>.</p><h3>III. Card Collection and Ownership</h3><p>What does it mean to own a card?</p><p>Good question.</p><p>In different games, with different principles, online and offline, it means different things.</p><p>Let us begin in good old paper Magic: The Gathering. What does it mean to own a card?</p><p>In its literal sense it means you <em>physically own a card. </em>This is my Black Lotus, or my Lightning Strike. The cardboard belongs to me and no one can take that away.</p><p>In a more interesting sense, it means I own a card that I can play in games of Magic, from the tabletop to the biggest tournaments.</p><p>There are catches.</p><p>The first category of catch is that the card might not be legal, or not legal in its current form.</p><p>Wizards can ban or restrict the card in various formats. Formats widely played can change, making the card no longer relevant or legal. Wizards can also <em>errata </em>the card, to change what it does.</p><p>To reassure players, Wizards has implemented a variety of measures against this.</p><p>They’ve stopped all power level errata; cards can be changed because to make them work as intended if a mistake is made, or to fix other rules or consistency issues, but not because the card is too good or too weak.</p><p>They’ve stopped restricting cards entirely outside of Vintage. They’ve promised not to ban (or in Vintage restrict) cards lightly, only doing so in the face of super strong evidence, and even then actively seek to lift such bans whenever feasible. When such moves do prove necessary, their announcements (<a href=\"https://magic.wizards.com/en/articles/archive/news/july-2-2018-banned-and-restricted-update-2018-07-02\">link to a recent example</a>) share their reasoning and data so players can better anticipate future actions.</p><p>Bans remain a concern, but Wizards has built a long track record of responsible behavior, so their impact is minimal except in cases where bans are likely for the right reasons.</p><p>A second category of concern is that what it rare today may be common tomorrow. Wizards might reprint your card, perhaps at a lower rarity, inflating the supply and depressing the price. This came to a head when Wizards printed large quantities of the Chronicles set, crashing the value of many cards. Wizards has since spent decades reassuring players that they will print cards, both new and old, responsibly.</p><p>The lynchpin of this is the <a href=\"https://mtg.gamepedia.com/Reserved_List\">reserve list</a>. Wizards has promised that key old cards will never be reprinted. This has become a valuable costly signal that Wizards will treat all of Magic responsibly. If Wizards were to break the reserve list, it would allow many more players to access the Legacy format, which extensively uses many cards from the reserve list, and Wizards would make a lot of money from selling the cards. But by doing so, Wizards would destroy the faith of its players in the long term value of their collections, card prices would crash, and a key pillar of the game would be gone. This would devastate all cards, not only the cards on the reserve list, because of the signal it would send about Wizards’ governance and future intentions.</p><p>Wizards has printed a number of sets in the Masters series, including Modern Masters, Vintage Masters and recently Ultimate Masters, which provide new copies of many of Magic’s most valuable and popular cards not on the reserve list, increasing player access to the Modern format. The resulting increasing popularity of Modern, combined with the new cards having new art and being printed in responsible quantities, meant that older copies of the same cards <em>increased </em>rather than decreased in value. The high prices on the Masters packs have, I believe, been a key part of this success. There is some reason for concern if such policies continue, but in the medium term this has (at least to me) reinforced Wizards’ reputation of being responsible shepherds.</p><p>A third category of concern is power creep. There is the temptation in each set to print cards more powerful than those seen previously, in order to shake things up, keeping decks and games fresh and new, and giving players motivation to buy the new cards. Doing this hurts the value of old cards, and by anticipation of new ones, and also damages the quality of your game as the balance is destroyed.</p><p>Another solution to all these concerns is provided by the Old School format, and other similar formats that preserve older cards and sets. These provide a permanent place for older cards, with their principles heavily invested in protecting that value. These formats are a key reason older cards are so valuable and that the prices of the oldest cards have exploded.</p><p>I can think of three additional concerns.</p><p>One is that Magic will lose its players and times and places to play, resulting in a loss of value and utility for its cards. A second is that Magic players will lose their respect for the officially printed cards and start widely using proxies and printed copies. A third is the threat of counterfeit cards. All are continued threats that will never be eliminated. I continue to be surprised and impressed that the problem of counterfeits in particular has not become far worse than it has.</p><p>As a resident of New York City, of course, I also have the issue of storage costs, and as an adult with children, sorting costs also loom large. These, rather than the above concerns or issues of cost, are what limit my physical Magic collection today.</p><p>Overall, Wizards has provided robust ownership of physical Magic cards.</p><p>Other physical games provided varying degrees of ownership. The biggest threat in these games is typically power creep.</p><p>All of this has echoes and implications as we move into the digital realm.</p><h3>IV. Digital Card Ownership</h3><p>Ownership of a digital card, like ownership of all digital goods, is a trickier, more slippery concept.</p><p>To what extent do you own a card on Magic Online? In Magic Arena? In Hearthstone or Eternal? In Pokemon? In the remarkably fun old Xbox 360 game <a href=\"https://en.wikipedia.org/wiki/Culdcept_Saga\">Culdcept Saga</a>?</p><p>In Culdcept Saga, your cards are yours, as part of your save file. No one can take that away, unless Microsoft forces an update upon you that does so, although that seems unlikely at this point. You don’t need a central server to play. That’s the good news. The bad news is that you can’t sell your cards other than by transferring your entire save file, and it would not do you any good since they aren’t worth anything, nor are there any assurances of rarity. It is easy to ‘mine’ whatever you want via grinding, if you so desire.</p><p>In Pokemon, as I learned from former fanatic Ari Lax, there is great news. You can transfer the Pokemon. You could also spend hours upon hours grinding to get those perfect Pokemon, as for true competition nothing less than a perfect Pokemon will do. Since it takes so long to generate the Pokemon for sale, there exists (or at least, there existed) a marketplace for them. Their price was presumably based on the cost to grind them. Or if you look at things from a crypto perspective, they required mining, and Pokemon works via proof of work. I’d say proof of play, but few if any ‘players’ crafting perfect Pokemon are going to make that miscategorization.</p><p>You even have a decent amount of protection against the Pokemon being modified or confiscated, since the game is played without a central server that can change what the cards do and rule your particular copy invalid.</p><p>Not half bad. In a real sense, one can be said to own your Pokemon. If you catch them all, you can keep them all.</p><p>Eternal and Hearthstone offer a less generous picture.</p><p>If I own a card in <a href=\"https://thezvi.wordpress.com/2018/02/10/eternal-and-hearthstone-economy-versus-magic-economy/\">Eternal</a> or Hearthstone, I only own it in the context of my account, on the company’s servers. If they change a card, which they frequently do, the old card is gone. If they banned the card, or rotated it out, it would vanish. If they confiscate your account, that’s it. If they take away a mode of play, you can’t get it back. Surplus copies are almost worthless, as they don’t permit variations where extra copies would be allowed, and you can’t transfer or sell them, only destroy them for a small fixed return in dust or shiftstone.</p><p>Packs are given away daily, with reward systems that sometimes hand them out like candy, while other times being stingy. When you get rewards, it feels worthwhile because it is saving you from having to spend money or lose access to cards, but the sum total of all possible rewards is a fully unlocked game until the next expansion rolls around. If the game were to fail, the servers would shut down, the game would become unplayable and your cards would be worthless in all senses.</p><p>That’s quite the grim picture. They take away buying and selling, transfer and independent play, and any practical expectation of respect for value earned, but still demand unlocking all cards one by one. This leaves games that demand a grind to keep you playing, and give you nothing in return except the right to keep playing. Even when there’s a fun game under there, most time spent with the game largely fails to find it. <a href=\"https://thezvi.wordpress.com/2018/03/30/the-eternal-grind/\">I’ve discussed my dislike for such models before</a>.</p><p>Don’t get me wrong. Things could be <em>so </em>much worse on all such fronts. True best-practices-following mobile games make you appreciate how truly generous, and not-utterly-utility-destroying, Eternal’s or Hearthstone’s business model is by comparison. As an example, see Plants vs. Zombies Heroes, a game I was pointed to by good friend Sam Black. There seems to be a solid lane game hidden under there, but we will never know. We’re too busy letting the phone play ads to get gems to buy a pack.</p><p><a href=\"https://magic.wizards.com/en/mtgarena\">Magic the Gathering: Arena</a> is a step up from Eternal and Hearthstone on the card ownership front. Cards can’t be modified due to their ties with physical Magic: The Gathering, and banning cards means banning them in physical play. This makes your ownership relatively secure.</p><p>That still leaves all the other weaknesses of the free to play economic model (minus some marginal other improvements Arena makes). Play is confined to Wizards’ servers, under their rules, and their judgment that you continue to own an account and its cards, with their database being the only judge on whether you own a card. You can’t buy, sell or transfer cards outside of buying gems in the official store. Wizards is free to give out cards like candy at some point in the future. Even the default formats of play are sometimes rotated in and out, and could disappear entirely. If you left your Arena collection alone for several years, you would be unlikely to have much of value or be able to competitively play anything upon your return.</p><p>There is tons of cool stuff one can do with Magic (or to a lesser extent Eternal or Hearthstone) cards. Many weekends on Arena feature one of these cool things! They charge you in-game currency to play. Then after a few days the opportunity is gone, perhaps never to return.</p><p>Magic Online attempts to render your cards as your cards. You can buy and sell them freely with other players, so long as you go through the game’s interface. Recent cards can even be redeemed for physical cards. Wizards brought its record and reputation as responsible steward and promised explicitly not to print more than a small additional amount of each product each year, to maintain collection value. A full Magic Online account, with four copies of each card, at one point was worth over $20,000.</p><p>Then came the treasure chests. Wizards removed and broke its promises about quantity of reprints. While there were positive short term effects on the Magic Online economy, and one can make a case that cheaper Magic is better long term for all, collections have tanked in value as they are inflated away. One is reminded of governments that realize they can print money, as the currency slowly is drained of its value. With Magic Arena’s new popularity, there is additional fear that Wizards will stop supporting Magic Online. With play only possible on its servers and all the accompanying restrictions, there is real danger that the platform will not even survive more than a few years out.</p><p>Magic Online extracted a ton of money from many players, myself included, by providing a way to play Magic on a computer and doing a good enough job with card ownership and collection value to justify the investment. I can’t say I didn’t get my money’s worth in good times and good practice, but it is sad to see that things could not be sustained. Better stewards, and better ways of protecting against corporate policy changes, proved necessary.</p><p>This brings us to Artifact.</p><p>Artifact makes its cards transferable. That’s huge. You can only sell them through the steam marketplace, which is bad, and it charges a hefty fee on each trade, which is worse. By default you get paid in steam credit. But even crippled, the fact that you can trade at all is huge. It is such a relief to be able to buy cards directly at reasonable prices. at all, and to sell even at a substantial discount.</p><p>Valve also, prior to the recent update, promised a philosophy of not modifying cards except in extreme situations, and made a point not to give away packs unless money was spent or tournament entry fees were risked. Combined with a commitment to a million dollar first prize tournament, and Valve’s long term credibility and track record, there was reason to trust an investment in an Artifact collection.</p><p>There were catches, of course. Other big limitations existed on card ownership. You can only play Artifact on Valve’s servers, with its official play modes. Your cards are entries in their database. They could in theory confiscate your steam account at any time, or more likely they or you could be hacked. The game could shut down and take your cards with it, or play modes could shift and rotate.</p><p>I have always been of two minds about whether I mostly own the games I have on steam and on similar download services, since I can’t lose or misplace or damage them and the kids can’t destroy them, or if I mostly own the games I have physical copies for. Same goes for music and other media. In practice I’ve been far happier with services like steam, but the omnipresent danger is definitely there. Our ownership, and in an important sense our civilization, grow more fragile.</p><p>I still can’t get my phone to reliably resume playing podcasts I had already started, if I go into the subway and lose internet access. It’s a problem. But I digress.</p><p>Last week’s changes take away two important pillars.</p><p>Players are now given packs and event tickets as a reward for play. The rewards are limited in scope, so in their current form this should not have too big an impact. There is already danger, if too many players who are exclusively playing limited get periodic card rewards and then dump them into the community market.</p><p>At current levels I expect this to not be a major impact. This concern is more about potential future rewards that would have a larger impact, and the anticipation by players of those future rewards and their economic impact. Once the central bank of Artifact proves willing to print money to give to players, its currency will forever be suspect as a result.</p><p>My first level model says that there should be a decent amount of ruin in the system here, as players substitute buying off other players for the purchase of packs, and the damage starts when the sum of this effect and the cards from packs won in the prize queues, and the collections of players who buy the game then dump all they have, adds up to beyond a critical amount versus the number of players. Thus, increasing the number of active players who keep and expand collections, and thus both increasing demand and reducing available supply, can easily more than compensate for this effect indefinitely as long as Valve is expected to behave responsibly. It would still caution me away from extra cards as a financial investment, but not be sufficient to drive me away from holding a full set for its mundane utility.</p><p>The other change is the one I <a href=\"https://thezvi.wordpress.com/2018/12/26/artifact-embraces-card-balance-changes/\">described in the first post</a>. Valve pivoted from a philosophy of modifying cards only in extreme cases, to a philosophy of regular, gradual card balance changes, which is more in line with how they approach a game like DOTA 2.</p><p>There are many good (and bad) arguments for and against power level changes on cards. If these changes drive player participation sufficiently, other considerations might overwhelm the downside risks even for card ownership.</p><p>That downside is the direct impact on ownership, both short and long term. It is not small.</p><p>If I expect cards that prove successful to be made less powerful, then that bodes quite poorly for the long term value of my investments, and is a crippling blow to speculation on the basis of card utility. My cards and deck might not only not be the most powerful option available, they might not even work the same way tomorrow. My ‘ownership quotient’ has gone down dramatically. There is a reason Valve is offering to buy back purchased copies of these cards at their post-announcement prices. We won’t have that fallback from future announcements, so if I was going to buy other cards likely to eventually get worse, such as Annihilation, I would severely discount the long term investment value, and evaluate mostly on the basis of short term utility.</p><h3>V. But Does Anyone Care?</h3><p>Degrees of card ownership and collectibility vary greatly from game to game.</p><p>You can have the ability to buy and sell freely (Magic: The Gathering on paper, or a future crypto-based game), buy and sell within the framework of the game (Magic: The Gathering Online), buy and sell within the framework of the game for a fee (Artifact), or not have that ability at all (Hearthstone, Eternal or Magic Arena).</p><p>You can have an absolute guarantee of rarity (older paper Magic: The Gathering cards, or perhaps a future crypto-based game), you can have a guarantee of cards beyond a fixed base being printed only when intentionally paid for (Artifact before the change, Magic: The Gathering Online before treasure chests, in-print Magic: The Gathering paper cards), you can have supply be restricted in real ways but with substantial free or incidental influx (Artifact after the change, Magic: The Gathering Online after treasure chests), or you can have lots of influx continuously (Magic Arena, Eternal, Hearthstone and similar games).</p><p>You can have the freedom to do what you want with your cards without a central authority (paper Magic, or a future crypto game), you can have reasonable dedication to allowing flexible constructed game forms indefinitely but not limited play or alternate rules sets or other weirdness (Magic: The Gathering Online), or you can be restricted to the few options currently offered to players in an attempt to solve coordination and critical mass problems (Magic Arena, Artifact, Eternal, Hearthstone and almost every digital game ever).</p><p>You can have insurance against cards changing or being banned on a whim (Magic, and pre-change Artifact), or confiscated from you, or not (most everyone else digital).</p><p>If you want to create enduring collectibles, these are crucial issues.</p><p><strong>But seriously: <em>does anyone care about any of that?</em></strong></p><p>Good question.</p><p><em>Some </em>people care. I care. Proof by example.</p><p>Collectively, do people care? Or do they mostly just want their game to be fun and people to play it?</p><p>My belief is that <em>they care if you give them a reason to care. </em>Or, <em>they <strong>want</strong> to care, but they don’t <strong>have </strong>to care</em> to enjoy your game.</p><p>Good collectibility is one potential asset among many. It has value. So do other things. It also has downsides, as it risks forcing a market mentality on a leisure activity.</p><p>Some people come for collectibility and card value almost exclusively. Some people don’t care about it at all, or actively dislike it. Others like it along with a wide variety of other things.</p><p>The reason I care about it is partly because it adds interesting strategic layers and options to the experience, and because they create value, reward players and allow games to charge a lot more money, allowing these games to be created, to exist and to support communities and prize pools.</p><h3>VI. Four Paths</h3><p>The bigger reason to care is because <em>most of the alternatives people implement are fundamentally broken, to the extent that they ruin games. </em></p><p>The concepts of randomized paid incremental play components has become best practices in games. Most implementations are a nightmare.</p><p>See <a href=\"https://thezvi.wordpress.com/2018/02/10/eternal-and-hearthstone-economy-versus-magic-economy/\">my</a> <a href=\"https://thezvi.wordpress.com/2018/03/30/the-eternal-grind/\">previous</a> <a href=\"https://thezvi.wordpress.com/2018/10/10/eternal-the-exit-interview/\">commentary</a> regarding Eternal, or <a href=\"https://www.facebook.com/notes/richard-garfield/a-game-players-manifesto/1049168888532667\">Richard Garfield’s A Game Player’s Manifesto</a>. Eternal’s model is comparatively generous and friendly, even when held against similar games like Hearthstone or Magic: The Gathering Arena, and it is a different order of magnitude of bad when compared to true mobile freemium hell that follows ‘best practices,’ which destroys <em>far more than all </em>of the value. Trying to play Shin Megami Tensei D-2 or Plants vs. Zombies Heroes, games that on important levels are trying to be my jam, made me cry.</p><p>Games have four known technologies regarding pricing, revenue and ownership. Each has its own logic.</p><p>Option one is true mobile freemium hell. I view this as a true deal with the devil. You bombard players with impetus to spend absurd amounts, make the game not fun, let a few big spenders get addicted and take their money. Such games should be avoided on principle, even when they still have some fun in them.</p><p>Option two is friendly freemium. This includes Hearthstone, Eternal and Magic: The Gathering Arena. I view this as a path dependent bad equilibrium. Players see the rewards in such games as being ‘paid to play’ in an emotionally resonant sense, and want to see numbers going up as they play. I understand and sympathize. Players thus demand and accept their competitive games becoming grinds, and violently protest (see Artifact’s reception) when games are not such grinds. Players thus talk about wanting a ‘generous business model’.</p><p>The friendly fremium model <em>dramatically reduces </em>the fun value of the game outside of the Skinner box of grinding, largely replacing it with the grind for most players. Such games need not be avoided entirely, if the game experience is sufficiently compelling. But one must proceed with caution, as you are facing addictive loops, and money spent is unlikely to go far or last long. Thus, one must have <em>a commitment to not spend money on such games unless the need is great and one’s eyes are fully open, and you know exactly what you are buying.</em></p><p>Option three is a fixed fee for access to the base game, and additional fees for expansions. This is also known as the living card game. It is also known as how most games (and other things!) are sold. This model is great. There are lots of games in the first two categories I would love to give $50 to if that let me fully enjoy what the game could be offering, <em>but there is no reasonable way to incrementally spend money.</em> A fixed fee that then allows the game to optimize around my enjoyment is much better.</p><p>Even here, I still care about maintaining ‘value’ even though that could reasonably be viewed as silly. Consider the Steam Sale. Multiple times a year, many computer games are offered at extreme, 50%+ discounts, and periodically base prices are lowered as well. That which costs $60 on release day is likely available for $30 within the year, and $10 not too long thereafter. Given my opportunity cost to play games, you could argue I should not care, but I am still a trader and the experience is super painful. Compare this to my experience buying games for the Nintendo Switch, where I want digital downloads so I need to pay full prices from the store with no expectation that I will get a better price by waiting. There, I pay far more, and yes this means I buy and try fewer items, but if anything I feel better about the experience.</p><p>The emergence of <a href=\"https://www.fantasyflightgames.com/en/products/android-netrunner-the-card-game/\">Android: Netrunner</a> and other living card games was a wonderful trend that I hope continues. I hope it also prospers online. <em>If you are not going to embrace the marketplace in option four, </em>this is what I want the business model to be. Pay <a href=\"https://thezvi.wordpress.com/2018/12/09/review-slay-the-spire/\">Slay the Spire</a> its $30, play the game as intended forever. Ideally, make a commitment to not discount that price, at least for a period of a few years, so I don’t feel like a schmuck every time I make a purchase.</p><p>There can still be entry fees and prizes, to give players motivation in their games, but the prizes need not take the form of cards. Like poker, the point is to give players motivation, so pay them in as close to money as possible.</p><p>Artifact does this for its limited play experience. You pay $20 for the core game, and can draft as much as you want. If you wish, you can risk $1 to keep it interesting and keep everyone motivated. Great!</p><p>The question I keep coming back to in the week following Artifact’s changes is, if we are going to balance cards continuously, <em>should it have simply been a living card game with prize tournaments attached? Could it still become one and we compensate people who bought in? Why are we trading these cards in the Steam marketplace at all, at this point? </em>Valve has already pioneered hugely successful ways of monetizing games like DOTA 2 that it could use again in Artifact.</p><p>The problems I have with Artifact’s new approach to card changes and awards are myriad, but the biggest is the <em>mismatch </em>it has with the business model of the full marketplace. How to properly embrace a true marketplace for a card game is a problem I have been extensively working on for many months. Continuously giving away and changing cards seems like the opposite of what one must do.</p><p>The fourth option is to create a marketplace. This is the solution for paper Magic: The Gathering, and Magic: The Gathering Online. It was also, to my delight, chosen by Artifact. Doing this involves a pact between publisher, players, traders and collectors. It also means taking good care of the market through effective monetary policy and thinking hard about issues of supply and demand. If that pact is maintained, the market is handled well and the game is a success, cards can be a great investment and everyone wins. Fall down on too many fronts, as Magic: The Gathering Online has sometimes done in the past and is now doing, and it goes badly.</p><p>For legal reasons, Hasbro has perpetuated the farce that it believes that Magic: The Gathering cards are not worth money. This has led to some absurd failures and poor functionality, particularly in Magic: The Gathering Online. It has also led to missed opportunities to embrace the marketplace and integrate it properly into the game experience. Countless hours would cease being wasted, transaction costs would shrink and new possibilities would open up.</p><p>I hope to soon get the opportunity to make a game of my own. My intention is that such a game will embrace the logic of the market, and integrate that into its design, its formats and its business model from the start. I am deeply excited to see what I can do with that, and with giving players full and real ownership of their cards. It comes with restrictions, but <a href=\"https://magic.wizards.com/en/articles/archive/making-magic/twenty-years-twenty-lessons-part-3-2016-06-13\">as Mark Rosewater often reminds us</a>, restrictions breed creativity. I work much better under restrictions than without them.</p><p>Most importantly, when you embrace the market fully, <em>cards become fully liquid, and collection management and acquisition ceases to be a huge time sink or the primary mode of game play. </em>You come full circle.</p><p>For those who mostly want to make a card game that plays great, as Artifact does, it may finally be time to go the other way. Charge your money up front, or make it on other things. Free us from the Skinner box grind of needing to slowly assemble components one by one and the time sink of collection management. Free us to experience core game play.</p><p></p><p></p><p></p>",
    "user": {
      "username": "Zvi",
      "slug": "zvi",
      "displayName": "Zvi"
    }
  },
  {
    "_id": "PHy6K7DwM9uWdYM8Y",
    "title": "On Disingenuity",
    "slug": "on-disingenuity",
    "pageUrl": "https://www.lesswrong.com/posts/PHy6K7DwM9uWdYM8Y/on-disingenuity",
    "postedAt": "2018-12-26T17:08:47.138Z",
    "baseScore": 28,
    "voteCount": 16,
    "commentCount": 3,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Suppose someone claims that all morality is relative, but when pressed on whether this would apply even to murder, they act evasive and refuse to give a clear answer. A critic might conclude that this person is disingenuous in refusing to accept the clear logical consequences of their belief.</p><p>However, imagine that there&apos;s a really strong social stigma against asserting that murder might not be bad, to the point of permanently damaging such a person&apos;s reputation, even though there&apos;s no consequence for making the actually stronger claim that all morality is relative. The relativist might therefore see the critic as the one who is disingenuous; trying to leverage social pressure against them instead of arguing on the basis of reason.</p><p>Thus in the right circumstances, each side can quite reasonably see the other as disingenuous. I suspect that everyone will have experienced both sides of the coin at different times depending on the issue being discussed.</p><p><em>This post was produced with the support of the <a href=\"http://eahotel.org/\">EA Hotel</a></em></p>",
    "user": {
      "username": "Chris_Leong",
      "slug": "chris_leong",
      "displayName": "Chris_Leong"
    }
  },
  {
    "_id": "vHLaESwKCSmtttx7k",
    "title": "Artifact Embraces Card Balance Changes",
    "slug": "artifact-embraces-card-balance-changes",
    "pageUrl": "https://www.lesswrong.com/posts/vHLaESwKCSmtttx7k/artifact-embraces-card-balance-changes",
    "postedAt": "2018-12-26T13:10:00.384Z",
    "baseScore": 11,
    "voteCount": 3,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Previously: <a href=\"https://thezvi.wordpress.com/2018/11/22/review-artifact/\">Review: Artifact</a></p>\n<p>Epistemic Status: <a href=\"https://www.youtube.com/watch?v=Y6ljFaKRTrI\">I’m making a note here. HUGE SUCCESS!</a></p>\n<p>Only a few weeks after the release of Artifact, <a href=\"https://steamcommunity.com/games/583950/announcements/detail/1714081669510213123\">a patch was released</a> making two huge changes to the game.</p>\n<p>The first was to introduce visible player rankings and award progression with free packs, free event tickets and player icons. Having numbers that can go up and a way to feel rewarded for playing was something Artifact clearly needed. This first version, while a work in progress on many levels and carrying economic implications that could be concerning, is far superior to nothing at all. It is a distinct topic, so I won’t say more about it here.</p>\n<p>The second change is what I will focus on here. Artifact changed eight cards. Two of them, Axe and Drow Ranger, were the most valuable two cards in the game. They got strictly worse in important ways. One good but hated card, Cheating Death, changed to a different card entirely, whose new power level is a matter of debate. Five unplayable (in constructed) cards got better, at least one of which, Jasper Daggers, is now interesting and potentially important.</p>\n<h3>I. Philosophical Changes</h3>\n<p>This represents a wealth of philosophical changes in how cards are created, balanced and modified.</p>\n<p>The old philosophy was that cards should be balanced to create a balance between colors and strategies, but not balanced to make all cards or heroes mostly equal.</p>\n<p>Axe being the most valuable card and strongest hero was most certainly not an accident or a surprise – when I wrote an analysis of what one might do if a card in Artifact became too expensive – I assumed the card in question was Axe, as there was no question it would be the most expensive hero, and likely the most expensive card overall. If you had told me another hero was more expensive, I would have assumed it was Drow Ranger and green as a color had proven superior to red.</p>\n<p>While the ubiquity of Red/Black in queue play was unfortunate before the patch, the deck was easy to beat and in my opinion not even that good. I had it at low tier one, with at least two decks clearly better. Prize tournaments reflected this opinion. The issue was more that Red/Black <em>appears </em>stronger when you first start playing, and most players were starting out.</p>\n<p>The new philosophy appears to be that players dislike seeing the same heroes all the time, so weak heroes should be made incrementally stronger, and heroes played all the time made incrementally weaker.</p>\n<p>It is not clear if the same approach applies to other card types. Jasper Daggers was likely a special case to provide a universally available answer to (among other things) hero silencing, rather than a card balance issue. The top non-hero cards might be safe for a long time, but the explanations of the philosophical change should give lovers of cards like Annihilation reason to worry.</p>\n<p>The old philosophy was that once a card is printed and made available for sale and purchase in the marketplace, the card should only be changed under extreme circumstances. Players need to be confident that their purchases will be usable and that their collections will retain value.</p>\n<p>The new philosophy is that such concerns aren’t worthless but most players mostly want the game to be the best it can be, so changes that improve the game happen. Players were given the opportunity, this time, to sell the changed cards back to Valve at their pre-nerf market prices, but the warning seems to be that next time such an option may not be available.</p>\n<p>The old philosophy was that Valve was the most patient and long term oriented company in gaming. Valve time is legendary, even more so than Blizzard time. A few weeks of players complaining or disappointing play numbers is nothing to worry about. Players always complain and demand quick action. Their feedback is valuable, but they don’t know what Valve knows. Keep your promises and your vision, and keep improving your customer experience. Huge stuff, like a million dollar first prize tournament, your ranking system and your first expansion set, lie in the future. Things will work out over time.</p>\n<p>The new philosophy <em>seems </em>to be that a few weeks of poor performance calls for a change in philosophy, and for promises to be broken. Give the people what they say they want lest you become dead on arrival. Work out later how this impacts the entire business model and economy of the game, and hope that making the cards more balanced is long term good for play.</p>\n<p>When I first heard about these changes, I thought they were very bad. Artifact has a well-considered philosophy, and an economy built on promises. It is already too complex and has too many things to remember, with a few top cards being played frequently and without the cards constantly changing. Its balance was crafted over months based on subtle things that the players do not yet have the experience or skill to appreciate.</p>\n<p>Changing cards like this threatens to blow up all of that, and didn’t seem to address the game’s actual problems at all. On top of that, by doing this along with a second major crowd-pleasing move, of giving lots of cards away and providing rankings, we’ll never know whether players even liked the new approach to card balancing.</p>\n<h3>II. Player and Metagame Reaction</h3>\n<p>The verdict was in quickly. Players <em>loved </em>the changes.</p>\n<p>The first response to the Twitter announcement was typical of what was said.</p>\n<div>\n<blockquote>\n<p>Its very brave to go against your game design ideas, like not balancing, no free packs. You did everything in a smart way as well. Limits on pack drops to keep the market safe and such. Proud of you guys!</p>\n<p>— bundew (@bunndew) <a href=\"https://twitter.com/bunndew/status/1075931153947901953?ref_src=twsrc%5Etfw\">December 21, 2018</a></p></blockquote>\n<p></p></div>\n<p>Multiple media sources described Artifact as previously being ‘dead on arrival’ but this being a game saving move. Players who review bombed the game are now talking about how to un-bomb it.</p>\n<p>It’s <em>hard </em>to find a negative reaction to the move, or even a mixed reaction.</p>\n<p>To the extent that reactions are mixed, the mix is usually of the form ‘it’s too bad you didn’t do this from the start’ or ‘this is a move in the right direction, but more will be needed.’</p>\n<p>I have seen concerns over the new card balance, especially players concerned that blue is now too powerful since its rivals got hurt in important ways, but blue was left unharmed. These concerns might well prove right, but there will always be such concerns.</p>\n<p>For now, the prize queue has a more diverse mix of decks, with a lot more blue decks.</p>\n<p>I have yet to see anyone worry about this not only not addressing the game’s complexity, but likely making it worse.</p>\n<p>I have yet to see anyone express concern over long term card value or collectibility, or other implications for the economy. Other than the silly issue of players who paid $30 for Axe because they couldn’t wait a few hours for prices to stabilize, and therefore had paid $30 for a thing now worth $10, and who would now only get $10 in return.</p>\n<p>The metagame diversity in the prize queue has improved somewhat, as well. This includes my own play, as I’ve switched primary decks and am quite enjoying the new concoction.</p>\n<p>This overwhelmingly positive (short term) reaction has huge implications for the future of collectible card games.</p>\n<p>That does not mean the changes worked out. It is too soon to tell, and we will never fully know. The overwhelmingly positive initial reaction, both in steam rank and online reaction, great as they are, are not sufficient. There are lots of long term implications to what happened here, and what data we have is hopelessly confounded.</p>\n<p>What we do have are valuable lessons, and lots of paths forward to explore.</p>\n<p>The individual questions raised are also interesting out of context, so I split those off and will be posting them on their own. Ideally I’ll then link to them at the bottom here.</p>\n<p> </p>",
    "user": {
      "username": "Zvi",
      "slug": "zvi",
      "displayName": "Zvi"
    }
  },
  {
    "_id": "2uYwrjGTZiw6TmHFs",
    "title": "A Section for Innovation?",
    "slug": "a-section-for-innovation",
    "pageUrl": "https://www.lesswrong.com/posts/2uYwrjGTZiw6TmHFs/a-section-for-innovation",
    "postedAt": "2018-12-26T12:37:36.048Z",
    "baseScore": 8,
    "voteCount": 4,
    "commentCount": 2,
    "meta": true,
    "question": false,
    "url": null,
    "htmlBody": "<p>Rationality is about a type of optimality, and optimal states are often found only after multiple steps within a domain of a function explored. There are communities running these processes (e.g., <a href=\"http://www.halfbakery.com\">halfbakery.com</a>), and encouraging them through things like humor, that leads to exploration of new extremes, eventually arriving at something more optimal than what was originally posted.</p><p>I wonder, if we could support something like that on LessWrong, where, say, someone comes up with a new mechanism or invention to do something (e.g., an invention of a new kind of immutable database using a pseudo-random number generator, initial seed, salting data, or some other sort of theoretical invention), that one believes that the invention should help us become more rational, yet the poster doesn&#x27;t have much time to elaborate very deeply, has not had time to complete the invention, and is not entirely sure of the viability of the invention, and whether it makes sense with respect to all possible aspects which community cares about.</p><p>Currently, in such a situation, the most sensible thing on LessWrong is probably writing a draft, and leaving it there until the time when one has more time to finish the writing.</p><p>However, in this fast-pacing world, it may well be that sharing early or sharing something is way better than sharing nothing. So, just like we have a section for &quot;meta&quot;, maybe we could have a section for sharing ideas, and encouraging more relaxed discussion, that is focused on humor and pragmatic correctness (exploration of possibilities), more than technical correctness (hygiene of reasoning)?</p>",
    "user": {
      "username": "Mindey",
      "slug": "mindey",
      "displayName": "Mindey"
    }
  },
  {
    "_id": "pBR6fotYeozwtEwkq",
    "title": "Bay Area: reading, writing, moving, celebrating",
    "slug": "bay-area-reading-writing-moving-celebrating",
    "pageUrl": "https://www.lesswrong.com/posts/pBR6fotYeozwtEwkq/bay-area-reading-writing-moving-celebrating",
    "postedAt": "2018-12-26T03:40:00.722Z",
    "baseScore": 8,
    "voteCount": 11,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>The interesting things going on in my life seem either too important to write about quickly, or too informative regarding some sensitive thing or another. This is perhaps exacerbated by my lack of a serious boyfriend: if I don’t have to regularly turn whatever is happening in my head into a communicable narrative, the plot arcs in my life seem to get stranger. Or perhaps I just don’t remember the less communicable plot arcs from earlier, or perhaps one always becomes stranger over time. Anyway, here are a few mundane and inoffensive things that happened lately in my life:</p><ul><li>I went to a party. I It was a relief when it ended because my face hurt from smiling for so long. I don’t know if I was happy. I liked it when a guy performed his favorite poem for me. </li>\n<li>I read <i>A Grief Observed</i>. For some reason I have always felt like C.S.Lewis was presumably my friend, or my fellow traveler or peer somehow, though I suppose I hadn’t read any of his writing except Narnia as a child. I was not disappointed. Which is strange, though it feels expected. I’m not sure what I particularly wanted, but something like a sensibility that is sincere and steady yet contends with the magic and wonder of the world, whatever those are. (Do most serious people seem less trustworthy and respectable in a way, because they set aside for respectability a swathe of what seems important and in need of contending with? Maybe.) I especially liked the idea in the book of appreciating the bracing resistance one meets when one’s imaginary world contacts reality, perhaps in meeting the real version of a person one often imagines, or in finding that one’s ideas of God or love do not match the real thing. It went well among my own thoughts lately about what one’s mental relationship with reality is or should be. </li>\n<li>C.S.Lewis apparently insisted on writing with a dip pen forever, in spite of fountain pens and typewriters being common in his lifetime. He said not to use typewriters, because ‘…the noise will destroy your sense of rhythm, which still needs years of training’. I was curious enough to try this, given my esteem for his writing. I can almost write with a dip pen now, and it is a decently different experience. If it turns out to be good, one explanation would be that it is slow, yet physically aesthetically pleasing, which means that I’m not bored but have time to think through each word several times over before I actually write it. (I’m reminded of the line “…and I want life in every word, to the extent that it’s absurd” and general associated aesthetic) He also apparently whispered the words out loud as he was writing, which I have also tried to do some. The jury is out.</li>\n<li>I’ve been trying to find a house to rent with a group of friends. I’m surprised how much our views about the relative merits of different houses and house components swing without much new information. We were fairly excited about a big, expensive, epic house with lovely living spaces and a tree house and all that a couple of days ago, and now are all much more excited for a place that is small and half the price per person. I’m not sure what about this feels confusing to me, but it reminds me of trying to understand people. Maybe some things—like people or houses—are sufficiently complicated that you can’t really keep all the characteristics in your head at once, so you have to have some kind of abstraction about them, and you can have quite different abstractions for the same underlying bunch of details, especially if the underlying details aren’t fundamentally thematic (e.g. if one place is just like another place except every aspect  of it is exactly twice as expensive, that is easier to understand than if everything is just different in different ways, and many are more expensive but not all, and they matter different amounts). So changing abstractions can cause huge swings in evaluation.</li>\n<li>There is a Christmas party outside my door, so I’m going to go to that now. My house has lots of Australians and other folk who for whatever reason don’t go home at Christmas, but we have made quite a go of celebrating it with each other. We have had hot chocolate, holiday anagrams, stockings, a photo, a tree, a white elephant, Christmas dresses and cricket, and soon there will be Chinese food. Hopefully it is good to celebrate Christmas.</li>\n</ul>",
    "user": {
      "username": "KatjaGrace",
      "slug": "katjagrace",
      "displayName": "KatjaGrace"
    }
  },
  {
    "_id": "cJ9foRmTLcxww6PMS",
    "title": "Role playing game based on HPMOR in Moscow",
    "slug": "role-playing-game-based-on-hpmor-in-moscow",
    "pageUrl": "https://www.lesswrong.com/events/cJ9foRmTLcxww6PMS/role-playing-game-based-on-hpmor-in-moscow",
    "postedAt": "2018-12-25T21:22:24.666Z",
    "baseScore": 7,
    "voteCount": 5,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Welcome to our custom made role playing game &quot;Hogwarts and methods of rationality&quot;, inspired by HPMOR story! It will be intellectual role playing game with rationality-based minigames. You can read more detailed announce and description here: <a href=\"https://goo.gl/WLU4p3\">https://goo.gl/WLU4p3</a></p>",
    "user": {
      "username": "Alexander230",
      "slug": "alexander230",
      "displayName": "Alexander230"
    }
  },
  {
    "_id": "EjssJnp9fNhvdDEdK",
    "title": "Reinterpreting \"AI and Compute\"",
    "slug": "reinterpreting-ai-and-compute",
    "pageUrl": "https://www.lesswrong.com/posts/EjssJnp9fNhvdDEdK/reinterpreting-ai-and-compute",
    "postedAt": "2018-12-25T21:12:11.236Z",
    "baseScore": 30,
    "voteCount": 9,
    "commentCount": 9,
    "meta": false,
    "question": false,
    "url": "https://aiimpacts.org/reinterpreting-ai-and-compute/",
    "htmlBody": "<p>Some arguments saying that the recent evidence about the speed at which compute has been increasing and has been responsible for rapid progress in machine learning, might mean that we should be less worried about short timelines, not more. </p><blockquote>[...] Overall, it seems pretty common to interpret the OpenAI data as evidence that we should expect extremely capable systems sooner than we otherwise would.</blockquote><blockquote>However, I think it’s important to note that the data can also easily be interpreted in the opposite direction. The opposite interpretation goes like this:</blockquote><blockquote>1. If we were previously underestimating the rate at which computing power was increasing, this means we were o<em>verestimating </em>the returns on it.</blockquote><blockquote>2. In addition, if we were previously underestimating the rate at which computing power was increasing, this means that we were <em>overestimating</em> how sustainable its growth is.</blockquote><blockquote><strong>3. </strong>Let’s suppose, as the original post does, that increasing computing power is currently one of the main drivers of progress in creating more capable systems. Then — barring any major changes to the status quo — it seems like we should expect progress to slow down pretty soon and we should expect to be underwhelmed by how far along we are when the slowdown hits.</blockquote>",
    "user": {
      "username": "habryka4",
      "slug": "habryka4",
      "displayName": "habryka"
    }
  },
  {
    "_id": "Mt9ZwedTgfeac4pD9",
    "title": "Alignment Newsletter #38",
    "slug": "alignment-newsletter-38",
    "pageUrl": "https://www.lesswrong.com/posts/Mt9ZwedTgfeac4pD9/alignment-newsletter-38",
    "postedAt": "2018-12-25T16:10:01.289Z",
    "baseScore": 9,
    "voteCount": 4,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Merry Christmas!</p><p>Find all Alignment Newsletter resources <a href=\"http://rohinshah.com/alignment-newsletter/\">here</a>. In particular, you can <a href=\"http://eepurl.com/dqMSZj\">sign up</a>, or look through this <a href=\"https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing\">spreadsheet</a> of all summaries that have ever been in the newsletter.</p><h2>Highlights</h2><p><strong><a href=\"https://futureoflife.org/2018/12/17/inverse-reinforcement-learning-and-the-state-of-ai-alignment-with-rohin-shah/\">AI Alignment Podcast: Inverse Reinforcement Learning and the State of AI Alignment with Rohin Shah</a></strong> <em>(Lucas Perry and Rohin Shah)</em>: Lucas interviewed me and we talked about a bunch of different topics. Some quick highlights, without the supporting arguments:</p><p>- If we want to use inverse reinforcement learning (IRL) to infer a utility function that we then optimize, we would have to account for systematic biases, and this is hard, and subject to an impossibility result.</p><p>- Humans do seem to be good at inferring goals of other humans, probably because we model them as planning in a similar way that we ourselves plan. It&#x27;s reasonable to think that IRL could replicate this. However, humans have very different ideas on how the future should go, so this seems not enough to get a utility function that can then be optimized over the long term.</p><p>- Another issue with having a utility function that is optimized over the long term is that it would have to somehow solve a whole lot of very difficult problems like the nature of identity and population ethics and metaphilosophy.</p><p>- Since human preferences seem to change as the environment changes, we could try to build AI systems whose goals are constantly changing by continuously running IRL. This sort of approach is promising but we don&#x27;t know how to get it working yet.</p><p>- IRL, agency and optimization all seem to require a notion of counterfactuals.</p><p>- One view of agency is that it is about how a search process thinks of itself, or about other search processes. This gives it a feeling of &quot;choice&quot;, even though the output of the search process is determined by physics. This can explain the debates over whether evolution is an optimization process -- on the one hand, it can be viewed as a search process, but on the other, we understand it well enough to think of it as a &quot;deterministic&quot; procedure.</p><p>- One way to view the AI alignment problem is to view it as a human-AI interaction problem, so that we get an AI that evolves over time along with us.</p><p>- Rather than building a function maximizer, we could aim to build an AI system that is corrigible, or one that follows norms.- Both iterated amplification and debate operate on an exponential deliberation tree, though in different ways, using reasoning learned from humans. If a human would have some desirable property (such as good epistemics), so too should their amplification.- Both iterated amplification and debate are based on <em>explicit</em> human reasoning, as opposed to intuitive reasoning.</p><p>- Value drift in the literal sense can be both positive and negative -- I certainly expect and want my stated preferences to change as I become more knowledgeable in the future.</p><p>- We only want the combined human-AI system to have a goal, which allows for a space of possibilities where the AI is not optimizing a goal.</p><p>- One of the problems that seems most troubling is the issue of inner optimizers, which will hopefully be described in a sequence soon.</p><p><strong><a href=\"https://aiimpacts.org/reinterpreting-ai-and-compute/\">Reinterpreting “AI and Compute”</a></strong> <em>(Ben Garfinkel)</em>: <a href=\"https://blog.openai.com/ai-and-compute/\">Data</a> from OpenAI showed that the amount of compute used by the most expensive projects had been growing exponentially with a doubling time of three months. While it is easy to interpret this trend as suggesting that we will get AGI sooner than expected, it is also possible to interpret this trend as evidence in the opposite direction. A surprisingly high rate of increase in amount of compute used suggests that we have been <em>overestimating</em> how helpful more compute is. Since this trend <a href=\"https://aiimpacts.org/interpreting-ai-compute-trends/\">can&#x27;t be sustainable over decades</a>, we should expect that progress will slow down, and so this data is evidence <em>against</em> near-term AGI.</p><p><strong>Rohin&#x27;s opinion:</strong> The surprising part of the data is how fast compute has been growing. One common part of AGI timelines is whether you think compute or algorithms are the bottleneck. Assuming you had a good sense of progress in AI, but were surprised by how fast compute grew, you should update against the relative benefits of compute.</p><p>This post seems to be about the way you relate compute to AGI timelines, ignoring algorithms altogether. If you think of AGI as requiring a specific amount of compute that is determined independently of current AI progress (for example, by estimating the compute used by a human brain), then the evidence should shorten your timelines. If you instead predict how close AGI is by looking at the rate of progress in AI and extrapolating over time, then to first order this data should not affect timelines (since compute is not part of the model), and to second order it should lengthen them for the reasons in this post.</p><p><strong>Read more:</strong> <a href=\"https://blog.openai.com/ai-and-compute/\">AI and Compute</a></p><h1>Previous newsletters</h1><p><a href=\"https://www.alignmentforum.org/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas\">Three AI Safety Related Ideas</a> and <a href=\"https://www.alignmentforum.org/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety\">Two Neglected Problems in Human-AI Safety</a> <em>(Wei Dai)</em>: Last week, I said that the problem of defending against intentional value corruption was an instance of the problem &quot;Competing aligned superintelligent AI systems could do bad things&quot;, and I wasn&#x27;t sure why we were focusing on value corruption in particular. In <a href=\"https://www.alignmentforum.org/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety#dykZxAXGr6sk7bh4e\">this comment</a>, Wei Dai argues that superintelligent AI systems could be really good at cooperating with each other, which solves <em>most</em> of the problems. However, the terms of such cooperation will probably reflect the balance of power between the AI systems, which may tend to benefit simpler value systems rather than ones with a proper amount of value complexity and moral uncertainty. This seems plausible to me, though I&#x27;m not confident one way or the other.</p><h1>Technical AI alignment</h1><h3>Technical agendas and prioritization</h3><p><strong><a href=\"https://futureoflife.org/2018/12/17/inverse-reinforcement-learning-and-the-state-of-ai-alignment-with-rohin-shah/\">AI Alignment Podcast: Inverse Reinforcement Learning and the State of AI Alignment with Rohin Shah</a></strong> <em>(Lucas Perry and Rohin Shah)</em>: Summarized in the highlights!</p><h3>Agent foundations</h3><p><a href=\"https://www.alignmentforum.org/posts/PgsxXNSDsyz4DFEuw/anthropic-paradoxes-transposed-into-anthropic-decision\">Anthropic paradoxes transposed into Anthropic Decision Theory</a> <em>(Stuart Armstrong)</em></p><p><a href=\"https://www.alignmentforum.org/posts/uzb3u3zMTkrSEhCaf/anthropic-probabilities-and-cost-functions\">Anthropic probabilities and cost functions</a> <em>(Stuart Armstrong)</em></p><h3>Learning human intent</h3><p><a href=\"http://arxiv.org/abs/1810.00821\">Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</a> <em>(Xue Bin Peng et al)</em>: Adversarial learning techniques require a delicate balance between the generator and the discriminator. If the discriminator is too weak, it cannot tell the difference between generated samples and true samples, and it cannot provide a learning signal for the generator. If the discriminator is too strong, small changes to the generator are not going to fool the discriminator, and so again the gradient is uninformative. This paper proposes to control the power of the discriminator using an <em>information bottleneck</em>.</p><p>Instead of providing data points directly to the discriminator, the data points are first encoded into a new representation, and the discriminator must work with the new representation. The representation is learned to be helpful for the discriminator under the constraint of an upper bound on the mutual information between the representation and the original data points. The choice of upper bound determines how much information the discriminator is allowed to access, which in turn determines how powerful the discriminator is.</p><p>They apply this idea to imitation learning (GAIL), inverse reinforcement learning (AIRL), and image generation (GANs), and find that it improves results.</p><h3>Forecasting</h3><p><strong><a href=\"https://aiimpacts.org/reinterpreting-ai-and-compute/\">Reinterpreting “AI and Compute”</a></strong> <em>(Ben Garfinkel)</em>: Summarized in the highlights!</p><p><a href=\"https://www.alignmentforum.org/posts/hSw4MNTc3gAwZWdx9/reasons-compute-may-not-drive-ai-capabilities-growth\">Reasons compute may not drive AI capabilities growth</a> <em>(Kythe)</em>: A common narrative (for example, <a href=\"https://blog.openai.com/ai-and-compute/\">at OpenAI</a>) is that AI progress will be driven by improvements in compute, but there are a few reasons we may not expect this to be the case. First, there are many known techniques to train faster that only require some engineering effort, that researchers often don&#x27;t use. Second, researchers still use grid searches to optimize hyperparameters rather than <a href=\"https://www.automl.org/blog_bohb/\">more efficient methods</a>. These two points suggest that researchers spend compute in order to avoid engineering effort, and so compute must not be the bottleneck.</p><p>In addition, the trends that have previously powered increasing levels of compute may be slowing down. For example, we had one-time gains by moving to GPUs and then to custom accelerators like TPUs, which probably will not happen again. In addition, many RL experiments require simulations on CPUs, and CPU improvements appear to be slowing down. GPU memory is often a bottleneck as well, though this could start increasing now that there is demand for larger memories, or we could get faster hardware interconnects that allow you to split models across multiple chips.</p><p><strong>Rohin&#x27;s opinion:</strong> I think the evidence in the first part suggesting an abundance of compute is mostly explained by the fact that academics expect that we need ideas and algorithmic breakthroughs rather than simply scaling up existing algorithms, so you should update on that fact rather than this evidence which is a downstream effect. If we <em>condition</em> on AGI requiring new ideas or algorithms, I think it is uncontroversial that we do not require huge amounts of compute to test out these new ideas.</p><p>The &quot;we are bottlenecked on compute&quot; argument should be taken as a statement about how to advance the state of the art in big unsolved problems in a sufficiently general way (that is, without encoding too much domain knowledge). Note that ImageNet is basically solved, so it does not fall in this category. At this point, it is a &quot;small&quot; problem and it&#x27;s reasonable to say that it has an overabundance of compute, since it <a href=\"https://blog.openai.com/ai-and-compute/\">requires</a><em>four orders of magnitude</em> less compute than AlphaGo (and probably Dota). For the unsolved general problems, I do expect that researchers do use efficient training tricks where they can find them, and they probably optimize hyperparameters in some smarter way. For example, AlphaGo&#x27;s hyperparameters were <a href=\"https://arxiv.org/abs/1812.06855\">trained via Bayesian optimization</a>.</p><p>More details in <a href=\"https://www.alignmentforum.org/posts/hSw4MNTc3gAwZWdx9/reasons-compute-may-not-drive-ai-capabilities-growth#AqaL4wutwNbocyR2u\">this comment</a>. I don&#x27;t know much about trends in hardware so I won&#x27;t comment on the second part.</p><h3>Field building</h3><p><a href=\"https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment\">The case for taking AI seriously as a threat to humanity</a> <em>(Kelsey Piper)</em>: This is an introduction to the problem of AI safety, from the perspective that it is hard to specify the &quot;right&quot; goal, and that goal-driven behavior leads to convergent instrumental subgoals that will likely be dangerous. It also addresses several common initial reactions that people have.</p><p><strong>Rohin&#x27;s opinion:</strong> I really like this introduction, it remains understandable while being technically accurate. It will probably be my new default article to introduce people to the problem.</p><h1>AI strategy and policy</h1><p><a href=\"https://arxiv.org/abs/1812.05979\">Scaling shared model governance via model splitting</a> <em>(Miljan Martic, Jan Leike et al)</em>: Suppose that two organizations want to develop a deep learning model together without allowing either one to unilaterally use the model. This can be done cryptographically using homomorphic encryption or secure multiparty computation, but this introduces several orders of magnitude of slowdown. What about the much simpler solution of letting each organization have half of the parameters, that are not shared with the other organization? For this to be secure, it should be prohibitively difficult to find the other organization&#x27;s parameters. In the least convenient world where each organization has access to all training data, hyperparameters etc., this is the security of the <em>model completion problem</em>, where given all of the normal setup for deep learning as well as half of the trained parameters for a model M, the goal is to create a new model that performs as well as M. Of course, we can simply rerun the training procedure that was used to create M, so the cost is bounded above by the cost to create M in the first place. We might be able to do better by leveraging the trained parameters that we know -- for example, by using those parameters as an initialization for the model instead of whatever initialization we normally use. The paper empirically investigates how well strategies like this can work. They find that it is relatively easy to create a model that achieves good performance (getting 80% of the way to the best performance), but quite difficult to achieve performance as good as that of M, typically requiring 40-100% of the time it took to create M.</p><p><strong>Rohin&#x27;s opinion:</strong> In the particular setting that they&#x27;re considering, let&#x27;s say that we require C compute to train M. Then one of the organizations had to contribute at least 0.5C, and that organization could defect by investing 1.5C. The first 0.5C is used to take part in the model splitting scheme so as not to arouse suspicion, and the remaining 1C is used to train a new version of M from scratch. So, security in this setting requires us to assume that the organization is unwilling to invest 3x the compute they are going to invest. This assumption seems questionable, but when it does hold, the evidence from the paper suggests that model splitting is relatively secure, since it typically takes an additional 0.4-1C in order to fully solve the model completion problem.</p><p>When there are N &gt;&gt; 2 parties, each party only has to contribute C/N. So, the assumption that no party will use C compute to recreate the model now translates to an assumption that no party will invest (N+1)C compute, which seems more reasonable for sufficiently large N.</p><p>I suspect we can get more mileage if each party had its own training data that it kept secret. It&#x27;s not clear how to train an AI system such that the training data remains secret, but if we could do that, and the model was split across each group, it would probably be impossible for any one group to recover a new model M&#x27; that achieved performance as good as that of M.</p><h1>News</h1><p><a href=\"https://www.alignmentforum.org/posts/a72owS5hz3acBK5xc/2018-ai-alignment-literature-review-and-charity-comparison\">2018 AI Alignment Literature Review and Charity Comparison</a> <em>(Larks)</em>: This post summarizes relevant papers in AI alignment over the last year, and uses them to compare different organizations working on AI alignment in order to choose which one to donate to.</p><p><strong>Rohin&#x27;s opinion:</strong> It&#x27;s a good roundup of papers, including several papers that I haven&#x27;t covered in this newsletter.</p>",
    "user": {
      "username": "rohinmshah",
      "slug": "rohinmshah",
      "displayName": "Rohin Shah"
    }
  },
  {
    "_id": "f4ABDXn9X5jbTtRpi",
    "title": "What are the axioms of rationality?",
    "slug": "what-are-the-axioms-of-rationality",
    "pageUrl": "https://www.lesswrong.com/posts/f4ABDXn9X5jbTtRpi/what-are-the-axioms-of-rationality",
    "postedAt": "2018-12-25T06:47:54.363Z",
    "baseScore": 2,
    "voteCount": 5,
    "commentCount": 8,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>I&#x27;m new here (my first post), i just started to get serious about rationality, and one of the questions that immediately came to my mind is &quot;What are the axioms of rationality?&quot;. I looked it up a bit, and didn&#x27;t find (even on this site) a post that&#x27;ll show them (and i&#x27;m quite sure there are).</p><p>So this is intended as a discussion, And I&#x27;ll make a post with the conclusions afterward. </p><p>curious to see your reply&#x27;s! (as well if you have feedback on how i asked the question) </p><p>thanks :)</p><p></p>",
    "user": {
      "username": "Yoav Ravid",
      "slug": "yoav-ravid",
      "displayName": "Yoav Ravid"
    }
  },
  {
    "_id": "2meuc3kPRkBcRpj3R",
    "title": "Contrite Strategies and The Need For Standards",
    "slug": "contrite-strategies-and-the-need-for-standards",
    "pageUrl": "https://www.lesswrong.com/posts/2meuc3kPRkBcRpj3R/contrite-strategies-and-the-need-for-standards",
    "postedAt": "2018-12-24T18:30:00.480Z",
    "baseScore": 135,
    "voteCount": 51,
    "commentCount": 5,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Epistemic Status: Confident</em></p>\n<p>There’s a really interesting paper from 1996 called <a href=\"https://homepage.univie.ac.at/Karl.Sigmund/JTB97a.pdf\">The Logic of Contrition</a>, which I’ll summarize here.  In it, the authors identify a strategy called “Contrite Tit For Tat”, which does better than either Pavlov or Generous Tit For Tat in Iterated Prisoner’s Dilemma.</p>\n<p>In Contrite Tit For Tat, the player doesn’t only look at what he and the other player played on the last term, but also another variable, the <em>standing </em>of the players, which can be good or bad.</p>\n<p>If Bob defected on Alice last round but Alice was in good standing, then Bob’s standing switches to bad, and Alice defects against Bob.</p>\n<p>If Bob defected on Alice last round but Alice was in <em>bad </em>standing, then Bob’s standing stays good, and Alice cooperates with Bob.</p>\n<p>If Bob cooperated with Alice last round, Bob keeps his good standing, and Alice cooperates.</p>\n<p>This allows two Contrite Tit For Tat players to recover quickly from accidental defections without defecting against each other forever;</p>\n<p>D/C -&gt; C/D -&gt; C/C</p>\n<p>But, unlike Pavlov, it consistently resists the “always defect” strategy</p>\n<p>D/C -&gt; D/D -&gt; D/D -&gt; D/D …</p>\n<p>Like TFT (Tit For Tat) and unlike Pavlov and gTFT (Generous Tit For Tat), cTFT (Contrite Tit For Tat) can invade a population of all Defectors.</p>\n<p>A related contrite strategy is Remorse.  Remorse cooperates only if it is in bad standing, or if both players cooperated in the previous round. In other words, Remorse is more aggressive; unlike cTFT, it can attack cooperators.</p>\n<p>Against the strategy “always cooperate”, cTFT always cooperates but Remorse alternates cooperating and defecting:</p>\n<p>C/C -&gt; C/D -&gt; C/C -&gt; C/D …</p>\n<p>And Remorse defends effectively against defectors:</p>\n<p>D/C -&gt; D/D -&gt; D/D -&gt; D/D…</p>\n<p>But if one Remorse accidentally defects against another, recovery is more difficult:</p>\n<p>C/D -&gt; D/C -&gt; D/D -&gt; C/D -&gt; …</p>\n<p>If the Prisoner’s Dilemma is repeated a large but finite number of times, cTFT is an evolutionarily stable state in the sense that <em>you can’t do better for yourself when playing against a cTFT player through doing anything that deviates from what cTFT would recommend. </em>This implies that no other strategy can successfully invade a population of all cTFT’s.</p>\n<p>REMORSE can sometimes be invaded by strategies better at cooperating with themselves, while Pavlov can sometimes be invaded by Defectors, depending on the payoff matrix; but for all Prisoner’s Dilemma payoff matrices, cTFT resists invasion.</p>\n<p>Defector and a similar strategy called Grim Trigger (if a player ever defects on you, keep defecting forever) are evolutionarily stable, but not good outcomes — they result in much lower scores for everyone in the population than TFT or its variants.  By contrast, a whole population that adopts cTFT, gTFT, Pavlov, or Remorse on average gets the payoff from cooperating each round.</p>\n<p>The bottom line is, adding “contrition” to TFT makes it quite a bit better, and allows it to keep pace with Pavlov in exploiting TFT’s, while doing better than Pavlov at exploiting Defectors.</p>\n<p>This is no longer true if we add noise in the <em>perception </em>of good or bad standing; contrite strategies, like TFT, can get stuck defecting against each other if they erroneously perceive bad standing.</p>\n<p>The moral of the story is that there’s a game-theoretic advantage to not only having <em>reciprocity </em>(TFT) but <em>standards </em>(cTFT), and in fact reciprocity alone is not enough to outperform strategies like Pavlov which don’t map well to human moral maxims.</p>\n<p>What do I mean by standards?</p>\n<p>There’s a difference between saying “Behavior X is better than behavior Y” and saying “Behavior Y is unacceptable.”</p>\n<p>The concept of “unacceptable” behavior functions like the concept of “standing” in the game theory paper.  If I do something “unacceptable” and you respond in some negative way (you get mad or punish me or w/e), I’m not supposed to retaliate against <em>your </em>negative response, I’m supposed to accept it.</p>\n<p>Pure reciprocity results in blood feuds — “if you kill one of my family I’ll kill one of yours” is perfectly sound Tit For Tat reasoning, but it means that we can’t <em>stop </em>killing once we’ve started.</p>\n<p><em>Arbitrary </em>forgiveness fixes that problem and allows parties to reconcile even if they’ve been fighting, but still leaves you vulnerable to an attacker who just won’t quit.</p>\n<p>Contrite strategies are like having a court system. (Though not an enforcement system!  They are still “anarchist” in that sense — all cTFT bots are equal.)  The “standing” is an assessment attached to each person of whether they are in the wrong and thereby restricted in their permission to retaliate.</p>\n<p>In general, for actions not covered by the legal system and even for some that are, we don’t <em>have </em>widely shared standards of acceptable vs. unacceptable behavior.  We’re aware (and especially so given the internet) that these standards differ from subculture to subculture and context to context, and we’re often aware that they’re arbitrary, and so we have enormous difficulty getting widely shared clarity on claims like “he was deceptive and that’s not OK”.  Because…was he deceptive in a way that counts as fraud? Was it just “puffery” of the kind that’s normal in PR?  Was it a white lie to spare someone’s feelings?  Was it “just venting” and thus not expected to be as nuanced or fact-checked as more formal speech?  What <em>level or standard </em>of honesty could he reasonably have been expected to be living up to?</p>\n<p>We can’t say “that’s not OK” without some kind of understanding that he had failed to live up to a shared expectation.  And <em>where is that bar?  </em>It’s going to depend who you ask and what local context they’re living in.  And not only that, but the fact that nobody is keeping track of where even the separate, local standards are, eventually standards will have to be dropped to the lowest common denominator if not made explicit.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Myers%E2%80%93Briggs_Type_Indicator\">MBTI </a>isn’t science but it’s illustrative descriptively, and it seems to me that the difference between “Perceivers” and “Judgers”, which is basically the difference between the kinds of people who get called “judgmental” in ordinary English and the people who don’t, is that “Judgers” have a clear idea of where the line is between “acceptable” and “unacceptable” behavior, while Perceivers don’t.  I’m a Perceiver, and I’ve often had this experience where someone is saying “that’s just Not OK” and I’m like “whoa, where are you getting that? I can certainly see that it’s <em>suboptimal</em>, this other thing would be better, but why are you drawing the line for acceptability here instead of somewhere else?”</p>\n<p>The lesson of cTFT is that <em>having </em>a line in the first place, having a standard that you can either be in line with or in violation of, has survival value.</p>\n<p> </p>",
    "user": {
      "username": "sarahconstantin",
      "slug": "sarahconstantin",
      "displayName": "sarahconstantin"
    }
  },
  {
    "_id": "AiBA5DsT2vKRHChks",
    "title": "Why is CBD everywhere? My Opinion\n",
    "slug": "why-is-cbd-everywhere-my-opinion",
    "pageUrl": "https://www.lesswrong.com/posts/AiBA5DsT2vKRHChks/why-is-cbd-everywhere-my-opinion",
    "postedAt": "2018-12-24T12:48:59.794Z",
    "baseScore": -24,
    "voteCount": 7,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p> </p><p>CBD oil has become popular but the change in opinion has been so sudden that many find it hard to pinpoint the exact time. From being viewed as a cannabis option for non-stoners it has become a cannabis option with healing properties.</p><p>View of CBD oil has changed, but there is a question that still needs to be answered. “Is CBD oil the next big thing in medical history or is another snake oil?” Supporters of CBD oil claim it can cure anything from depression to cancer. These claims are such that people might be forgiven if they believe that CBD oil can cure many of the problems we face in the 21st century.</p><p>The problems we are facing today range from climate change to changes in our views of society.  These changes are hard for many to handle and that is why many of them are reaching out for CBD oil products.</p><p>Apart from this, people always worried about where to buy CBD oil online and which is the best place to buy CBD, I also met with the same confustion while buying online at first time, After reading the reviews from <a href=\"https://www.cbdpush.com/\">cbdpush.com</a> I got a clear idea to choose the best vendor in online.</p><p><strong>Cannabis plant is no longer limited to stoners</strong></p><p>Till a few years back very few people had any idea about CBD oil products and now the number of CBD oil products has grown exponentially. The hype surrounding it is similar to one that surrounds yoga. Even though many have heard of CBD, very few are even aware that the full form of CBD is cannabidiol and that it comes from the cannabis plant. However, unlike it cousin THC, CBD will not be able to stone you.</p><p>The effects of CBD oil has been compared to the effects of practicing yoga or performing intense meditation. Many people have claimed that CBD oil has cured them of their anxiety problems.</p><p>Each generation has something that it claimed could help it and it seems that for this generation it CBD oil. The problem with this generation is not the lack of information, but the abundance of information. Unfortunately, the access to so much information has not reduced the anxiety people face. Instead, it has increased it.</p><p><strong>Latest Fad or is it?</strong></p><p>Many people consider CBD oil to be a little more than a fad. Fortunately or unfortunately the people who follow this fad are not limited to millennials. A large number of people from the baby boomer generation are using CBD oil for their health problems.</p><p>However, CBD oil is not being marketed as a recreational drug. In fact, it is being marketed as a drug that can counteract the effects of both THC and alcohol. Many are trying to develop CBD into a drink that can not only prevent hangovers but also protect the liver from the damages.</p><p><strong>Can we consider CBD oil as wonder drug or is it another snake oil</strong></p><p>There are people who consider CBD oil to the 21st-century version of the snake oil. In other words, it is all hype and no substance. However, these people will be surprised to find that CBD oil is being studied because of the potential it has shown for treating conditions including insomnia. Also, the compound has shown promise as a substance that weans people from <a href=\"https://www.drugabuse.gov/drugs-abuse/opioids\">opioids </a>that are addictive.</p><p>It must be noted if the CBD oil industry is going to have a future it must be based on facts, not fantasy.</p>",
    "user": {
      "username": "Richard McGual",
      "slug": "richard-mcgual",
      "displayName": "Richard McGual"
    }
  },
  {
    "_id": "ozFisYqNcErR8ooGW",
    "title": "Testing Rationality Apps for Science",
    "slug": "testing-rationality-apps-for-science",
    "pageUrl": "https://www.lesswrong.com/posts/ozFisYqNcErR8ooGW/testing-rationality-apps-for-science",
    "postedAt": "2018-12-24T10:46:20.229Z",
    "baseScore": 26,
    "voteCount": 7,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><strong>TLDR</strong>: Do you want to contribute to science by trying out new productivity apps? Then send a mail to re-experiments-on@tuebingen.mpg.de.</p><p><strong>Longer version</strong>: Do you want to achieve your personal goals more effectively, become more focused and productive at work, improve your planning skills, or learn how to make better decisions?</p><p>Then we have good news for you: You can be the first to try out our brand-new productivity and self-improvement apps by participating in our research.</p><p>We regularly conduct online studies and are looking for participants willing to try out our productivity apps. You would be amongst the first to test our newly developed innovative applications that foster personal growth and/or productivity.</p><p>We are the Max Planck Research Group for Rationality Enhancement at the MPI for Intelligent Systems in Tübingen. Our scientific mission is to lay the cognitive and technological foundations for helping people become more effective. To this end, we investigate how people learn how to think, develop cognitively-informed machine learning methods for discovering rational heuristics, and build cognitive tutors that help people learn faster and cognitive prostheses that assist people in goal setting and goal achievement.</p><p>If you would like to contribute to our research and participate in our upcoming studies, please subscribe to our mailing list by sending an email to re-experiments-on@tuebingen.mpg.de.</p><p>If you can think of other people or communities who might be interested in using our apps and participating in our research, then please feel to spread the word by sharing or forwarding this message to them.</p><p>Many thanks in advance for your support!</p>",
    "user": {
      "username": "BayesianMind",
      "slug": "bayesianmind",
      "displayName": "BayesianMind"
    }
  },
  {
    "_id": "hvrx55TMwpyHK6RcD",
    "title": "State Machines and the Strange Case of Mutating API",
    "slug": "state-machines-and-the-strange-case-of-mutating-api",
    "pageUrl": "https://www.lesswrong.com/posts/hvrx55TMwpyHK6RcD/state-machines-and-the-strange-case-of-mutating-api",
    "postedAt": "2018-12-24T05:50:00.599Z",
    "baseScore": 8,
    "voteCount": 3,
    "commentCount": 5,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>State machines are widely used to implement network protocols, or, generally, objects that have to react to external events.</p><p>Consider TCP state machine:</p><span><figure><img src=\"http://250bpm.wdfiles.com/local--files/blog:25/events3.png\" class=\"draft-image \" style=\"\" /></figure></span><p></p><p>During its lifetime TCP socket moves throught different states in the diagram. When you start connecting it&#x27;s in SYN SENT state, when the initial handshake is over, it&#x27;s in ESTABLISHED state and so on.</p><p>And here&#x27;s an interesting observation: The API of the socket changes as you move from one state to another. For example, it doesn&#x27;t make sense to receive data while you are still connecting. But once you are connected, receiving data is all right.</p><p>To give a more illustrative example, have a look at <a href=\"https://tools.ietf.org/html/rfc1929\">SOCKS5</a> protocol. It&#x27;s basically a TCP or UDP proxy protocol. It&#x27;s used, for example, by Tor. It starts with authentication phase, supporting different kinds of authentication. Then it moves to connection establishment phase. Once again there are different ways to connect. You can connect to an IPv4 address, to a IPv6 address or to a hostname. Finally, the state machine moves to one of the working states. This can be a TCP connection or an UDP connection.</p><span><figure><img src=\"http://250bpm.wdfiles.com/local--files/blog:142/mutate2.png\" class=\"draft-image \" style=\"\" /></figure></span><p></p><p>Note how API changes between the states. In CLOSED state you can call functions such as connect_unauthenticated or connect_password. In AUTHENTICATED state you can call connect_tcp, bind_tcp or open_udp. In TCP ESTABLISHED you can do normal stream socket operations, while in UDP ESTABLISHED you can do datagram operations.</p><p>This requirement of mutating API is at odds with how the state machines are normally implemented: There&#x27;s a single object representing the connection during it&#x27;s entire lifetime. Therefore, a single object must support different APIs.</p><p>What it leads to is code like this:</p><p> <code>void Socks5::connect_tcp(Addr addr) { if(state != AUTHENTICATED) throw &quot;Cannot connect is this state.&quot;; ... }</code> </p><p>Which, let&#x27;s be frank, is just an implementation of dynamically typed language on top of statically-typed one.</p><p>In other words, by implementing state machines this way we are giving up proper type checking. While compiler would be perfectly able to warn us if connect_tcp was called in CLOSED state, we give up on the possibility and we check the constraint at runtime.</p><p>This sounds like bad coding style, but it turns out that the programming languages we use fail to provide tools to handle this kind of scenarios. It&#x27;s not network programmers who are at fault, but rather programming language designers.</p><p>The closest you can get is having a different interface for each state and whenever state transition happens closing the old interface and opening a new one:</p><p> <code>auto i1 = socks5_socket(); auto i2 = i1-&gt;connect_unauthenticated(proxy_addr); // i1 is an invalid pointer at this point auto i3 = i1-&gt;connect_tcp(addr); // i2 is an invalid pointer at this point</code> </p><p>But note how ugly the code is. You have there undefined variables (i1, i2) hanging around. If you accidentally try to use them, you&#x27;ll get a runtime error. And imagine how would the code closing the socket have to look like!</p><p>So you try to &quot;undeclare&quot; those variables, but the only way to do &quot;undeclare&quot; is it let the variable fall out of scope:</p><p> <code>socks5_tcp_established *i3; { socks5_authenticated *i2; { auto i1 = socks5_socket(); i2 = i1-&gt;connect_unauthenticated(proxy_addr); } i3 = i1-&gt;connect_tcp(addr); }</code> </p><p>You&#x27;ve got what you wanted — only i3 is declared when you get to the end of the block — but you aren&#x27;t better off. Now you have undefined variables at the beginning. And I am not even speaking of how ugly the code looks like.</p><p>Anyway, this rant is addressed to programming language designers: What options do we have to support such mutating API at the moment. And can we do better?</p>",
    "user": {
      "username": "sustrik",
      "slug": "sustrik",
      "displayName": "Martin Sustrik"
    }
  },
  {
    "_id": "LafvFDyzHgsbbPxM9",
    "title": "[Video] Why Not Just: Think of AGI Like a Corporation? (Robert Miles)",
    "slug": "video-why-not-just-think-of-agi-like-a-corporation-robert",
    "pageUrl": "https://www.lesswrong.com/posts/LafvFDyzHgsbbPxM9/video-why-not-just-think-of-agi-like-a-corporation-robert",
    "postedAt": "2018-12-23T21:49:06.438Z",
    "baseScore": 17,
    "voteCount": 4,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": "https://www.youtube.com/watch?v=L5pUA3LsEaw",
    "htmlBody": "<p>Robert Miles has been creating AI-Alignment related videos for a while now, but I found this one particularly good. </p><p>Here is the automatically generated Youtube transcript. Obviously it&#x27;s not very good, but at least it makes the post searchable (In case Robert reads this and has a transcript for the video lying around, I would love to replace this with one that has proper capitalization and punctuation marks and other luxuries): </p><p>hi so I sometimes see people saying</p><p>things like okay so your argument is</p><p>that at some point in the future we&#x27;re</p><p>going to develop intelligent agents that</p><p>are able to reason about the world in</p><p>general and take actions in the world to</p><p>achieve their goals</p><p>these agents might have superhuman</p><p>intelligence that allows them to be very</p><p>good at achieving their goals and this</p><p>is a problem because they might have</p><p>different goals from us but don&#x27;t we</p><p>kind of have that already corporations</p><p>can be thought of as super intelligent</p><p>agents they&#x27;re able to think about the</p><p>world in general and they can outperform</p><p>individual humans across a range of</p><p>cognitive tasks and they have goals</p><p>namely maximizing profits or shareholder</p><p>value or whatever and those goals aren&#x27;t</p><p>the same as the overall goals of</p><p>humanity so corporations are a kind of</p><p>misaligned super intelligence the people</p><p>who say this having established the</p><p>metaphor at this point tend to diverge</p><p>mostly along political lines some say</p><p>corporations are therefore a clear</p><p>threat to human values and goals in the</p><p>same way that misaligned super</p><p>intelligences are and they need to be</p><p>much more tightly controlled if not</p><p>destroyed all together others say</p><p>corporations are like misaligned super</p><p>intelligences but corporations have been</p><p>instrumental in the huge increases in</p><p>human wealth and well-being that we&#x27;ve</p><p>seen over the last couple of centuries</p><p>with pretty minor negative side effects</p><p>overall if that&#x27;s the effect of</p><p>misaligned super intelligences I don&#x27;t</p><p>see why we should be concerned about AI</p><p>and others say corporations certainly</p><p>have their problems but we seem to have</p><p>developed systems that keep them under</p><p>control well enough that they&#x27;re able to</p><p>create value and do useful things</p><p>without literally killing everyone so</p><p>perhaps we can learn something about how</p><p>to control or align super intelligences</p><p>by looking at how we handle corporations</p><p>so we&#x27;re gonna let the first to fight</p><p>amongst themselves and we&#x27;ll talk to the</p><p>third guy so how good is this metaphor</p><p>our corporations really like misaligned</p><p>artificial general super intelligences</p><p>quick note before we start we&#x27;re going</p><p>to be comparing corporations to AI</p><p>systems and this gets a lot more</p><p>complicated when you consider that</p><p>corporations in fact use AI systems so</p><p>for the sake of simplicity we&#x27;re going</p><p>to assume that corporations don&#x27;t use AI</p><p>systems because otherwise the problem</p><p>gets recursive and like not in a cool</p><p>way</p><p>first off our corporations agents in the</p><p>relevant way I would say yeah pretty</p><p>much I think that it&#x27;s reasonably</p><p>productive to think of a corporation as</p><p>an agent</p><p>they do seem to make decisions and take</p><p>actions in the world in order to achieve</p><p>goals in the world but I think you face</p><p>a similar problem thinking of</p><p>corporations as agents as you do when</p><p>you try to think of human beings as</p><p>agents in economics it&#x27;s common to model</p><p>human beings as agents that want to</p><p>maximize their money in some sense and</p><p>you can model corporations in the same</p><p>way and this is useful but it is kind of</p><p>a simplification in that human beings in</p><p>practice want things that aren&#x27;t just</p><p>money</p><p>and while corporations are more directly</p><p>aligned with profit maximizing than</p><p>individual human beings are it&#x27;s not</p><p>quite that simple so yes we can think of</p><p>corporations as agents but we can&#x27;t</p><p>treat their stated goals as being</p><p>exactly equivalent to their actual goals</p><p>in practice more on that later so</p><p>corporations are more or less agents are</p><p>they generally intelligent agents again</p><p>yeah I think so I mean corporations are</p><p>made up of human beings so they have all</p><p>the same general intelligence</p><p>capabilities that human beings have so</p><p>then the question is are they super</p><p>intelligent this is where things get</p><p>interesting because the answer is kind</p><p>of like SpaceX is able to design a</p><p>better rocket than any individual human</p><p>engineer could design rocket design is a</p><p>cognitive task and SpaceX is better at</p><p>that than any human being therefore</p><p>SpaceX is a super intelligence in the</p><p>domain of rocket design but a calculator</p><p>is a super intelligence in the domain of</p><p>arithmetic that&#x27;s not enough our</p><p>corporation&#x27;s general super</p><p>intelligences do they outperform humans</p><p>across a wide range of cognitive tasks</p><p>as an AGI code in practice it depends on</p><p>the task consider playing a strategy</p><p>game for the sake of simplicity let&#x27;s</p><p>use a game that humans still beat AI</p><p>systems at like Starcraft if a</p><p>corporation for some reason had to win</p><p>at Starcraft it could perform about as</p><p>well as the best human players it would</p><p>do that by hiring the best human players</p><p>but you won&#x27;t achieve superhuman play</p><p>that way a human player acting on behalf</p><p>of the corporation is just a human</p><p>player and the corporation doesn&#x27;t</p><p>really have a way to do much better than</p><p>that</p><p>a team of reasonably good Starcraft</p><p>players working together to control one</p><p>army will still lose to a single very</p><p>good player working alone this seems to</p><p>be true for a lot of strategy games the</p><p>classic example is the game of Kasparov</p><p>versus the world where Garry Kasparov</p><p>played against the entire rest of the</p><p>world cooperating on the Internet</p><p>the game was kind of weird but Kasparov</p><p>ended up winning and the kind of real</p><p>world strategy that corporations have to</p><p>do seems like it might be similar as</p><p>well when companies outsmart their</p><p>competition it&#x27;s usually because they</p><p>have a small number of decision makers</p><p>who are unusually smart rather than</p><p>because they have a hundred reasonably</p><p>smart people working together for at</p><p>least some tasks teams of humans are not</p><p>able to effectively combine their</p><p>intelligence to achieve highly</p><p>superhuman performance so corporations</p><p>are limited to around human level</p><p>intelligence of those tasks to break</p><p>down where this is let&#x27;s look at some</p><p>different options corporations have four</p><p>ways to combine human intelligences one</p><p>obvious way is specialization if you can</p><p>divide the task into parts that people</p><p>can specialize in you can outperform</p><p>individuals you can have one person</p><p>who&#x27;s skilled at engine design one who&#x27;s</p><p>great at aerodynamics one who knows a</p><p>lot about structural engineering and one</p><p>who&#x27;s good at avionics can you tell I&#x27;m</p><p>not a rocket surgeon anyway if these</p><p>people with their different skills are</p><p>able to work together well with each</p><p>person doing what they&#x27;re best at the</p><p>resulting agent will in a sense have</p><p>superhuman intelligence no single human</p><p>could ever be so good at so many</p><p>different things but this mechanism</p><p>doesn&#x27;t get you superhumanly high</p><p>intelligence just superhumanly broad</p><p>intelligence whereas super intelligence</p><p>software AGI might look like this so</p><p>specialization yields a fairly limited</p><p>form of super intelligence if you can</p><p>split your task up but that&#x27;s not easy</p><p>for all tasks for example the task of</p><p>coming up with creative ideas or</p><p>strategies isn&#x27;t easy to split up you</p><p>either have a good idea or you don&#x27;t but</p><p>as a team you can get everyone to</p><p>suggest a strategy or idea and then pick</p><p>the best one that way a group can</p><p>perform better than any individual human</p><p>how much better though and how does that</p><p>change with the size of the team I got</p><p>curious about exactly how this works so</p><p>I came up with a toy model now I&#x27;m not a</p><p>statistician I&#x27;m a computer scientist so</p><p>rather than working it out properly I</p><p>just simulated it a hundred million</p><p>times because that was quicker okay so</p><p>here&#x27;s the idea quality distribution for</p><p>an individual human will model it as a</p><p>normal distribution with a mean of 100</p><p>and a standard deviation of 20 so what</p><p>this means is you ask a human for a</p><p>suggestion and sometimes they do really</p><p>well and come up with a hundred</p><p>30-level strategy sometimes they screw</p><p>up and can only give you a 70 idea but</p><p>most of the time it&#x27;s around 100 now</p><p>suppose we had a second person whose</p><p>intelligence is the same as the first we</p><p>have both of them come up with ideas and</p><p>we keep whichever idea is better the</p><p>resulting team of two people combined</p><p>looks like this</p><p>on average the ideas are better the mean</p><p>is now 107 and as we keep adding people</p><p>the performance gets better here&#x27;s 5</p><p>people 10 20 50 100</p><p>remember these are probability</p><p>distributions so the height doesn&#x27;t</p><p>really matter the point is that the</p><p>distributions move to the right and get</p><p>thinner the average idea quality goes up</p><p>and the standard deviation goes down so</p><p>we&#x27;re coming up with better ideas and</p><p>more reliably but you see how the</p><p>progress is slowing down we&#x27;re using a</p><p>hundred times as much brain power here</p><p>but our average ideas are only like 25%</p><p>better what if we use a thousand people</p><p>ten times more resources again only gets</p><p>us up to around a hundred and thirty</p><p>five diminishing returns so what does</p><p>this mean for corporations well first</p><p>off to be fair this team of a thousand</p><p>people is clearly super intelligent the</p><p>worst ideas it ever has are still so</p><p>good that an individual human will</p><p>hardly ever manage to think of them but</p><p>it&#x27;s still pretty limited there&#x27;s all</p><p>this space off to the right of the graph</p><p>that it would take vast team sizes to</p><p>ever get into if you&#x27;re wondering how</p><p>this would look with seven billion</p><p>humans well you have to work out the</p><p>statistical solution yourself the point</p><p>is the team isn&#x27;t that super intelligent</p><p>because it&#x27;s never going to think of an</p><p>idea that no human could think of which</p><p>is kind of obvious when you think about</p><p>it but AGI is unlimited in that way and</p><p>in practice even this model is way too</p><p>optimistic for corporations firstly</p><p>because it assumes that the quality of</p><p>suggestions for a particular problem is</p><p>uncorrelated between humans which is</p><p>clearly not true and secondly because</p><p>you have to pick out the best suggestion</p><p>but how can you be sure that you&#x27;ll know</p><p>the best idea when you see it it happens</p><p>to be true a lot of the time for a lot</p><p>of problems that we care about that</p><p>evaluating solutions is easier than</p><p>coming up with them you know Homer it&#x27;s</p><p>very easy to criticize machine learning</p><p>relies pretty heavily on this like</p><p>writing a program that differentiates</p><p>pictures of cats and dogs is really hard</p><p>but evaluating such a program is fairly</p><p>simple you</p><p>show it lots of pictures of cats and</p><p>dogs and see how well it does the clever</p><p>bit is in figuring out how to take a</p><p>method for evaluating solutions and use</p><p>that to create good solutions anyway</p><p>this assumption isn&#x27;t always true and</p><p>even when it is the fact that evaluation</p><p>is easier or cheaper than generation</p><p>doesn&#x27;t mean that evaluation is easy or</p><p>cheap</p><p>like I couldn&#x27;t generate a good rocket</p><p>design myself but I can tell you that</p><p>this one needs work so evaluation is</p><p>easier than generation but that&#x27;s a very</p><p>expensive way to find out and I wouldn&#x27;t</p><p>have been able to do it the cheap way by</p><p>just looking at the blueprints the</p><p>skills needed to evaluate in advance</p><p>whether a given rocket design will</p><p>explode are very closely related to the</p><p>skills needed to generate a non</p><p>exploding rocket design so yeah even if</p><p>a corporation could somehow get around</p><p>being limited to the kind of ideas that</p><p>humans are able to generate they&#x27;re</p><p>still limited to the kind of ideas that</p><p>humans are able to recognize as good</p><p>ideas just how serious is this</p><p>limitation how good are the strategies</p><p>and ideas that corporations are missing</p><p>out on well take a minute to think of an</p><p>idea that&#x27;s too good for any human to</p><p>recognize it as good got one well it was</p><p>worth a shot we actually do have an</p><p>example of this kind of thing in move 37</p><p>from alphago&#x27;s 2016 match with world</p><p>champion Lisa doll this kind of</p><p>evaluation value that&#x27;s a very that&#x27;s a</p><p>very surprising move I thought I thought</p><p>it was I thought it was a mistake yeah</p><p>that turned out to be pretty much the</p><p>move that won the game but you&#x27;re go</p><p>playing corporation is never going to</p><p>make move 37 even if someone happens to</p><p>suggest it it&#x27;s almost certainly not</p><p>going to be chosen</p><p>normally human we never play this one</p><p>because it&#x27;s not enough for someone in</p><p>your corporation to have a great idea</p><p>the people at the top need to recognize</p><p>that it&#x27;s a great idea that means that</p><p>there&#x27;s a limit on the effective</p><p>creative or strategic intelligence of a</p><p>corporation which is determined by the</p><p>intelligence of the decision-makers and</p><p>their ability to know a good idea when</p><p>they see one okay what about speed</p><p>that&#x27;s one of the things that makes AI</p><p>systems so powerful and one of the ways</p><p>that software IGI is likely to be super</p><p>intelligent the general trend is we go</p><p>from computer</p><p>can&#x27;t do this at all two computers can</p><p>do this much faster than people not</p><p>always but in general so I wouldn&#x27;t be</p><p>surprised if that pattern continues with</p><p>AGI how does the corporation rate on</p><p>speed again it kind of depends</p><p>this is closely related to something</p><p>we&#x27;ve talked about before parallelizable</p><p>ax t some tasks are easy to split up and</p><p>work on in parallel and some aren&#x27;t</p><p>for example if you&#x27;ve got a big list of</p><p>a thousand numbers and you need to add</p><p>them all up it&#x27;s very easy to paralyze</p><p>if you have ten people you can just say</p><p>okay you take the first hundred numbers</p><p>you take the second hundred you take the</p><p>third and so on have everybody add up</p><p>their part of the list and then at the</p><p>end you add up everyone&#x27;s totals however</p><p>long the list is you can throw more</p><p>people at it and get it done faster much</p><p>faster than any individual human code</p><p>this is the kind of task where it&#x27;s easy</p><p>for corporations to achieve superhuman</p><p>speed but suppose instead of summing a</p><p>list you have a simple simulation that</p><p>you want to run for say a thousand</p><p>seconds you can&#x27;t say okay you work out</p><p>the first hundred seconds of the</p><p>simulation you do the next hundred and</p><p>you do the next hundred and so on</p><p>because obviously the person who&#x27;s</p><p>simulating second 100 needs to know what</p><p>happened at the end of second 99 before</p><p>they can get started so this is what&#x27;s</p><p>called an inherently serial task you</p><p>can&#x27;t easily do it much faster by adding</p><p>more people you can&#x27;t get a baby in less</p><p>than nine months by hiring two pregnant</p><p>women</p><p>you know most real-world tasks are</p><p>somewhere in between you get some</p><p>benefits from adding more people but</p><p>again you hit diminishing returns some</p><p>parts of the task can be split up and</p><p>worked on in parallel some parts need to</p><p>happen one after the other so yes</p><p>corporations can achieve superhuman</p><p>speed add some important cognitive tasks</p><p>but really if you want to talk about</p><p>speed in a principled way you need to</p><p>differentiate between throughput how</p><p>much goes through the system within a</p><p>certain time and latency how long it</p><p>takes a single thing to go through the</p><p>system these ideas are most often used</p><p>in things like networking and I think</p><p>that&#x27;s the easiest way to explain it so</p><p>basically let&#x27;s say you need to send</p><p>someone a large file and you can either</p><p>send it over a dial-up internet</p><p>connection or you can send them a</p><p>physical disk through the postal system</p><p>the dial-up connection is low latency</p><p>each bit of the file goes through the</p><p>system quickly but it&#x27;s also low</p><p>throughput the rate at which you can</p><p>send data is pretty low whereas sending</p><p>the physical disk is high latency it</p><p>might take days for the first</p><p>to arrive but it&#x27;s also high-throughput</p><p>you can put vast amounts of data on the</p><p>disk so your average data sent per</p><p>second could actually be very good</p><p>corporations are able to combine human</p><p>intelligences to achieve superhuman</p><p>throughput so they can complete large</p><p>complex tasks faster than individual</p><p>humans could but the thing is a system</p><p>can&#x27;t have lower latency than its</p><p>slowest component and corporations are</p><p>made of humans so corporations aren&#x27;t</p><p>able to achieve superhuman latency and</p><p>in practice as you&#x27;ve no doubt</p><p>experienced is quite the opposite so</p><p>corporate intelligence is kind of like</p><p>sending the physical disk corporations</p><p>can get a lot of cognitive work done in</p><p>a given time but they&#x27;re slow to react</p><p>and that&#x27;s a big part of what makes</p><p>corporations relatively controllable</p><p>they tend to react so slowly that even</p><p>governments are sometimes able to move</p><p>fast enough to deal with them</p><p>software super intelligence is on the</p><p>other hand could have superhuman</p><p>throughput and superhuman latency which</p><p>is something we&#x27;ve never experienced</p><p>before in a general intelligence so our</p><p>corporations super intelligent agents</p><p>well they&#x27;re pretty much generally</p><p>intelligent agents which are somewhat</p><p>super intelligent in some ways and</p><p>somewhat below human performance in</p><p>others so yeah kinda the next question</p><p>is are they misaligned but this video is</p><p>already like 14 and a half minutes long</p><p>so we&#x27;ll get to that in the next video</p><p>[Music]</p><p>I want to end the video by saying a big</p><p>thank you to my excellent patrons it&#x27;s</p><p>all of these people here in this video</p><p>I&#x27;m especially thanking Pablo area or</p><p>Pablo a de aluminio Sushil recently I&#x27;ve</p><p>been putting a lot of time into some</p><p>projects that I&#x27;m not able to talk about</p><p>but as soon as I can and the patrons</p><p>will be the first to know</p><p>thank you again so much for your</p><p>generosity and thank you all for</p><p>watching I&#x27;ll see you next time</p><p>[Music]</p>",
    "user": {
      "username": "habryka4",
      "slug": "habryka4",
      "displayName": "habryka"
    }
  },
  {
    "_id": "X9RTd4pGoCXwQ2gdC",
    "title": "Boston Solstice 2018 Retrospective",
    "slug": "boston-solstice-2018-retrospective",
    "pageUrl": "https://www.lesswrong.com/posts/X9RTd4pGoCXwQ2gdC/boston-solstice-2018-retrospective",
    "postedAt": "2018-12-23T20:04:40.244Z",
    "baseScore": 44,
    "voteCount": 10,
    "commentCount": 4,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Last night we hosted the Boston Secular Solstice, which I wrote about planning <a href=\"https://www.jefftk.com/p/boston-solstice-2018\">a couple months ago</a>. It felt like it went really well! Several people told me it was their favorite one we&#x27;ve had in Boston.</p><p>The past few years we&#x27;ve been at the MIT Chapel. It&#x27;s a really nice space for this sort of event, but you need more people for it to feel right. Comfortably full is a much better feeling than spread thin. This year we decided to have it at my house, and I was thinking we might have ~20 people.</p><p>A few days before I was looking at a <a href=\"https://www.facebook.com/events/177150809894419/\">fb event</a> with 36 &quot;yes&quot; and 61 &quot;interested&quot;, plus several other people who had told me they were coming. I tried to figure out how to configure our space for maximum capacity:</p><p></p><span><figure><img src=\"https://www.jefftk.com/2018-solstice-floor-plans-big.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p>This meant a lot of moving things around, but I think mostly worked. I set up boards across a few chairs as a way to get a bit denser seating than you can with just chairs alone, and I set up a futon without the frame to get people in front a bit lower:</p><p></p><span><figure><img src=\"https://www.jefftk.com/board-bench-chairs-big.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>Panoramic view:</p><p></p><span><figure><img src=\"https://www.jefftk.com/2018-solstice-panorama-big.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>We ended up having ~40 people, which was a lot. There were empty seats, but only ones that had basically no ability to see the projector. At the beginning I told people how to follow along on their phones if they couldn&#x27;t see well, but I&#x27;m not sure whether that was useful.</p><p>To get the projector image high enough on the wall without blocking the view of people behind it, I put up a temporary shelf:</p><p></p><span><figure><img src=\"https://www.jefftk.com/2018-solstice-projector-shelf-big.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>It&#x27;s back down now, though I still need to spackle the holes. Helpful things about having your own place!</p><p>The overall structure we followed was about 20min of people arriving and getting settled, with snacks, and then a first half of relatively light things. We took a break for intermission, with a dessert potluck, and then when we came back we did progressively darker things culminating in a reading of <a href=\"http://secularsolstice.com/beyond-the-reach-of-god-spoken-word-version/\">Beyond the Reach of God</a> and singing <a href=\"https://humanistculture.bandcamp.com/track/brighter-than-today\">Brighter than Today</a>.</p><p></p><span><figure><img src=\"https://www.jefftk.com/2018-solstice-during-big.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>We ended with a few more songs and one more reading, and then people hung out for ~2hr until I pushed the stragglers out so we could go to bed.</p><p></p><span><figure><img src=\"https://www.jefftk.com/2018-solstice-after-big.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>The program was:</p><ul><li>Welcome</li><li>All sing: <em>Lean on Me</em> (<a href=\"https://www.jefftk.com/solstice-2018/01--lean-on-me.mp3\">mp3</a>)</li><li>Introduction to <em>Why Did the Sun Shine?</em></li><li>All sing: <em>Why Did the Sun Shine?</em> (<a href=\"https://www.jefftk.com/solstice-2018/02--why-did-the-sun-shine.mp3\">mp3</a>)</li><li>Reading: <em><a href=\"https://thewholesky.wordpress.com/2017/12/22/what-i-learned-about-risk-from-parenting/\">What I Learned About Risk from Parenting</a></em>, adapted</li><li>All sing: <em>Uplift</em> (<a href=\"https://www.jefftk.com/solstice-2018/03--uplift.mp3\">mp3</a>)</li><li>Introduction to <em>Her Mysteries</em></li><li>All sing: <em>Her Mysteries</em> (<a href=\"https://www.jefftk.com/solstice-2018/04--her-mysteries.mp3\">mp3</a>)</li><li>Reading: <em><a href=\"http://www.planetary.org/explore/space-topics/earth/pale-blue-dot.html\">Pale Blue Dot</a></em></li><li>All sing: <em>Chasing Patterns</em> (<a href=\"https://www.jefftk.com/solstice-2018/05--chasing-patterns.mp3\">mp3</a>)</li><li>Introduction to <em>Why Does the Sun Shine?</em></li><li>All sing: <em>Why Does the Sun Shine?</em> (<a href=\"https://www.jefftk.com/solstice-2018/06--why-does-the-sun-shine.mp3\">mp3</a>)</li><li>Intermission</li><li>Brief words on fire safety, light candles, house lights out</li><li>Reading: <em><a href=\"https://siderea.dreamwidth.org/1439898.html\">How We Remember and What</a></em>, <a href=\"https://docs.google.com/document/d/18w21H7FLHRvN0VdvE_fBIVSthaLkOJuT0Pi5kCiXdYM/preview\">adapted</a></li><li>Solo: <em>Somebody Will</em> (<a href=\"https://www.jefftk.com/solstice-2018/07--somebody-will.mp3\">mp3</a>)</li><li>Introduction to <em>Hymn to Breaking Strain</em></li><li>All sing: <em>Hymn to Breaking Strain</em> (<a href=\"https://www.jefftk.com/solstice-2018/08--hymn-to-breaking-strain.mp3\">mp3</a>)</li><li>Lights and projector out</li><li>All sing, a capella and without lyrics: <em><a href=\"https://www.jefftk.com/p/sinner-man-pandemic\">Sinner Man (Pandemic)</a></em> (<a href=\"https://www.jefftk.com/solstice-2018/09--sinner-man.mp3\">mp3</a>)</li><li>Reading: <em><a href=\"https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god\">Beyond the Reach of God</a></em>, <a href=\"http://secularsolstice.com/beyond-the-reach-of-god-spoken-word-version/\">adapted</a></li><li>Relight candles, projector on, house lights up during following song</li><li>All sing: <em>Brighter than Today</em> <em>(<a href=\"https://www.jefftk.com/solstice-2018/10--brighter-than-today.mp3\">mp3</a>)</em></li><li>Introduction to <em>Why Does the Sun Really Shine?</em></li><li>All sing: <em>Why Does the Sun Really Shine?</em> (<a href=\"https://www.jefftk.com/solstice-2018/11--why-does-the-sun-really-shine.mp3\">mp3</a>)</li><li>Reading: <em><a href=\"https://www.lesswrong.com/posts/pGvyqAQw6yqTjpKf4/the-gift-we-give-to-tomorrow\">The Gift We Give to Tomorrow</a></em>, <a href=\"https://docs.google.com/document/d/1wTQhHdjA1YmLZyRGnrKvrZdEeTGgRdG6rMluJNjacLw/preview\">adapted</a></li><li>All sing: <em>What a Wonderful World</em> (<a href=\"https://www.jefftk.com/solstice-2018/12--what-a-wonderful-world.mp3\">mp3</a>)</li><li>Reading: <em><a href=\"https://blog.jaibot.com/500-million-but-not-a-single-one-more/\">500 Million, But Not a Single One More</a></em></li><li>All sing: <em>Old Devil Time</em> (<a href=\"https://www.jefftk.com/solstice-2018/13--old-devil-time.mp3\">mp3</a>)</li></ul><p><em><a href=\"https://docs.google.com/presentation/d/1FERDzvznBPOkAPECQcXhq_lWRSP6a3ZPOSxMteao6tc/preview\">lyrics slides</a>,</em> <em><a href=\"https://docs.google.com/presentation/d/1L_mH5FCnddw6U_cspLyWjoYyjbkUlQW46bwJgHJBAxI/preview\">musician slides</a>,</em> <em><a href=\"https://docs.google.com/document/d/1ZRhwSgtO7icrdcgqy6WJcr8jEZurCodk4qsouRWqWP4/preview\">transition words</a></em></p><p>Thoughts:</p><ul><li>Ray tried to convince us to get big tall candles ahead of time, but I liked the idea of using a quirky collection of weird ones I had. In retrospect the weird ones did really poorly with lighting candles from each other, and tall ones would have been a lot better.</li><li>Being in a place where we could comfortably hang out afterwards was great. Lots of interesting discussions.</li><li>Overall, I really liked having candles, which is something we weren&#x27;t allowed to have at the MIT Chapel.</li><li>Of ~40 people, only three needed parking. Which was good, because I only could park four! With enough notice I probably could have gotten parking consideration, which is where the city suspends residential parking enforcement around your house because you&#x27;re hosting something.</li><li>Every year I wish we could have the music settled farther in advance so we&#x27;d have time to actually rehearse. Several people led songs where I was accompanying them completely cold, and as often happens with open signups not everyone who volunteered could carry a tune.</li><li>Relighting the candles took long enough that some people got bored, and it would have worked better if we started Brighter than Today while people were still passing the light around.</li><li>Past solstices I&#x27;ve played a piano, which has meant having my back to the audience. This time I played keyboard, which was so much better. I really like being able to see people, and it also makes it easier for people to follow me if I&#x27;m leading the singing.</li><li>There are a surprisingly large number of versions of Brighter than Today, and the one we&#x27;ve landed on is kind of an unusual one. Ray is working on standardizing this some, and I&#x27;ll look back next year to see what this is like.</li><li>A lot of these songs have tunes that are pretty complex, and this was after trying to pick simpler ones than in past years. Combined with our historical difficulty finding strong songleaders this has been tricky. Filks of common songs are one way to handle this (the Paleolithic version of <em>Why Does the Sun Shine?</em>, the pandemic version of <em>Sinner Man</em>, various reworded Christmas carols we&#x27;ve done at times), as is composing new songs with simple melodies (<em>Chasing Patterns</em>). This is a similar set of constraints (with less tolerance for filks) that led to <a href=\"https://en.wikipedia.org/wiki/Contemporary_worship_music\">praise music</a> (<a href=\"https://www.youtube.com/watch?v=F8umfBRlwW8\">examples</a>), and those songs are generally reasonably easy for people to pick up.</li><li>I looked at what it would take to get this program (and past Boston programs) onto the <a href=\"https://secularsolstice.github.io/\">Secular Solstice Resources</a> page, but after getting the dependencies installed and starting to add something it seemed like much more typing than I was up for.</li><li>Other people probably have better pictures than I took? None of mine came out especially well.</li><li>Starting right in with <em>Why Did the Sun Shine</em> would have been better in terms of feeling, but we didn&#x27;t think enough people knew the tune. Turns out we were wrong!</li><li>Taymon did a huge amount of work, which was critical for this to go as well as it did.</li></ul><p><em>follow-up to <a href=\"https://www.lesswrong.com/posts/ERboWueanAyqwKbiQ\">Boston Solstice 2018</a>, cross-posted from <a href=\"https://www.jefftk.com/p/boston-solstice-2018-retrospective\">jefftk.com</a></em></p>",
    "user": {
      "username": "jkaufman",
      "slug": "jkaufman",
      "displayName": "jefftk"
    }
  },
  {
    "_id": "jGmSMHmELjZaHm44k",
    "title": "Best arguments against worrying about AI risk?",
    "slug": "best-arguments-against-worrying-about-ai-risk",
    "pageUrl": "https://www.lesswrong.com/posts/jGmSMHmELjZaHm44k/best-arguments-against-worrying-about-ai-risk",
    "postedAt": "2018-12-23T14:57:09.905Z",
    "baseScore": 15,
    "voteCount": 7,
    "commentCount": 16,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>Since so many people here (myself included) are either working to reduce AI risk or would love to enter the field, it seems worthwhile to ask what are the best arguments against doing so. This question is intended to focus on existential/catastrophic risks and not things like technological unemployment and bias in machine learning algorithms. </p>",
    "user": {
      "username": "Chris_Leong",
      "slug": "chris_leong",
      "displayName": "Chris_Leong"
    }
  },
  {
    "_id": "WdMHxAxEjmiihJbf9",
    "title": "Why Don't Creators Switch to their Own Platforms?",
    "slug": "why-don-t-creators-switch-to-their-own-platforms",
    "pageUrl": "https://www.lesswrong.com/posts/WdMHxAxEjmiihJbf9/why-don-t-creators-switch-to-their-own-platforms",
    "postedAt": "2018-12-23T04:46:47.047Z",
    "baseScore": 42,
    "voteCount": 16,
    "commentCount": 18,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>Almost every content creator rationalists follow owns their platform: podcasters like Sam Harris and the Julia Galef, bloggers like Scott (and <a href=\"https://putanumonit.com/\">myself</a>), all the nerdy webcomics. And yet, outside the rationalsphere every creator seems engaged in an endless fight against censorship and harassment by the platforms that are supposed to enable them: Facebook, Twitter, Tumblr, Patreon... So why do they stay on those platforms? Other than Sam Harris giving Patreon the middle finger, no one else seems to do much except protest platforms on the platforms themselves.</p><p>This questions really came up for me after reading the <a href=\"https://quillette.com/2018/12/20/pewdiepies-battle-for-the-soul-of-the-internet/\">saga of Pewdiepie and YouTube</a>. Currently, pewdiepie.com redirects to his YouTube page, where he posts videos protesting YouTube. This is crazy. The technology that YouTube provides was hard to build when YouTube started a decade and a half ago, but surely today it&#x27;s not a huge challenge. PDP has 20 billion total views. He doesn&#x27;t need traffic from the algorithm suggesting his videos, everyone else is trying to game the algorithm to get redirected by PDP! Switching to his own platform would allow him to capture a higher percentage of revenue, be immune to any kind of censorship, and make him a legend if he starts an exodus from YouTube. He can host all the other non-PC comedians on his own platform. How is that not worth losing a bit of traffic as viewers readjust?</p><p></p>",
    "user": {
      "username": "Jacobian",
      "slug": "jacob-falkovich",
      "displayName": "Jacob Falkovich"
    }
  },
  {
    "_id": "yBGcu4kFzvLHRjZnk",
    "title": "Cognitive Bias of AI Researchers?",
    "slug": "cognitive-bias-of-ai-researchers",
    "pageUrl": "https://www.lesswrong.com/posts/yBGcu4kFzvLHRjZnk/cognitive-bias-of-ai-researchers",
    "postedAt": "2018-12-22T09:20:52.045Z",
    "baseScore": 9,
    "voteCount": 6,
    "commentCount": 7,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>I find it inconvenient that many AI discussions circle around &quot;<strong>agents</strong>&quot;, “<strong>environments</strong>” and “<strong>goals</strong>”. These are <strong>non-mathematical words</strong>, and by using this vocabulary we are anthropomorphizng natural world&#x27;s phenomena.<br/><br/>While an &quot;agent&quot; may well be a set of interacting processes, that produce emergent phenomena (like volition, cognition, action), it is not a fundamental and pragmatic mathematical concept. The truly fundamental pragmatic mathematical concepts may be:<br/><br/>(1) <strong>states</strong>: a set of possible world states (each being a sets of conditions).<br/>(2) <strong>processes</strong>: a set of world processes progressing towards some of those states. <br/><br/>If so, how could we avoid that anthropomorphic cognitive bias in our papers, and discussions?</p><p>Would the (1), (2) terms be a good alternative for our discussions, and to express the ideas in most AI research papers? E.g., <strong>Bob is a process</strong>, and <strong>Alice is processes</strong>,... and they collectively are progressing towards some  <strong><span style=\"text-decoration:line-through\">desired state</span></strong>  <strong><em>convergent state</em></strong>, defined by process addition.</p><p>What fundamental concepts would you consider to be a better alternative to talk formally about the AI domain?</p>",
    "user": {
      "username": "Mindey",
      "slug": "mindey",
      "displayName": "Mindey"
    }
  },
  {
    "_id": "NQgWL7tvAPgN2LTLn",
    "title": "Spaghetti Towers",
    "slug": "spaghetti-towers",
    "pageUrl": "https://www.lesswrong.com/posts/NQgWL7tvAPgN2LTLn/spaghetti-towers",
    "postedAt": "2018-12-22T05:29:47.551Z",
    "baseScore": 233,
    "voteCount": 133,
    "commentCount": 41,
    "meta": false,
    "question": false,
    "url": "https://eukaryotewritesblog.com/2018/12/21/spaghetti-towers/",
    "htmlBody": "<p>Here’s a pattern I’d like to be able to talk about. It might be known  under a certain name somewhere, but if it is, I don’t know it. I call  it a Spaghetti Tower. It shows up in large complex systems that are built haphazardly.</p><p>Someone or something builds the first Part A.</p><p></p><span><figure><img src=\"https://eukaryotewritesblog.files.wordpress.com/2018/12/20181220_204411.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>Later, someone wants to put a second Part B on top of Part A, either out of convenience (a common function, just somewhere to put it) or as a refinement to Part A.</p><p></p><span><figure><img src=\"https://eukaryotewritesblog.files.wordpress.com/2018/12/20181220_204450.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>Now, suppose you want to tweak Part A. If you do that, you might break Part B, since it interacts with bits of Part A. So you might instead build Part C on top of the previous ones.</p><p></p><span><figure><img src=\"https://eukaryotewritesblog.files.wordpress.com/2018/12/20181220_204759.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>And by the time your system looks like this, it’s much harder to tell what changes you can make to an earlier part without crashing some component, so you’re basically relegated to throwing another part on top of the pile.</p><p></p><span><figure><img src=\"https://eukaryotewritesblog.files.wordpress.com/2018/12/bkajfeakfje.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>I call these spaghetti towers for two reasons: One, because they tend to quickly take on circuitous knotty tangled structures, like what programmers call “spaghetti code”. (Part of the problem with spaghetti code is that it can lead to spaghetti towers.)</p><p>Especially since they’re usually interwoven in multiple dimensions, and thus look more like this:</p><p></p><span><figure><img src=\"https://eukaryotewritesblog.files.wordpress.com/2018/12/20181220_205553.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>“Can you just straighten out the yellow one without touching any of the others? Thanks.”</p><p>Second, because shortsightedness in the design process is a crucial part of spaghetti machines. In order to design a spaghetti system, you <a href=\"https://www.urbandictionary.com/define.php?term=spaghetti%20test\">throw spaghetti against a wall and see if it sticks</a>. Then, when you want to add another part, you throw more spaghetti until it sticks to that spaghetti. And later, you throw more spaghetti. So it  goes. And if you decide that you want to tweak the bottom layer to make it a little more useful – which you might want to do because, say, it was built out of spaghetti – without damaging the next layers of gummy partially-dried spaghetti, well then, good luck.</p><p>Note that all systems have load-bearing, structural pieces. This does not make them spaghetti towers. The distinction about spaghetti towers  is that they have a lot of shoddily-built structural components that are  completely unintentional. A bridge has major load-bearing components – they’re pretty obvious, strong, elegant, and efficiently support the  rest of the structure. A spaghetti tower is more like this.</p><span><figure><img src=\"https://eukaryotewritesblog.files.wordpress.com/2018/12/SpaghettiFix-1.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p><em>Image from the always-delightful <a href=\"https://www.reddit.com/r/diwhy/\">r/DiWHY</a>. </em></p><p>(The motto of the spaghetti tower is “Sure, it works fine, as long as you never run lukewarm water through it and unplug the washing machine during thunderstorms.”)</p><p>Where do spaghetti towers appear?</p><ul><li>Basically all of biology works like this. Absolutely all of  evolution is made by throwing spaghetti against walls and seeing what sticks. (More accurately, throwing nucleic acid against harsh reality  and seeing what successfully makes more nucleic acid.) We are 3.5  billion years of hacks in fragile trench coats. </li><ul><li><a href=\"https://www.tumblr.com/dashboard/blog/slatestarscratchpad\">Scott Star Codex</a> describes the phenomenon in neurotransmitters, but it’s true for all of molecular biology:</li></ul></ul><blockquote>You know those stories about clueless old people who get  to their Gmail account by typing “Google” into Bing, clicking on Google in the Bing search results, typing “Gmail” into Google, and then  clicking on Gmail in the Google search results?</blockquote><blockquote> I am reading about serotonin transmission now, and everything in the  human brain works on this principle. If your brain needs to downregulate  a neurotransmitter, it’ll start by upregulating a completely different neurotransmitter, which upregulates the first neurotransmitter, which hits autoreceptors that downregulate the first neurotransmitter, which then cancel the upregulation, and eventually the neurotransmitter gets downregulated.</blockquote><blockquote>Meanwhile, my patients are all like “How come this drug that was  supposed to cure my depression is giving me vision problems?” and at least on some level the answer is “how come when Bing is down your  grandfather can’t access Gmail?</blockquote><ul><li>My programming friends tell me that spaghetti towers are  near-universal in the codebases of large companies. Where it would theoretically be nice if every function was neatly ordered, but actually, the thing you’re working on has three different dependencies, two of which are unmaintained and were abandoned when the guy who built them went to work at Google, and you can never be 100% certain that your code tweak won’t crash the site.</li><li>I think this also explains some of why bureaucracies look and act the way they do, and are so hard to change.</li></ul><p>I think there are probably a lot of examples of spaghetti towers, and they probably have big ramifications for things like, for instance, what systems evolution can and can’t build.</p><p>I want to do a much deeper and more thoughtful analysis about what  exactly the implications here are, but this has been kicking around my  brain for long enough and all I want to do is get the concept out there.</p><p>Does this feel like a meaningful concept? Where do you see spaghetti towers?</p>",
    "user": {
      "username": "eukaryote",
      "slug": "eukaryote",
      "displayName": "eukaryote"
    }
  },
  {
    "_id": "6J4GwrRCFxCgTdWqR",
    "title": "Boundaries enable positive material-informational feedback loops",
    "slug": "boundaries-enable-positive-material-informational-feedback",
    "pageUrl": "https://www.lesswrong.com/posts/6J4GwrRCFxCgTdWqR/boundaries-enable-positive-material-informational-feedback",
    "postedAt": "2018-12-22T02:46:48.938Z",
    "baseScore": 36,
    "voteCount": 13,
    "commentCount": 26,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p><em>(Cross-posted from <a href=\"https://unstableontology.com/2018/12/22/boundaries-enable-positive-material-informational-feedback-loops/\">my blog</a>)</em></p>\n<p>[epistemic status: obvious once considered, I think]</p>\n<p>If you want to get big things done, you almost certainly need positive feedback loops.  Unless you can already do all the necessary things, you need to do/make things that allow you to do/make more things in the future.  This dynamic can be found in RPG and economy-management games, and in some actual economic systems, such as industrializing economies.</p>\n<h1>Material, information, and economy</h1>\n<p>Some goods that can be used in a positive feedback loop, such as software and inventions, are informational.  Once produced, they can be used indefinitely in the future.  In economic terms, they are nonrivalous.</p>\n<p>Other goods are material, such as manufactured goods and energy.  They can't be copied cheaply.  In economic terms, they are rivalrous.</p>\n<p>In practice, any long-lasting positive feedback loop contains both informational and material goods, as production of information requires a physical substrate.  While ensuring that informational goods can be used in the future is an organization and communication problem (a subject beyond the scope of this post), the problem of ensuring that material goods can be used in the future is additionally a security problem.</p>\n<p>An important question to ask is: why haven't material-informational positive feedback loops already taken over the world?  Why don't we have so much stuff by now that providing for people's material needs (such as food and housing) is trivial?</p>\n<p>To some extent, material-informational positive feedback loops <em>have</em> taken over the world, but they seem <em>much</em> slower than one would naively expect.  See <a href=\"http://slatestarcodex.com/2017/02/09/considerations-on-cost-disease/\">cost disease</a>.  As an example of cost disease, the average cost of a new house in the USA has <a href=\"https://www.census.gov/hhes/www/housing/census/historic/values.html\">quadrupled</a> over a 60-year period (adjusted for inflation!), whereas models of capitalism based on economy-management games such as <a href=\"https://en.wikipedia.org/wiki/Factorio\">Factorio</a> (or, more academically, according to the <a href=\"https://en.wikipedia.org/wiki/On_the_Principles_of_Political_Economy_and_Taxation\">labor/capital based economic models</a> of classical economists such as <a href=\"https://en.wikipedia.org/wiki/David_Ricardo\">David Ricardo</a>) would suggest that houses would be plentiful by now.  (And no, this isn't just because of land prices; it costs <a href=\"https://www.homeadvisor.com/cost/architects-and-engineers/build-a-house/\">about $300K</a> to build a house in the US in 2018)</p>\n<h1>Security and boundaries</h1>\n<p>I've already kind of answered this question by saying that ensuring that material goods can be used in the future is a security problem.  If you use one of your material goods to produce another material good, and someone takes this new good, then you can't put this good back into your production process.  Thus, what would have been a positive feedback loop is instead a negative feedback loop, as it leaks goods faster than it produces them.</p>\n<p>Solving security issues generally requires boundaries.  You need to draw a boundary in material space somewhere, differentiating the inside from the outside, such that material goods (such as energy) on the inside don't leak out, and can potentially have positive feedback loops.  There are many ways to prevent leaks across a boundary while still allowing informational and material to pass through sometimes, such as semiporous physical barriers and active policing.  Regardless of the method to enforce the boundary, the boundary has to exist in some geometrical sense for it to make sense to say that e.g. energy increases within this system.</p>\n<p>Not all security issues are from other agents; some are from non-agentic processes.  Consider a homeostatic animal.  If the animal expends energy to warm its body, and this warmth escapes, the animal will fail to realize gains from the energy expenditure.  Thus, the animal has a boundary (namely, skin) to solve this \"security problem\".  The cold air particles that take away heat from the animal are analogous to agents that directly take resources, though obviously less agentic.  While perhaps my usage of the word \"security\" to include responses to nonagentic threats is nonstandard, I hope it is clear that these are on the same spectrum as agentic threats, and can be dealt with in some of the same ways.</p>\n<p>It is also worth thinking about semi-agentic entities, such as microorganisms.  One of the biggest threats to a food store is microorganisms (i.e. rotting), and slowing the negative feedback loops depleting food stores requires solving this security problem using a boundary (such as a sealed container or a subset of the air that is colder than the outside air, such as in a refrigerator).</p>\n<p>Property rights are a simple example of boundaries.  Certain goods are considered to be \"owned\" by different parties, such that there is common agreement about who owns what, and people are for one reason or another not motivated to take other people's stuff.  Such division of goods into sets owned by different parties is a set of boundaries enabling positive feedback loops, which are especially salient in capitalism.</p>\n<p>What about trust between different entities?  A complex ecosystem will contain entities satisfying a variety of niches, which include parasitism and predation (which are on the same spectrum).  A trust network can be thought of as a way for different entities to draw various boundaries, often fuzzy ones, that mostly exclude parasites/predators, such that there are few leaks from inside this boundary to outside this boundary (which would include parasitism/predation by entities outside the boundary).  There are \"those who you trust\" and \"those who you don't trust\" (both fuzzy sets), and you assign more utility to giving resources to those you trust, as this allows for positive feedback loops within a system that contains you (namely, the trust network).</p>\n<h1>Externalities and sustainability</h1>\n<p>Since no subsystem of the world is causally closed, all positive feedback loops have externalities.  By definition, the outside world is only directly affected by these externalities, and is only affected by what happens within the boundary to the extent that this eventually leads to externalities.  A wise designer of a positive feedback loop will anticipate its externalities, and set it up such that the externalities are overall desirable to the designer.  After all, there is no point to creating a positive feedback loop unless its externalities are mostly positive [EDIT: unless the boundary contains things that have intrinsic value].</p>\n<p>A positive feedback loop's externalities modify its environment, affecting its own ability to continue; for example, a positive feedback loop of microorganisms eating food will exhaust itself by consuming the food.  So, different positive feedback loops are environmentally sustainable to different extents.  Both production and conquest generate positive feedback loops, as Ben Hoffman discusses in <a href=\"http://benjaminrosshoffman.com/talents/\">this post</a>, but production is much more environmentally sustainable than conquest.</p>\n<p>One way to increase environmental sustainability is to move more processes to the inside of the boundary.  For example, a country that is consuming large amounts of iron (driving up iron prices) may consider setting up its own iron mines.  Thus, the inside of the boundary becomes more like an economy of its own.  This is sometimes known as <a href=\"https://en.wikipedia.org/wiki/Import_replacement\">import replacement</a>.</p>\n<p>Of course, the environmental sustainability of a positive feedback loop can also be a negative, as it is better for some processes (such as rotting) to limit or exhaust themselves, thus transitioning to negative feedback or a combination of positive and negative feedback.  Processes that include intentionally-designed positive and negative feedback can be much more environmentally sustainable than processes that only have positive feedback loops designed in, since they can limit their growth when such growth would be unsustainable.</p>\n<p>While in theory the philosophy of effective altruism would imply a strong (and likely overwhelming) emphasis on creating and maintaining environmentally sustainable positive feedback loops with positive externalities, typically-recommended EA practices (such as giving away 10% of one's income) are <em>negative</em> feedback loops (the more you make, the more you give away).  While in theory the place the resources are given to could have a faster positive feedback loop than just <a href=\"https://rationalconspiracy.com/2012/12/29/if-youre-young-dont-give-to-charity/\">investing in yourself, your friends, and your projects</a>, in practice I rarely believe <a href=\"https://blog.givewell.org/2013/05/15/flow-through-effects/\">claims of this form that come from the EA movement</a>; for example, if a country has a high rate of poverty, that indicates that the <em>negative</em> feedback loops (such as corruption) are likely stronger than the positive ones, and that giving resources is ineffective.  Thus, I cannot in good conscience allow anything like current EA ideology to substantially control resource allocation in most systems I create, even though EA philosophy taken to its logical conclusion would get the right answer on the importance of boundaries and positive feedback loops.</p>\n<h1>Policy suggestions</h1>\n<p>How do these ideas translate to action?  One suggestion is that, if you are trying to do something big, you use one or more positive feedback loops, and ask yourself the following questions about each one:</p>\n<ol>\n<li>What's the generator of my positive feedback loop (i.e. what's the process that turns stuff into more stuff)?</li>\n<li>What is the boundary within which the positive feedback increases resources?</li>\n<li>How am I reducing leakage across this boundary?</li>\n<li>What are the externalities of this positive feedback loop?</li>\n<li>How environmentally sustainable is this positive feedback loop?</li>\n<li>Are there built-in negative feedback loops that increase environmental sustainability?</li>\n</ol>\n<hr>\n<p>(thanks to Bryce Hidysmith for a conversation that led to this post)</p>\n</body></html>",
    "user": {
      "username": "jessica.liu.taylor",
      "slug": "jessica-liu-taylor",
      "displayName": "jessicata"
    }
  },
  {
    "_id": "MerZuGnCNYcKSF8Qe",
    "title": "LessWrong 35c3 meetup",
    "slug": "lesswrong-35c3-meetup",
    "pageUrl": "https://www.lesswrong.com/events/MerZuGnCNYcKSF8Qe/lesswrong-35c3-meetup",
    "postedAt": "2018-12-21T22:18:27.589Z",
    "baseScore": 3,
    "voteCount": 2,
    "commentCount": 2,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>There&#x27;s going to be a LessWrong meetup at the 35th Chaos Communication Congress. </p><p>This is an annual (with increasing numbers) event organised from the Chaos Communication Club. This year it&#x27;s going to be roughly 17k enthusiasts about computer science, technology and other related subjects. More information can be found here: https://events.ccc.de/congress/2018/wiki/index.php/Main_Page</p><p>Look up the actual meetup location in the self-organised-session part of the wiki: https://events.ccc.de/congress/2018/wiki/index.php/Session:LessWrong_meetup</p><p>(updates will follow)</p>",
    "user": {
      "username": "felix-karg",
      "slug": "felix-karg",
      "displayName": "Felix Karg"
    }
  },
  {
    "_id": "uzb3u3zMTkrSEhCaf",
    "title": "Anthropic probabilities and cost functions",
    "slug": "anthropic-probabilities-and-cost-functions",
    "pageUrl": "https://www.lesswrong.com/posts/uzb3u3zMTkrSEhCaf/anthropic-probabilities-and-cost-functions",
    "postedAt": "2018-12-21T17:54:20.921Z",
    "baseScore": 16,
    "voteCount": 5,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head><style type=\"text/css\">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></head><body><p>I've claimed that anthropic probabilities like <a href=\"https://en.wikipedia.org/wiki/Self-indication_assumption\">SIA</a> and <a href=\"https://en.wikipedia.org/wiki/Self-sampling_assumption\">SSA</a> don't actually exist - or, more properly, that you need to include some details of preferences in order to get any anthropic probabilities, and thus that anthropic issues should be <a href=\"https://arxiv.org/abs/1110.6437\">approached from the perspective of decision theory</a>.</p>\n<p>What do I mean by this? Well, informally, what are probabilities? If I said that <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span> (a very visible event) would happen with a probability <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span></span></span>, then I would expect to see events like <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span> happen about a tenth of the time.</p>\n<p>This makes a lot of sense. Why can't it be transposed into anthropic situations? Well, the big problem is the \"I\" in \"I would expect\". Who is this \"I\" - me, my copies, some weighted average of us all?</p>\n<p>In non-anthropic situations, we can formalise \"I would expect to see\" with a cost function. Let me choose a number <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p(X)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span> to be whatever I want; then, if <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span> doesn't happen I pay a cost of <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(p(X))^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span>, while if it does happen, I pay a cost of <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(p(X)-1)^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span> (this is exactly equal to <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"-(p(X)-I_X)^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.064em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;\">I</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span>, for <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"I_X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.064em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;\">I</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span></span></span> the <a href=\"https://en.wikipedia.org/wiki/Indicator_function\">indicator function</a> of <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span>).</p>\n<p>Then, for this cost function, I minimize my losses by setting \"<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p(X)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>\" to be equal to my subjective opinion of the probability of <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span> (note there are many eliciting <a href=\"https://jfinocchiaro.files.wordpress.com/2018/05/cvx-elic4.pdf\">cost functions</a> we could have used, not just the quadratic loss, but the results are the same in for all of them).</p>\n<p>In the informal setting, we didn't know how to deal with \"I\" when expecting future outcomes. In the formal setting, we don't know how to aggregate the cost when multiple copies could all have to pay the cost.</p>\n<p>There are two natural methods of aggregation: the first is to keep <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"-(p(X)-I_X)^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.064em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;\">I</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span>, as above, as the cost for every copy. Thus each copy has the <em>average</em> cost of all the copies (this also allows us to generalise to situations where different copies would see different things). In this case, the probability that develops from this is SSA.</p>\n<p>Alternatively, we could add up all the costs, giving a <em>total</em> cost of <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"-n(p(X)-I_X)^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.064em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;\">I</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span> if there were <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span> copies (this also generalises to situations where different copies see different things). In this case, the probability that develops from this is SIA.</p>\n<p>So probability might be an estimate of what I expect to see, or a cost-minimiser for errors of prediction, but anthropic probabilities differ depending on how one extends \"I\" and \"cost\" to situations of multiple copies.</p>\n</body></html>",
    "user": {
      "username": "Stuart_Armstrong",
      "slug": "stuart_armstrong",
      "displayName": "Stuart_Armstrong"
    }
  },
  {
    "_id": "7LR5F4RhbbaLsra5y",
    "title": "Standing on a pile of corpses",
    "slug": "standing-on-a-pile-of-corpses",
    "pageUrl": "https://www.lesswrong.com/posts/7LR5F4RhbbaLsra5y/standing-on-a-pile-of-corpses",
    "postedAt": "2018-12-21T10:36:50.454Z",
    "baseScore": 35,
    "voteCount": 24,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>[In the darkest day of 2018, it is proper to think about the darkness that surrounds us]</p><p>When we think about the history of humanity, we focus on its highlights. </p><p>Galileo discovering the moons of Jupiter. Edward Jenner developing the first pox vaccine. Emmy Noether setting the mathematical foundation of modern physics.</p><p>We stand on the shoulders of giants, we say, that have elevated us over the clouds so we can see the stars above.</p><p>I believe a more apt metaphor is that we stand on a pile of nameless corpses.</p><p>Because for each great human that made it to the history books, a million have been forgotten. And each of them, knowingly or unknowingly, it is part of our legacy.</p><p>And most of them are dead.</p><p>100 billion humans have been born and died, most in undignified circumstances of sickness and age, and most by no choice of their own. They are now part of the pile of corpses.</p><p>7 billion people remain alive. And even though our lives are much better than those of the people from 1000 years ago, from 100 years ago and even from 10 years ago, we still suffer.</p><p>Many of our living kin live in sickness and hunger. Even amongst the most fortunate we wrestle with mental illness and accidents and the plights of aging.</p><p>We do not stand proud, but afraid and resigned to become yet another layer of the pile of corpses.</p><p>For not a single human lives free of the tyranny of death. We can choose to embrace it early, but we cannot still postpone it, not for much.</p><p>And so the pile of corpses grows, too fast for us to properly mourn the fallen.</p><p>There is a glimmer of hope, that the pile of corpses will stop growing as we come of age as a civilization. That we will stop the non consensual suffering we experience, and death will be no more for those who want to defy it.</p><p>That we will have, for the first time in history, time to breathe and reflect and remember all the nameless people who came before. To properly ponder without having to constantly struggle over our survival, and leaving behind extreme suffering.</p><p>To finally give a proper burial to the pile of corpses we stand on, and decide what to do with our piece of the universe.</p><p>But we are not guaranteed this happy ending - the book of humanity might suddenly end, without us having a chance to dictate the final words.</p><p>The worst we have endured is not a good predictor of the worst that it is to come, and for all we know the horrors ahead may be the ones that end us.</p><p>And then the pile of corpses will stop growing, but so will our civilization.</p><p>And all there will be left is an inanimate pile of corpses floating through the cosmos, surrounded by the cold and uncaring void.</p><p><em>Thanks to Tam Borine for proofreading the text. This text was used as part of a 2018 private secular solstice celebration in Madrid, Spain.</em></p>",
    "user": {
      "username": "Jsevillamol",
      "slug": "jsevillamol",
      "displayName": "Jsevillamol"
    }
  },
  {
    "_id": "q25bajee6DeL9wFqm",
    "title": "Systems Engineering and the META Program",
    "slug": "systems-engineering-and-the-meta-program",
    "pageUrl": "https://www.lesswrong.com/posts/q25bajee6DeL9wFqm/systems-engineering-and-the-meta-program",
    "postedAt": "2018-12-20T20:19:25.819Z",
    "baseScore": 31,
    "voteCount": 14,
    "commentCount": 3,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>I periodically look for information on <a href=\"https://en.wikipedia.org/wiki/Systems_engineering\">systems engineering</a>. This time I came across a powerpoint presentation from the MIT Open Courseware course <a href=\"https://ocw.mit.edu/courses/aeronautics-and-astronautics/16-842-fundamentals-of-systems-engineering-fall-2015/lecture-notes/MTI16_842F15_Ses12_FutofSE.pdf\">Fundamentals of Systems Engineering</a>. Professor de Weck, who taught the course, had done some research on state-of-the-art methods developed as part of DARPA&#x27;s META Program.</p><p>A few years ago DARPA wrapped up the program, designed to speed up delivery of cyber-electro-mechanical systems (war machines) by 5x. Since the parent program <a href=\"https://www.darpa.mil/program/adaptive-vehicle-make\">Adaptive Vehicle Make</a> seems to have concluded without producing a vehicle, I infer the META Program lost its funding at the same time.</p><p>The work it produced appears to be adjacent to our interests along several dimensions though, so I thought I would bring it to the community&#x27;s attention. The pitch for the program, taken from the abstract of <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1016.1278&rep=rep1&type=pdf\">de Weck&#x27;s paper</a>:</p><blockquote>The method claims to achieve this speedup by a combination of three main mechanisms:</blockquote><blockquote>1. The deliberate use of layers of abstraction. High-level functional   requirements are used to explore architectures immediately  rather  than waiting for downstream level 2,3,4 ... requirements to be defined.</blockquote><blockquote>2. The  development  and  use  of  an  extensive  and  trusted component  (C2M2L)  model  library.  Rather  than  designing  all components from scratch, the META process allows importing component  models  directly  from  a  library  in  order  to  quickly compose functional designs.</blockquote><blockquote>3. The  ability  to  find  emergent  behaviors  and  problems ahead of time during virtual Verification and Validation (V&amp;V) and generating designs that are correct-by-construction allows a more  streamlined  design  process  and  avoids  costly  design iterations that often lead to expensive design changes.</blockquote><p>Which is to say they very carefully architect the system, use known-to-be-good components, and employ formal verification to catch problems early. In the paper a <em>simulation</em> of the META workflow successfully achieved a 4.4x development speedup compared to the same project&#x27;s actual development using traditional methods.</p><p>There are a bunch of individual directions explored which are of interest. Some that struck me were:</p><ul><li>A <a href=\"https://cps-vo.org/node/2667\">metric for complexity</a>.</li><li>A <a href=\"https://apps.dtic.mil/dtic/tr/fulltext/u2/a552867.pdf\">metric for adaptability</a>.</li><li>Stuff for <a href=\"http://qav.cs.ox.ac.uk/projects/darpa-prismatic/publications.php\">quantitative verification methods</a>.</li><li>Stuff for <a href=\"http://loonwerks.com/projects/meta.html\">complexity reduction and verification</a>.</li></ul><p>It looks like the repository for the program&#x27;s outputs is <a href=\"https://cps-vo.org/node/2243/browser\">here</a>. A missile engineer trying to apply the method to healthcare is <a href=\"https://www.incose.org/docs/default-source/working-groups/healthcare/public-library/2018-se-in-healthcare-conference/improving-operating-room-design-through-innovative-systems-engineering-methodology.pdf?sfvrsn=f50897c6_2\">here</a>. Reducing healthcare costs <em>is</em> rocket science!</p><p>In a nutshell it contains a whole bunch of things we have long discussed all in a single package, and now there are a few people out and about trying to get parts into practice.</p>",
    "user": {
      "username": "ryan_b",
      "slug": "ryan_b",
      "displayName": "ryan_b"
    }
  },
  {
    "_id": "3rxMBRCYEmHCNDLhu",
    "title": "The Pavlov Strategy",
    "slug": "the-pavlov-strategy",
    "pageUrl": "https://www.lesswrong.com/posts/3rxMBRCYEmHCNDLhu/the-pavlov-strategy",
    "postedAt": "2018-12-20T16:20:00.542Z",
    "baseScore": 287,
    "voteCount": 154,
    "commentCount": 14,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Epistemic Status: Common knowledge, just not to me</em></p>\n<p><a href=\"https://ncase.me/trust/\">The Evolution of Trust</a> is a deceptively friendly little interactive game.  Near the end, there’s a “sandbox” evolutionary game theory simulator. It’s pretty flexible. You can do quick experiments in it without writing code. I highly recommend playing around.</p>\n<p>One of the things that surprised me was a strategy the game calls Simpleton, also known in the literature as Pavlov.  In certain conditions, it works pretty well — even better than tit-for-tat or tit-for-tat with forgiveness.</p>\n<p>Let’s set the framework first. You have a Prisoner’s dilemma type game.</p>\n<ul>\n<li>If both parties cooperate, they each get +2 points.</li>\n<li>If one cooperates and the other defects, the defector gets +3 points and the cooperator gets -1 point</li>\n<li>If both defect, both get 0 points.</li>\n</ul>\n<p>This game is <em>iterated </em>— you’re randomly assigned to a partner and you play many rounds.   Longer rounds reward more cooperative strategies; shorter rounds reward more defection.</p>\n<p>It’s also <em>evolutionary </em>— you have a proportion of bots each playing their strategies, and after each round, the bots with the most points replicate and the bots with the least points die out.  Successful strategies will tend to reproduce while unsuccessful ones die out.  In other words, this is the <a href=\"https://thezvi.wordpress.com/2017/11/15/the-darwin-game/\">Darwin Game.</a></p>\n<p>Finally, it’s <em>stochastic</em> — there’s a small probability that any bot will make a mistake and cooperate or defect at random.</p>\n<p>Now, how does Pavlov work?</p>\n<p>Pavlov starts off cooperating.  If the other player cooperates with Pavlov, Pavlov keeps doing whatever it’s doing, even if it was a mistake; if the other player defects, Pavlov switches its behavior, even if it was a mistake.</p>\n<p>In other words, Pavlov:</p>\n<ul>\n<li>cooperates when you cooperate with it, except by mistake</li>\n<li>“pushes boundaries” and keeps defecting when you cooperate, until you retaliate</li>\n<li>“concedes when punished” and cooperates after a defect/defect result</li>\n<li>“retaliates against unprovoked aggression”, defecting if you defect on it while it cooperates.</li>\n</ul>\n<p>If there’s any randomness, Pavlov is better at cooperating with itself than Tit-For-Tat. One accidental defection and two Tit-For-Tats are stuck in an eternal defect cycle, while Pavlov’s forgive each other and wind up back in a cooperate/cooperate pattern.</p>\n<p>Moreover, Pavlov can exploit CooperateBot (if it defects by accident, it will keep greedily defecting against the hapless CooperateBot, while Tit-For-Tat will not) but still exerts <em>some </em>pressure against DefectBot (defecting against it half the time, compared to Tit-For-Tat’s consistent defection.)</p>\n<p>The interesting thing is that Pavlov can beat Tit-For-Tat <em>or </em>Tit-for-Tat-with-Forgiveness in a wide variety of scenarios.</p>\n<p>If there are only Pavlov and Tit-For-Tat bots, Tit-For-Tat has to start out outnumbering Pavlov quite significantly in order to win. The same is true for a population of Pavlov and Tit-For-Tat-With-Forgiveness.  It doesn’t change if we add in some Cooperators or Defectors either.</p>\n<p>Why?</p>\n<p>Compared to Tit-For-Tat, Pavlov cooperates better with itself.  If two Tit-For-Tat bots are paired, and one of them accidentally defects, they’ll be stuck in a mutual defection equilibrium.  However, if one Pavlov bot accidentally defects against its clone, we’ll see</p>\n<p>C/D -&gt; D/D -&gt; C-&gt;C</p>\n<p>which recovers a mutual-cooperation equilibrium and picks up more points.</p>\n<p>Compared to Tit-For-Tat-With-Forgiveness, Pavlov cooperates *worse* with itself (it takes longer to recover from mistakes) but it “exploits” TFTWF’s patience better. If Pavlov accidentally defects against TFTWF, the result is</p>\n<p>D/C -&gt; D/C -&gt; D/D -&gt; C/D -&gt; D/D -&gt; C/C,</p>\n<p>which leaves Pavlov with a net gain of 1 point per turn, (over the first five turns before a cooperative equilibrium) compared to TFTWF’s 1/5 point per turn.</p>\n<p>If TFTWF accidentally defects against Pavlov, the result is</p>\n<p>C/D -&gt; D/C -&gt; D/C -&gt; D/D -&gt; C/D</p>\n<p>which cycles eternally (until the next mistake), getting Pavlov an average of 5/4 points per turn, compared to TFTWF’s 1 point per turn.</p>\n<p>Either way, Pavlov eventually overtakes TFTWF.</p>\n<p>If you add enough DefectBots to a mix of Pavlovs and TFT’s (and it has to be a large majority of the total population being DefectBots) TFT can win, because it’s more resistant against DefectBots than Pavlov is.  Pavlov cooperates with DefectBots half the time; TFT never does except by mistake.</p>\n<p>Pavlov isn’t perfect, but it performs well enough to hold its own in a variety of circumstances.  An adapted version of Pavlov won the <a href=\"https://bib.irb.hr/datoteka/583494.1818-A-Review-of-Iterated-Prisoner-s-Dilemma-Strategies-v1_2.pdf\">2005 iterated game theory tournament</a>.</p>\n<p>Why, then, don’t we actually talk about it, the way we talk about Tit-For-Tat?  If it’s true that moral maxims like the Golden Rule emerge out of the fact that Tit-For-Tat is an effective strategy, why aren’t there moral maxims that exemplify the Pavlov strategy?  Why haven’t I even <em>heard </em>of Pavlov until now, despite having taken a game theory course once, when <em>everybody </em>has heard of Tit-For-Tat and has an intuitive feeling for how it works?</p>\n<p>In Wedekind and Milinski’s 1996 <a href=\"https://www.pnas.org/content/pnas/93/7/2686.full.pdf\">experiment</a> with human subjects, playing an iterated prisoner’s dilemma game, a full 70% of them engaged in Pavlov-like strategies.  The human Pavlovians were smarter than a pure Pavlov strategy — they eventually recognized the DefectBots and stopped cooperating with them, while a pure-Pavlov strategy never would — but, just like Pavlov, the humans kept “pushing boundaries” when unopposed.</p>\n<p>Moreover, humans basically divided themselves into Pavlovians and Tit-For-Tat-ers; they didn’t switch strategies between game conditions where one strategy or another was superior, but just played the same way each time.</p>\n<p>In other words, it seems fairly likely not only that Pavlov performs well in computer simulations, but that humans <em>do </em>have some intuitive model of Pavlov.</p>\n<p>Human players are <a href=\"https://www.pnas.org/content/pnas/95/23/13755.full.pdf\">more likely</a> to use generous Tit-For-Tat strategies rather than Pavlov when they have to play a working-memory game at the same time as they’re playing iterated Prisoner’s Dilemma.  In other words, Pavlov is probably more costly in working memory than generous Tit for Tat.</p>\n<p>If you look at all 16 theoretically possible strategies that only have memory of the previous round, and let them evolve, evolutionary dynamics can wind up quite <a href=\"https://www.pnas.org/content/pnas/90/11/5091.full.pdf\">complex and oscillatory.</a></p>\n<p>A population of TFT players will be invaded by more “forgiving” strategies like Pavlov, who in turn can be invaded by DefectBot and other uncooperative strategies, which again can be invaded by TFT, which thrives in high-defection environments.  If you track the overall rate of cooperation over time, you get very regular oscillations, though these are quite sensitive to variation in the error and mutation rates and nonperiodic (chaotic) behavior can occur in some regimes.</p>\n<p>This is strangely reminiscent of Peter Turchin’s theory of <a href=\"http://peterturchin.com/secular-cycles/\">secular cycles</a> in history.  Periods of peace and prosperity alternate with periods of conflict and poverty; empires rise and fall.  Periods of low cooperation happen at the fall of an empire/state/civilization; this enables new empires to rise when a subgroup has better ability to <a href=\"http://peterturchin.com/cliodynamica/the-dune-hypothesis/\">cooperate with itself and fight off its enemies</a> than the surrounding warring peoples; but in peacetime, at the height of an empire, more forgiving and exploitative strategies like Pavlov can emerge, which themselves are vulnerable to the barbaric defectors.  This is a vastly simplified story compared to the actual mathematical dynamics <em>or </em>the actual history, of course, but it’s an illustrative gist.</p>\n<p>The big takeaway from learning about evolutionary game theory is that it’s genuinely complicated from a <a href=\"https://srconstantin.wordpress.com/2018/12/14/player-vs-character-a-two-level-model-of-ethics/\">player-perspective.</a></p>\n<p>“It’s complicated” sometimes functions as a curiosity-stopper; you conclude “more research is needed” instead of looking at the data you have and drawing preliminary conclusions, if you want to protect your intellectual “territory” without putting yourself out of a job.</p>\n<p>That isn’t the kind of “complexity” I’m talking about here.  Chaos in dynamical systems has a specific meaning: the system is so sensitive to initial conditions that even a small measurement error in determining where it starts means you cannot even approximately predict where it will end up.</p>\n<p><a href=\"http://Chaos: When the present determines the future, but the approximate present does not approximately determine the future.\">“Chaos: When the present determines the future, but the approximate present does not approximately determine the future.”</a></p>\n<p>Optimal strategy depends sensitively on who else is in the population, how many errors you make, and how likely strategies are to change (or enter or leave).  There are a lot of moving parts here.</p>",
    "user": {
      "username": "sarahconstantin",
      "slug": "sarahconstantin",
      "displayName": "sarahconstantin"
    }
  },
  {
    "_id": "Xt22Pqut4c6SAdWo2",
    "title": "What self-help has helped you?",
    "slug": "what-self-help-has-helped-you",
    "pageUrl": "https://www.lesswrong.com/posts/Xt22Pqut4c6SAdWo2/what-self-help-has-helped-you",
    "postedAt": "2018-12-20T03:31:52.497Z",
    "baseScore": 39,
    "voteCount": 18,
    "commentCount": 22,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>Simply, what self-help techniques have you tried and found to help you? Bonus points if you can say something about the context in which you did the self-help and any speculations you have about mechanism of action.</p><p>Please give one technique per answer, multiple answers accepted per person (at least, I hope LW allows that, but I&#x27;m guessing it will), please read existing answer to avoid duplication, and comment on existing answers about a technique if you have more to say about that technique.</p><p>I&#x27;ll put a couple of my own answers in below as examples and because I have answers to my own question!</p>",
    "user": {
      "username": "gworley",
      "slug": "gordon-seidoh-worley",
      "displayName": "Gordon Seidoh Worley"
    }
  },
  {
    "_id": "jPxcAjapd7w43tdoc",
    "title": "Defining Freedom",
    "slug": "defining-freedom",
    "pageUrl": "https://www.lesswrong.com/posts/jPxcAjapd7w43tdoc/defining-freedom",
    "postedAt": "2018-12-20T02:41:38.865Z",
    "baseScore": 8,
    "voteCount": 8,
    "commentCount": 7,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p> </p><p>I’ve always found the concept of Freedom confusing.</p><p>There’s a level on which it makes sense. When William Wallace is talking about being free from the yoke of Edward Longshanks, it’s obvious what he means – the English king and his goons regularly come to town, order people around, take their stuff and beat them up. Being free of English rule means they don’t do that anymore, and William Wallace and his friends can just go around farming and living their peaceful lives without having to worry about anything worse than the soul-crushing depression of living in Scotland.</p><p>On the other hand, what the heck does the word free mean in the context of “the land of the free and the home of the brave”? Is there even a reasonable definition? The original song is about being free from English rule, but it’s been two and a half  centuries since Edward Longshanks and his goons were ruling America, and not-being-ruled-by-the-King isn’t really a core property of most Americans’ identities these days.</p><p>I settle the first one by thinking of freedom as defined relative to a constraint. You can be free of something if you don’t have to worry about it when making decisions. This matches the common use of freedom – you’re free of having to worry about parking if you don’t have a car, free of a tyrant if you aren’t constrained by him telling you what to do, etc. This also explains why the second use seems so weird – it’s trying to use a fundamentally relative term without using it in relation to anything. So my response to the second use used to be to roll my eyes at people throw around empty deep-sounding terms.</p><p>But now I think we can resolve this. We start with the mathematical definition of degrees of freedom – your total freedom level at a given time is the number of options you have available to you at that time (if you want to sound all mathy, you can call this the local dimension of your options space or something)[1].</p><p>But there’s a fundamental problem here: At the end of the day you’ll only pick one of the options you had, because you can only pick one – once you eliminate the big constraints, you’ll just be subject to smaller and smaller constraints until you’re down to one option. Even if there’s no law forcing you to wear black socks, you’ll end up wearing black socks by the constraint that they were a dollar cheaper on Amazon and you were too lazy to scroll down. If you actually incorporate every single constraint you have, you end up having one action. Can we solve this using more math?</p><p>Yes. Let <em>f(x)</em> be the utility function on the space of possible positions you can be in. In this definition, the choice you make in a given position is simply to take a step in whichever direction increases <em>f(x)</em>the most. We generally think of constraints as cliffs in the utility function – if you disobey the tyrant he’ll probably kill you, so the “disobey tyrant” direction of decision space has a massive drop in <em>f(x)</em>. Smaller constraints, like “white socks are a dollar more and involve scrolling down on Amazon”  are only minor dips in <em>f(x)</em>.</p><p>In this case, we can think of absolute freedom as a measure of flatness of <em>f</em> – Some measure of how many directions you can go without falling down too big a cliff. There are a lot of ways to formally measure this, but the idea is that this should be a value that goes down more by wider or steeper cliffs (although steepness has diminishing returns – the difference between a specific option getting you badly injured or killing you is fairly minor if you can easily just not take that option). In a sense, this is just a measure of resilience – how many non-terrible routes do you have? Because your position and options are constantly changing, there’s a lot of value in having multiple non-terrible paths.</p><p>Finally, note that under this definition freedom definitely isn’t all we want – we also like having opportunities to massively increase our utility (instead of just saving ourselves from decreasing it too much). We can think of Welfare vs. Freedom as increasing our EV vs. reducing our variance.</p><p></p><p>[1] There’s an issue here with how to count/measure options – e.g. “go to the ball/don’t go to the ball” is clearly a freer choice than “go to the ball wearing white socks/go to the ball wearing black socks” – but like most measure issues you can mostly just ignore it and use your intuition.</p>",
    "user": {
      "username": "pku",
      "slug": "pku",
      "displayName": "pku"
    }
  },
  {
    "_id": "hSw4MNTc3gAwZWdx9",
    "title": "Reasons compute may not drive AI capabilities growth",
    "slug": "reasons-compute-may-not-drive-ai-capabilities-growth",
    "pageUrl": "https://www.lesswrong.com/posts/hSw4MNTc3gAwZWdx9/reasons-compute-may-not-drive-ai-capabilities-growth",
    "postedAt": "2018-12-19T22:13:34.474Z",
    "baseScore": 42,
    "voteCount": 17,
    "commentCount": 10,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p>How long it will be before humanity is capable of creating general AI is an important factor in discussions of the importance of doing AI alignment research as well as discussions of which research avenues have the best chance of success. One frequently discussed model for estimating AI timelines is that AI capabilities progress is essentially driven by growing compute capabilities. For example, <a href=\"https://blog.openai.com/ai-and-compute/\">the OpenAI article on AI and Compute</a> presents a compelling narrative, which shows a trend of well-known results in machine learning using exponentially more compute over time. This is an interesting model because if valid we can do some quantitative forecasting, due to somewhat smooth trends in compute metrics which can be extrapolated. However, I think there are a number of reasons to suspect AI progress to be driven more by engineer and researcher effort than compute.</p>\n<p>I think there's a spectrum of models between:</p>\n<ul>\n<li>We have an abundance of ideas that aren't worth the investment to try out yet. Advances in compute capability unlock progress by make researching more expensive techniques economically feasible. We'll be able to create general AI soon after we have enough compute to do it.</li>\n<li>Research proceeds at its own pace and makes use of as much compute is convenient to save researcher time on optimization and achieve flashy results. We'll be able to create general AI once we come up with all the right ideas behind it, and either:\n<ul>\n<li>We'll already have enough compute to do it</li>\n<li>We won't have enough compute and we'll start optimizing, invest more in compute, and possibly start truly being bottlenecked on compute progress.</li>\n</ul>\n</li>\n</ul>\n<p>My research hasn't pointed too solidly in either direction, but below I discuss a number of the reasons I've thought of that might point towards compute not being a significant driver of progress right now.</p>\n<h2>There's many ways to train more efficiently that aren't widely used</h2>\n<p>Starting October of 2017, the Stanford DAWNBench contest challenged teams to come up with the fastest and cheapest ways to train neural nets to solve certain tasks.</p>\n<p>The most interesting was the <a href=\"https://dawn.cs.stanford.edu/benchmark/ImageNet/train.html\">ImageNet training time contest</a>. The baseline entry took 10 days and cost $1112; less than one year later the best entries (all by the <a href=\"https://fast.ai/\">fast.ai</a> team) were down to 18 minutes for $35, 19 minutes for $18 or 30 minutes for $14[^1]. This is ~800x faster and ~80x cheaper than the baseline.</p>\n<p>Some of this was just using more and better hardware, the winning team used 128 V100 GPUs for 18 minutes and 64 for 19 minutes, versus eight K80 GPUs for the baseline. However, substantial improvements were made even on the same hardware. The training time on a <code>p3.16xlarge</code> AWS instance with eight V100 GPUs went down from 15 hours to 3 hours in 4 months. The training time on a single Google Cloud TPU went down from 12 hours to 3 hours as the Google Brain team tuned their training and incorporated ideas from the fast.ai team. An even larger improvement was seen on <a href=\"https://dawn.cs.stanford.edu/benchmark/CIFAR10/train.html\">the CIFAR10 contest</a> recently, with times on a <code>p3.2xlarge</code> improving by 60x with <a href=\"https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet/\">the accompanying blog series</a> still mentioning multiple improvements left on the table due to effort constraints. He also speculates that many of the optimizations would also improve the ImageNet version.</p>\n<p>The main <a href=\"https://www.fast.ai/2018/04/30/dawnbench-fastai/\">techniques</a> <a href=\"http://www.fast.ai/2018/08/10/fastai-diu-imagenet/\">used</a> for fast training were all known techniques: progressive resizing, mixed precision training, removing weight decay from batchnorms, scaling up batch size in the middle of training, and gradually warming up the learning rate. They just required engineering effort to implement and weren't already implemented in the library defaults.</p>\n<p>Similarly, the improvement due to scaling from eight K80s to many machines with V100s was partially hardware but also required lots of engineering effort to implement: using mixed precision fp16 training (required to take advantage of the V100 Tensor Cores), efficiently using the network to transfer data, implementing the techniques required for large batch sizes, and writing software for supervising clusters of AWS spot instances.</p>\n<p>These results seem to show that it's possible to train much faster and cheaper by applying knowledge and sufficient engineering effort. Interestingly not even a team at Google Brain working to show off TPUs initially had all the code and knowledge required to get the best available performance, and had to gradually work for it.</p>\n<p>I would suspect that in a world where we were bottlenecked hard on training times that these techniques would be more widely known about and applied, and implementations of them readily available for every major machine learning library. Interestingly, in postscripts to both of <a href=\"https://www.fast.ai/2018/04/30/dawnbench-fastai/\">his</a> <a href=\"http://www.fast.ai/2018/08/10/fastai-diu-imagenet/\">articles</a> on how fast.ai managed to achieve such fast times, Jeremy Howard notes that he doesn't believe large amounts of compute are required for important ML research, and notes that many foundational discoveries were available with little compute.</p>\n<p>[^1]: Using spot/preemptible instance pricing instead of the on-demand pricing the benchmark page lists, due to much lower prices and the lack of need for on-demand instances given the short time. The authors of the winning solution <a href=\"https://www.fast.ai/2018/04/30/dawnbench-fastai/\">wrote software to effectively use spot instances</a> and actually used them for their tests. It may seem unfair to use spot prices for the winning solution but not for the baseline, but a lot of the improvement in the contest came from actually using all the techniques for faster/cheaper training available despite inconvenience, and they had to write software to easily use spot instances and had short enough training times that it was viable without fancy software to automatically transfer training to new machines.</p>\n<h2>Hyperparameter grid searches are inefficient</h2>\n<p>I've heard hyperparameter grid searches mentioned as a reason why ML research needs way more compute than it would appear based on the training time of the models used. However, I can also see the use of grid searches as evidence of an abundance of compute rather than a scarcity.</p>\n<p>As far as I can tell it's possible to find hyperparameters much more efficiently than a grid search, it just takes more human time and engineering implementation effort. There's a large literature of <a href=\"https://www.automl.org/blog_bohb/\">more efficient hyperparameter search methods</a> but as far as I can tell they aren't very popular (I've never heard of anyone using one in practice, and all open source implementations of these kind of things I can find have few Github stars).</p>\n<p>Researcher <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Smith%2C+L+N\">Leslie Smith</a> also has a number of papers with little-used ideas on principled approaches to choosing and searching for optimal hyperparameters with much less effort, including a <a href=\"https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate\">fast automatic procedure for finding optimal learning rates</a>. This suggests that it's possible to substitute hyperparameter search time for more engineering, human decision-making and research effort.</p>\n<p>There's also likely room for improvement in the factorization of the hyper-parameters we use so that they're more amenable to separate optimization. For example, L2 regularization is usually used in place of weight decay because they theoretically do the same thing, but <a href=\"https://arxiv.org/pdf/1711.05101.pdf\">this paper</a> points out that not only do they not do the same thing with ADAM and using weight decay causes ADAM to surpass the more popular SGD with momentum in practice, but that weight decay is a better hyper-parameter since the optimal weight decay is more independent of learning rate than L2 regularization strength is.</p>\n<p>All of this suggests that most researchers might be operating under an abundance of cheap compute relative to their problems that leads to them not investing the effort required to more efficiently optimize their hyperparameters and just do so haphazardly or with grid searches instead.</p>\n<h2>The types of compute we need may not improve very quickly</h2>\n<p>Improvements in computing hardware are not uniform and there are many different hardware attributes that can be bottlenecks for different things. AI progress may rely on one or more of these that don't end up improving quickly, becoming bottlenecked on the slowest one rather than experiencing exponential growth.</p>\n<h3>Machine learning accelerators</h3>\n<p>Modern machine learning is largely composed of large operations that are either directly matrix multiplies or can be decomposed into them. It's also possible to train using much lower precision than full 32-bit floating point using some tricks. This allows the creation of specialized training hardware like Google's TPUs and Nvidia Tensor Cores. A number of other companies have also announced they're working on custom accelerators.</p>\n<p>The first generation of specialized hardware delivered a large one-time improvement, but we can also expect continuing innovation in accelerator architecture. There will likely be sustained innovations in training with <a href=\"https://code.fb.com/ai-research/floating-point-math/\">different number formats</a> and architectural optimizations for faster and cheaper training. I expect this will be the area our compute capability will grow the most, but may flatten like CPUs have once we figure out enough of the easily discoverable improvements.</p>\n<h3>CPUs</h3>\n<p>Reinforcement learning simulations like the OpenAI Five DOTA bot, and various physics playgrounds, often use CPU-heavy serial simulations. OpenAI Five uses 128,000 CPU cores and only 256 GPUs. At current Google Cloud preemptible prices the CPUs cost 5-10x more than the GPUs in total. Improvements in machine learning training ability will still leave the large cost of the CPUs. If the use of expensive simulations that run best on CPUs becomes an important part of training advanced agents, progress may become bottlenecked on CPU cost.</p>\n<p>Additionally, improvement in CPU compute costs may be slowing. <a href=\"http://blog.zorangagic.com/2017/07/aws-ec2-historical-pricing.html\">Cloud CPU costs only decreased 45% from 2012 to 2017 and performance per dollar for buying the hardware only improved 2x.</a>. Google Cloud Compute prices have <a href=\"https://www.kapwing.com/blog/cloud-costs-arent-actually-dropping-dramatically/\">only dropped 25% from 2014-2018</a>. Although the introduction of preemptible prices 30% of full price in 2016 was a big improvement, and that decreased to 20% of full price in 2017.</p>\n<h3>GPU/accelerator memory</h3>\n<p>Another scarce resource is memory on the GPU/accelerator used for training. The memory must be large enough to store all the model parameters, the input, the gradients, and other optimization parameters.</p>\n<p>This is one of the most frequent limits I see referenced in machine learning papers nowadays. For example the new large BERT language model <a href=\"https://github.com/google-research/bert#out-of-memory-issues\">can only be trained properly on TPUs</a> with their 64GB of RAM. The <a href=\"https://blog.openai.com/glow/\">Glow</a> paper needs to use gradient checkpointing and an alternative to batchnorm so that they can use gradient accumulation, because only a single sample of gradients fits on a GPU.</p>\n<p>However there are <a href=\"https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255\">ways to address this limitation</a> that aren't frequently used. Glow already uses the two best ones, gradient checkpointing and gradient accumulation, but did not implement an optimization they mentioned which would make the amount of memory the model takes constant in the number of layers instead of linear, likely because it would be difficult to engineer into existing ML frameworks. The BERT implementation uses none of the techniques because they just use a TPU with enough memory, in fact <a href=\"https://github.com/huggingface/pytorch-pretrained-BERT\">a reimplementation of BERT</a> implemented 3 such techniques and got it to fit on a GPU. Thus it still seems that in a world with less RAM these might still have happened, just with more difficulty or smaller demonstration models.</p>\n<p>Interestingly, the maximum available RAM per device barely changed from 2014 through 2017 with the NVIDIA K80's 24GB, but then shot up in 2018 to 48GB with the RTX 8000 as well as the 64GB TPU v2 and 128GB TPU v3. Probably both because of demand for larger device memories for machine learning training, as well as the availability of high capacity HBM memory. It's unclear to me if this rapid rise will continue or if it was mostly a one-time change reflecting new demands for the largest possible memories reaching the market.</p>\n<p>It's also possible that per-device memory will cease to be a constraint on model size due to faster hardware interconnects that allow sharing a model across the memory of multiple devices like <a href=\"https://ai.intel.com/nervana-nnp/\">Intel's Nervana</a> and <a href=\"https://arxiv.org/abs/1811.02084v1\">Tensorflow Mesh</a> plan to do. It also seems likely that techniques for splitting models across devices to fit in memory, like the original AlexNet did, will become more popular. It may be the case that the fact that we don't split models across devices like AlexNet anymore is evidence that we're not constrained by RAM much but I'm not sure.</p>\n<h2>Limited ability to exploit parallelism</h2>\n<p>As discussed extensively in <a href=\"https://arxiv.org/pdf/1811.03600.pdf\">a new paper from Google Brain</a>, there seems to be a limit on how much data parallelism in the form of larger batch sizes we can currently extract out of a given model. If this constraint isn't worked around, wall time to train models could stall even if compute power continues to grow.</p>\n<p>However the paper mentions that various things like model architecture and regularization affect this limit and I think it's pretty likely that techniques to increase this limit will continue to be discovered so it isn't a bottleneck. A <a href=\"https://blog.openai.com/science-of-ai/\">newer paper by OpenAI</a> finds that more difficult problems also tolerate larger batch sizes. Even if the limit remains, increasing compute would allow training more different models in parallel, potentially just meaning that more parameter search and evolution gets layered on top of the training. I also suspect that just using ever-larger models may allow use of more compute without increasing batch sizes.</p>\n<p>At the moment, it seems that we know how to train effectively with batch sizes large enough to saturate large clusters, for example <a href=\"https://arxiv.org/abs/1807.11205\">this paper about training ImageNet in 7 minutes with a 64k batch size</a>. But this requires extra tuning and implementing some tricks, <a href=\"http://www.fast.ai/2018/08/10/fastai-diu-imagenet/\">even just to train on mid-size clusters</a>, so as far as I know only a small fraction of all machine learning researchers regularly train on large clusters (anecdotally, I'm uncertain about this).</p>\n<h2>Conclusion</h2>\n<p>These all seem to point towards compute being abundant and ideas being the bottleneck, but not solidly. For the points about training efficiency and grid searches this could just be an inefficiency in ML research and all the major AGI progress will be made by a few well-funded teams at the boundaries of modern compute that have solved these problems internally.</p>\n<p><a href=\"https://www.lesswrong.com/users/vaniver\">Vaniver</a> commented on a draft of this post that it's interesting to consider the case where training time is the bottleneck rather than ideas, but massive engineering effort is highly effective at reducing training time. In this case an increase in investment in AI research which lead to hiring more engineers to apply techniques to speed up training could lead to rapid progress. This world might also lead to more sizable differences in capabilities between organizations, if large somewhat serial software engineering investments are required to make use of the most powerful techniques, rather than a well-funded newcomer being able to just read papers and buy all the necessary hardware.</p>\n<p>The course of various compute hardware attributes seems uncertain both in terms of how fast they'll progress and whether or not we'll need to rely on anything other than special-purpose accelerator speed. Since the problem is complex with many unknowns, I'm still highly uncertain, but all of these points did move me to varying degrees in the direction of continuing compute growth not being a driver of dramatic progress.</p>\n<p><em>Thanks to <a href=\"https://www.lesswrong.com/users/vaniver\">Vaniver</a> and <a href=\"http://shlegeris.com/\">Buck Shlegeris</a> for discussions that lead to some of the thoughts in this post.</em></p>\n</body></html>",
    "user": {
      "username": "Kythe",
      "slug": "tristan-h",
      "displayName": "Tristan H"
    }
  },
  {
    "_id": "PgsxXNSDsyz4DFEuw",
    "title": "Anthropic paradoxes transposed into Anthropic Decision Theory",
    "slug": "anthropic-paradoxes-transposed-into-anthropic-decision",
    "pageUrl": "https://www.lesswrong.com/posts/PgsxXNSDsyz4DFEuw/anthropic-paradoxes-transposed-into-anthropic-decision",
    "postedAt": "2018-12-19T18:07:42.251Z",
    "baseScore": 19,
    "voteCount": 11,
    "commentCount": 23,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head><style type=\"text/css\">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></head><body><p><a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Anthropic_Decision_Theory_Tech_Report.pdf\">Anthropic Decision Theory</a> (ADT) replaces anthropic probabilities (<a href=\"https://en.wikipedia.org/wiki/Self-indication_assumption\">SIA</a> and <a href=\"https://en.wikipedia.org/wiki/Self-sampling_assumption\">SSA</a>) with a decision theory that doesn't need anthropic probabilities to function. And, roughly speaking, ADT shows that total utilitarians will have a behaviour that looks as if it was using SIA, while average utilitarians look like they are using SSA.</p>\n<p>That means that the various paradoxes of SIA and SSA can be translated into ADT format. This post will do that, and show how the paradoxes feel a lot less counter-intuitive under ADT. Some of these have been presented before, but I wanted to gather them in one location. The paradoxes examined are:</p>\n<ol>\n<li>The <a href=\"https://en.wikipedia.org/wiki/Doomsday_argument\">Doomsday Argument</a>.</li>\n<li>The <a href=\"http://www.anthropic-principle.com/preprints/mys/mysteries.pdf\">Adam and Eve</a> problem.</li>\n<li>The <a href=\"http://www.anthropic-principle.com/preprints/cau/paradoxes.html\">UN++</a> problem.</li>\n<li>The <a href=\"http://www.anthropic-principle.com/preprints/mys/mysteries.pdf\">Presumptuous Philosopher</a>.</li>\n<li>Katja Grace's  <a href=\"https://meteuphoric.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">SIA doomsday argument</a>.</li>\n</ol>\n<p>The first three are are paradoxes of SSA (which increases the probability of \"small\" universes with few observers), while the last three are paradoxes of SIA (which increases the probability of \"large\" universes with many observers).</p>\n<h2>No Doomsday, just a different weighting of rewards</h2>\n<p>The famous Doomsday Argument claims that, because of SSA's preferences for small numbers of observers, the end of the human species is closer than we might otherwise think.</p>\n<p>How can we translate that into ADT? I've found it's generally harder to translate SSA paradoxes into ADT that SIA ones, because average utilitarianism is a bit more finicky to work with.</p>\n<p>But here is a possible formulation: a disaster may happen 10 years from now, with 50% probability, and will end humanity with a total of <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\omega\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ω</span></span></span></span></span></span> humans. If humans survive the disaster, there will be <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Omega\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span></span></span></span></span> humans total.</p>\n<p>The agent has the option of consuming <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span> resources now, or consuming <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Y\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span></span></span></span></span> resources in 20 years time. If this were a narrow-minded selfish agent, then it will consume early if <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X>Y/2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&gt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span>, and late if <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X<Y/2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span>.</p>\n<p>However, if the agent is an average utilitarian, the amount of expected utility they derive from from consuming early is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(X/\\omega + X/\\Omega)/2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ω</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span> (the expected average utility of <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span>, averaged over survival and doom), while the expected utility for consuming late is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(Y/\\Omega)/2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span> (since consuming late means survival).</p>\n<p>This means that the breakeven point for the ADT average utilitarian is when:</p>\n<ul>\n<li><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Y=X(1+\\Omega/\\omega)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ω</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>.</li>\n</ul>\n<p>If <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Omega\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span></span></span></span></span> is much larger than <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\omega\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ω</span></span></span></span></span></span>, then the ADT agent will only delay consumption if <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Y\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span></span></span></span></span> is similarly larger than <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span>.</p>\n<p>This <em>looks like</em> a narrow-minded selfish agent that is convinced that doom is almost certain. But it's only because of the weird features of average utilitarianism.</p>\n<h2>Adam and Eve and differentially pleasurable sex and pregnancy</h2>\n<p>In the Adam and Eve thought experiment, the pair of humans want to sleep together, but don't want to get pregnant. The snake reassures them that because a pregnancy would lead to billions of descendants, SSA's preferences for small universes means that this is almost impossibly unlikely, so, time to get frisky.</p>\n<p>There are two utilities to compare here: the positive utility of sex (<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"+S\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span></span></span></span>), and the negative utility of pregnancy (<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"-P\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span></span></span></span>). Assume a <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"50\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">50</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span></span></span> chance of pregnancy from having sex, and a subsequent <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Omega\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span></span></span></span></span> descendants.</p>\n<p>Given an average utilitarian ADT couple, the utility derived from sex is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(S/2 + S/(2+\\Omega))/2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span>, while the disutility from pregnancy is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"-P/(2+\\Omega)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>. For large enough <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Omega\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span></span></span></span></span>, those terms will be approximately <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S/4\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">4</span></span></span></span></span></span> and <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span>.</p>\n<p>So the disutility of pregnancy is buried in the much larger population.</p>\n<p>There are more extreme versions of the Adam and Eve problem, but they are closely related to the next paradox.</p>\n<h2>UN++: more people to dilute the sorrow</h2>\n<p>In the UN++ thought experiment, a future world government seeks to prevent damaging but non-fatal gamma ray bursts by committing to creating many many more humans, if the bursts happen. The paradox is that SSA implies that this should lower the probability of the bursts.</p>\n<p>In ADT, this behaviour is perfectly rational: if we assume that the gamma ray-bursts will cause pain to the current population, then creating a lot of new humans (of same baseline happiness) will dilute this pain, by averaging it out over a larger population.</p>\n<p>So in ADT, the SSA paradoxes just seem to be artefacts of the weirdness of average utilitarianism.</p>\n<h2>Philosopher: not presumptuous, but gambling for high rewards</h2>\n<p>We turn now to SIA, replacing our average utilitarian ADT agent with a total utilitarian one.</p>\n<p>In the Presumptuous Philosopher thought experiment, there are only two possible theories about the universe: <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span> and <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span>. Both posit large universes, but <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span> posits a much larger universe than <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span>, with trillions of times more observers.</p>\n<p>Physicists are about to do an experiment to see which theory is true, but the SIA-using Presumptuous Philosopher (PP) interrupts them, saying that <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span> is almost certain because of SIA. Indeed, they are willing to bet on <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span> at odds of up to a trillion-to-one.</p>\n<p>With that betting idea, the problem is quite easy to formulate in ADT. Assume that all PP are total utilitarians towards each other, and will all reach the same decision. Then there are a trillion times more PPs in <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span> than in <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span>. Which means that winning a bet in <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span> is a trillion times more valuable than winning it in <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span>.</p>\n<p>Thus, under ADT, the Presumptuous Philosopher will indeed bet on <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"T_2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span></span></span> at odds of up to a trillion to one, but the behaviour is simple to explain: they are simply going for a low-probability, high-utility bet with higher expected utility than the opposite. There does not seem to be any paradox remaining.</p>\n<h2>SIA Doomsday: care more about mosquito nets in large universes</h2>\n<p>Back to SIA. The SIA Doomsday Argument, somewhat simplified, is since SIA means that we should expect there to be a lot of observers like ourselves, then it is more likely that the <a href=\"https://en.wikipedia.org/wiki/Fermi_paradox\">Fermi paradox</a> is explained by a late <a href=\"https://en.wikipedia.org/wiki/Great_Filter\">Great Filter</a> (which kills civilizations that are <em>more</em> advanced than us) than a early Great Filter (which kills life at an earlier stage or stops it from evolving in the first place). The reason for this is that, obviously, there are more observers like us for a late Great Filter than an early one.</p>\n<p>To analyse this in decision theory, use the same setup as for the standard Doomsday Argument: choosing between consuming <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span> now (or donating <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span></span></span></span> to <a href=\"https://www.againstmalaria.com/\">AMF</a>, or similar), or <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Y\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span></span></span></span></span> in twenty years, with a risk of human extinction in ten years.</p>\n<p>To complete the model, assume that if the Great Filter is early, there will be no human extinction, while if it is late, there is a <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"99\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">99</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span></span></span> chance of extinction. If the Great Filter is late, there are <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Omega\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span></span></span></span></span> advanced civilizations across the universe, while if it is early, there are only <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\omega < \\Omega\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ω</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span></span></span></span></span>. Assume that the agent currently estimates late-vs-early Great Filters as 50-50.</p>\n<p>With the usual ADT agent assuming that all their almost-copies reach the same decision in every civilization, the utility from early consumption is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\omega X/2 + \\Omega X/2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ω</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span> (total utility averaged over late vs early Great Filters), while the utility from late consumption is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\omega Y/2 + \\Omega Y/(2 \\times 100)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ω</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">100</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>.</p>\n<p>So a total utilitarian ADT agent will be more likely to go for early consumption than the objective odds would imply. And the more devastating the late Great Filter, the stronger this effect.</p>\n<p>For large <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Omega\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span></span></span></span></span>, these approximate to <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Omega X/2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span></span> and <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Omega Y(2\\times 100)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Ω</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">×</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">100</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>.</p>\n</body></html>",
    "user": {
      "username": "Stuart_Armstrong",
      "slug": "stuart_armstrong",
      "displayName": "Stuart_Armstrong"
    }
  },
  {
    "_id": "DLG9Mk5WYTnJfGPJi",
    "title": "Games in Kocherga club: Fallacymania, Tower of Chaos, Scientific Discovery",
    "slug": "games-in-kocherga-club-fallacymania-tower-of-chaos",
    "pageUrl": "https://www.lesswrong.com/events/DLG9Mk5WYTnJfGPJi/games-in-kocherga-club-fallacymania-tower-of-chaos",
    "postedAt": "2018-12-18T22:12:07.533Z",
    "baseScore": 2,
    "voteCount": 1,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Welcome to Moscow LW community makeshift games! In that games, some rationality skills are involved, so you can practise while you playing!<br/>* Fallacymania: it is a game where you guess logical fallacies in arguments, or practise using logical fallacies yourself (depending on team in which you will be).<br/>* Tower of Chaos: funny game with guessing the rules of human placement on a Twister mat.<br/>* Scientific Discovery: modified version of Zendo with simultaneous turns for all players.<br/>Details about the games: <a href=\"https://bit.ly/2J2T5o8\">https://bit.ly/2J2T5o8</a><br/>Come to antikafe &quot;Kocherga&quot;, ul.B.Dorogomilovskaya, 5-2. The map is here: <a href=\"https://kocherga-club.ru/#contacts\">https://kocherga-club.ru/#contacts</a><br/>Games begin at 19:40, the length is 3 hours.</p>",
    "user": {
      "username": "Alexander230",
      "slug": "alexander230",
      "displayName": "Alexander230"
    }
  },
  {
    "_id": "adGXMdYuTdj7EojHe",
    "title": "Games in Kocherga club: Fallacymania, Tower of Chaos, Scientific Discovery",
    "slug": "games-in-kocherga-club-fallacymania-tower-of-chaos",
    "pageUrl": "https://www.lesswrong.com/events/adGXMdYuTdj7EojHe/games-in-kocherga-club-fallacymania-tower-of-chaos",
    "postedAt": "2018-12-18T22:10:23.666Z",
    "baseScore": 2,
    "voteCount": 1,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Welcome to Moscow LW community makeshift games! In that games, some rationality skills are involved, so you can practise while you playing!<br/>* Fallacymania: it is a game where you guess logical fallacies in arguments, or practise using logical fallacies yourself (depending on team in which you will be).<br/>* Tower of Chaos: funny game with guessing the rules of human placement on a Twister mat.<br/>* Scientific Discovery: modified version of Zendo with simultaneous turns for all players.<br/>Details about the games: <a href=\"https://bit.ly/2J2T5o8\">https://bit.ly/2J2T5o8</a><br/>Come to antikafe &quot;Kocherga&quot;, ul.B.Dorogomilovskaya, 5-2. The map is here: <a href=\"https://kocherga-club.ru/#contacts\">https://kocherga-club.ru/#contacts</a><br/>Games begin at 19:40, the length is 3 hours.</p>",
    "user": {
      "username": "Alexander230",
      "slug": "alexander230",
      "displayName": "Alexander230"
    }
  },
  {
    "_id": "Ngii4TX6F9wtzXS8C",
    "title": "Solstice Album Crowdfunding",
    "slug": "solstice-album-crowdfunding",
    "pageUrl": "https://www.lesswrong.com/posts/Ngii4TX6F9wtzXS8C/solstice-album-crowdfunding",
    "postedAt": "2018-12-18T20:51:31.183Z",
    "baseScore": 39,
    "voteCount": 11,
    "commentCount": 7,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>For several years, people have been singing songs at <a href=\"https://www.lesswrong.com/posts/CGkZEQdeBZZXbBT7o/on-rationalist-solstice-and-epistemic-caution\">Solstice</a>, and some of those songs have been recorded, but always as a live performance which means the quality isn&#x27;t as good and the musicians are recording-in-one-take-that-includes-all-mistakes.</p><p>So, I&#x27;m finally putting together a professional Solstice Album. I recorded an initial 12 songs for $8,000 (paying for musicians and studio time), recently recorded another few songs for $2,000. Ideally I&#x27;d like at least a bit more studio time to layer in some more vocal tracks so that 2/3 of the songs aren&#x27;t just me singing.</p><p>The <a href=\"https://www.indiegogo.com/projects/secular-solstice-album/x/4446313#/\">IndieGogo campaign is here</a>. Since I&#x27;ve already spent a bunch of money, it&#x27;s a flexible fund campaign (however much we raise, the album will still come out, and I&#x27;ll get reimbursed or whatever people put it). But the more we raise the higher quality the songs will be end up being and the more likely I&#x27;ll be to take on similar projects in the future.</p><p>If you choose the Early Access tier and I know who you are (in particular, I have your email), I&#x27;ll send you a link to the google drive folder with the rough cuts as soon as I can, rather than waiting till after the IndieGogo wraps up.</p><hr class=\"dividerBlock\"/><h2>The Songs</h2><p>These are the songs that are already recorded and guaranteed to be in the album. Some of them have links to snippets so that you can hear what the instrumentation is like:</p><ul><li>Bold Orion</li><li>Bring the Light</li><li>Gather Round</li><li><a href=\"https://soundcloud.com/raymond-arnold/bitter-wind-snippet?in=raymond-arnold/sets/solstice-snippets\">Bitter Wind Lullaby</a></li><li>Chasing Patterns</li><li><a href=\"https://soundcloud.com/raymond-arnold/stardust-snippet\">Stardust</a></li><li>Time Wrote the Rocks</li><li>Do You Realize</li><li>Hymn to the Breaking Strain</li><li>Bitter Wind March</li><li><a href=\"https://soundcloud.com/raymond-arnold/brighter-than-today-snippet?in=raymond-arnold/sets/solstice-snippets\">Brighter Than Today</a></li><li><a href=\"https://soundcloud.com/raymond-arnold/endless-lights-snippet\">Endless Light</a></li><li><a href=\"https://soundcloud.com/raymond-arnold/here-and-now-snippet\">Here and Now</a></li><li>Forever Young</li><li><a href=\"https://soundcloud.com/raymond-arnold/uplift-snippet\">Uplift</a></li><li>Five Thousand Years</li></ul><p>There will be bonus tracks available at higher tiers that include rough recordings of some of the off-beat silly songs or songs that get suggested by people on the &quot;suggest a song&quot; tier. </p><p>The bonus tracks will also include <strong>instrumental versions of most songs</strong> so that people can sing along karaoke style, or use the backing-track for their own Solstice events if their community doesn&#x27;t have musicians.</p><h2>Suggest a Bonus Song</h2><p>Higher level tiers include &quot;Suggest a Bonus Song&quot; and &quot;Costly Signal a Bonus Song&quot;. These do not guarantee a particular bonus song, because things are expensive and the world is complicated and the campaign might not even recover the money to pay the existing expenses.</p><p>However, if there is a lot of support for a particular song getting included I will do my best to at least get a rough recording of it done, and in general, I will prioritize the bonus songs that got Signaled Hardest.</p><p></p>",
    "user": {
      "username": "Raemon",
      "slug": "raemon",
      "displayName": "Raemon"
    }
  },
  {
    "_id": "Lr2MAFLsfmayBhJnC",
    "title": "18-month follow-up on my self-concept work",
    "slug": "18-month-follow-up-on-my-self-concept-work",
    "pageUrl": "https://www.lesswrong.com/posts/Lr2MAFLsfmayBhJnC/18-month-follow-up-on-my-self-concept-work",
    "postedAt": "2018-12-18T17:40:03.941Z",
    "baseScore": 62,
    "voteCount": 24,
    "commentCount": 4,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>About eighteen months ago, I found Steve Andreas’s book <a href=\"https://www.amazon.com/Transforming-Your-Self-Becoming-Want/dp/0911226435\">Transforming Your Self</a>, and applied its techniques to fixing a number of issues in my self-concepts which had contributed to my depression and anxiety. Six weeks after those changes, I posted a report called “<a href=\"https://kajsotala.fi/2017/07/how-i-found-fixed-the-root-problem-behind-my-depression-and-anxiety-after-20-years/\">How I found &amp; fixed the root problem behind my depression and anxiety after 20+ years</a>”. I figured that by now it would be time for a follow-up on how those effects have lasted.</p>\n<h1><b>Overall summary and general considerations </b></h1>\n<p>Looking back, this was definitely a major milestone in improving my mental health. I feel like since 2014, I have been ongoing a process of completely transforming myself from the depression- and anxiety-ridden person who was convinced that he had no other option than becoming a total failure, to someone calmly confident who has the option of constructing his life to his taste. I don’t claim to be there yet, but I feel like I’m constantly getting closer. I feel like the self-concept work discussed in my post, was one of the largest engines powering this transition. (Other major ones being me <a href=\"https://kajsotala.fi/2015/08/change-blindness/\">getting antidepressants</a>, <a href=\"https://www.lesswrong.com/posts/R6KFnJXyk79Huvmrn/two-arguments-for-not-thinking-about-ethics-too-much\">changing how I thought about ethics</a>, and <a href=\"https://kajsotala.fi/2014/11/event-report-cfars-rationality-workshop-england/\">learning a new mindset from CFAR</a> in 2014, properly learning <a href=\"https://en.wikipedia.org/wiki/Focusing_(psychotherapy)\">Focusing</a> and <a href=\"https://www.amazon.com/Core-Transformation-Reaching-Wellspring-Within/dp/0911226338\">Core Transformation</a> as well as starting to meditate according to <a href=\"http://slatestarcodex.com/2018/11/28/book-review-the-mind-illuminated/\">The Mind Illuminated</a> system in 2017, and starting to apply <a href=\"https://selfleadership.org/about-internal-family-systems.html\">Internal Family Systems</a> this year.)</p>\n<p>There are two difficulties evaluating my self-concept post afterwards. First is that I have a poor emotional memory, so it’s a little hard for me to remember what I felt before these changes. The second is that after doing self-concept work, I’ve also done plenty of other things, such as meditation and moving together with some housemates, which have also had a definite impact on my mental health. I can’t know how well the self-concept work would have stuck around, if I hadn’t also implemented those other changes. It’s possible and even likely that some of my current results are because of those other changes instead.</p>\n<p>At the same time, the self-concept work is also not independent from everything that I’ve done later. For instance, I think that being able to eliminate the feelings of pointless shame has been a major reason <i>why</i> I’ve been able to live with housemates and find them a definite net positive. Previously the feelings of shame would have made it too draining to have to engage in social interaction in my home on a regular basis, whereas now social interaction has tended to be much more energizing than it did in the past. But then again, there are also <a href=\"https://kajsotala.fi/2015/10/two-conversationalist-tips-for-introverts/\">other skills</a> which have made social fatigue less of an issue than sometime in the past, and which I’ve also been gradually training up.</p>\n<p>But still, at least I can report on the various things in the post, and on how they’ve held up.</p>\n<h1><b>Things that seem to have been fixed for good</b></h1>\n<p><b>Generalized feelings of shame; being afraid of thinking that thoughts that might trigger feelings of shame; needing constant validation in order to avoid feelings of shame. </b>I described the following in my post from last year:</p>\n<p><i>I realized that I had a sense of unease, a vague feeling of shame… as if there was something shameful about me that I knew, but was trying to avoid thinking about. And I knew that I had felt this same vague shame many times before, often particularly when I was tired. </i>[…]</p>\n<p><i>… there’s always an underlying insecurity, a sense of unease from the fact that anything might cause your attention to swing back to the [memories of being a terrible person]. You need a constant stream of external validation and evidence in order to keep your attention anchored on the examples [of being a good person]; the moment it ceases, your attention risks swinging to the [memories of being a terrible person] again.</i></p>\n<p>As far as I can tell, this kind of thing simply doesn’t happen anymore. I still get feelings of guilt, if I have screwed up in some way, but there’s no shame or feeling of being a horrible person. Nor is there any need for external validation in this regard. I just know that I’m always doing the best that I can, and if I make a mistake that I need to learn from, then I feel the amount of guilt that’s necessary to motivate me to make amends and/or remember to act differently in the future. And that’s that.</p>\n<p><b>Being motivated by a desire to prove to myself that I’m a good person. </b><i>Previously I was trying to do a lot of things, but basically everything was strongly driven by a motivation to feel better about myself, and whenever it looked like something wasn’t likely to help with that goal, I would get demotivated. </i>[…] <i>Previously when I was trying to do things to “save the world”, there was a strong component of doing it for the sake of guilt, feeling bad, or trying to win respect or status from others.</i></p>\n<p>Basically fixed; this caused a period of readjustment, in that I had been doing things which had been optimized for looking good in the eyes of people that I admired, even when I personally hadn’t felt on a gut level that they made much sense. It took a while to readjust and find things which felt worth doing, but now I mostly feel like I’m doing things because they are genuinely derived from my values, rather than to avoid shame.</p>\n<p>I still occasionally have something-like-guilt as a factor in thinking about what I want to do, but it mostly pops up when I notice that I’m not satisfying all of my <i>own</i> values and neglecting something that I actually care about. I’m no longer doing things “for the sake of guilt”, in the sense that I would do something and then keep feeling guilty regardless. <a href=\"http://mindingourway.com/dont-steer-with-guilt/\">If you find yourself regularly experiencing guilt, then you are using guilt incorrectly</a>; in this respect as well, I’m using guilt much more correctly now.</p>\n<p><b>Insecurity in relationships and with romantic partners; very detailed escapist romantic fantasies. </b><i>If I was in a relationship, I would tend to very strongly highlight some qualities that I felt I had and which I felt bad about, in an attempt to get my partner to explicitly express being okay with them. […]</i></p>\n<p><i>… much of my desire and need to be in a relationship was another way of trying to look for external validation, some kind of evidence that there was somebody who would accept me and would want to be with me. I used to have a lot of pretty detailed romantic fantasies; a lot of them lost their appeal after I fixed my self-concept.</i></p>\n<p>Evaluating this is slightly harder since I haven’t actually been in a relationship since writing that post. However, judging from the way that I’ve felt towards and interacted with <i>potential</i> romantic partners as well as women I’ve been intimate friends with, and how I’ve felt about relationships in general, this feels basically fixed. Being single is far from my ideal preference, but it’s not particularly terrible either, and I don’t spend much time absorbed in detailed fantasies when I could be doing something else. I’m also much more comfortable with intimate friendships which are ambiguous about whether or not they might turn more romantic; I can be genuinely happy either way.</p>\n<h1><b>Mostly fixed, might still pop up a bit</b></h1>\n<p><b>Obsessive sexual fantasies. </b><i>Without going into too much detail, previously my sexuality and fantasies had been very strongly entwined around a few paraphilias, which provided a great deal of emotional comfort. A lot of those fantasies were obsessive to the point of being bothersome.</i></p>\n<p>At the time of writing my post, I reported that these basically disappeared. They remained gone for a while, but eventually some (not all) of them came back, though considerably transformed. They are fun to engage with occasionally, and they might get a bit bothersome if I think about them too much. But whenever they start getting that mildly obsessive flavor, it tends to act as a natural disincentive for me to continue thinking about them, and then they quiet down again.</p>\n<h1><b>Partially fixed, but with other causes as well</b></h1>\n<p><b>Feelings of anxiety and a need to escape. </b><i>It feels that, large parts of the time, my mind is constantly looking for an escape, though I’m not entirely sure what exactly it is trying to escape from. But it wants to get away from the current situation, whatever the current situation happens to be. To become so engrossed in something that it forgets about everything else.</i></p>\n<p><i>Unfortunately, this often leads to the opposite result. My mind wants that engrossment right now, and if it can’t get it, it will flinch away from whatever I’m doing and into whatever provides an immediate reward. Facebook, forums, IRC, whatever gives that quick dopamine burst. That means that I have difficulty getting into books, TV shows, computer games: if they don’t grab me right away, I’ll start growing restless and be unable to focus on them. Even more so with studies or work, which usually require an even longer “warm-up” period before one gets into flow.</i></p>\n<p>This kind of a thing still happens; apparently the anxiety from poor self-concepts was only one of its causes. I now think that it’s more of an <a href=\"https://en.wikipedia.org/wiki/Executive_dysfunction\">executive dysfunction</a> symptom, in that various causes of stress or feeling bad can trigger a <a href=\"http://slatestarcodex.com/2014/04/11/going-loopy/\">self-reinforcing loop</a> of feeling bad, trying to escape that badness, feeling even more bad for failing to escape it, etc. My feelings of shame were definitely one cause, but many other things can also trigger it. Meditation and <a href=\"https://en.wikipedia.org/wiki/Focusing_(psychotherapy)\">Focusing</a> / <a href=\"https://selfleadership.org/about-internal-family-systems.html\">IFS</a> work have been a major aid in fixing several other causes.</p>\n<p><b>Insecurities based on shame vs. instrumental considerations. </b><i>Suppose that you have an unstable self-concept around “being a good person”, and you commit some kind of a faux pas. Or even if you haven’t actually committed one, you might just be generally unsure of whether others are getting a bad impression of you or not. Now, there are four levels on which you might feel bad about the real or imagined mistake:</i></p>\n<ol>\n<li><i> Feeling bad because you think you’re an intrinsically bad person</i></li>\n<li><i> Feeling bad because you suspect others think bad of you and that this is intrinsically bad (if other people think bad of you, that’s terrible, for its own sake)</i></li>\n<li><i> Feeling bad because you suspect others think bad of you and that this is instrumentally bad (other people thinking bad of you can be bad for various social reasons)</i></li>\n<li><i> Feeling bad because you might have hurt or upset someone, and you care about what others feel</i></li>\n</ol>\n<p><i>Out of these, #3 and #4 are reasonable, #1 and #2 less so. When I fixed my self-concept, reaction #1 mostly vanished. But interestingly, reaction #2 stuck around for a while… or at least, a fear of #2 stuck around for a while.</i></p>\n<p>#1 and #2 seem to indeed have disappeared; however, I’ve still continued to experience insecurities which have taken the forms of what seems like excessive worries of #3 and #4 (thinking that I’ve displeased someone in a way which will make them like me less, as well as worrying that someone might have felt upset over something that they in all likelihood won’t even remember). These seem to be the kinds of issues that can’t be fixed by internal work alone, since they are about the external world: in order to evaluate how justified these are, I need to actually <i>test</i> the extent to which something e.g. makes other people dislike me.</p>\n<p>This work is still ongoing, but I’ve been making progress. Major contributors to current progress are the skills of <a href=\"https://kajsotala.fi/2018/10/on-insecurity-as-a-friend/\">integrating the cautions from my insecurities</a> and <a href=\"https://kajsotala.fi/2018/11/tentatively-considering-emotional-stories-ifs-and-getting-into-self/\">tentatively considering emotional stories</a>. These seem to have the effect that parts of my mind which have long held extreme beliefs about how cautious I should be, get listened to in a fairer way, causing them to update their beliefs to less extreme ones.</p>\n<p><b>Difficulties in self-motivation. </b><i>Besides being able to work at all, I’m also able to consistently work from home. This was often basically impossible: the impulse to escape was just too strong, and I needed to go elsewhere, preferably co-work with somebody else. Now I’ve cut down on co-working a lot, because leaving my home would take time, and I get more done if I don’t need to spend that time on travel.</i></p>\n<p>This varies; implementing these fixes seems to have provided a temporary motivational boost allowing me to get a lot of work done with just the reward of financial security. When I find things to do that I’m significantly motivated by, then I seem to be able to work on them pretty well, even from home. However, anything that I’m <i>not</i> significantly motivated by still requires a lot of external structure for me to get anything done. Again, this seems like a manifestation of executive dysfunction issues more generally.</p>\n<p>My initial motivation boost expired for a while, and I soon ran into new problems (I’ll discuss these below). It has taken a while to find promising new directions and figure out my new motivations so that I can do work more consistently, but (again thanks to meditation and Focusing / IFS work) in the last few months I’ve been starting to feel more consistently self-motivated.</p>\n<h1><b>In progress of being fixed after being made worse </b><b><i>by</i></b><b> the self-concept work</b></h1>\n<p><b>Lack of motivation once escaping the pain was no longer as motivating. </b><i>For a while, there was a sense that my life had gotten more boring. Remember that analogy about being hungry all the time and focusing all your energies on food, and then being transformed into an android which didn’t need to eat? Your previous overriding priority of finding food being gone, you wouldn’t know what to do anymore. You’d feel okay, and it would be a steady okay – no lows, but also no particular highs.</i></p>\n<p>The fixes in the post had the problem that I no longer felt actively bad; but eventually I started to notice that, having largely structured my life, habits and brain around escaping the badness, I didn’t have any particularly wholesome ways of feeling <i>good</i>. Even though I had fixed a major cause behind my depression and <a href=\"https://kajsotala.fi/2017/01/on-my-burnout/\">burnouts</a>, they had still left pretty deep marks in my brain. After a while, I started to feel acutely <a href=\"https://en.wikipedia.org/wiki/Anhedonia\">anhedonic</a> – limited in my ability to get pleasure from anything. The fact that many of my previous obsessive fantasies had been eliminated probably made this worse, since they had at least been a source of pleasure and motivation.</p>\n<p>But this is still a good development. <a href=\"https://www.facebook.com/sebastmarsh/posts/1265332690156628\">The goal of life isn’t to be free of problems; it’s to have more interesting problems</a>, and this is definitely a much more interesting problem. I’ve been trying new things, from <a href=\"https://kajsotala.fi/2018/08/finland-museum-tour-1-tampere-art-museum/\">going to museums</a> to generally being more open to stuff. I’m working on fixing the remaining mental blocks that are keeping me in place rather than experiencing stuff.</p>\n<p>I’m gradually relearning to genuinely enjoy things. And that feels good: I feel like I’m just getting started in the process of rebuilding myself.</p>\n<p>Can’t wait to see where I’ll be in a few year’s time.</p>\n<p> </p>",
    "user": {
      "username": "Kaj_Sotala",
      "slug": "kaj_sotala",
      "displayName": "Kaj_Sotala"
    }
  },
  {
    "_id": "ZPvCWXasmf23sudDb",
    "title": "Good arguments against \"cultural appropriation\"",
    "slug": "good-arguments-against-cultural-appropriation",
    "pageUrl": "https://www.lesswrong.com/posts/ZPvCWXasmf23sudDb/good-arguments-against-cultural-appropriation",
    "postedAt": "2018-12-18T17:23:52.900Z",
    "baseScore": 24,
    "voteCount": 12,
    "commentCount": 12,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>[Originally <a href=\"https://www.facebook.com/tyrrell.mcallister/posts/10106390274370763\">posted to Facebook</a>.]</p><p>I&#x27;m collecting steel-man arguments that the concept of &quot;cultural appropriation&quot; describes a real problem. Below are three arguments that seem somewhat reasonable to me in some cases. They seem to point to plausibly real costs of cross-cultural sharing and re-interpretation.</p><p>(Just to lay my cards on the table: I currently think that the benefits of cross-cultural sharing so often outweigh these cost that cultural appropriation as such should not be stigmatized.)</p><p>I&#x27;ll abbreviate <em>cultural appropriation</em> as <em>CA</em> from here on out.</p><p><strong>1. Some CA is taken to be a kind of mockery.</strong> Such CA is thought to result in diminished status and power for people in the &quot;appropriated&quot; culture. Alleged examples are team mascots, Halloween costumes, and Charlie Chan.</p><p>On the one hand, such arguments should be scrutinized skeptically and accepted only tentatively, because they rest on claims about difficult-to-measure effects on the vaguely defined status of amorphous social groups.</p><p>On the other hand, and for the same reason, you can&#x27;t be certain that such claims are false. And you shouldn&#x27;t just trust your own intuition on this kind of thing, because you don&#x27;t have a bird&#x27;s eye view of the entire complicated network of power and status that makes up our super-culture and all of its various subcultures. So it does make sense to listen to how other people think so-called CA affects their social standing.</p><p><strong>2. Some CA amounts to diluting a piece of the cultural commons</strong> from which people in that culture were benefiting.</p><p>For example, people choose their clothes based on how they want to be seen by others. Tie-dye, for example, has a certain meaning in our culture. You wear tie-dye if you want people to see you in a certain way. But suppose that our culture found itself immersed in some larger surrounding culture, and people in the larger culture started wearing tie-dye without any knowledge of the whole system of sartorial signification within which tie-dye is embedded in our own culture. Now there&#x27;s a bunch of people walking around wearing tie-dye who don&#x27;t mean to signal what tie-dye signaled for us. As a result, tie-dye loses its signaling value for us. Having lost this signaling tool, we are that much poorer.</p><p>In some cultures, such markers of meaning are much more potent than they are in ours. So the loss of these markers results in a correspondingly greater loss of value to the people in these cultures. This seems to be part of the objection to the appropriation of clothing, jewelry, and hair styles.</p><p><strong>3. Some CA is seen as a kind of theft of intellectual property</strong>, where gains in status and material wealth go to people outside the culture that ought to have gone to people inside the culture. Mere users of a cultural innovation (anglo consumers of Mexican food, say) are resented insofar as they patronize outsiders rather than insiders. But the real resentment is directed toward the outsiders who sell the innovation, or who gain status as &quot;trendsetters&quot;. The profits and the status, on this view, ought to have gone to the people within the culture, who deserve a kind of corporate credit for the innovation.</p><hr class=\"dividerBlock\"/><p>There is a fourth kind of argument that is conspicuously absent from my list: &quot;People within appropriated cultures take offense at CA. So, you shouldn&#x27;t do it if you care about not putting people through that painful experience&quot;.</p><p>I want to set &quot;offense&quot; arguments aside for the moment. That&#x27;s not because I dismiss them out of hand. Rather, it&#x27;s because offense arguments raise lots of issues that require special care to treat properly.</p><p>For one thing, offense arguments have a kind of recursive tendency to be self-fulfilling. Under certain circumstances, they can even bootstrap themselves into validity from practically nothing. Katja Grace has a couple really good posts on how this can happen:</p><p><a href=\"https://meteuphoric.com/2016/07/14/iterations-of-hurt/\">Iterating hurt</a></p><p><a href=\"https://meteuphoric.com/2017/08/30/what-you-cant-say-to-a-sympathetic-ear/\">What you can’t say to a sympathetic ear</a></p><p>For that reason, I want to see how far the pro-CA case can get prior to an appeal to offensiveness. If you like, I&#x27;m looking for non-recursive reasons for finding CA to be offensive—reasons other than &quot;because that practice is already understood to be offensive&quot;. This is not to suggest that such reasons aren&#x27;t real or can be ignored in the final analysis. Regardless, it seems valuable to know what is left when you set this kind of argument aside.</p>",
    "user": {
      "username": "Tyrrell_McAllister",
      "slug": "tyrrell_mcallister",
      "displayName": "Tyrrell_McAllister"
    }
  },
  {
    "_id": "oDADigGrmZuXMHHmX",
    "title": "Experiences of Self-deception",
    "slug": "experiences-of-self-deception",
    "pageUrl": "https://www.lesswrong.com/posts/oDADigGrmZuXMHHmX/experiences-of-self-deception",
    "postedAt": "2018-12-18T11:10:26.965Z",
    "baseScore": 15,
    "voteCount": 5,
    "commentCount": 3,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>It seems to me that self-deception can describe two different things - conscious and unconscious self-deception. </p><p>Sometimes the elephant believes something untrue all by itself without the rider ever getting a look in. The claims of <a href=\"http://elephantinthebrain.com/\">elephant in the brain</a> seem to focus on this type of unconscious self-deception.</p><p>At other times the rider is complicit in endorsing a particular known untrue belief. The elephant analyses a situation, determines what it is beneficial to believe and motivates the rider to believe this. The rider has access to information which indicates that this isn&#x27;t true. If the rider brings this information to full attention then it is one of those rare occasions where he can override the elephant&#x27;s desires. However the rider also has the option to push the information to the side and believe a beneficial lie. It is possible to do this well enough that the information is forgotten or completely overridden with new, inaccurate, information. </p><p>In pushing the information to the side, the rider can sometimes just never bring the information to full attention. Failing that, it can drown the information out by presenting other information (which agrees with its favoured interpretation) as loudly as possible in order to doubt/ignore/forget the information which it doesn&#x27;t like.</p><p>At least, this is something I experience but I don&#x27;t know whether other people do. I have a few examples where this has happened and have even experimented with allowing myself to start down the route of conscious self-deception to see what it feels like. To me it feels like cognitive dissonance (feeling hot, brain feeling &quot;fuzzy&quot;, adrenaline kicking in) whilst the rider works on counteracting the information. I guess this would be followed by the relief of resolving said dissonance when the rider starts to believe the lie but I haven&#x27;t experimented that far!</p><p>The literature appears to be understandably non-committal on whether the subjects are consciously aware of their self-deception - I guess that would be pretty hard to determine. </p><p>So my question is - do other people recognise this as something which happens to them? How would you describe the experience? Is it something which you&#x27;ve trained yourself to recognise when it starts?</p>",
    "user": {
      "username": "Bucky",
      "slug": "bucky",
      "displayName": "Bucky"
    }
  },
  {
    "_id": "a72owS5hz3acBK5xc",
    "title": "2018 AI Alignment Literature Review and Charity Comparison",
    "slug": "2018-ai-alignment-literature-review-and-charity-comparison",
    "pageUrl": "https://www.lesswrong.com/posts/a72owS5hz3acBK5xc/2018-ai-alignment-literature-review-and-charity-comparison",
    "postedAt": "2018-12-18T04:46:55.445Z",
    "baseScore": 190,
    "voteCount": 64,
    "commentCount": 26,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em><a href=\"https://forum.effectivealtruism.org/posts/BznrRBgiDdcTwWWsB/2018-ai-alignment-literature-review-and-charity-comparison\">Cross-posted</a> to the EA forum.</em></p><h1><strong>Introduction</strong></h1><p><u><a href=\"https://forum.effectivealtruism.org/posts/XKwiEpWRdfWo7jy7f/2017-ai-safety-literature-review-and-charity-comparison\">Like last year</a></u> and <u><a href=\"https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison\">the year before</a></u>, I’ve attempted to review the research that has been produced by various organisations working on AI safety, to help potential donors gain a better understanding of the landscape. This is a similar role to that which GiveWell performs for global health charities, and somewhat similar to an securities analyst with regards to possible investments.  It appears that once again no-one else has attempted to do this, to my knowledge, so I&#x27;ve once again undertaken the task. </p><p>This year I have included several groups not covered in previous years, and read more widely in the literature.</p><p>My aim is basically to judge the output of each organisation in 2018 and compare it to their budget. This should give a sense for the organisations&#x27; average cost-effectiveness. We can also compare their financial reserves to their 2019 budgets to get a sense of urgency.</p><p>Note that this document is quite long, so I encourage you to just read the sections that seem most relevant to your interests, probably the sections about the individual organisations. I do <em>not</em> recommend you skip to the conclusions!</p><p>I’d like to apologize in advance to everyone doing useful AI Safety work whose contributions I may have overlooked or misconstrued.</p><h1><strong>Methodological Considerations</strong></h1><h2>Track Records</h2><p>Judging organisations on their historical output is naturally going to favour more mature organisations. A new startup, whose value all lies in the future, will be disadvantaged. However, I think that this is correct. The newer the organisation, the more funding should come from people with close knowledge. As organisations mature, and have more easily verifiable signals of quality, their funding sources can transition to larger pools of less expert money. This is how it works for startups turning into public companies and I think the same model applies here.</p><p>This judgement involves analysing a large number papers relating to Xrisk that were produced during 2018. Hopefully the year-to-year volatility of output is sufficiently low that this is a reasonable metric. I also attempted to include papers during December 2017, to take into account the fact that I&#x27;m missing the last month&#x27;s worth of output from 2017, but I can&#x27;t be sure I did this successfully.</p><p>This article focuses on AI risk work. If you think other causes are important too, your priorities might differ. This particularly affects GCRI, FHI and CSER, who both do a lot of work on other issues.</p><p>We focus on papers, rather than outreach or other activities. This is partly because they are much easier to measure; while there has been a large increase in interest in AI safety over the last year, it’s hard to work out who to credit for this, and partly because I think progress has to come by persuading AI researchers, which I think comes through technical outreach and publishing good work, not popular/political work. </p><h2><strong>Politics</strong></h2><p>My impression is that policy on technical subjects (as opposed to issues that attract strong views from the general population) is generally made by the government and civil servants in consultation with, and being lobbied by, outside experts and interests. Without expert (e.g. top ML researchers at Google, CMU &amp; Baidu) consensus, no useful policy will be enacted. Pushing directly for policy seems if anything likely to hinder expert consensus. Attempts to directly influence the government to regulate AI research seem very adversarial, and risk being pattern-matched to ignorant opposition to GM foods or nuclear power. We don&#x27;t want the &#x27;us-vs-them&#x27; situation, that has occurred with climate change, to happen here. AI researchers who are dismissive of safety law, regarding it as an imposition and encumbrance to be endured or evaded, will probably be harder to convince of the need to voluntarily be extra-safe - especially as the regulations may actually be totally ineffective. The only case I can think of where scientists are relatively happy about punitive safety regulations, nuclear power, is one where many of those initially concerned were scientists themselves.  Given this, I actually think policy outreach to the general population is probably negative in expectation.</p><p>If you’re interested in this I’d recommend you read <u><a href=\"https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work\">this blog post</a></u> (also reviewed below).</p><h2><strong>Openness</strong></h2><p>I think there is a strong case to be made that openness in AGI capacity development is bad. As such I do not ascribe any positive value to programs to ‘democratize AI’ or similar. </p><p>One interesting question is how to evaluate non-public research. For a lot of safety research, openness is clearly the best strategy. But what about safety research that has, or potentially has, capabilities implications, or other infohazards? In this case it seems best if the researchers do not publish it. However, this leaves funders in a tough position – how can we judge researchers if we cannot read their work? Maybe instead of doing top secret valuable research they are just slacking off. If we donate to people who say “trust me, it’s very important and has to be secret” we risk being taken advantage of by charlatans; but if we refuse to fund, we incentivize people to reveal possible infohazards for the sake of money. (Is it even a good idea to publicise that someone else is doing secret research?)</p><p>With regard published research, in general I think it is better for it to be open access, rather than behind journal paywalls, to maximise impact. Reducing this impact by a significant amount in order for the researcher to gain a small amount of prestige does not seem like an efficient way of compensating researchers to me. Thankfully this does not occur much with CS papers as they are all on arXiv, but it is an issue for some strategy papers.  </p><p>More prosaically, organisations should make sure to upload the research they have published to their website! Having gone to all the trouble of doing useful research it is a shame how many organisations don’t take this simple step to significantly increase the reach of their work.</p><h2><strong>Research Flywheel</strong></h2><p>My basic model for AI safety success is this:</p><ol><li>Identify interesting problems</li><ol><li>As a byproduct this draws new people into the field through nerd-sniping</li></ol><li>Solve interesting problems</li><ol><li>As a byproduct this draws new people into the field through credibility and prestige</li></ol><li>Repeat</li></ol><p>One advantage of this model is that it produces both object-level work and field growth.</p><p>There is also some value in arguing for the importance of the field (e.g. Bostrom’s Superintelligence) or addressing criticisms of the field.</p><p>Noticeably absent are strategic pieces. In previous years I have found these helpful; however, lately fewer seem to yield incremental updates to my views, so I generally ascribe lower value to these. This does not apply to <em>technical strategy</em> pieces, about e.g. whether CIRL or Amplification is a more promising approach.</p><h2><strong>Near vs Far Safety Research</strong></h2><p>One approach is to research things that will make contemporary ML systems more safe, because you think AGI will be a natural outgrowth from contemporary ML, and this is the only way to get feedback on your ideas. I think of this approach as being exemplified by<a href=\"https://arxiv.org/abs/1606.06565\"> <u>Concrete Problems</u></a>. You might also hope that even if ML ends up leading us into another AI Winter, the near-term solutions will generalize in a useful way, though this is of course hard to judge. To the extent that you endorse this approach, you would probably be more likely to donate to CHAI.</p><p>Another approach is to try to reason directly about the sorts of issues that will arise with superintelligent AI, and won’t get solved anyway / rendered irrelevant as a natural side effect of ordinary ML research. To the extent that you endorse this approach, you would probably be more likely to donate to MIRI, especially for their<a href=\"https://intelligence.org/files/TechnicalAgenda.pdf\"> <u>Agent Foundations</u></a> work.</p><p>I am not sure how to relatively value these two things.</p><p>There are a number of other topics that often get mentioned as AI Safety issues. I generally do not think it is important to support organisations or individuals working on these issues unless there is some direct read-through to AGI safety.</p><p>I have heard it argued that we should become experts in these areas in order to gain credibility and influence for the real policy work. However, I am somewhat sceptical of this, as I suspect that as soon as a domain is narrow-AI-solved it will cease to be viewed as AI.</p><h3>Autonomous Cars</h3><p>My view is that the localised nature of any tragedies plus the strong incentive alignment mean that private companies will solve this problem by themselves.</p><h3>Unemployment</h3><p>While technological advance continually mechanise and replace labour in individual categories, it also opens up new ones. Contemporaneous unemployment has more to do with poor macroeconomic policy and inflexible labour markets than robots. AI strong enough to replace humans in basically every job is basically AGI-complete. At that point we should be worried about survival, and if we solve the alignment problem well enough to prevent extinction we will have likely also solved it well enough to also prevent mass unemployment (or at least the negative effects of such, if you believe the two can be separated).</p><p>There has been an increase in interest in a ‘Basic Income’ – an unconditional cash transfer given to all citizens – as a solution to AI-driven unemployment. I think this is a big mistake, and largely motivated reasoning by people who would have supported it anyway. In a Hansonian scenario, all meat-based humanity has is our property rights. If property rights are strong, we will become very rich. If they are weak, and the policy is that every agent gets a fair share, all the wealth will be eaten up as Malthusian EMs massively outnumber physical humans and driving the basic income down to the price of some cycles on AWS.</p><h3>Bias</h3><p>The vast majority of discussion in this area seems to consist of people who are annoyed at ML systems are learning based on the data, rather than based on the prejudices/moral views of the writer. While in theory this could be useful for teaching people about the difficulty of the alignment problem, the complexity of human value, etc., in practice I doubt this is the case. <u><a href=\"https://www.chrisstucchio.com/pubs/slides/crunchconf_2018/slides.pdf\">This presentation</a></u> is one of the better I have seen on the subject.</p><h2><strong>Other Existential Risks</strong></h2><p>Some of the organisations described below also do work on other existential risks, for example GCRI, FLI and CSER. I am not an expert on other Xrisks so they are hard for me to evaluate work in, but it seems likely that many people who care about AI Alignment will also care about them, so I will mention publications in these areas. The exception is climate change, which is highly non-neglected.</p><h2><strong>Financial Reserves</strong></h2><p>Charities like having financial reserves to provide runway, and guarantee that they will be able to keep the lights on for the immediate future. This could be justified if you thought that charities were expensive to create and destroy, and were worried about this occurring by accident due to the whims of donors.</p><p>Donors prefer charities to not have too much reserves. Firstly, those reserves are cash that could be being spent on outcomes now, by either the specific charity or others. Valuable future activities by charities are supported by future donations; they do not need to be pre-funded. Additionally, having reserves increases the risk of organisations ‘going rogue’, because they are insulated from the need to convince donors of their value.</p><p>As such, in general I do not give full credence to charities saying they need more funding because they want more than a year of runway in the bank. A year’s worth of reserves should provide plenty of time to raise more funding.</p><p>It is worth spending a moment thinking about the equilibrium here. If donors target a lower runway number than charities, charities might curtail their activities to allow their reserves to last for longer. At this lower level of activities, donors would then decide a lower level of reserves are necessary, and so on, until eventually the overly conservative charity ends up with a budget of zero, with all the resources instead given to other groups who turn donations into work more promptly. This is allows donor funds to be turned into research more quickly.</p><p>I estimated reserves = (cash and grants) / (2019 budget – committed annual funding). In general I think of this as something of a measure of urgency. This is a simpler calculation than many organisations (MIRI, CHAI etc.) shared with me, because I want to be able to compare consistently across organisations. I attempted to compare the amount of reserves different organisations had, but found this rather difficult. Some organisations were extremely open about their financing (thank you CHAI!). Others were less so. As such these should be considered suggestive only.</p><h2><strong>Donation Matching</strong></h2><p>In general I believe that charity-specific donation matching schemes<a href=\"https://forum.effectivealtruism.org/posts/a2gYyTnAP36TxqdQp/matching-donation-fundraisers-can-be-harmfully-dishonest\"> <u>are somewhat dishonest</u></a>, despite my having provided matching funding for at least one in the past.</p><p>Ironically, despite this view being<a href=\"https://blog.givewell.org/2011/12/15/why-you-shouldnt-let-donation-matching-affect-your-giving/\"> <u>espoused by GiveWell</u></a> (albeit in 2011), this is basically of OpenPhil’s policy of, at least in some cases, artificially limiting their funding to 50% of a charity’s need, which some charities argue (though not by OpenPhil themselves that I recall) effectively provides a 1:1 match for outside donors. I think this is bad. In the best case this forces outside donors to step in, imposing marketing costs on the charity and research costs on the donors. In the worst case it leaves valuable projects unfunded.</p><p>Obviously cause-neutral donation matching is different and should be exploited. Everyone should max out their corporate matching programs if possible, and things like the<a href=\"https://www.eagivingtuesday.org/\"> <u>annual Facebook Match</u></a> and the<a href=\"https://spring.wetrust.io/\"> <u>quadratic-voting match</u></a> were great opportunities.</p><h2><strong>Poor Quality Research </strong></h2><p>Partly thanks to the efforts of the community, the field of AI safety is considerably more well respected and funded than was previously the case, which has attracted a lot of new researchers. While generally good, one side effect of this (perhaps combined with the fact that many low-hanging fruits of the insight tree have been plucked) is that a considerable amount of low-quality work has been produced. For example, there are a lot of papers which can be accurately summarized as asserting “just use ML to learn ethics”. Furthermore, the conventional peer review system seems to be extremely bad at dealing with this issue.</p><p>The standard view here is just to ignore low quality work. This has many advantages, for example 1) it requires little effort, 2) it doesn’t annoy people. This conspiracy of silence seems to be the strategy adopted by most scientific fields, except in extreme cases like anti-vaxers.</p><p>However, I think there are some downsides to this strategy. A sufficiently large miliu of low-quality work might degrade the reputation of the field, deterring potentially high-quality contributors. While low-quality contributions might help improve<a href=\"https://arxiv.org/abs/1606.06565\"> <u>Concrete Problems</u></a>’ citation count, they may use up scarce funding.</p><p>Moreover, it is not clear to me that ‘just ignore it’ really generalizes as a community strategy. Perhaps you, enlightened reader, can judge that <em>“How to solve AI Ethics: Just use RNNs”</em> is not great. But is it really efficient to require everyone to independently work this out? Furthermore, I suspect that the idea that we can all just ignore the weak stuff is somewhat an example of typical mind fallacy. Several times I have come across people I respect according respect to work I found blatantly rubbish. And several times I have come across people I respect arguing persuasively that work I had previously respected was very bad – but I only learnt they believed this by chance! So I think it is quite possible that many people will waste a lot of time as a result of this strategy, especially if they don’t happen to move in the right social circles.</p><p>Finally, I will note that the two examples which spring to mind of cases where the EA community has forthrightly criticized people for producing epistemically poor work – namely<a href=\"http://effective-altruism.com/ea/12z/concerns_with_intentional_insights/\"> <u>Intentional Insights</u></a> and<a href=\"http://effective-altruism.com/ea/1so/concerns_with_ace_research/\"> <u>ACE</u></a> – seem ex post to have been the right thing to do, although in both cases the targets were inside the EA community, rather than vaguely-aligned academics.</p><p>Having said all that, I am not a fan of unilateral action, so will largely continue to abide by this non-aggression convention. My only deviation here is to make it explicit – though see<a href=\"https://80000hours.org/articles/accidental-harm/\"> <u>this</u></a> by 80,000 Hours.</p><h2>The Bay Area</h2><p>Much of the AI and EA communities, and especially the EA community concerned with AI, is located in the Bay Area, especially Berkeley and San Francisco. This is an extremely expensive place, and is dysfunctional both politically and socially. A few months ago I read a series of stories about abuse in the bay and was struck by how many things I considered abhorrent were in the story merely as background. In general I think the centralization is bad, but if there must be centralization I would prefer it be almost anywhere other than Berkeley. Additionally, I think many funders are geographically myopic, and biased towards funding things in the Bay Area. As such, I have a mild preference towards funding non-Bay-Area projects. If you’re interested in this topic I recommend you reading<a href=\"https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/\"> <u>this</u></a> or<a href=\"https://www.lesswrong.com/posts/zAqoj79A7QuhJKKvi/the-berkeley-community-and-the-rest-of-us-a-response-to-zvi\"> <u>this</u></a> or<a href=\"https://rationalconspiracy.com/2017/04/22/moving-to-the-bay-area/\"> <u>this</u></a>.</p><h1><strong>Organisations and Research</strong></h1><h2><strong>MIRI: The Machine Intelligence Research Institute</strong></h2><p><u><a href=\"https://intelligence.org/\">MIRI</a></u> is the largest pure-play AI existential risk group. Based in Berkeley, it focuses on mathematics research that is unlikely to be produced by academics, trying to build the foundations for the development of safe AIs. They were founded by Eliezer Yudkowsky and lead by Nate Soares.</p><p>Historically they have been responsible for much of the germination of the field, including advocacy, but are now focused on research. In general they do very ‘pure’ mathematical work, in comparison to other organisation with more ‘applied’ ML or strategy focuses. I have historically been impressed with their research.</p><p>Their agent foundations work is basically trying to develop the correct way of thinking about agents and learning/decision making by spotting areas where our current models fail and seeking to improve them.</p><h3>Research</h3><p>Garrabrant and Demski&#x27;s <u><a href=\"https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh\">Embedded Agency Sequence</a></u> is a short sequence of blog posts outlining MIRI&#x27;s thinking about Agent Foundations. It describes the issues about how to reason about agents that are embedded in their environment. I found it to be a very intuitive explanation of many issues that MIRI is working on. However, little of it will be new to someone who has worked through MIRI&#x27;s previous, less accessible work on the subject.</p><p>Yudkowsky and Christiano&#x27;s <u><a href=\"https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal\">Challenges to Christiano&#x27;s Capability Amplification Proposal</a></u> discusses Eliezer&#x27;s objections to Paul&#x27;s Amplification agenda in back-and-forth blog format. Eliezer has a couple of objections. At a high level, Paul is attempting a more direct solution, working largely within the existing ML framework, vs MIRI&#x27;s desire to work on things like agent foundations first. Eliezer is concerned that most aggregation/amplification methods do not preserve alignment, and that finding one that does (and building the low level agents) is essentially as hard as solving the alignment problem. Any loss of alignment would be multiplied with every level of amplification. Thirdly, there may be many problems that need sequential work - additional bandwidth does not suffice. Additionally, he objects that Paul&#x27;s ideas would likely be far too slow, due to the huge amount of human input required. This was an interesting post, but I think could have been more clear. Researchers from OpenAI were also named authors on the paper.</p><p>Yudkowsky&#x27;s <u><a href=\"https://intelligence.org/2018/10/03/rocket-alignment/\">The Rocket Alignment Problem</a></u> is a blog post presenting a Galileo-style dialogue/analogy for why MIRI is taking a seemingly indirect approach to AI Safety. It was enjoyable, but I&#x27;m not sure how convincing it would be to outsiders. I guess if you thought a deep understanding of the target domain was never necessary it could provide an existence proof.</p><p>Demski&#x27;s <u><a href=\"https://www.lesswrong.com/posts/CvKnhXTu9BPcdKE4W/an-untrollable-mathematician-illustrated\">An Untrollable Mathematician Illustrated</a></u> provides a very accessible explanation to some results about logical induction. </p><p>MIRI researchers also appeared as co-authors on:</p><ul><li>Manheim and Garrabrant&#x27;s <u><a href=\"https://arxiv.org/abs/1803.04585\">Categorizing Variants of Goodheart&#x27;s Law</a></u><br/><br/></li></ul><h3>Non-disclosure policy</h3><p>Last month MIRI announced their new policy of<a href=\"https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section3\"> <u>nondisclosure-by-default</u></a>:</p><p><em>[G]oing forward, most results discovered within MIRI will remain internal-only unless there is an explicit decision to release those results, based usually on a specific anticipated safety upside from their release.</em></p><p>This is a significant change from their previous policy. As of circa a year ago my understanding was that MIRI would be doing secret research largely <em>in addition to</em> their current research programs, not that <em>all </em>their programs would become essentially secret.</p><p>At the same time secrecy at MIRI is not entirely new. I’m aware of at least one case from 2010 where they decided not to publish something for similar reasons; as far as I’m aware this thing has never been ‘declassified’ – indeed perhaps it has been forgotten.</p><p>In any case, one consequence of this is that for 2018 MIRI has published essentially nothing. (Exceptions to this are discussed above).</p><p>I find this very awkward to deal with.</p><p>On the one hand, I do not want people to be pressured into premature disclosure for the sake of funding. This space is sufficiently full of infohazards that secrecy might be necessary, and in its absence researchers might prudently shy away from working on potentially risky things - in the same way that no-one in business sends sensitive information over email any more. MIRI are in exactly the sort of situation that you would expect might give rise to the need for extreme secrecy. If secret research is a necessary step en route to saving the world, it will have to be done by someone, and it is not clear there is anyone much better.</p><p>On the other hand, I don’t think we can give people money just because they say they are doing good things, because of the risk of abuse. There are many other reasons for not publishing anything. A some simple ones would be “we failed to produce anything publishable” or “it is fun to fool ourselves into thinking we have exciting secrets” or “we are doing bad things and don’t want to get caught.” </p><p>Additionally, by hiding the highest quality work we risk impoverishing the field, making it look unproductive and unattractive to potential new researchers. </p><p>One possible solution would be for the research to be done by impeccably deontologically moral people, whose moral code you understand and trust. Unfortunately I do not think this is the case with MIRI. (I also don’t think it is the case with many other organisations, so this is not a specific criticism of MIRI, except insomuchas you might have held them to a higher standard than others).</p><p>Another possible solution would be for major donors to be insiders, who read the secret stuff and can verify it is worth supporting. If the organisation also wanted to keep small donors the large donors could give their seal of approval; otherwise the organisation could simply decide it did not need them any more. However, if MIRI are adopting this strategy they are keeping it a secret from<a href=\"https://intelligence.org/topcontributors/\"> <u>me</u></a>! Perhaps this is reassuring about their ability to keep secrets.</p><p>Perhaps we hope that MIRI employees would leak information of any wrongdoing, but not leak potential info-hazards?</p><p>Finally, I will note that MIRI are have been very generous with their time in attempting to help me understand what they are doing.</p><h3>Finances</h3><p>According to MIRI they have around 1.5 years of expenses in reserve, and their 2019 estimated budget is around $4.8m. This does not include the potential purchase of a new office they are considering.</p><p>There is <em>prima facie </em>counterfactually valid matching funding available from REG’s<a href=\"https://doubleupdrive.com/\"> <u>Double Up Drive</u></a>.</p><p>If you wanted to donate to MIRI, <u><a href=\"https://intelligence.org/donate/\">here</a></u> is the relevant web page.</p><h2><strong>FHI: The Future of Humanity Institute</strong></h2><p><u><a href=\"https://www.fhi.ox.ac.uk/\">FHI</a></u> is a well-established research institute, affiliated with Oxford and led by Nick Bostrom. Compared to the other groups we are reviewing they have a large staff and large budget. As a relatively mature institution they produced a decent amount of research over the last year that we can evaluate. They also do a significant amount of outreach work.</p><p>Their research is more varied than MIRI&#x27;s, including strategic work, work directly addressing the value-learning problem, and corrigibility work.</p><h3>Research</h3><p>Armstrong and O&#x27;Rourke&#x27;s <u><a href=\"https://arxiv.org/pdf/1712.06365.pdf\">‘Indifference’ methods for managing agent rewards</a></u> provides an overview of Stuart&#x27;s work on Indifference. These are methods that try to prevent agents from manipulating a certain event, or ignore it, or change utility function without trying to fight it. In the paper they lay out extensive formalism and prove some results. Some but not all will be familiar to people who have been following his other work in the area. The key to understanding the why the utility function in the example is defined the way it is, and vulnerable to the problem described in the paper, is that we do not directly observe age - hence the need to base it on wristband status. I found the example a little confusing because it could also be solved by just scaling up the punishment for mis-identification that is caught, in line with Becker&#x27;s Crime and Punishment: An Economic Approach (1974), but this approach wouldn&#x27;t work if you didn&#x27;t know the probabilities ahead of time. Overall I thought this was an excellent paper. Researchers from ANU were also named authors on the paper.</p><p>Armstrong and Mindermann&#x27;s <u><a href=\"https://arxiv.org/abs/1712.05812\">Impossibility of deducing preferences and rationality from human policy</a></u> argues that you cannot infer human preferences from the actions of people who may be irrational in unknown ways. The basic point is quite trivial - that arbitrary irrationalities can mean that any set of values could have produced the observed actions - but at the same time I hadn&#x27;t internalised why this would be a big problem for the IRL framework, and in any case it is good to have important things written down. More significant is they also showed that &#x27;simplicity&#x27; assumptions will not save us - the &#x27;simplest&#x27; solution will (almost definitely) be degenerate. This suggests we do need to &#x27;hard code&#x27; some priors about human values into the AI - they suggest beliefs about truthful human utterances (though of course as speech acts are acts all the same, it seems that some of the same problems occur again at this level of meta). Alternatives (not mentioned in the paper) could be to look to psychology or biology (e.g. Haidt or evolutionary biology). Overall I thought this was an excellent paper.</p><p>Armstrong and O&#x27;Rourke&#x27;s <u><a href=\"https://arxiv.org/pdf/1711.05541.pdf\">Safe Uses of AI Oracles</a></u> suggests two possible safe Oracle designs. The first takes advantage of Stuart&#x27;s trademark indifference results to build an oracle whose reward is only based on cases where the output after being automatically verified is deleted, and hence cannot attempt to manipulate humanity. I thought this was clever, and it&#x27;s nice to see some payoff from the indifference machinery he&#x27;s been working on, though this Oracle only works for NP-style questions, and assumes the verifier cannot be manipulated - which is a big assumption. The paper also includes a simulation of such an Oracle, showing how the restriction affects performance. The rest of the paper describes the more classic technique of restricting an Oracle to give answers simple enough that we hope they&#x27;re not potentially manipulative, and frequently re-starting the Oracle. Researchers from ANU were also named authors on the paper.</p><p>Dafoe&#x27;s <u><a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/GovAIAgenda.pdf\">AI Governance: A Research Agenda</a></u> is an introduction to the issues faced in AI governance for policy future researchers. It seems to do a good job of this. As lowering barriers to entry is important for new fields, this is potentially a very valuable document if you are highly concerned about the governance side of AI. In particular, it covers policy work to address threats from general artificial intelligence as well as near-term narrow AI issues, which is a major plus to me. In some ways it feels similar to Superintelligence.</p><p>Sandberg&#x27;s <u><a href=\"http://oxfordre.com/naturalhazardscience/view/10.1093/acrefore/9780199389407.001.0001/acrefore-9780199389407-e-293\">Human Extinction from Natural Hazard Events</a></u> provides a detailed overview of extinction risks from natural events. The paper is both detailed and broad, and is something of an updated version of part of Bostrom and Cirkovic&#x27;s Global Catastrophic Risks. His conclusion is broadly than man-made risks are significantly larger than natural ones. As with any Anders paper it contains a number of interesting anecdotes - for example I also hadn&#x27;t realised that people in 1910 were concerned that Halley&#x27;s Comet might poison the atmosphere!</p><p>Schulze and Evans&#x27;s <u><a href=\"https://arxiv.org/abs/1803.04926\">Active Reinforcement Learning with Monte-Carlo Tree Search</a></u> provide an algorithm for efficient reinforcement-learning when learning the reward is costly. In most RL designs the agent always sees the reward; however, this would not be the case with CIRL, because the rewards require human input, which is expensive, so we have to ration it. Here Sebastian and Owain produce a new algorithm, BAMCP++ that tries to address this in an efficient way. The paper provides simulations to show the near-optimality of this algorithm in some scenarios vs failure of rivals, and some theoretical considerations for why things like Thompson Sampling would struggle.</p><p>Brundage et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1802.07228\">The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation</a></u> is a massively collaborative policy document on the threats posed by narrow AI. Aimed primarily at policymakers, it does a good job of introducing a wide variety of potential threats. However, it does not really cover existential risks at all, so I suspect the main benefit (from our point of view) is that of credibility-building for later. However, I am in general sceptical of politicians&#x27; ability to help with AI safety, so I relatively downweight this. But if you were concerned about bad actors using AI to attack, this is a good paper for you. Researchers from OpenAI, CSER were also named authors on the paper.</p><p>Bostrom&#x27;s <u><a href=\"https://nickbostrom.com/papers/vulnerable.pdf\">The Vulnerable World Hypothesis</a></u> introduces and discusses the idea of worlds that will be destroyed &#x27;by default&#x27; when they reach a certain level of technological advancement. He distinguishes between a variety of different cases, like if it is easy for individuals to develop weapons of mass destruction, with intuitive names like &#x27;Type-2b vulnerability&#x27;, and essentially argues for a global police state (or similar) to reduce the risk. It contained a bunch of interesting anecdotes - for example I hadn&#x27;t realised what little influence the scientists in the Manhattan Project had on the eventual political uses of nukes. However, given its origin I actually found this paper didn&#x27;t add much new. The areas where it could have added - for example, discussing novel ways of using cryptography to enable surveillance without totalitarianism, discussing Value Drift as a form of existential risk that might be impossible to solve without something like this, or the risks of global surveillance itself being an existential risk (as ironically covered in Caplan&#x27;s chapter of Global Catastrophic Risks) - were left with only cursory discussion. Additionally, given the nature of governments, I do not think that supporting surveillance is a very neglected area.</p><p>Lewis et al.&#x27;s <u><a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.13235\">Information Hazards in Biotechnology</a></u> discusses issues around dangerous biology research. They provide an overview, including numerous examples of dangerous discoveries and the policies that were used and their merits. </p><p>FHI researchers also appeared as co-authors on:</p><ul><li>Carey&#x27;s <u><a href=\"https://aiimpacts.org/interpreting-ai-compute-trends/\">Interpreting AI Compute Trends</a></u> </li><li>Baum et al.&#x27;s <u><a href=\"http://gcrinstitute.org/papers/trajectories.pdf\">Long-Term Trajectories of Human Civilization</a></u> </li><li>Evans et al.&#x27;s <u><a href=\"https://ought.org/papers/predicting-judgments-tr2018.pdf\">Predicting Human Deliberative Judgments with Machine Learning</a></u> </li><li>Shah et al.&#x27;s <u><a href=\"https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh\">Value Learning Sequence</a></u> </li><li>Duettmann et al.&#x27;s <u><a href=\"https://fs1-bb4c.kxcdn.com/wp-content/uploads/2018/11/AGI-Coordination-Geat-Powers-Report.pdf\">Artificial General Intelligence: Coordination and Great Powers</a></u><br/><br/></li></ul><h3>Finances</h3><p><u><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/miscellaneous/future-humanity-institute-work-on-global-catastrophic-risks\">OpenPhil awarded FHI $13.4m</a></u> earlier this year, spread out over 3 years, largely (but not exclusively) to fund AI safety research. Unfortunately the write-up I found on the website was even more minimal than<a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/future-humanity-institute-general-support\"> <u>last year’s</u></a> and so is unlikely to be of much assistance to potential donors.</p><p>They are currently in the process of moving to a new larger office just west of Oxford.</p><p>FHI didn’t reply to my emails about donations, and seem to be more limited by talent (though there are <u><a href=\"https://forum.effectivealtruism.org/posts/hBxp5xQtFGisTaKaH/cross-post-think-twice-before-talking-about-talent-gaps\">problems </a></u>with this phrase) than by money, so the case for donating here seems weaker. But it could be a good place to work!</p><p>If you wanted to donate to them, <u><a href=\"https://www.fhi.ox.ac.uk/support-fhi/\">here</a></u> is the relevant web page.</p><h2><strong>CHAI: The Center for Human-Compatible AI</strong></h2><p><u><a href=\"https://humancompatible.ai/\">The Center for Human-Compatible AI</a></u>, founded by Stuart Russell in Berkeley, launched in August 2016. They have produced a lot of interesting work, especially focused around inverse reinforcement learning. They are significantly more applied and ML-focused than MIRI or FHI (who are more ‘pure’) or CSER or CGRI (who are more strategy-focused). They also do work on non-xrisk related AI issues, which I generally think are less important, but which perhaps have solutions that can be re-used for AGI safety.</p><h3>Research</h3><p>Shah&#x27;s <u><a href=\"https://rohinshah.com/alignment-newsletter/\">AI Alignment Newsletter</a></u> is a weekly email of interesting new developments relevant to AI Alignment. It is amazingly detailed. I struggle writing this; I don&#x27;t know how he keeps on track of it all. Overall I thought is an excellent project.</p><p>Mindermann and Shah et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1809.03060\">Active Inverse Reward Design</a></u> turns the reward design process into an interactive one where the agent can &#x27;ask&#x27; questions. The idea, as I understand it, is that instead of the programmers creating a one-and-done training reward function which the agent learns about, instead the agent learns from the reward function, is cognizant of its uncertainties (Inverse Reward Design) and then queries the designer in such a way as to reduce its uncertainty. This seems like exploring the designers value space in the same way that an RL agent explores its environmental space. It seems like a very clever idea to me, though I would have liked to see more examples in the paper.</p><p>Hadfield-Menell and Hadfield&#x27;s <u><a href=\"https://arxiv.org/abs/1804.04268\">Incomplete Contracting and AI alignment</a></u> analogises the problem of AI alignment with the economics literature on incentive alignment (for humans). The analysis is generally good, and might lead to useful followups, though most of the readthroughs they drew from the principal-agent literature seem like they are already appreciated in the AI safety community. There was some somewhat novel stuff about signalling models, and about Aghion &amp; Tirole&#x27;s 1997 paper on incomplete contracting that seemed interesting but I didn&#x27;t really understand or have time to look into. It also did a nice job of pointing out how much the human problem of incomplete contracting is solved by humans being embedded in a moral and social order, and thus able and willing to do what &#x27;obviously&#x27; is &#x27;common sense&#x27; in unclear situations - a solution which unfortunately seems no FAI-complete for our case. Researchers from OpenAI were also named authors on the paper.</p><p>Reddy et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1805.08010\">Where Do You Think You&#x27;re Going?: Inferring Beliefs about Dynamics from Behaviour</a></u> attempt to infer values from agents with incorrect world-models (pace Armstrong and Mindermann&#x27;s Impossibility paper). They attempt to avoid the impossibility result by first deducing agent beliefs on a task with known goals, and then using those beliefs to infer goals on a new task. While there might not be any tasks with known human goals, you might hope that there are different areas where human goals and beliefs are more or less well understood, which could be utilised by a related approach. As such I was quite pleased by this paper. They also have a n=12 user trial.</p><p>Tucker et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1810.10593\">Inverse Reinforcement Learning for Video Games</a></u> apply an IRL algorithm to an Atari game. Given that proving that alignment-congeniality can be achieved with little loss of efficacy is important for convincing the field, and how much status is applied to success at video games, I think this is a good area to pursue.</p><p>Filan&#x27;s <u><a href=\"https://www.greaterwrong.com/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers\">Bottle Caps aren&#x27;t Optimisers</a></u> is a short blog post about how to identify agents. It argues this is important because we don&#x27;t want to accidentally create agents.</p><p>Milli et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1807.05185\">Model Reconstruction from Model Explanations</a></u> show it is easier to reconstruct a model with queries about gradients than levels. Asking &quot;what are the partial derivatives at this point?&quot; gives more information, and hence makes it easier to reverse-engineer the model, than asking &quot;what is the output at this point?&quot;. The paper is framed as being about the desire by some people to make AI models &#x27;accountable&#x27; by making them &#x27;explain&#x27; their decisions. I think this is not very important, but it does seem to have some relevance to efficiently reconstructing latent *human* value models. Given that we can only query humans so many times, it is important to make efficient use of these queries. Instead of asking &quot;Would you pull the lever?&quot; many times, instead ask &quot;Which factors would make you more likely to pull the lever?&quot;. In some sense asking for partial derivatives seems like n queries (for an n-dimensional space), but given that many (most?) of these are likely to be locally negligible this might be an efficient way to help extract human preferences. </p><p>Shah et al.&#x27;s <u><a href=\"https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh\">Value Learning Sequence</a></u> is a short sequence of blog posts outlining the specification problem. This is basically how to specify even in theory what we might want to AI to do. It is a nice introduction to many of the issues, like why imitation learning is not enough. Most of what has been published so far is not that new, though apparently it is still ongoing. Researchers from FHI were also contributed posts.</p><p>Reddy et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1802.01744\">Shared Autonomy via Deep Reinforcement Learning</a></u> desire an RL system that is intended to operate simultaneously with a human, preventing the human from taking very bad actions, despite not fully understanding the humans goals. </p><p>Hadfield-Menell et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1811.01267\">Legible Normativity for AI Alignment: The Value of Silly Rules</a></u> build a RL/Game Theory model for why we might want AI agents to obey and enforce even &#x27;silly&#x27; rules. Basically the idea is that fidelity to, and enforcement of, silly rules provides credible signals that important rules will also be enforced - and their failure to be enforced is also useful information that the group is not strong enough to defend itself so agents can quit earlier. I was a little confused by the conclusion, which suggested that agents would have to learn the difference between silly and non-silly rules. Wouldn&#x27;t this undermine the signalling value?</p><p>CHAI researchers also appeared as co-authors on:</p><ul><li>Ratner et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1806.02501\">Simplifying Reward Design through Divide-and-Conquer</a></u> </li><li>Basu&#x27;s Do You Want Your Autonomous Car to Drive Like You? </li><li>Liu et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1802.01780\">Goal Inference Improves Objective and Perceived Performance in Human-Robot Collaboration</a></u> </li><li>Wu et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1806.02027\">Discrete-Continuous Mixtures in Probabilistic Programming: Generalised Semantics and Inference Algorithms</a></u> </li><li>Zhou et al.&#x27;s Expressive Robot Motion Timing </li><li>Sadigh et al.&#x27;s <u><a href=\"https://people.eecs.berkeley.edu/~sastry/pubs/Pdfs%20of%202016/SadighPlanning2016.pdf\">Planning for Autonomous Cars that Leverage Effects on Human Actions</a></u> </li></ul><h3>Finances</h3><p>Based on detailed financials they shared with me I estimate they have around 2 years worth of expenses in reserve (including grants promised but not yet disbursed), with a 2019 budget of around $3m.</p><p>If you wanted to donate to them, <u><a href=\"https://give.berkeley.edu/egiving/index.cfm?Fund=FN3330000\">here</a></u> is the relevant web page.</p><h2><strong>CSER: The Center for the Study of Existential Risk </strong></h2><p><u><a href=\"http://cser.ac.uk/\">CSER</a></u> is an existential risk focused group located in Cambridge. Like GCRI they do work on a variety of existential risks, with more of a focus on strategy than FHI, MIRI or CHAI. </p><p>Strategic work is inherently tied to outreach, like lobbying the UK government, which is hard to evaluate and assign responsibility for.</p><p>In the past I have criticised them for a lack of output. It is possible they had timing issues whereby a substantial amount of work was done in earlier years but only released more recently. In any case they have published more in 2018 than in previous years.</p><p>CSER’s researchers seem to select a somewhat eclectic group of research topics, which I worry may reduce their effectiveness. </p><h3>Research</h3><p>Liu and Price&#x27;s <u><a href=\"http://philsci-archive.pitt.edu/14972/\">Ramsey and Joyce on deliberation and prediction</a></u> discusses whether agents can have credences on which decision they&#x27;ll make while they&#x27;re in the process of deciding. This builds on their previous work in Heart of DARCness. The relevance to AI safety is presumably via MIRI&#x27;s 5-10 problem, and how to model agents who think about themselves as part of the world, which I didn&#x27;t appreciate when I read Heart of DARCness. In particular, it discusses agents with sub agents. Having said that, a lot of the paper seemed to rest on terminological distinctions.</p><p>Currie&#x27;s <u><a href=\"http://philsci-archive.pitt.edu/14800/\">Existential Risk, Creativity &amp; Well-Adapted Science</a></u> argues that the professionalisation of science encourages &#x27;cautious&#x27; research, whereas Xrisk requires more creativity. Essentially it argues that many institutional factors push scientists towards exploitation over exploration. In general I found this convincing, though <em>pace</em> Currie I think the small number of Professorships compared to the number of PhDs actually *encourages* risk-taking, as the value out-of-the-money call options increases with volatility. I found his argument that Xrisk research needing unusually large amounts of creativity not entirely convincing - while I agree that novel threats like AI require this, his example of solar flares seems like the sort of threat that could be addressed in a diligent, rather than genius, fashion. The paper has some pertience for how we fund the Xrisk movement - in particular I think it pulls in favour of many small grants to &#x27;citizen scientists&#x27;, rather than large grants towards organisations.</p><p>Rees&#x27;s <u><a href=\"https://www.amazon.com/Future-Prospects-Humanity-Martin-Rees-ebook/dp/B07CSD5BG9\">On The Future</a></u> is a quick-read pop-sci book about the future of humanity. It includes a brief discussion of AI risk, and the section on the risks posed by high-energy physics experiments was new to me. Many topics are discussed only in a very cursory way however, and I agree with <u><a href=\"http://www.overcomingbias.com/2018/11/on-the-future-by-rees.html\">Robin&#x27;s review </a></u>- the book would have benefited from being proofread by an economist, or simply someone who does not share the author&#x27;s political views.</p><p>Shahar and Shapira&#x27;s <u><a href=\"https://www.cser.ac.uk/news/civilization-v-video-game-mod-superintelligent-ai/\">Civ V AI Mod</a></u> is a mod for Civ V (PC game) that adds superintelligence research into the game. This is the novel publicity effort I alluded to last year. It generated some media attention, which seemed less bad than I expected. </p><p>Currie&#x27;s <u><a href=\"http://philsci-archive.pitt.edu/15066/\">Introduction: Creativity, Conservatism &amp; the Social Epistemology of Science</a></u> is a general introduction to some issues about how risk-taking (or not) institutional science is. </p><p>Shahar&#x27;s <u><a href=\"http://philsci-archive.pitt.edu/15058/\">Mavericks and Lotteries</a></u> describes various ways in which allocating research funding by lottery, rather than through peer review, might be better. In particular he argues it would make institutional science less conservative. I am sceptical of this, however: the proposals still feature filtering proposals for being &quot;good enough&quot;, and in equilibrium the standard for being &quot;good enough&quot; may just rise to where the peer review standard was before. Additionally, I&#x27;m not sure I see a very strong link to existential risk - I guess OpenPhil could adopt randomisation? Expecting to reform all of science funding as a path to Xrisk reduction seems *very* indirect.</p><p>Currie&#x27;s <u><a href=\"http://philsci-archive.pitt.edu/14607/\">Geoengineering Tensions</a></u> discusses the pros and cons of geoengineering, and the difficulties of doing experiments in the field. It discusses two tensions: firstly the moral hazard risk, and secondly the difficulty of doing the necessary experiments given the conservatism of institutional science.<br/><br/>Adrian Currie edited a ‘special issue’, <a href=\"https://www.sciencedirect.com/journal/futures/vol/102/suppl/C\"><u><em>Futures of Research in Catastrophic and Existential Risk</em></u> </a>which I think is basically a journal of articles they in some sense commissioned or collected. Currie and Ó hÉigeartaigh&#x27;s <u><a href=\"https://www.dropbox.com/s/bh6okdz8pvrxzc6/Working%20together%20to%20face%20humanity%E2%80%99s%20greatest%20threats%20preprint.pdf?dl=0\">Working together to face humanity&#x27;s greatest threats: Introduction to The Future of Research on Catastrophic and Existential Risk</a></u> provides an overview of the topics discussed in the edition. In general these are not so much concerned with object-level existential risks as with the meta-work of developing the field. Unfortunately I have not had time to review all the articles it contains that were not authored by CSER researchers, though Jones et al.&#x27;s <u><a href=\"https://www.sciencedirect.com/science/article/pii/S0016328717301179\">Representation of future generations in United Kingdom policy-making</a></u> which advocated for a Parliamentory committee for future generations, looks interesting, as one was indeed subsequently created. CSER claim, as seems plausible, that many of these papers would not have counterfactually existed without CSER’s role as a catalyst. The topics discussed include a variety of existential risks.</p><p>CSER researchers also appeared as co-authors on the following papers:</p><ul><li>Brundage et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1802.07228\">The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation</a></u> (joint first authorship)</li><li>Cave and Ó hÉigeartaigh&#x27;s <u><a href=\"http://www.aies-conference.com/wp-content/papers/main/AIES_2018_paper_163.pdf\">An AI Race for Strategic Advantage: Rhetoric and Risks</a></u> </li><li>Martinez-Plumed et al&#x27;s <u><a href=\"https://www.ijcai.org/proceedings/2018/0718.pdf\">The Facets of Artificial Intelligence: A Framework to Track the Evolution of AI</a></u> </li></ul><h3>Finances</h3><p>Based on some very rough numbers shared with me I estimate they have around 1.25 years worth of expenses in reserve, with an annual budget of around $1m. </p><p>If you wanted to donate to them, <u><a href=\"https://www.cser.ac.uk/support-us/\">here</a></u> is the relevant web page.</p><h2><strong>GCRI: Global Catastrophic Risks Institute</strong></h2><p>The<a href=\"http://gcrinstitute.org/\"> <u>Global Catastrophic Risks Institute</u></a> is a geographically dispersed group run by Seth Baum. They have produced work on a variety of existential risks, including AI and non-AI risks. Within AI they do a lot of work on the strategic landscape, and are very prolific. </p><p>They are significantly smaller organisation than most of the others reviewed here, and in 2018 only one of their researchers (Seth) was full time. In the past I have been impressed with their high research output to budget ratio, and that continued this year. At the moment they seem to be somewhat subscale as an organisation - Seth seems to have been responsible for a large majority of their 2018 work - and are trying to grow. </p><p><u><a href=\"http://gcrinstitute.org/summary-of-gcris-2018-2019-accomplishments-plans-and-fundraising/\">Here </a></u>is their annual write-up.</p><p>Adam Gleave, winner of the 2017 donor lottery, chose to give some money to GCRI; <u><a href=\"https://forum.effectivealtruism.org/posts/SYeJnv9vYzq9oQMbQ/2017-donor-lottery-report\">here</a></u> is his thought process. He was impressed with their nuclear war work (which I’m not qualified to judge), and recommend GCRI focus more on quality and less on quantity, which seems plausible to me. GCRI tell me they are attentive to the issue and have made institutional changes to try to affect change.</p><p>GCRI also shared some other considerations with me that I cannot disclose, which may have affected my overall conclusion in addition to the considerations listed above.</p><h3>Research</h3><p>Baum et al.&#x27;s <u><a href=\"http://gcrinstitute.org/papers/trajectories.pdf\">Long-Term Trajectories of Human Civilization</a></u> provides an analysis of possible ways the future might go. They discuss four broad trajectories: status quo, catastrophe, technological transformation, and astronomical colonisation. The scope is very broad but the analysis is still quite detailed; it reminds me of Superintelligence a bit. I think this paper has a strong claim to becoming the default reference for the topic. Researchers from FHI, FRI were also named authors on the paper.</p><p>Baum&#x27;s <u><a href=\"https://irgc.epfl.ch/wp-content/uploads/2018/11/Baum-for-IRGC-Resilience-Guide-Vol-2-2018.pdf\">Resilience to Global Catastrophe</a></u> provides a brief introduction to ideas around resilience to disasters. The points it made seem true, but are obviously more applicable to non-AGI based threats that leave more scope for recovery.</p><p>Baum&#x27;s <u><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3218342\">Uncertain Human Consequences in Asteroid Risk Analysis and the Global Catastrophe Threshold</a></u> discusses the consequences of Asteroid impact. He reviews some of the literature, and discusses the idea of important thresholds for impact. One idea I hadn&#x27;t come across before was the risk that an asteroid impact might be mistaken as a nuclear attack and cause a war - an interesting risk because all we need to do to avoid it is see the asteroid coming. However, I&#x27;m not an expert in the field, so struggle to judge how novel or incremental the paper is.</p><p>Baum and Barrett&#x27;s <u><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3155983\">A Model for the Impacts of Nuclear War</a></u> goes through the various impacts of nuclear war. It seems diligent and useful for future researchers or policymakers as a reference, though it is not my area of expertise.</p><p>Baum et al.&#x27;s <u><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3137081\">A Model for the Probability of Nuclear War</a></u> describes and decomposes the many possible routes to nuclear war. It also contains an interesting and extensive database of &#x27;near-miss&#x27; scenarios.</p><p>Baum&#x27;s <u><a href=\"https://www.mdpi.com/2078-2489/9/9/209\">Superintelligence Skepticism as a Political Tool</a></u> discusses the risk of motivated scepticism about AI risks in order to protect funding for researchers and avoid regulation for corporation. This seems like a plausible risk, though we should be careful attributing disingenuous motivations to opponents - though it is certainly true that the AI safety community seems to be the target of more misinformation than you might expect. I think the paper could might have benefitted from contrasting this with the risks of regulatory capture, which seem to operate in the other direction. Without doing so the political discussion was somewhat partisan - in both misinformation papers virtually all the examples bad actor were right wing groups, though perhaps most readers might find this is agreeable!</p><p>Baum&#x27;s <u><a href=\"https://www.mdpi.com/2078-2489/9/10/244\">Countering Superintelligence Misinformation</a></u> discusses ways to improve debate around superintelligence through countering misinformation.  These are mainly different forms of education, plus criticism of people for saying false things. I thought that the sections about ways of addressing misinformation once it exists were generally quite sophisticated, though I am sceptical of some of them as I don&#x27;t think AI safety is very amenable to popular or state pressure.</p><p>Baum et al.&#x27;s <u><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3104645\">Modelling and Interpreting Expert Disagreement about Artificial Intelligence</a></u> attempts to put numbers of Bostrom and Goertzel&#x27;s credences for various AI risk factors and compare. They try to break down the disagreement into three statements, interpret the two thinkers&#x27; statements as probabilities for those statements, and then assign their own probability for which thinker is correct. I&#x27;m a bit confused by the last step - it seems that by doing so you&#x27;re basically ensuring the output will be equal to your own credence (by the law of total probability). </p><p>Umbrello and Baum&#x27;s <u><a href=\"https://www.researchgate.net/publication/324715437_Evaluating_Future_Nanotechnology_The_Net_Societal_Impacts_of_Atomically_Precise_Manufacturing\">Evaluating Future nanotechnology: The Net Societal Impacts of Atomically Precise Manufacturing</a></u> discusses the possible impacts of nanotechnology on society. Most of the discussion is quite broad, and could apply to economic growth in general. I was surprised how little value the authors assigned to greatly increasing the wealth of humanity.</p><h3>Finances</h3><p>GCRI spent around $140k in 2018, and are aiming to raise $1.5m to cover the next three years, for a target annual budget of ~$500k. This would allow them to employ their (3) key staff full time and have some money for additional hiring.</p><p>This large jump makes it a little hard to calculate runway in a comparable fashion to other organisations. They currently have around $280k, having recently received a $250k donation. But is it unfair to include this donation, given they received it subsequently to some other organisations telling me about their finance? All organisations should look progressively better funded as giving season goes on!</p><p>In any case it seems relatively clear that they have been and probably continue to be at the moment more funding constrained than most other organisations. The part-time nature of many of their staff makes their cost structure more variable and less fixed, suggesting this limited runway is less of an existential threat than it would be at some other organisations – they’re not about to disband - though clearly this is still undesirable.</p><p>It seems credible that more funding would allow them to hire their researchers full time, which seems like a relatively low-risk method of scaling. If they can preserve their current productivity this could be valuable, though my impression is many small organisations become less productive as they scale, as high initial productivity may be due to founder effects that revert to the mean.</p><p>If you want to donate to GCRI, <u><a href=\"http://gcrinstitute.org/donate/\">here</a></u> is the relevant web page.</p><h2><strong>GPI: The Global Priorities Institute</strong></h2><p>The <u><a href=\"https://globalprioritiesinstitute.org/\">Global Priorities Institute</a></u> is an academic research institute, lead by <a href=\"http://users.ox.ac.uk/~mert2255/\">Hilary Greaves,</a> working on EA philosophy within Oxford. I think of their mission as attempting to provide a home so that high quality academics can have a respectable academic career while working on the most important issues. At the moment they mainly employee philosophers, but they tell me they are planning to hire more economists in the future.</p><p>They are relatively new but many of their employees are extremely impressive and their working papers (linked on the EA forum, not on their main website) seem very good to me. At this stage I wouldn’t expect them to have reached run-rate productivity, so would expect this to increase in 2019.</p><p>They shared with me abstracts of a number of papers and so on they were working on which seemed interesting and useful. As academic philosophy goes it is very tightly focused on important, decision-relevant issues - however it is not directly AI Safety work.</p><p>They allow their employees to spend 50% (!) of their time working on non-GPI projects, to help attract talent. However, the Trammell paper mentioned below was one of these projects, and I thought it was very good, so maybe in practice this does not represent a halving of their cost-effectiveness. </p><p>CEA are also spawning a new independant <u><a href=\"https://forum.effectivealtruism.org/posts/vxwcxwiDKCnyHJhbz/announcing-the-new-forethought-foundation-for-global\">Forethought Foundation for Global Priorities Research</a></u>, which seems to be very similar to GPI except not part of Oxford.</p><h3>Research</h3><p>Mogensen&#x27;s <u><a href=\"https://unioxfordnexus-my.sharepoint.com/personal/exet1753_ox_ac_uk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fexet1753%5Fox%5Fac%5Fuk%2FDocuments%2FGlobal%20Priorities%20Institute%2FOperations%2FWebsite%2FWorking%20papers%2FLongtermism%20and%20risk%20aversion%20v3%2Epdf&parent=%2Fpersonal%2Fexet1753%5Fox%5Fac%5Fuk%2FDocuments%2FGlobal%20Priorities%20Institute%2FOperations%2FWebsite%2FWorking%20papers&slrid=10daaa9e-b098-7000-a41a-599fb32c6ff4\">Long-termism for risk averse altruists</a></u> argues that risk-averse should make altruists *more*, not *less*, interested in preventing existential risks. This is basically for the same reason that risk aversion causes people to buy insurance. You should be risk averse in outcomes, not in the direct impacts of your actions. This argument is totally obvious now but I&#x27;d never heard anyone mention it until two months ago, which suggests it is real progress. Overall I thought this was an excellent paper.</p><p>Trammell&#x27;s <u><a href=\"https://unioxfordnexus-my.sharepoint.com/personal/exet1753_ox_ac_uk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fexet1753%5Fox%5Fac%5Fuk%2FDocuments%2FGlobal%20Priorities%20Institute%2FOperations%2FWebsite%2FWorking%20papers%2Fdecision%5Ftheory%5Fregress%2Epdf&parent=%2Fpersonal%2Fexet1753%5Fox%5Fac%5Fuk%2FDocuments%2FGlobal%20Priorities%20Institute%2FOperations%2FWebsite%2FWorking%20papers&slrid=14daaa9e-3069-7000-a41a-5aa6302f7c36\">Fixed-Point Solutions to the Regress Problem in Normative Uncertainty</a></u> argues that we can avoid infinite metaethical regress through fixed-point results. This seems like an alternative to Will&#x27;s work on Moral Uncertainty in some senses. Basically the idea is that if the &#x27;choiceworthiness&#x27; of different theories are cardinal at every level in their hierarchy, we can prove a unique fixed point. This is significant to the extent we think that AIs are going to have to learn how to do moral reasoning, perhaps without the aid of humans&#x27; convenient &quot;just don&#x27;t think about it&quot; hack. It&#x27;s also in some ways a nice response to <u><a href=\"http://slatestarcodex.com/2018/09/12/in-the-balance/\">this</a></u> SlateStarCodex article.</p><h3>Finances</h3><p>They have a 2019 budget of around $1.5m dollars, and shared with me a number of examples of types of people they might like to hire in the future, with additional funding.</p><p>Apparently Oxford University rules mean that all their hires have to be pre-funded for their entire duration of their (4-5 year) contract.</p><p>If you wanted to donate to GPI, <a href=\"https://www.campaign.ox.ac.uk/philosophy\">here</a> is the link.</p><h2><strong>ANU: Australian National University </strong></h2><p>Australian National University has produced a surprisingly large number of relevant papers and researchers over time. </p><h3>Research</h3><p>Everitt et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1805.01109\">AGI Safety Literature Review AGI Safety Literature Review</a></u> - I was glad to see someone else attempting to do the same thing I have! Readers of this article might enjoy reading it, as it has much the same purpose. For academics new to the field it could function as a useful overview, introducing but not really arguing for many important points. It’s main value probably comes from one-sentence descriptions of a large number of papers, which could be a useful launching point for research. Literature reviews can also help raise the status of the field. However, it is less likely to add much new insight to those familiar with the field, as it doesn’t really engage with any of the arguments in depth. </p><p>Everitt et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1705.08417\">Reinforcement Learning with a Corrupted Reward Channel</a></u> examines how noisy reward inputs can drastically degrade reinforcement learner performance, and some possible solutions. Unsurprisingly, CIRL features as a possible solution. It&#x27;s also nice to see ANU-Deepmind collaboration. This paper was actually written last year, but I mention it here for completeness as I think I missed it previously; I haven&#x27;t reviewed it in depth. Researchers from Deepmind were also named authors on the paper.</p><p><em>EDIT: one paper redacted on author request, pending improved second version.</em></p><p>ANU researchers were also named as co-authors on the following papers:</p><ul><li>Leike et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1711.09883\">AI Safety Gridworlds</a></u> </li><li>Armstrong and O&#x27;Rourke&#x27;s <u><a href=\"https://arxiv.org/pdf/1712.06365.pdf\">‘Indifference’ methods for managing agent rewards</a></u> </li><li>Armstrong and O&#x27;Rourke&#x27;s <u><a href=\"https://arxiv.org/pdf/1711.05541.pdf\">Safe Uses of AI Oracles</a></u> </li></ul><h3>Finances</h3><p>Given their position as part of ANU I suspect it would be difficult for individual donations to appreciably support their work. Additionally, one of their top researchers, Tom Everitt, has now joined Deepmind.</p><h2><strong>BERI: The Berkeley Existential Risk Initiative</strong></h2><p><em>EDIT: After publishing, the <u><a href=\"http://existence.org/\">Berkeley Existential Risk Initiative</a></u> requested I remove this section. As a professional courtesy I am reluctantly complying, and rescind any suggestion that BERI may be a good place to donate. I apologize for any inconvenience caused to readers.</em></p><h2><strong>Ought</strong></h2><p><u><a href=\"https://ought.org/\">Ought</a></u> is a San Francisco based non-profit are researching the viability of automating human-like cognition. The focus is on approaches that are “scalable” in the sense that better ML or more compute makes them increasingly helpful for supporting and automating deliberation without requiring additional data generated by humans. The idea, as with amplification, is that we can achieve safety guarantees by making agents that reason in individual explicit and comprehensible steps, iterated many times over, as opposed to the dominant more black-box approaches of mainstream ML. Ought does research on computing paradigms that support this approach and experiments with human participants to determine whether this class of approaches is promising. But I admit I understand what they do less well than with other groups.</p><p>Their work doesn’t fit neatly into the model of the above groups - they’re not focused on publishing research papers, at least at the moment. Partly as a result of this, and as a new group, I feel like I don’t have quite as good a grasp on exactly their status as with other groups - which is of course primarily a fact about my epistemic state, rather than them.</p><h3>Research</h3><p>Stuhlmüller&#x27;s <u><a href=\"https://ought.org/presentations/factored-cognition-2018-05\">Factored Cognition</a></u> outlines the ideas behind their implementation of Christiano-style amplification. They built a web app where people take questions and recursively break them down into simpler questions that can be solved in isolation. At the moment this is for humans, to try to test whether this sort of amplification of distillation and answering could work. It seems like they have put a fair bit of thought into the ontology.</p><p>Evans et al.&#x27;s <u><a href=\"https://ought.org/papers/predicting-judgments-tr2018.pdf\">Predicting Human Deliberative Judgments with Machine Learning</a></u> attempts to make progress on building ML systems remain well-calibrated (i.e. the system &quot;knows what it knows&quot;) in AI-complete settings (i.e. in settings where current ML algorithms can’t possibly do well on every possible input). To do this they collect a dataset of human judgements on complex issues (weird fermi estimations and political fact-checking) and then look at how people&#x27;s estimates for these questions changed as they were allowed more time. This is important because someone&#x27;s rapid judgement of an issue is evidence as to what their eventual slow judgement will be. In some cases you might be able to predict that there is no need to give the human more time; their 30 second answer is probably good enough. This could be useful if you are trying to produce a large training set of judgements about complex topics. I also admire the author&#x27;s honesty that the results of their ML system was less good than they expected. They also discussed problems with their dataset; this was definitely my experience when trying to use the site. Researchers from FHI were also named authors on the paper.</p><h3>Finances</h3><p>Based on numbers they shared with me I estimate they have around half a year’s worth of expenses in reserve, with an projected 2019 budget of around $1m.</p><p>Additional funding sounds like it would go towards reserves and additional researchers and programers, including a web developer, probably mainly continuing working on Factored Cognition.</p><p>Ought ask me to point out that they have applied for an OpenPhil grant renewal but expect to still have room for more funding afterwards.</p><h2><strong>AI Impacts</strong></h2><p><u><a href=\"https://aiimpacts.org/\">AI Impacts</a></u> is a small Berkeley-based group that does high-level strategy work, especially on AI timelines, somewhat associated with MIRI. </p><p>Adam Gleave, winner of the 2017 donor lottery, chose to give some money to AI Impacts; <u><a href=\"https://forum.effectivealtruism.org/posts/SYeJnv9vYzq9oQMbQ/2017-donor-lottery-report\">here</a></u> is his thought process. He was impressed with their work, although sceptical of their ability to scale.</p><h3>Research</h3><p>Carey wrote <u><a href=\"https://aiimpacts.org/interpreting-ai-compute-trends/\">Interpreting AI Compute Trends</a></u>, which argues that cutting-edge ML research projects have been getting dramatically more expensive. So much so that the trend will have to stop, suggesting that (one driver of) AI progress will slow down over the next 3.5-10 years. Additionally, he points out that we are also nearing the processing capacity (though not scanning capacity) required to model human brains. (Note that this was a guest post by Ryan, who works for FHI)</p><p>Grace&#x27;s <u><a href=\"https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/\">Likelihood of discontinuous progress around the development of AGI</a></u> discusses a 11 different arguments for AGI to have a discontinuous impact, and finds them generally unconvincing. This is important from a strategy point of view because it suggests we should have more time to see AGI coming, potentially also making it clear to sceptics. Overall I found the article clear and generally convincing.</p><p>McCaslin&#x27;s <u><a href=\"https://aiimpacts.org/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths/\">Transmitting fibers in the brain: Total length and distribution of lengths</a></u> analyses how much neural fibre there is in the human brain, and the distribution of long vs short. My understanding is this is related to how many neurons in human brains are dedicated to moving information around, rather than computation, which might be important because it is an additional form of capacity that is often overlooked when people talk about FLOPS and MIPS, and so might affect your estimates for when we have enough hardware capacity for neuromorphic AI. However, I might be misunderstanding, as I found the motivation a little unclear.</p><p>Grace&#x27;s <u><a href=\"https://aiimpacts.org/human-level-hardware-timeline/\">Human Level Hardware Timeline</a></u> attempts to estimate how long until we have human-level hardware at human cost. Largely based on earlier work, they estimate &quot;a 30% chance we are already past human-level hardware (at human cost), a 45% chance it occurs by 2040, and a 25% chance it occurs later.&quot;</p><p>They have gathered a collection of examples of discontinuous progress in history, to attempt to produce something of a reference class for how likely this is with AGI - see for example the <u><a href=\"https://aiimpacts.org/discontinuity-from-the-burj-khalifa/\">Burj Khalifa</a></u>, the <u><a href=\"https://aiimpacts.org/discontinuity-from-the-eiffel-tower/\">Eiffel Tower</a></u>, <u><a href=\"https://aiimpacts.org/discontinuity-in-altitude-records/\">rockets</a></u>. It would be nice to see how many possible examples they investigated and found were not discontinuous.</p><h3>Finances</h3><p>According to numbers they shared with me, AI Impacts spent around $90k in 2018 on two part-time employees. In 2019 they plan to significantly increase, to ~$360k and hire multiple new workers. They have just over $400k in current funding, suggesting a bit over a year of runway at this elevated rate, or many years at their 2018 rate.</p><p>Similar to GCRI, there is some risks that small groups may have a high productivity due to founder effects, and this might revert to the mean as they scale.</p><p>MIRI seems to administer their finances on their behalf; donations can be made <u><a href=\"https://aiimpacts.org/donate/\">here</a></u>.</p><h2><strong>Open AI</strong></h2><p><u><a href=\"https://openai.com/\">OpenAI</a></u> is a San Francisco based AGI startup charity, with a large focus on safety. It was founded in 2015 with money largely from Elon Musk.</p><h3>Research</h3><p>Christiano et al. &#x27;s <u><a href=\"https://arxiv.org/abs/1810.08575\">Supervising Strong Learners by Amplifying Weak Experts</a></u> lays out Paul&#x27;s amplification ideas in a paper - or at least one implementation of them. Basically the idea is that there are many problems where it is too expensive to produce training signals directly, so we will do so indirectly. We do this by iteratively breaking up the task into sub-tasks, using the agent to help with each sub-task, and then training the agent on the human&#x27;s overall judgement, aided by the agent&#x27;s output on the subtasks. Hopefully as the agent becomes strong it also gets better at the subtasks, improving the training set further. We also train a second agent to be able to predict good subtasks to go for, and to predict how the human will use the outputs from the subtasks. I&#x27;m not sure I understand why we don&#x27;t train the agent on its performance of the subtasks (except that it is expensive to evaluate there?) I think the paper might have been a bit clearer if it had included an example of the algorithm being used in practice with a human in the loop, rather than purely algorithmic examples. Hopefully this will come in the future. Nonetheless this was clearly a very important paper. Overall I thought this was an excellent paper.</p><p>Irving, Christiano and Amodei&#x27;s <u><a href=\"https://arxiv.org/abs/1805.00899\">AI Safety via Debate</a></u> explore adversarial &#x27;debate&#x27; between two or more advanced agents, competing to be judged the most helpful by a trusted but limited agent. This is very clever. It&#x27;s an extension of the grand Christiano project of trying to devise ways of amplifying simple, trusted agents (like humans) into more powerful ones - designing a system that takes advantage of our trust in the weak agent to ensure compliance in the stronger. Imagine we basically have a courtroom situation, where two highly advanced legal teams, with vast amounts of legal and forensic expertise, try to convince a simple but trusted agent (the jury) that they&#x27;re in the right. Each side is trying to make its &#x27;arguments&#x27; as simple as possible, and point out the flaws in the other&#x27;s. As long as refuting lies is easy relative to lying, honesty should be the best strategy... so agents constrained in this way will be honest, and not even try dishonesty! Like a courtroom where both legal teams decide to represent the same side. The paper contains some nice examples, including AlphaGo as an analogy and a neat MNIST simulation, and an interactive website. Overall I thought this was an excellent paper.</p><p><u><a href=\"https://blog.openai.com/openai-charter/\">The OpenAI Charter</a></u> is their statement of values with regard AGI research. It seems to contain the things you would want it to: benefit of all, fiduciary duty to humanity. Most interestingly, it also includes &quot; if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project. We will work out specifics in case-by-case agreements, but a typical triggering condition might be “a better-than-even chance of success in the next two years”&quot;, a clause which seems very sensible. Finally, it also notes that, like MIRI, they anticipate reducing their conventional publishing.</p><p>Amodei and Hernandez&#x27;s <u><a href=\"https://blog.openai.com/ai-and-compute/\">AI and Compute</a></u> attempts to quantify the computing power used for recent major AI developments like ResNets and AlphaGo. They find it has been doubling approximately every 3-4 months, dramatically faster than you would expect from Moore’s law – especially if you had been reading articles about the end of Moore’s law! This is due to a combination of the move to specialist hardware (initially GPUs, and now AI ASICs) and companies simply spending a lot more dollars. This is not a theory paper, but has direct relevance for timeline prediction and strategy that depends on whether or not there will be a hardware overhang. </p><p>Christiano&#x27;s <u><a href=\"https://ai-alignment.com/universality-and-security-amplification-551b314a3bab\">Universality and Security Amplification</a></u> describes how Amplification hopes to enhance security by protecting against adversarial inputs (attacks). The hope is that the process of breaking down queries into sub-queries that is at the heart of the Amplification idea can leave us with queries of sufficiently low complexity that they are human-secure. I&#x27;m not sure I really understood what this posts adds to others in Paul&#x27;s arsenal, mainly because I haven’t been following these as closely as perhaps I should have.</p><p>Researchers from OpenAI were also named as coauthors on:</p><ul><li>Hadfield-Menell and Hadfield&#x27;s <u><a href=\"https://arxiv.org/abs/1804.04268\">Incomplete Contracting and AI alignment</a></u> </li><li>Brundage et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1802.07228\">The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation</a></u> </li><li>Yudkowsky and Christiano&#x27;s <u><a href=\"https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal\">Challenges to Christiano&#x27;s Capability Amplification Proposal</a></u> </li></ul><h3>Finances </h3><p>Given the strong funding situation at OpenAI, as well as their safety team’s position within the larger organisations, I think it would be difficult for individual donations to appreciably support their work. However it could be an excellent place to apply to work.</p><h2><strong>Google Deepmind</strong></h2><p>As well as being arguably the most advanced AI research shop in the world, Google’s London-based <u><a href=\"https://deepmind.com/\">Deepmind </a></u>has a very sophisticated AI Safety team.</p><h3>Research</h3><p>Leike et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1711.09883\">AI Safety Gridworlds</a></u> introduces an open-source set of environments for testing ML algorithms for safetyness. Progress in ML has been considerable aided by the availability of common toolsets like MNIST or the Atari games. Here the Deepmind safety team have produced a set of environments designed to test algorithms ability to avoid a number of safety-related failure modes, like Interruptibility, Side Effects, Distributional Shifts and Reward Hacking. This hopefully not only makes such testing more accessible, it also makes these issues more concrete. Ideally it would shift the overton window: maybe one day it will be weird to read an ML paper that does not contain a section describing performance on the Deepmind Gridworlds. This is clearly not a panacea; it is easily to &#x27;fake&#x27; passing the test by giving the agent information it shouldn&#x27;t have, it is better to prove safety results than tack them on, and there is always a risk of Goodhearting. But this seems to me to be clearly a significant step forward. My enthusiasm is only slightly tempered by the fact that only one paper published in the following year citing the paper made use of the Gridworld suite, though Alex Turner&#x27;s excellent post on Impact measures did as well. Overall I thought this was an excellent paper. Researchers from ANU were also named authors on the paper.</p><p>Krakovna&#x27;s <u><a href=\"https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/\">Specification Gaming Examples in AI</a></u> provides a collection of different cases where agents have optimised their reward function in surprising/undesirable fashion. The spreadsheet of 45 examples might have some research value, but my guess is most of the value is as evidence of the problem.</p><p>Krakovna et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1806.01186\">Measuring and avoiding side effects using relative reachability</a></u> invents a new way of defining &#x27;impact&#x27;, which is important if you want to minimise it, based on how many states&#x27; achievability are affected. Essentially it takes some the set of possible states, and then punishes the agent for reducing the attainability of these states.  The post also includes a few simulations in the AI Gridworld.</p><p>Leike et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1811.07871\">Scalable agent alignment via reward modeling: a research direction</a></u> outlines the Deepmind agenda for bootstrapping human evaluations to provide feedback for RL agents. Similar in some ways to the Christiano project, the idea is that your main RL agent simultaneously learns its reward function and about the world. The human&#x27;s ability to provide good reward feedback is improved by training smaller agents who help him judge which rewards to provide. The paper goes into a number of potential familiar problems, and potential avenues of attack on those issues. I think the news here is more that the Deepmind (Safety) team is focusing on this, rather than the core ideas themselves. The paper also reviews a lot of related work.</p><p>Gasparik et al.&#x27;s <u><a href=\"https://deepmind.com/blog/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control/\">Safety-first AI for autonomous data centre cooling and industrial control</a></u> describes the mainly safety measures Google put in place to ensure their ML-driven datacenter cooling system didn&#x27;t go wrong. </p><p>Ibarz et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1811.06521\">Reward Learning from Human Preferences and Demonstrations in Atari</a></u> combines RL and IRL as two different sources of information for the agent. If you think both ideas have some value, it makes sense that combining them further improves performance.</p><p>Leibo et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1801.08116\">Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents</a></u> creates an environment for comparing humans and RL agents on the same tasks. Given the goal of getting AI agents to behave in ways humans approve of is closely related to the goal of making them behave like humans, this seems like a potentially useful tool.</p><p>Ortega et al.&#x27;s <u><a href=\"https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1\">Building safe artificial intelligence: specification, robustness and assurance</a></u>  provide an introduction to various problems in AI Safety. The content is unlikely to be new to readers here; it is significant insomuchas it represents a summary of the (worthwhile) priorities of Deepmind(&#x27;s safety team). They decompose the issue into specification, robustness and assurance.</p><p>Researcher’s from Deepmind were also named as coauthors on the following papers:</p><ul><li>Everitt et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1705.08417\">Reinforcement Learning with a Corrupted Reward Channel</a></u> </li><li>Rainforth et al.&#x27;s <u><a href=\"https://arxiv.org/abs/1802.04537\">Tighter Variational Bounds are Not Necessarily Better</a></u> </li><li>Ngo and Pace&#x27;s <u><a href=\"https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work\">Some cruxes on impactful alternatives to AI policy work</a></u> </li></ul><h3>Finances</h3><p>Being part of Google, I think it would be difficult for individual donors to directly support their work. However it could be an excellent place to apply to work.</p><h2><strong>Google Brain</strong></h2><p><u><a href=\"https://ai.google/research/teams/brain\">Google Brain</a></u> is Google’s other highly successful AI research group.</p><h3>Research</h3><p>Kurakin et al. wrote <u><a href=\"https://arxiv.org/pdf/1804.00097.pdf\">Adversarial Attacks and Defences Competition</a></u>, which summarises the NIPS 2017 competition on Adversarial Attacks, including many of the strategies used. If you&#x27;re not familiar with the area this could be a good introduction.</p><p>Brown and Olsson wrote <u><a href=\"https://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html\">Introducing the Unrestricted Adversarial Examples Challenge</a></u>, which launches a new 2-sided challenge, for designing systems resistant to adversarial examples, and then finding adversarial examples. The difference here is in allowing a much broader class of adversarial examples, rather than just small perturbations. This seems like a significantly more important class, so it is good they are attempting to move the field in this direction.</p><p>Gilmer et al. wrote <u><a href=\"https://arxiv.org/abs/1807.06732\">Motivating the Rules of the Game for Adversarial Example Research</a></u>, which argue that the adversarial example literature has overly-focused on a narrow class of imperceptibly-changed images. In most realistic cases the adversary has a much wider scope of possible attacks. Importantly for us, the general question is also more similar to the sorts of distributional shift issues that are likely to arise with AGI. To the extent this paper helps push researchers towards more relevant research it seems quite good.</p><h3>Finances</h3><p>Being part of Google, I think it would be difficult for individual donors to directly support their work. However it could be an excellent place to apply to work.</p><h2><strong>EAF / FRI: The Effective Altruism Foundation / Foundational Research Institute</strong></h2><p><u><a href=\"https://ea-foundation.org/\">EAF</a></u> is a German/Swiss group effective altruist group, lead by Jonas Vollmer and Stefan Torges, that undertakes a number of activities. They do research on a number of fundamental long-term issues, many related how to reduce the risks of very bad AGI outcomes, published through the <u><a href=\"https://ea-foundation.org/foundational-research-institute/\">Foundational Research Institute</a></u> (FRI). Their website suggests that FRI and WAS (<u><a href=\"https://was-research.org/\">Wild Animal Suffering</a></u>) are two equal sub-organisations, but apparently this is not the case - essentially everything EAF does is FRI now, and they just let WAS use their legal entity and donation interface. EAF also have Raising for Effective Giving, which encourages professional poker players to donate to effective charities, including MIRI.</p><p>In the past they have been rather negative utilitarian, which I have always viewed as an absurd and potentially dangerous doctrine. If you are interested in the subject I recommend <u><a href=\"http://www.amirrorclear.net/academic/ideas/negative-utilitarianism/\">Toby Ord’s piece on the subject</a></u>. However, they have produced research on why it is <u><a href=\"https://foundational-research.org/gains-from-trade-through-compromise/#What_about_moral_advocacy\">good to cooperate with other value systems</a></u>, making me somewhat less worried.</p><p>Research</p><p>Oesterheld&#x27;s <u><a href=\"https://casparoesterheld.files.wordpress.com/2017/12/rldt.pdf\">Approval-directed agency and the decision theory of Newcomb-like problems</a></u> analyses which decision theories are instantiated by RL agents. The paper analyses the structure of RL agents of various kinds and maps them mathematically to either Evidential or Causal Decision theory. Given how much we discuss decision theory it is surprising in retrospect that no-one (to my knowledge) had previously looked to see which ones our RL agents were actually instantiating. As such I found this an interesting paper.</p><p>Baumann&#x27;s <u><a href=\"https://foundational-research.org/using-surrogate-goals-deflect-threats/\">Using Surrogate Goals to Deflect Threats</a></u> discusses using a decoy utility function component as to protect against threats. The idea is that agents run the risk of counter-optimisation at the hands of an extortionist, but this could be protected against by redefining their utility function to add a pointless secondary goal (like avoiding the creation of a certain dimensioned platinum sphere). An opponent would find it easier to extort the agent by negatively optimising the surrogate goal. This doesn&#x27;t prevent the agent from giving in to the threats, but it does reduce the damage if the attacker has to follow-through on their threat. The paper discusses many additional details, including the multi-agent case, and the interaction between this and other defence mechanisms. My understanding is that they and Eliezer both (independently?) came up with this idea. One thing I didn&#x27;t quite understand is the notional of attacker-hostile surrogates - surely they would just be ignored? </p><p>Sotala and Gloor&#x27;s <u><a href=\"http://www.informatica.si/index.php/informatica/article/view/1877\">Superintelligence as a Cause or Cure for Risks of Astronomical Suffering</a></u> is a review article for the various ways the future might contain a lot of suffering. It does a good job of going through possibilities, though I felt it was overly focused on suffering as a bad outcome - there are many other bad things too!</p><p>Sotala&#x27;s <u><a href=\"https://www.lesswrong.com/posts/FkZCM4DMprtEp568s/shaping-economic-incentives-for-collaborative-agi\">Shaping economic incentives for collaborative AGI</a></u> argues that encouraging collaborative norms in AI with regard narrow AI will encourage those norms in the future for AGI due to cultural lock-in. Unfortunately it is not clear how to go about doing this. Researchers from FHI, were also named authors on the paper.</p><h3>Finances</h3><p>Based on their <u><a href=\"https://forum.effectivealtruism.org/posts/DkQyGkPBb9yePuPjQ/effective-altruism-foundation-plans-for-2019\">blog post</a></u>, they currently have around a year and a half’s worth of reserves, with a 2019 budget of $925,000.</p><p>As EAF have in the past worked on a variety of cause areas, donors might worry about fungibility. EAF tell me that they are now basically entirely focused on AI related work, and that WAS research is funded by specifically allocated donations, which would imply this is not a concern, though I note that several WAS people are still listed on their <u><a href=\"https://ea-foundation.org/team/\">team page</a></u>.</p><p>Readers who want to donate to EAF/FRI can do so <u><a href=\"https://ea-foundation.org/2018-fundraiser/\">here</a></u>.</p><h2><strong>Foresight Institute</strong></h2><p>The <u><a href=\"https://foresight.org/\">Foresight Institute</a></u> is a Palo-Alto based group focusing on AI and nanotechnology. Originally founded in 1986 (!), they seem to have been somewhat re-invigorated recently by Allison Duettmann. Unfortunately I haven’t had time to review them in detail.</p><p>A large part of their activity seems to be in organising ‘salon’ discussion / workshop events.</p><p>Duettmann et al.&#x27;s <u><a href=\"https://fs1-bb4c.kxcdn.com/wp-content/uploads/2018/11/AGI-Coordination-Geat-Powers-Report.pdf\">Artificial General Intelligence: Coordination and Great Powers</a></u> summarises the discussion at the 2018 Foresight Institute Strategy Meeting on AGI.  Researchers from FHI and FLI were also named authors on the paper.</p><p>Readers who want to donate to Foresight can do so <u><a href=\"https://foresight.org/donate/donate-or-sponsor/\">here</a></u>.</p><h2><strong>FLI: The Future of Life Institute</strong></h2><p>The Future of Life Institute was founded to do outreach, including run the<a href=\"http://futureoflife.org/2015/10/12/ai-safety-conference-in-puerto-rico/\"> <u>Puerto Rico conference</u></a>. Elon Musk donated $10m for the organisation to re-distribute; given the size of the donation it has rightfully come to somewhat dominate their activity. </p><p>In 2018 they ran a <u><a href=\"https://futureoflife.org/ai-safety-research/\">second grantmaking round</a></u>, giving $2m split between 10 different people. These grants were more focused on AGI than the previous round, which included a large number of narrow AI projects. In general the grants went to university professors. They have now awarded most of the $10m.</p><p>Unfortunately I haven’t had time to review them in detail.</p><p>Readers who want to donate to FLI can do so <u><a href=\"https://futureoflife.org/get-involved/\">here</a></u>.</p><h2><strong>Median Group</strong></h2><p>The <u><a href=\"http://mediangroup.org/\">Median Group </a></u>is a new group for research on global catastrophic risks, with researchers from MIRI, OpenPhil and Numerai. As a new group they lack the sort of track record that would make them easily amenable to analysis. Current projects they’re working on include AI timelines, forest fires, and climate change impacts on geopolitics.</p><p>I don’t know that much about them because the contact email listed on the website does not work.</p><h3>Research</h3><p>Taylor et al. wrote <u><a href=\"http://mediangroup.org/insights\">Insight-based AI timeline model</a></u>, which made an insight-based model for the time to AGI. They first produced a list of important insights that have (plausibly) contributed towards AGI. Surprisingly, they find there has been a roughly constant rate of insight production since 1945. They then model time-to-AGI using a pareto distribution for the number of insights required. This is a novel (to me, at least) method that I liked.</p><h2><strong>Convergence Analysis</strong></h2><p><u><a href=\"http://convergenceanalysis.org/\">Convergence Analysis</a></u> is a new group, lead by Justin Shovelain, aiming to do strategic work. They are too new to have any track record.</p><h2><strong>Other Research</strong></h2><p>I would like to emphasis that there is a lot of research I didn&#x27;t have time to review, especially in this section, as I focused on reading organisation-donation-relevant pieces. For example, Kosoy&#x27;s <a href=\"https://agentfoundations.org/item?id=1816\">The Learning-Theoretic AI Alignment Research Agenda</a> seems like a worthy contribution.  </p><h3>Papers </h3><p>Lipton and Steinhardt&#x27;s <u><a href=\"https://arxiv.org/abs/1807.03341\">Troubling Trends in Machine Learning Scholarship</a></u> critiques a number of developments in the ML literature that they think are bad. Basically, they argue that a lot of papers obfuscate explanation vs speculation, obscure the true source of improvement in their papers (often just hyper-parameter tuning), use maths to impress rather than clarify, and use common english words for complex terms, thereby smuggling in unnecessary connotations. It&#x27;s unclear to me, however, to what extent these issues retard progress on safety vs capabilities. I guess to the extent that safety requires clear understanding, whereas capabilities can be achieved in a more messy fashion, these trends are bad and should be pushed back ok.</p><p>Jilk&#x27;s <u><a href=\"http://www.informatica.si/index.php/informatica/article/view/1875\">Conceptual-Linguistic Superintelligence</a></u> discusses the need for AGI to have a conceptual-linguistic facility. Contra recent AI developments - e.g. AlphaZero does not have a linguistic ability - he argues that AIs will need linguistic ability to understand much of the human world. He also discusses the difficulties that Rice&#x27;s theorem imposes on AI self-improvement, though this has been well discussed before.</p><p>Cave and Ó hÉigeartaigh&#x27;s <u><a href=\"http://www.aies-conference.com/wp-content/papers/main/AIES_2018_paper_163.pdf\">An AI Race for Strategic Advantage: Rhetoric and Risks</a></u> argues that framing AI development as a &#x27;race&#x27;, or an &#x27;arms race&#x27;, is bad. Much of their reasoning is not new, and was previously published by e.g. Baum&#x27;s On the Promotion of Safe and Socially Beneficial Artificial Intelligence. Instead I think of the target audience here as being policymakers and other AI researchers: this is a paper aiming to influence global strategy, not research EA strategy. Having said that, their discussion of why we should actively confront AI race rhetoric, rather than trying to simply avoid it, was novel, at least to me. It also apparently won best paper at the AAAI/ACM conference on Artificial Intelligence, Ethics, and Society. Researchers from CSER were also named authors on the paper.</p><p>Liu et al.&#x27;s <u><a href=\"https://ieeexplore.ieee.org/document/8290925\">A Survey on Security Threats and Defensive Techniques of Machine Learning: A Data Driven View</a></u> reviews security threats to contemporary ML systems. This is basically addresses the concerns raised in Amodei et al.&#x27;s Concrete Problems about Distributional Shifts between training and test data, and how to ensure robustness.</p><p>Sarma and Hay&#x27;s <u><a href=\"https://arxiv.org/abs/1708.02553\">Robust Computer Algebra, Theorem Proving, and Oracle AI</a></u> discuss computer algorithm systems as potentially important classes of Oracles, and try to provide concrete safety-related work that could be done. Their overview of Question-Answering-Systems, Computer-Algebra-Systems and Interactive-Theorem-Provers was interesting to me, as I didn&#x27;t have much familiarity thereof. They argue that CAS use heuristics that lead to invalid inferences sometimes, while ITPs are very inefficient, and suggest projects to help integrate the two, to produce more reliable math oracles. I think of this paper as being a bit like a specialised version of Amodei et al&#x27;s Concrete Problems, but the connection between the projects here and the end goal of FAI is a little harder for me to grasp. Additionally, the paper seems to have been in development since 2013?</p><p>Manheim and Garrabrant&#x27;s <u><a href=\"https://arxiv.org/abs/1803.04585\">Categorizing Variants of Goodheart&#x27;s Law</a></u> classifies different types of situations where a proxy measures ceases to be a good proxy when you start relying on it. This is clearly an important topic for AI safety, insomuch as we are hoping to design AIs that will not fall victim to it. The paper provides a nice disambiguation of different kinds of situation, bringing conceptual clarity even if it&#x27;s not a deep mathematical result. Researchers from MIRI were also named authors on the paper.</p><p>Ngo and Pace&#x27;s <u><a href=\"https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work\">Some cruxes on impactful alternatives to AI policy work</a></u> discuss the advantages and disadvantages of AI policy work. They try to find the &#x27;crux&#x27; of their disagreement - the small number of statements they disagree about which determine which side of the issue they come down on. Researchers from Deepmind were also named authors on the paper.</p><p>Awad et al.&#x27;s <u><a href=\"https://www.nature.com/articles/s41586-018-0637-6\">The Moral Machine Experiment</a></u> did a massive online interactive survey of 35 *million* people to determine their moral preferences with regard autonomous cars. They found that people prefer: saving more people rather than fewer; saving humans over animals; saving young (including unborn children) over old; lawful people over criminals; executives over homeless; fit over fat; females over males; and pedestrians over passengers. I thought this was very interesting, and applaud them for actually looking for people&#x27;s moral intuitions, rather than just substituting the values of the programmers/politicians. They also analyse how these values differ between cultures. Overall I thought this was an excellent paper.<br/></p><p><figure><img src=\"https://lh3.googleusercontent.com/JwVsgl212szH1nHJYwhpZWW6IUmYJ-40w8xauisBiz59blFffHYI7df7SLEWqf9iOZoUxkb0wUu2NDXR1sy3k9kqtaMkRSCvqGHZkqEcfkQb2m-RRd13t61VG01lzpACDlBpRCVu\" class=\"draft-image \" style=\"width:624%\" /></figure></p><p>Green&#x27;s <u><a href=\"http://apcz.umk.pl/czasopisma/index.php/SetF/article/view/SetF.2018.015\">Ethical Reflections on Artificial Intelligence</a></u> reviews various ethical issues about AI from a christian perspective. Given the dominance of utilitarian thinking on the subject, it was nice to see an explicitly Christian contribution that displayed familiarity with the literature, with safety as #1 and #3 on the list of issues. &quot;therefore it must be the paramount goal of ethics to maintain human survival.&#x27;</p><p>Eth&#x27;s <u><a href=\"http://www.informatica.si/index.php/informatica/article/view/1874\">The Technological Landscape Affecting Artificial General Intelligence and the Importance of Nanoscale Neural Probes</a></u> presents arguments for favouring whole-brain-emulation as a pathway to human-level AI over de novo AGI, and suggests that nanoscale neural probe research could be a good way to differentially advance WBE vs merely human-inspired Neuromorphic AGI. The paper builds on a lot of arguments in Bostrom&#x27;s Superintelligence. It seems clear that neuromorphic AGI is undesirable - the question is between de novo and WBE, which unfortunately seem to have neuromorphic &#x27;in between&#x27; them from a technological requirement point of view. Daniel presents some good arguments for the relative safety of WBE (some of which were already in Bostrom), for example that WBEs would help provide training data from de novo AGI, though I was sceptical of the idea that the identity of the first WBEs would be determined by public debate. An especially good point was that even if nanoscale neural probes accelerate neuromorphic almost as much as WBEs, because the two human-inspired paths are closely linked and hence more likely to hit closer in time than de novo, neural probe research is more likely to cause WBE to overtake neuromorphic than neuromorphic to overtake de novo.</p><p>Turchin&#x27;s <u><a href=\"https://philpapers.org/rec/TURCSW\">Could slaughterbots wipe out humanity? Assessment of the global catastrophic risk posed by autonomous weapons</a></u>, provides a series of fermi-calculation like estimates of the danger posed by weaponised drones. He concludes that while they are very difficult to defend against, and their cost is coming down, it is unlikely they would be the driving force behind human extinction.</p><p>Bogosian&#x27;s <u><a href=\"https://link.springer.com/article/10.1007/s11023-017-9448-z\">Implementation of Moral Uncertainty in Intelligent Machines</a></u>, argues for using Will&#x27;s metanormativity approach to moral uncertainty as a way for addressing moral disagreement in AI design. I&#x27;m always glad to see more attention given to Will&#x27;s thesis, which I thought was very good, and the application to AI is an interesting one. I&#x27;m not quite sure how it would interact with a value-learning system - is the idea that the agent is updating all of its moral theories as new evidence comes in? Or that it has some value-learning approaches that are sharing credence with pre-programmed non-learning systems? I was a bit confused by his citing Greene (2001) as comparing the dispersion of issue and theory level disagreement on moral issues, but I don&#x27;t think this actually affects the conclusions of the paper at all, and am less concerned than Kyle is about the scaling properties of the algorithm. I also liked his prudential argument for why moral partisans should agree to this compromise, though I note that virtue ethicists, for whom the character of the agent (not merely the results) matters, may not be convinced. Finally, I think he actually understated the extent to which debates about decision procedures are less vicious than those about object-level issues, as virtually all the emotion about voting systems seems to be generated by object-level partisans who believe that changing the voting system will help them achieve their object-level political goals.</p><p>rk and Sempere&#x27;s <u><a href=\"https://www.lesswrong.com/posts/bkG4qj9BFEkNva3EX/ai-development-incentive-gradients-are-not-uniformly\">AI development incentive gradients are not uniformly terrible</a></u> argue that the &#x27;openness is bad&#x27; conclusion from Armstrong et al&#x27;s Racing to the Precipice is basically because of the discontinuity in success probability in their model. This seems true to me, and reduced my credence that openness was bad. Researchers from FHI were also named authors on the paper.</p><p>Liu et al.&#x27;s <u><a href=\"https://www.sciencedirect.com/science/article/pii/S0016328717301623\">Governing Boring Apocalypses: A new typology of existential vulnerabilities and exposures for existential risk research</a></u> discusses the broad risk landscape. They provide a number of breakdowns of possible risks, including many non-AI. I think the main use is the relatively policymaker-friendly framing.</p><p>Bansal and Weld&#x27;s <u><a href=\"https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17110\">A Coverage-Based Utility Model for Identifying Unknown Unknowns</a></u> design a model for efficiently utilising a scarce human expert to discover false-positive regions. </p><p>Dai&#x27;s <u><a href=\"https://www.lesswrong.com/posts/idb5Ppp9zghcichJ5/a-general-model-of-safety-oriented-ai-development\">A general model of safety-oriented AI development</a></u> provides a very brief generalisation of the sort of inductive strategies for AI safety I had been referring to as &#x27;Christiano-like&#x27; <br/></p><h3>Books</h3><p>Roman Yampolskiy edited a 500-page anthology on AI Safety, available for purchase <u><a href=\"https://www.amazon.com/Artificial-Intelligence-Security-Chapman-Robotics/dp/0815369824\">here</a></u>. Unfortunately I haven’t had time to read every article; <u><a href=\"https://www.lesswrong.com/posts/a4tcqr7QBAgMHLbcz/book-review-ai-safety-and-security\">here</a></u> is a review by someone who has.</p><p>The first half of the book, Concerns of Luminaries, is basically re-prints of older articles. As such readers will probably mainly be interested in the second half, which I think are all original to this volume.</p><h1><strong>Misc other news</strong></h1><p>OpenPhil<a href=\"https://www.openphilanthropy.org/giving/grants/centre-for-effective-altruism-new-discretionary-fund\"> <u>gave Carl Shulman $5m to re-grant</u></a>, of which some seems likely to end up funding useful AI safety work. Given Carl’s intellect and expertise this seems like a good use of money to me.</p><p>OpenPhil are also funding seven ML PhD students ($1.1m over five years) through their<a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/announcing-2018-ai-fellows\"> <u>‘AI Fellows’ program</u></a>. I have read their published research and some of it seems quite interesting – I found Noam’s<a href=\"https://arxiv.org/abs/1705.02955\"> <u>Safe and Nested Subgame Solving for Imperfect-Information Games</u></a> particularly interesting, partly as I didn’t have much prior familiarity with the subject. Most of their work thus far does not seem very AI Safety relevant, with some exceptions like this blog post by<a href=\"http://www.foldl.me/2018/conceptual-issues-ai-safety-paradigmatic-gap/\"> <u>Jon Gauthier</u></a>. But given the timeline for academic work and the mid-year announcement of the fellowships I think it’s probably too early to see if they will produce any AI Safety relevant work.</p><p>If you like podcasts, you might enjoy these 80,000 Hours podcasts. If not, they all have complete transcripts.</p><ul><li><u><a href=\"https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/\">Paul Christiano</a></u></li><li><u><a href=\"https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/\">Hilary Greaves</a></u></li><li><u><a href=\"https://80000hours.org/podcast/episodes/jan-leike-ml-alignment/\">Jan Leike</a></u></li><li><u><a href=\"https://80000hours.org/podcast/episodes/allan-dafoe-politics-of-ai/\">Alan Dafoe</a></u></li></ul><p>80,000 Hours also wrote a guide on<a href=\"https://80000hours.org/articles/ml-engineering-career-transition-guide/\"> <u>how to transition from programming or CS into ML</u></a>.</p><p>Last year I mentioned that EA Long Term Future Fund did not seem to be actually making grants. After a series of criticism on the EA forum by<a href=\"http://effective-altruism.com/ea/1k9/ea_funds_hands_out_money_very_infrequently_should/\"> <u>Henry Stanley</u></a> and<a href=\"http://effective-altruism.com/ea/1qx/the_ea_community_and_far_future_ea_funds_are_not/\"> <u>Evan Gaensbauer</u></a>, CEA has now<a href=\"http://effective-altruism.com/ea/1uu/announcing_new_ea_funds_management_teams/\"> <u>changed the management of the funds and committed to a regular series of grantmaking</u></a>. However, I’m skeptical this will solve the underlying problem. Presumably they organically came across plenty of possible grants – if this was truly a ‘lower barrier to giving’ vehicle than OpenPhil they would have just made those grants. It is possible, however, that more managers will help them find more non-controversial ideas to fund. <a href=\"https://app.effectivealtruism.org/funds/far-future/payouts/3JnNTzhJQsu4yQAYcKceSi\">Here</a> is a link to their recent grants round.</p><p>If you’re reading this, you probably already read SlateStarCodex. If not, you might enjoy <u><a href=\"http://slatestarcodex.com/2018/01/15/maybe-the-real-superintelligent-ai-is-extremely-smart-computers/\">this article</a></u> he wrote this year about AI Safety.</p><p>In an early proof of the viability of cryonics,<a href=\"https://www.lesswrong.com/\"> <u>LessWrong</u></a> has been brought back to life. If like me you find the new interface confusing you can view it through<a href=\"https://www.greaterwrong.com/\"> <u>GreaterWrong</u></a>. Relatedly there is integration with the<a href=\"https://www.alignmentforum.org/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq\"> <u>Alignment Forum</u></a>, to provide a place for discussion of AI Alignment issues that is linked to LessWrong. This seems rather clever to me.</p><p>Zvi Mowshowitz and Vladimir Slepnev have been organizing<a href=\"https://www.lesswrong.com/posts/YDLGLnzJTKMEtti7Z/announcing-the-ai-alignment-prize\"> <u>a</u></a><a href=\"https://www.lesswrong.com/posts/SSEyiHaACSYDHcYZz/announcement-ai-alignment-prize-round-2-winners-and-next\"> <u>series</u></a><a href=\"https://www.lesswrong.com/posts/juBRTuE3TLti5yB35/announcement-ai-alignment-prize-round-3-winners-and-next\"> <u>of</u></a> AI Safety prizes, giving out money for the articles they were most impressed with in a certain time frame.</p><p>Deepmind’s work on <u><a href=\"https://deepmind.com/blog/alphafold/\">Protein Folding</a></u> proved quite successful, winning the big annual competition by a significant margin. This seemed significant to me mainly because ‘solving the protein folding problem’ has been one of the prototypical steps between ‘recursively self-improving AI’ and ‘singleton’ since at least 2001.</p><p>Berkley offered a<a href=\"http://inst.eecs.berkeley.edu/~cs294-149/fa18/\"> <u>graduate-level course in AGI Safety</u></a>.</p><p><u><a href=\"https://vast.ai/console/create/\">Vast.ai</a></u> are attempting to create a two-sided marketplace where you can buy or sell idle GPU capacity. This seems like the sort of thing that probably will not succeed, but if something like it did that’s another piece of evidence for hardware overhang.</p><p>The US department of commerce suggested an<a href=\"https://qz.com/1469008/the-us-could-regulate-ai-in-the-name-of-national-security/\"> <u>ban on AI exports</u></a>, presumably inspired by previous bans on cryptography exports.</p><h1><strong>Conclusions</strong></h1><p>The size of the field continues to grow, both in terms of funding and researchers. Both make it increasingly hard for individual donors.</p><p>As I have once again failed to reduce charity selection to a science, I’ve instead attempted to subjectively weigh the productivity of the different organisations against the resources they used to generate that output, and donate accordingly.</p><p>My constant wish is to promote a lively intellect and independent decision-making among my readers; hopefully my laying out the facts as I see them above will prove helpful to some readers. Here is my eventual decision,<a href=\"http://www.rot13.com/\"> rot13&#x27;d</a> so you can do come to your own conclusions first if you wish:</p><p> </p><p>Qrfcvgr univat qbangrq gb ZVEV pbafvfgragyl sbe znal lrnef nf n erfhyg bs gurve uvtuyl aba-ercynprnoyr naq tebhaqoernxvat jbex va gur svryq, V pnaabg va tbbq snvgu qb fb guvf lrne tvira gurve ynpx bs qvfpybfher. Nqqvgvbanyyl, gurl nyernql unir n ynetre ohqtrg guna nal bgure betnavfngvba (rkprcg creuncf SUV) naq n ynetr nzbhag bs erfreirf.</p><p>Qrfcvgr SUV cebqhpvat irel uvtu dhnyvgl erfrnepu, TCV univat n ybg bs cebzvfvat cncref va gur cvcryvar, naq obgu univat uvtuyl dhnyvsvrq naq inyhr-nyvtarq erfrnepuref, gur erdhverzrag gb cer-shaq erfrnepuref’ ragver pbagenpg fvtavsvpnagyl vapernfrf gur rssrpgvir pbfg bs shaqvat erfrnepu gurer. Ba gur bgure unaq, uvevat crbcyr va gur onl nern vfa’g purnc rvgure.</p><p>Guvf vf gur svefg lrne V unir nggrzcgrq gb erivrj PUNV va qrgnvy naq V unir orra vzcerffrq jvgu gur dhnyvgl naq ibyhzr bs gurve jbex. V nyfb guvax gurl unir zber ebbz sbe shaqvat guna SUV. Nf fhpu V jvyy or qbangvat fbzr zbarl gb PUNV guvf lrne.</p><p>V guvax bs PFRE naq TPEV nf orvat eryngviryl pbzcnenoyr betnavfngvbaf, nf 1) gurl obgu jbex ba n inevrgl bs rkvfgragvny evfxf naq 2) obgu cevznevyl cebqhpr fgengrtl cvrprf. Va guvf pbzcnevfba V guvax TPEV ybbxf fvtavsvpnagyl orggre; vg vf abg pyrne gurve gbgny bhgchg, nyy guvatf pbafvqrerq, vf yrff guna PFRE’f, ohg gurl unir qbar fb ba n qenzngvpnyyl fznyyre ohqtrg. Nf fhpu V jvyy or qbangvat fbzr zbarl gb TPEV ntnva guvf lrne.</p><p>NAH, Qrrczvaq naq BcraNV unir nyy qbar tbbq jbex ohg V qba’g guvax vg vf ivnoyr sbe (eryngviryl) fznyy vaqvivqhny qbabef gb zrnavatshyyl fhccbeg gurve jbex. </p><p>Bhtug frrzf yvxr n irel inyhnoyr cebwrpg, naq V nz gbea ba qbangvat, ohg V guvax gurve arrq sbe nqqvgvbany shaqvat vf fyvtugyl yrff guna fbzr bgure tebhcf.</p><p>NV Vzcnpgf vf va znal jnlf va n fvzvyne cbfvgvba gb TPEV, jvgu gur rkprcgvba gung TPEV vf nggrzcgvat gb fpnyr ol uvevat vgf cneg-gvzr jbexref gb shyy-gvzr, juvyr NV Vzcnpgf vf fpnyvat ol uvevat arj crbcyr. Gur sbezre vf fvtavsvpnagyl ybjre evfx, naq NV Vzcnpgf frrzf gb unir rabhtu zbarl gb gel bhg gur hcfvmvat sbe 2019 naljnl. Nf fhpu V qb abg cyna gb qbangr gb NV Vzcnpgf guvf lrne, ohg vs gurl ner noyr gb fpnyr rssrpgviryl V zvtug jryy qb fb va 2019. </p><p>Gur Sbhaqngvbany Erfrnepu Vafgvghgr unir qbar fbzr irel vagrerfgvat jbex, ohg frrz gb or nqrdhngryl shaqrq, naq V nz fbzrjung zber pbaprearq nobhg gur qnatre bs evfxl havyngreny npgvba urer guna jvgu bgure betnavfngvbaf. </p><p>V unira’g unq gvzr gb rinyhngr gur Sberfvtug Vafgvghgr, juvpu vf n funzr orpnhfr ng gurve fznyy fvmr znetvany shaqvat pbhyq or irel inyhnoyr vs gurl ner va snpg qbvat hfrshy jbex. Fvzvyneyl, Zrqvna naq Pbairetrapr frrz gbb arj gb ernyyl rinyhngr, gubhtu V jvfu gurz jryy.</p><p>Gur Shgher bs Yvsr vafgvghgr tenagf sbe guvf lrne frrz zber inyhnoyr gb zr guna gur cerivbhf ongpu, ba nirentr. Ubjrire, V cersre gb qverpgyl rinyhngr jurer gb qbangr, engure guna bhgfbhepvat guvf qrpvfvba.</p><p>V nyfb cyna gb fgneg znxvat qbangvbaf gb vaqvivqhny erfrnepuref, ba n ergebfcrpgvir onfvf, sbe qbvat hfrshy jbex. Gur pheerag fvghngvba, jvgu n ovanel rzcyblrq/abg-rzcyblrq qvfgvapgvba, naq hcsebag cnlzrag sbe hapregnva bhgchg, frrzf fhobcgvzny. V nyfb ubcr gb fvtavsvpnagyl erqhpr bireurnq (sbe rirelbar ohg zr) ol abg univat na nccyvpngvba cebprff be nal erdhverzragf sbe tenagrrf orlbaq univat cebqhprq tbbq jbex. Guvf jbhyq or fbzrjung fvzvyne gb <u><a href=\"https://impactpurchase.org/\">Vzcnpg Pregvsvpngrf</a></u>, juvyr ubcrshyyl nibvqvat fbzr bs gurve vffhrf. </p><p>However I wish to emphasis that all the above organisations seem to be doing good work on the most important issue facing mankind. It is the nature of making decisions under scarcity that we must prioritize some over others, and I hope that all organisations will understand that this necessarily involves negative comparisons at times.</p><p>Thanks for reading this far; hopefully you found it useful. Apologies to everyone who did valuable work that I excluded; I have no excuse other than procrastination, Crusader Kings II, and a starting work at a new hedge fund.</p><h1><strong>Disclosures</strong></h1><p>I have not in general checked all the proofs in these papers, and similarly trust that researchers have honestly reported the results of their simulations.</p><p>I was a Summer Fellow at MIRI back when it was SIAI, volunteered briefly at GWWC (part of CEA) and previously applied for a job at FHI. I am personal friends with people at MIRI, FHI, CSER, CHAI, GPI, BERI, OpenAI, Deepmind, Ought and AI Impacts but not really at ANU, EAF/FRI, GCRI, Google Brain, Foresight, FLI, Median, Convergence (so if you’re worried about bias you should overweight them… though it also means I have less direct knowledge) (also sorry if I’ve forgotten any friends who work for the latter set!). However I have no financial ties beyond being a donor and have never been romantically involved with anyone who has ever been at any of the organisations.</p><p>I shared drafts of the individual organisation sections with representatives from MIRI, FHI, CHAI, CSER, GCRI, GPI, BERI, Ought, AI Impacts, and EAF/FRI.</p><p>I’d like to thank Greg Lewis and my anonymous reviewers for looking over this. Any remaining mistakes are of course my own. I would also like to thank my wife for tolerating all the time I have invested/wasted on this.</p><p> <em>EDIT: Removed language about BERI, at their request.</em> </p><h1><strong>Sources</strong></h1><p>Amodei, Dario and Hernandez, Danny - AI and Compute - 2018-05-16 - https://blog.openai.com/ai-and-compute/</p><p>Armstrong, Stuart; O&#x27;Rourke, Xavier - ‘Indifference’ methods for managing agent rewards - 2018-01-05 - https://arxiv.org/pdf/1712.06365.pdf</p><p>Armstrong, Stuart; O&#x27;Rourke, Xavier - Safe Uses of AI Oracles - 2018-06-05 - https://arxiv.org/pdf/1711.05541.pdf</p><p>Armstrong, Stuart; Soren, Mindermann - Impossibility of deducing preferences and rationality from human policy - 2017-12-05 - https://arxiv.org/abs/1712.05812</p><p>Avin, Shahar; Wintle, Bonnie; Weitzdorfer, Julius;  Ó hÉigeartaigh, Seán; Sutherland, William; Rees, Martin - Classifying Global Catastrophic Risks - 2018-02-23 - https://www.sciencedirect.com/science/article/pii/S0016328717301957#tbl0010</p><p>Awad, Edmond; Dsouza, Sohan; Kim, Richard; Schulz, Jonathan; Henrich, Joseph; Shariff, Azim; Bonnefon, Jean-Francois; Rahwan, Iyad - The Moral Machine Experiment - 2018-10-24 - https://www.nature.com/articles/s41586-018-0637-6</p><p>Bansal, Gagan; Weld, Daniel - A Coverage-Based Utility Model for Identifying Unknown Unknowns - 2018-04-25 - https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17110</p><p>Basu, Chandrayee; Yang, Qian; Hungerman, David; Mukesh, Singhal; Dragan, Anca - Do You Want Your Autonomous Car to Drive Like You? - 2018-02-05 - </p><p>Batin, Mikhail; Turchin, Alexey; Markov, Sergey; Zhila, Alisa; Denkenberger, David - Artificial Intelligence in Life Extension: from Deep Learning to Superintelligence - 2017-08-31 - http://www.informatica.si/index.php/informatica/article/view/1797</p><p>Baum, Seth - Countering Superintelligence Misinformation - 2018-09-09 - https://www.mdpi.com/2078-2489/9/10/244</p><p>Baum, Seth - Resilience to Global Catastrophe - 2018-11-29 - https://irgc.epfl.ch/wp-content/uploads/2018/11/Baum-for-IRGC-Resilience-Guide-Vol-2-2018.pdf</p><p>Baum, Seth - Superintelligence Skepticism as a Political Tool - 2018-08-22 - https://www.mdpi.com/2078-2489/9/9/209</p><p>Baum, Seth - Uncertain Human Consequences in Asteroid Risk Analysis and the Global Catastrophe Threshold - 2018-07-28 - https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3218342</p><p>Baum, Seth; Armstrong, Stuart; Ekenstedt, Timoteus; Haggstrom, Olle; Hanson, Robin; Kuhlemann, Karin; Maas, Matthijs; Miller, James; Salmela, Markus; Sandberg, Anders; Sotala, Kaj; Torres, Phil; Turchi, Alexey; Yampolskiy, Roman - Long-Term Trajectories of Human Civilization - 2018-08-08 - http://gcrinstitute.org/papers/trajectories.pdf</p><p>Baum, Seth; Barrett, Anthony - A Model for the Impacts of Nuclear War - 2018-04-03 - https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3155983</p><p>Baum, Seth; Barrett, Anthony; Yampolskiy, Roman - Modelling and Interpreting Expert Disagreement about Artificial Intelligence - 2018-01-27 - https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3104645</p><p>Baum, Seth; Neufville, Robert; Barrett, Anthony - A Model for the Probability of Nuclear War - 2018-03-08 - https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3137081</p><p>Baumann, Tobias - Using Surrogate Goals to Deflect Threats - 2018-02-20 - https://foundational-research.org/using-surrogate-goals-deflect-threats/</p><p>Becker, Gary - Crime and Punishment: An Economic Approach - 1974-01-01 - https://www.nber.org/chapters/c3625.pdf</p><p>Bekdash, Gus - Using Human History, Psycology and Biology to Make AI Safe for Humans - 2018-04-01 - </p><p>Berberich, Nicolas; Diepold, Klaus - The Virtuous Machine - Old Ethics for New Technology - 2018-06-27 - https://arxiv.org/abs/1806.10322</p><p>Blake, Andrew; Bordallo, Alejandro; Hawasly, Majd; Penkov, Svetlin; Ramamoorthy, Subramanian; Silva, Alexandre  - Efficient Computation of Collision Probabilities for Safe Motion Planning - 2018-04-15 - https://arxiv.org/abs/1804.05384</p><p>Bogosian, Kyle - Implementation of Moral Uncertainty in Intelligent Machines - 2017-12-01 - https://link.springer.com/article/10.1007/s11023-017-9448-z</p><p>Bostrom, Nick - The Vulnerable World Hypothesis - 2018-11-09 - https://nickbostrom.com/papers/vulnerable.pdf</p><p>Brown, Noam; Sandholm, Tuomas - Safe and Nested Subgame Solving for Imperfect-Information Games - 2017-05-08 - https://arxiv.org/abs/1705.02955</p><p>Brown, Noam; Sandholm, Tuomas - Solving Imperfect-Information Games via Discounted Regret Minimization - 2018-09-11 - https://arxiv.org/abs/1809.04040</p><p>Brown, Tom; Olsson, Catherine; Google Brain Team, Research Engineers - Introducing the Unrestircted Adversarial Examples Challenge - 2018-09-03 - https://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html</p><p>Carey, Ryan - Interpreting AI Compute Trends - 2018-07-10 - https://aiimpacts.org/interpreting-ai-compute-trends/</p><p>Cave, Stephen; Ó hÉigeartaigh, Seán  - An AI Race for Strategic Advantage: Rhetoric and Risks - 2018-01-16 - http://www.aies-conference.com/wp-content/papers/main/AIES_2018_paper_163.pdf</p><p>Christiano, Paul - Techniques for Optimizing Worst-Case Performance - 2018-02-01 - https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99</p><p>Christiano, Paul - Universality and Security Amplification - 2018-03-10 - https://ai-alignment.com/universality-and-security-amplification-551b314a3bab</p><p>Christiano, Paul; Shlegeris, Buck; Amodei, Dario - Supervising Strong Learners by Amplifying Weak Experts - 2018-10-19 - https://arxiv.org/abs/1810.08575</p><p>Cohen, Michael; Vellambi, Badri; Hutter, Marcus - Algorithm for Aligned Artificial General Intelligence - 2018-05-25 - https://cs.anu.edu.au/courses/CSPROJECTS/18S1/reports/u6357432.pdf</p><p>Cundy, Chris; Filan, Daniel - Exploring Hierarchy-Aware Inverse Reinforcement Learning - 2018-07-13 - https://arxiv.org/abs/1807.05037</p><p>Currie, Adrian - Existential Risk, Creativity &amp; Well-Adapted Science - 2018-07-22 - http://philsci-archive.pitt.edu/14800/</p><p>Currie, Adrian - Geoengineering Tensions - 2018-04-30 - http://philsci-archive.pitt.edu/14607/</p><p>Currie, Adrian - Introduction: Creativity, Conservatism &amp; the Social Epistemology of Science - 2018-09-27 - http://philsci-archive.pitt.edu/15066/</p><p>Currie, Adrian; Ó hÉigeartaigh, Seán - Working together to face humanity&#x27;s greatest threats: Introduction to The Future of Research on Catastrophic and Existential Risk - 2018-03-26 - <u><a href=\"https://www.dropbox.com/s/bh6okdz8pvrxzc6/Working%20together%20to%20face%20humanity%E2%80%99s%20greatest%20threats%20preprint.pdf?dl=0\">https://www.dropbox.com/s/bh6okdz8pvrxzc6/Working%20together%20to%20face%20humanity%E2%80%99s%20greatest%20threats%20preprint.pdf?dl=0</a></u></p><p>Dafoe, Allen - AI Governance: A Research Agenda - 2018-08-27 - https://www.fhi.ox.ac.uk/wp-content/uploads/GovAIAgenda.pdf</p><p>Dai, Wei - A general model of safety-oriented AI development - 2018-06-11 - <u><a href=\"https://www.lesswrong.com/posts/idb5Ppp9zghcichJ5/a-general-model-of-safety-oriented-ai-development\">https://www.lesswrong.com/posts/idb5Ppp9zghcichJ5/a-general-model-of-safety-oriented-ai-development</a></u></p><p>Demski, Abram - An Untrollable Mathematician Illustrated - 2018-03-19 - https://www.lesswrong.com/posts/CvKnhXTu9BPcdKE4W/an-untrollable-mathematician-illustrated</p><p>DeVries, Terrance; Taylor, Graham - Leveraging Uncertainty Estimates for Predicting Segmentation Quality - 2018-07-02 - https://arxiv.org/abs/1807.00502</p><p>Dobbe, Roel; Dean, Sarah; Gilbert, Thomas; Kohli, Nitin - A Broader View on Bias in Automated Decision-Making: Reflecting on Epistemology and Dynamics - 2018-07-06 - https://arxiv.org/abs/1807.00553</p><p>Doshi-Velez, Finale; Kim, Been - Considerations for Evaluation and Generalization in Interpretable Machine Learning - 2018-08-24 - https://finale.seas.harvard.edu/publications/considerations-evaluation-and-generalization-interpretable-machine-learning</p><p>Duettmann, Allison; Afanasjeva, Olga; Armstrong, Stuart; Braley, Ryan; Cussins, Jessica; Ding, Jeffrey; Eckersley, Peter; Guan, Melody; Vance, Alyssa; Yampolskiy, Roman - Artificial General Intelligence: Coordination and Great Powers - 1900-01-00 - https://fs1-bb4c.kxcdn.com/wp-content/uploads/2018/11/AGI-Coordination-Geat-Powers-Report.pdf</p><p>Erdelyi, Olivia ; Goldsmith, Judy - Regulating Artificial Intelligence: Proposal for a Global Solution - 2018-02-01 - http://www.aies-conference.com/wp-content/papers/main/AIES_2018_paper_13.pdf</p><p>Eth, Daniel - The Technological Landscape Affecting Artificial General Intelligence and the Importance of Nanoscale Neural Probes - 2017-08-31 - http://www.informatica.si/index.php/informatica/article/view/1874</p><p>Evans, Owain; Stuhlmuller, Andreas; Cundy, Chris; Carey, Ryan; Kenton, Zachary; McGrath, Thomas; Schreiber, Andrew - Predicting Human Deliberative Judgments with Machine Learning - 2018-07-13 - https://ought.org/papers/predicting-judgments-tr2018.pdf</p><p>Everitt, Tom; Krakovna, Victoria; Orseau, Laurent; Hutter, Marcus; Legg, Shane - Reinforcement Learning with a Corrupted Reward Channel - 2017-05-23 - https://arxiv.org/abs/1705.08417</p><p>Everitt, Tom; Lea, Gary; Hutter, Marcus - AGI Safety Literature Review - 2018-05-22 - AGI Safety Literature Review</p><p>Filan, Daniel - Bottle Caps aren&#x27;t Optimisers - 2018-11-21 - https://www.greaterwrong.com/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers</p><p>Fisac, Jaime; Bajcsy, Andrea; Herbert, Sylvia; Fridovich-Keil, David; Wang, Steven; Tomlin, Claire; Dragan, Anca - Probabilistically Safe Robot Planning with Confidence-Based Human Predictions - 2018-05-31 - https://arxiv.org/abs/1806.00109</p><p>Garnelo, Marta; Rosenbaum, Dan; Maddison, Chris; Ramalho, Tiago; Saxton, David; Shanahan, Murray; The, Yee Whye; Rezende, Danilo; Eslami, S M Ali - Conditional Neural Processes - 2018-07-04 - </p><p>Garrabrant, Scott; Demski, Abram - Embedded Agency Sequence - 2018-10-29 - https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh</p><p>Gasparik, Amanda; Gamble, Chris; Gao, Jim - Safety-first AI for autonomous data centre cooling and industrial control - 2018-08-17 - https://deepmind.com/blog/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control/</p><p>Gauthier, Jon; Ivanova, Anna - Does the brain represent words? An evaluation of brain decoding studies of language understanding - 2018-06-02 - https://arxiv.org/abs/1806.00591</p><p>Ghosh, Shromona; Berkenkamp, Felix; Ranade, Gireeja; Qadeer, Shaz; Kapoor, Ashish - Verifying Controllers Against Adversarial Examples with Bayesian Optimization - 2018-02-26 - https://arxiv.org/abs/1802.08678</p><p>Gilmer, Justin; Adams, Ryan; Goodfellow, Ian; Andersen, David, Dahl, George - Motivating the Rules of the Game for Adversarial Example Research - 2018-07-20 - https://arxiv.org/abs/1807.06732</p><p>Grace, Katja - Human Level Hardware Timeline - 2017-12-22 - https://aiimpacts.org/human-level-hardware-timeline/</p><p>Grace, Katja - Likelihood of discontinuous progress around the development of AGI - 2018-02-23 - https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/</p><p>Green, Brian Patrick - Ethical Reflections on Artificial Intelligence - 2018-06-01 - http://apcz.umk.pl/czasopisma/index.php/SetF/article/view/SetF.2018.015</p><p>Hadfield-Menell, Dylan; Andrus, McKane; Hadfield, Gillian - Legible Normativity for AI Alignment: The Value of Silly Rules - 2018-11-03 - <u><a href=\"https://arxiv.org/abs/1811.01267\">https://arxiv.org/abs/1811.01267</a></u></p><p>Hadfield-Menell, Dylan; Hadfield, Gillian - Incomplete Contracting and AI alignment - 2018-04-12 - https://arxiv.org/abs/1804.04268</p><p>Haqq-Misra, Jacob - Policy Options for the radio Detectability of Earth - 2018-04-02 - https://arxiv.org/abs/1804.01885</p><p>Hoang, Lê Nguyên - A Roadmap for the Value-Loading Problem - 2018-09-04 - https://arxiv.org/abs/1809.01036</p><p>Huang, Jessie; Wu, Fa; Precup, Doina; Cai, Yang - Learning Safe Policies with Expert Guidance - 2018-05-21 - https://arxiv.org/abs/1805.08313</p><p>Ibarz, Borja; Leike, Jan; Pohlen, Tobias; Irving, Geoffrey; Legg, Shane; Amodei, Dario - Reward Learning from Human Preferences and Demonstrations in Atari - 2018-11-15 - https://arxiv.org/abs/1811.06521</p><p>IBM - Bias in AI: How we Build Fair AI Systems and Less-Biased Humans - 2018-02-01 - https://www.ibm.com/blogs/policy/bias-in-ai/</p><p>Irving, Geoffrey; Christiano, Paul; Amodei, Dario - AI Safety via Debate - 2018-05-02 - https://arxiv.org/abs/1805.00899</p><p>Janner, Michael; Wu, Jiajun; Kulkarni, Tejas; Yildirim, Ilker; Tenenbaum, Joshua - Self-Supervised Intrinsic Image Decomposition - 2018-02-05 - https://arxiv.org/abs/1711.03678</p><p>Jilk, David - Conceptual-Linguistic Superintelligence - 2017-07-31 - http://www.informatica.si/index.php/informatica/article/view/1875</p><p>Jones, Natalie; O’Brien, Mark; Ryan, Thomas - Representation of future generations in United Kingdom policy-making - 2018-03-26 - <u><a href=\"https://www.sciencedirect.com/science/article/pii/S0016328717301179\">https://www.sciencedirect.com/science/article/pii/S0016328717301179</a></u></p><p>Koller, Torsten; Berkenkamp, Felix; Turchetta, Matteo; Krause, Andreas - Learning-based Model Predictive Control for Safe Exploration - 2018-09-22 - https://arxiv.org/abs/1803.08287</p><p>Krakovna, Victoria - Specification Gaming Examples in AI - 2018-04-02 - https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/</p><p>Krakovna, Victoria; Orseau, Laurent; Martic, Miljan; Legg, Shane - Measuring and avoiding side effects using relative reachability - 2018-06-04 - https://arxiv.org/abs/1806.01186</p><p>Kurakin, Alexey; Goodfellow, Ian; Bengio, Samy; Dong, Yinpeng; Liao, Fangzhou; Liang, Ming; Pang, Tianyu ; Zhu, Jun; Hu, Xiaolin; Xie, Cihang; Wang, Jianyu; Zhang, Zhishuai; Ren, Zhou; Yuille, Alan; Huang, Sangxia; Zhao, Yao; Zhao, Yuzhe; Han, Zhonglin; Long, Junjiajia; Berdibekov, Yerkebulan; Akiba, Takuya; Tokui, Seiya; Abe Motoki  - Adversarial Attacks and Defences Competition - 2018-03-31 - https://arxiv.org/pdf/1804.00097.pdf</p><p>Lee, Kimin; Lee, Kibok; Lee, Honglak; Shin, Jinwoo - A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks - 2018-10-27 - https://arxiv.org/abs/1807.03888</p><p>Lehman, Joel; Clune, Jeff; Misevic, Dusan - The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities - 2018-08-14 - https://arxiv.org/abs/1803.03453</p><p>Leibo, Joel; de Masson d&#x27;Autume, Cyprien; Zoran, Daniel; Amos, David; Beattie, Charles; Anderson, Keith; Castañeda, Antonio García; Sanchez, Manuel; Green, Simon; Gruslys, Audrunas, Legg, Shane, Hassabis, Demis, Botvinick, Matthew - Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents - 2018-02-04 - https://arxiv.org/abs/1801.08116</p><p>Leike, Jan; Kruegar, David; Everitt, Tom; Martic, Miljan; Maini, Vishal; Legg, Shane - Scalable agent alignment via reward modeling: a research direction - 2018-11-19 - https://arxiv.org/abs/1811.07871</p><p>Leike, Jan; Martic, Miljan; Krakovna, Victoria; Ortega, Pedro; Everitt, Tom; Lefrancq, Andrew; Orseau, Laurent; Legg, Shane - AI Safety Gridworlds - 2017-11-28 - https://arxiv.org/abs/1711.09883</p><p>Lewis, Gregory; Millett, Piers; Sandberg, Anders; Snyder-Beattie; Gronvall, Gigi - Information Hazards in Biotechnology - 2018-11-12 - https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.13235</p><p>Lipton, Zachary; Steinhardt, Jacob - Troubling Trends in Machine Learning Scholarship - 2018-07-26 - https://arxiv.org/abs/1807.03341</p><p>Liu, Chang; Hamrick, Jessica; Fisac, Jaime; Dragan, Anca; Hedrick, J Karl; Sastry, S Shankar; Griffiths, Thomas - Goal Inference Improves Objective and Perceived Performance in Human-Robot Collaboration - 2018-02-06 - https://arxiv.org/abs/1802.01780</p><p>Liu, Hin-Yan; Lauta, Kristian Cedervall; Mass, Matthijs Michiel - Governing Boring Apocalypses: A new typology of existential vulnerabilities and exposures for existential risk research - 2018-03-26 - https://www.sciencedirect.com/science/article/pii/S0016328717301623</p><p>Liu, Qiang; Li, Pan; Zhao, Wentao; Cai, Wei; Yu, Shui; Leung, Victor - A Survey on Security Threats and Defensive Techniques of Machine Learning: A Data Driven View - 2018-02-13 - https://ieeexplore.ieee.org/document/8290925</p><p>Liu, Yang; Price, Huw - Ramsey and Joyce on deliberation and prediction - 2018-08-30 - http://philsci-archive.pitt.edu/14972/</p><p>Lütjens, Björn; Everett, Michael; How, Jonathan  - Safe Reinforcement Learning with Model Uncertainty Estimates - 2018-10-19 - https://arxiv.org/abs/1810.08700</p><p>Malinin, Andrey; Gales, Mark - Predictive Uncertainty Estimation via Prior Networks - 2018-10-08 - https://arxiv.org/abs/1802.10501</p><p>Manheim, David; Garrabrant, Scott - Categorizing Variants of Goodheart&#x27;s Law - 2018-04-10 - https://arxiv.org/abs/1803.04585</p><p>Martinez-Plumed, Fernando; Loe, Bao Sheng; Flach, Peter; Ó hÉigeartaigh, Seán; Vold, Karina; Hernandez-Orallo, Jose - The Facets of Artificial Intelligence: A Framework to Track the Evolution of AI - 2018-08-21 - https://www.ijcai.org/proceedings/2018/0718.pdf</p><p>McCaslin, Tegan - Transmitting fibers in the brain: Total length and distribution of lengths - 2018-03-29 - https://aiimpacts.org/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths/</p><p>Menda, Kunal; Driggs-Campbell, Katherine; Kochenderfer, Mykel - EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning - 2018-07-22 - https://arxiv.org/abs/1807.08364</p><p>Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy, Dario Amodei - The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation - 2018-02-20 - https://arxiv.org/abs/1802.07228</p><p>Milli, Smitha; Schmidt, Ludwig; Dragan, Anca; Hardt, Moritz - Model Reconstruction from Model Explanations - 2018-07-13 - <u><a href=\"https://arxiv.org/abs/1807.05185\">https://arxiv.org/abs/1807.05185</a></u></p><p>Mindermann, Soren; Shah, Rohin; Gleave, Adam; Hadfield-Menell, Dylan - Active Inverse Reward Design - 2018-11-16 - https://arxiv.org/abs/1809.03060</p><p>Mogensen, Andreas - Long-termism for risk averse altruists - 1900-01-00 - https://unioxfordnexus-my.sharepoint.com/personal/exet1753_ox_ac_uk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fexet1753%5Fox%5Fac%5Fuk%2FDocuments%2FGlobal%20Priorities%20Institute%2FOperations%2FWebsite%2FWorking%20papers%2FLongtermism%20and%20risk%20aversion%20v3%2Epdf&amp;parent=%2Fpersonal%2Fexet1753%5Fox%5Fac%5Fuk%2FDocuments%2FGlobal%20Priorities%20Institute%2FOperations%2FWebsite%2FWorking%20papers&amp;slrid=10daaa9e-b098-7000-a41a-599fb32c6ff4</p><p>Ngo, Richard; Pace, Ben - Some cruxes on impactful alternatives to AI policy work - 2018-10-10 - https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work</p><p>Noothigattu, Ritesh; Bouneffouf, Djallel; Mattei, Nicholas; Chandra, Rachita; Madan, Piyush; Varshney, Kush; Campbell, Murray; Singh, Moninder; Rossi, Francesca  - Interpretable Multi-Objective Reinforcement Learning through Policy Orchestration - 2018-09-21 - https://arxiv.org/abs/1809.08343</p><p>Nushi, Besmira; Kamar, Ece; Horvitz, Eric - Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure - 2018-09-19 - https://arxiv.org/abs/1809.07424</p><p>Oesterheld, Caspar - Approval-directed agency and the decision theory of Newcomb-like problems - 2017-12-21 - https://casparoesterheld.files.wordpress.com/2017/12/rldt.pdf</p><p>OpenAI - OpenAI Charter - 2018-04-09 - https://blog.openai.com/openai-charter/</p><p>Ortega, Pedro; Maini, Vishal; Safety Team, Deepmind - Building safe artificial intelligence: specification, robustness and assurance - 2018-09-27 - https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1</p><p>Papernot, Nicolas; McDaniel, Patrick - Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning - 2018-03-13 - https://arxiv.org/pdf/1803.04765.pdf</p><p>Raghunathan, Aditi; Steinhardt, Jacob; Liang, Percy - Certified Defenses Against Adversarial Examples - 2018-01-29 - https://arxiv.org/abs/1801.09344</p><p>Rainforth, Tom; Kosiorek, Adam; Anh Le, Tuan; Maddison, Chris; Igl, Maximilian; Wood, Frank; Whe Teh, Yee - Tighter Variational Bounds are Not Necessarily Better - 2018-06-25 - https://arxiv.org/abs/1802.04537</p><p>Ratner, Ellis; Hadfield-Menell, Dylan; Dragan, Anca - Simplifying Reward Design through Divide-and-Conquer - 2018-06-07 - https://arxiv.org/abs/1806.02501</p><p>Reddy, Siddharth; Dragan, Anca; Levine, Sergey - Shared Autonomy via Deep Reinforcement Learning - 2018-05-23 - https://arxiv.org/abs/1802.01744</p><p>Reddy, Siddharth; Dragan, Anca; Levine, Sergey - Where Do You Think You&#x27;re Going?: Inferring Beliefs about Dynamics from Behaviour - 2018-10-20 - https://arxiv.org/abs/1805.08010</p><p>Rees, Martin - On The Future - 2018-10-16 - https://www.amazon.com/Future-Prospects-Humanity-Martin-Rees-ebook/dp/B07CSD5BG9</p><p>rk; Sempere, Nuno - AI development incentive gradients are not uniformly terrible - 2018-11-12 - https://www.lesswrong.com/posts/bkG4qj9BFEkNva3EX/ai-development-incentive-gradients-are-not-uniformly</p><p>Ruan, Wenjie; Huang, Xiaowei; Kwiatkowska, Marta - Reachability Analysis of Deep Neural Networks with Provable Guarantees - 2018-05-06 - https://arxiv.org/abs/1805.02242</p><p>Sadigh, Dorsa; Sastry, Shankar; Seshia, Sanjit; Dragan, Anca - Planning for Autonomous Cars that Leverage Effects on Human Actions - 2016-06-01 - https://people.eecs.berkeley.edu/~sastry/pubs/Pdfs%20of%202016/SadighPlanning2016.pdf</p><p>Sandberg, Anders - Human Extinction from Natural Hazard Events - 2018-02-01 - http://oxfordre.com/naturalhazardscience/view/10.1093/acrefore/9780199389407.001.0001/acrefore-9780199389407-e-293</p><p>Sarma, Gopal; Hay, Nick - Mammalian Value Systems - 2017-12-31 - https://arxiv.org/abs/1607.08289</p><p>Sarma, Gopal; Hay, Nick - Robust Computer Algebra, Theorem Proving, and Oracle AI - 2017-12-31 - https://arxiv.org/abs/1708.02553</p><p>Sarma, Gopal; Hay, Nick; Safron, Adam - AI Safety and Reproducibility: Establishing Robust Foundations for the Neuropsychology of Human Values - 2018-09-08 - https://arxiv.org/abs/1712.04307</p><p>Schulze, Sebastian; Evans, Owain - Active Reinforcement Learning with Monte-Carlo Tree Search - 2018-03-13 - https://arxiv.org/abs/1803.04926</p><p>Shah, Rohin - AI Alignment Newsletter - 1905-07-10 - https://rohinshah.com/alignment-newsletter/</p><p>Shah, Rohin; Christiano, Paul; Armstrong, Stuart; Steinhardt, Jacob; Evans, Owain - Value Learning Sequence - 2018-10-29 - https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh</p><p>Shahar, Avin - Mavericks and Lotteries - 2018-09-25 - http://philsci-archive.pitt.edu/15058/</p><p>Shahar, Avin;  Shapira, Shai - Civ V AI Mod - 2018-01-05 - https://www.cser.ac.uk/news/civilization-v-video-game-mod-superintelligent-ai/</p><p>Shaw, Nolan P.; Stockel, Andreas; Orr, Ryan W.; Lidbetter, Thomas F.; Cohen, Robin - Towards Provably Moral AI Agents in Bottom-up Learning Frameworks - 2018-03-15 - http://www.aies-conference.com/wp-content/papers/main/AIES_2018_paper_8.pdf</p><p>Sotala, Kaj - Shaping economic incentives for collaborative AGI - 2018-06-29 - https://www.lesswrong.com/posts/FkZCM4DMprtEp568s/shaping-economic-incentives-for-collaborative-agi</p><p>Sotala, Kaj; Gloor, Lukas - Superintelligence as a Cause or Cure for Risks of Astronomical Suffering - 2017-08-31 - http://www.informatica.si/index.php/informatica/article/view/1877</p><p>Stuhlmuller, Andreas - Factored Cognition - 2018-04-25 - https://ought.org/presentations/factored-cognition-2018-05</p><p>Taylor, Jessica; Gallagher, Jack; Maltinsky, Baeo  - Insight-based AI timeline model - 1905-07-10 - http://mediangroup.org/insights</p><p>The Future of Life Institute - Value Alignment Research Landscape - 1900-01-00 - https://futureoflife.org/valuealignmentmap/</p><p>Trammell, Philip - Fixed-Point Solutions to the Regress Problem in Normative Uncertainty - 2018-08-29 - https://unioxfordnexus-my.sharepoint.com/personal/exet1753_ox_ac_uk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fexet1753%5Fox%5Fac%5Fuk%2FDocuments%2FGlobal%20Priorities%20Institute%2FOperations%2FWebsite%2FWorking%20papers%2Fdecision%5Ftheory%5Fregress%2Epdf&amp;parent=%2Fpersonal%2Fexet1753%5Fox%5Fac%5Fuk%2FDocuments%2FGlobal%20Priorities%20Institute%2FOperations%2FWebsite%2FWorking%20papers&amp;slrid=14daaa9e-3069-7000-a41a-5aa6302f7c36</p><p>Tucker, Aaron; Gleave, Adam; Russell, Stuart - Inverse Reinforcement Learning for Video Games - 2018-10-24 - https://arxiv.org/abs/1810.10593</p><p>Turchin, Alexey - Could slaughterbots wipe out humanity? Assessment of the global catastrophic risk posed by autonomous weapons - 2018-03-19 - https://philpapers.org/rec/TURCSW</p><p>Turchin, Alexey; Denkenberger, David - Classification of Global Catastrophic Risks Connected with Artificial Intelligence - 2018-05-03 - https://link.springer.com/article/10.1007/s00146-018-0845-5</p><p>Turner, Alex - Towards a New Impact Measure - 2018-09-18 - https://www.alignmentforum.org/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure</p><p>Umbrello, Steven; Baum, Seth - Evaluating Future nanotechnology: The Net Societal Impacts of Atomically Precise Manufacturing - 2018-04-30 - https://www.researchgate.net/publication/324715437_Evaluating_Future_Nanotechnology_The_Net_Societal_Impacts_of_Atomically_Precise_Manufacturing</p><p>Vonitzer, Vincent; Sinnott-Armstrong, Walter; Borg, Jana Schaich; Deng, Yuan; Kramer, Max - Moral Decision Making Frameworks for Artificial Intelligence - 2017-02-12 - https://users.cs.duke.edu/~conitzer/moralAAAI17.pdf</p><p>Wang, Xin; Chen, Wenhu; Wang, Yuan-Fang ; Yang Wang, William  - No Metrics are Perfect: Adversarial Reward Learning for Visual Storytelling - 2018-07-09 - https://arxiv.org/abs/1804.09160</p><p>Wu, Yi; Siddharth, Srivastava; Hay, Nicholas; Du, Simon; Russell, Stuart - Discrete-Continuous Mixtures in Probabilistic Programming: Generalised Semantics and Inference Algorithms - 2018-06-13 - https://arxiv.org/abs/1806.02027</p><p>Wu, Yueh-Hua; Lin, Shou-De - A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents - 2018-09-10 - https://arxiv.org/abs/1712.04172</p><p>Yu, Han; Shen, Zhiqi; Miao, Chunyan; Leung, Cyril; Lesser, Victor; Yang, Qiang - Building Ethics into Artificial Intelligence - 2018-07-13 - http://www.ntulily.org/wp-content/uploads/conference/Building_Ethics_into_Artificial_Intelligence_accepted.pdf</p><p>Yudkowsky, Eliezer - The Rocket Alignment Problem - 2018-10-03 - https://intelligence.org/2018/10/03/rocket-alignment/</p><p>Yudkowsky, Eliezer; Christiano, Paul - Challenges to Christiano&#x27;s Capability Amplification Proposal - 2018-05-19 - https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal</p><p>Zhou, Allen; Hadfield-Menell, Dylan; Nagabandi, Anusha; Dragan, Anca - Expressive Robot Motion Timing - 2018-02-05 - </p>",
    "user": {
      "username": "Larks",
      "slug": "larks",
      "displayName": "Larks"
    }
  },
  {
    "_id": "NmL2NdrrDosX7nEyz",
    "title": "Equivalence of State Machines and Coroutines",
    "slug": "equivalence-of-state-machines-and-coroutines",
    "pageUrl": "https://www.lesswrong.com/posts/NmL2NdrrDosX7nEyz/equivalence-of-state-machines-and-coroutines",
    "postedAt": "2018-12-18T04:40:00.750Z",
    "baseScore": 12,
    "voteCount": 4,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<div> <p>In the past I often referred to the equivalence between state machines and coroutines as a kind of obvious fact that doesn't need any additional explanation. It was brought to my attention, however, that that may not always be the case.</p> <p>This article therefore doesn't attempt to express and deep and ground-breaking truth, rather, it illustrates the equivalence of finite state machines and coroutines using a practical example.</p> <p>The example is stolen from <a href=\"https://en.wikipedia.org/wiki/Finite-state_machine\">Wikipedia's article on finite state machines</a>:</p> <blockquote> <p>A turnstile, used to control access to subways and amusement park rides, is a gate with three rotating arms at waist height, one across the entryway. Initially the arms are locked, blocking the entry, preventing patrons from passing through. Depositing a coin or token in a slot on the turnstile unlocks the arms, allowing a single customer to push through. After the customer passes through, the arms are locked again until another coin is inserted.</p> </blockquote> <p>Wikipedia presents the state machine in question in the following manner:</p> <div><img src=\"http://250bpm.wdfiles.com/local--files/blog:141/fsm3.png\" /></div> <p>And here's my attempt to rewrite the state machine as a coroutine in Go:</p> <div><img src=\"http://250bpm.wdfiles.com/local--files/blog:141/fsm2.png\" /></div> ",
    "user": {
      "username": "sustrik",
      "slug": "sustrik",
      "displayName": "Martin Sustrik"
    }
  },
  {
    "_id": "b2rGxCsDPzqZjy3JF",
    "title": "You can be wrong about what you like, and you often are",
    "slug": "you-can-be-wrong-about-what-you-like-and-you-often-are",
    "pageUrl": "https://www.lesswrong.com/posts/b2rGxCsDPzqZjy3JF/you-can-be-wrong-about-what-you-like-and-you-often-are",
    "postedAt": "2018-12-17T23:49:39.935Z",
    "baseScore": 30,
    "voteCount": 10,
    "commentCount": 21,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Meta: I&#x27;m not saying anything new here. There has been a lot of research on the topic, and popular books like </em><a href=\"https://www.amazon.com/Stumbling-Happiness-Daniel-Gilbert/dp/1400077427\">Stumbling on Happiness</a><em> have been written. Furthermore, I don&#x27;t think I have explained any of this particularly well, or provided particularly enlightening examples. Nevertheless, I think these things are worth saying because a) a lot of people have an &quot;I know what I like&quot; attitude, and b) this attitude seems pretty harmful. Just be sure to treat this as more of an exploratory post than an authoritative one.</em></p><p>I think that the following attitudes are very common:</p><ul><li>I&#x27;m just not one of those people who enjoys &quot;deeper&quot; activities like reading a novel. I like watching TV and playing video games.</li><li>I&#x27;m just not one of those people who likes healthy foods. <em>You</em> may like salads and swear by them, but <em>I</em> am different. <em>I</em> like pizza and french fries.</li><li>I&#x27;m just not an intellectual person. I don&#x27;t enjoy learning.</li><li>I&#x27;m just not into that early retirement stuff. I need to maintain my current lifestyle in order to be happy.</li><li>I&#x27;m just not into &quot;good&quot; movies/music/art. I like the Top 50 stuff.</li></ul><p>Imagine what would happen if you responded to someone who expressed one of these attitudes by saying &quot;I think that you&#x27;re wrong.&quot; Often times, the response you&#x27;ll get is something along the lines of:</p><blockquote>Who are you to tell me what I do and don&#x27;t like? How can you possibly know? <em>I&#x27;m</em> the one who&#x27;s in my own head. <em>I</em> know how these things make me feel.</blockquote><p>When I think about that response, I think about optical illusions. Consider this one:</p><span><figure><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a6/Grey_square_optical_illusion.svg/2000px-Grey_square_optical_illusion.svg.png\" class=\"draft-image \" style=\"\" /></figure></span><p>When I think about that response, I think about the following dialog:</p><blockquote>Me: A and B are the <a href=\"https://en.wikipedia.org/wiki/Checker_shadow_illusion\">same</a> shade of gray.</blockquote><blockquote>Person: No they&#x27;re not! WTF are you talking about? How can you say that they are? I can <em>see</em> with my <em>eyes</em> that they&#x27;re <em>not</em>!</blockquote><p>I understand the frustration. It <em>feels</em> like they&#x27;re different shades. It feels like it is stupidly obvious that they&#x27;re different shades.</p><p>And if feels like you know what you like.</p><p>But sometimes, sometimes your brain lies to you.</p><p>The image of the squares is an optical illusion. Neuroscientists and psychologists study them. And they write books like <a href=\"https://www.amazon.com/Vision-Science-Phenomenology-Stephen-Palmer/dp/0262161834\">this</a> about them.</p><p>The question of knowing what you like can be a <em>hedonic</em> illusion (that&#x27;s what I&#x27;ll decide to call it anyway). Neuroscientists and psychologists study these illusions too. And they write books like <a href=\"https://www.amazon.com/Stumbling-Happiness-Daniel-Gilbert/dp/1400077427\">this</a> about them.</p><p>They have found that we&#x27;re actually really bad at knowing what will make us happy. At knowing what we do, and don&#x27;t like.</p><p>Some quotes from <em><a href=\"https://harvardmagazine.com/2007/01/the-science-of-happiness.html\">The Science of Happiness</a></em>:</p><ul><li>“One big question was, Are beautiful people happier?” Etcoff says. “Surprisingly, the answer is no! This got me thinking about happiness and what makes people happy.”</li><li>His book <em><a href=\"http://www.powells.com/partner/30264/biblio/1400042666\">Stumbling on Happiness</a></em> became a national bestseller last summer. Its central focus is “prospection”—the ability to look into the future and discover what will make us happy. The bad news is that humans aren’t very skilled at such predictions; the good news is that we are much better than we realize at adapting to whatever life sends us.</li><li>The reason is that humans hold fast to a number of wrong ideas about what will make them happy. Ironically, these misconceptions may be evolutionary necessities. “Imagine a species that figured out that children don’t make you happy,” says Gilbert. “We have a word for that species: <em>extinct.</em></li></ul><p>That last one was pretty powerful, wow.</p><p>I think that the implications of this are all pretty huge. We all want to be happy. We all want to thrive. We make thousands and thousands of little decisions to this end. We decide to have fried chicken for dinner, and that having salads isn&#x27;t worth the effort, despite whatever long term health benefits. We decide that video games are a nice, fun, relaxing way to decompress after work. We decide that working a corporate job is worth it because we &quot;need the money&quot;.</p><p>All of these decisions shape our lives. If we&#x27;re getting them wrong, well, then we&#x27;re not doing a good job of shaping our lives.</p><p>And if we&#x27;re basing these decisions off of our <em>intuitions</em>, according to the positive psychology research, we&#x27;re probably screwing up a lot.</p><p>So then, I propose that we approach these sorts of questions with more <strong>curiosity</strong>.</p><blockquote>The first virtue is curiosity. A burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance. <em>If in your heart you believe you already know</em>, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer. The glory of glorious mystery is to be solved, after which it ceases to be mystery. Be wary of those who speak of being open-minded and modestly confess their ignorance. There is a time to confess your ignorance and a time to relinquish your ignorance.</blockquote><p>And with more <strong>humility</strong>.</p><blockquote>The eighth virtue is humility. To be humble is to take specific actions in anticipation of your own errors. To confess your fallibility and then do nothing about it is not humble; it is boasting of your modesty. Who are most humble? Those who most skillfully prepare for the deepest and most catastrophic errors in their own beliefs and plans. Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities. But it is useless to be superior: Life is not graded on a curve. The best physicist in ancient Greece could not calculate the path of a falling apple. There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse. If you compare yourself to others you will not see the biases that all humans share. To be human is to make ten thousand errors. No one in this world achieves perfection.</blockquote><p>You can be wrong about what you like, and you often are.</p><p><a href=\"https://www.youtube.com/watch?v=KmC1btSZP7U\">https://www.youtube.com/watch?v=KmC1btSZP7U</a></p><p>(My grandpa used to read this to me all of the time when I was younger. And he still bugs me about it to this day. It&#x27;s cool that I&#x27;m finally starting to understand it.)</p>",
    "user": {
      "username": "adamzerner",
      "slug": "adamzerner",
      "displayName": "Adam Zerner"
    }
  },
  {
    "_id": "wrMqGrGWaFfcAyvBW",
    "title": "Alignment Newsletter #37",
    "slug": "alignment-newsletter-37",
    "pageUrl": "https://www.lesswrong.com/posts/wrMqGrGWaFfcAyvBW/alignment-newsletter-37",
    "postedAt": "2018-12-17T19:10:01.774Z",
    "baseScore": 25,
    "voteCount": 7,
    "commentCount": 4,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Find all Alignment Newsletter resources <a href=\"http://rohinshah.com/alignment-newsletter/\">here</a>. In particular, you can <a href=\"http://eepurl.com/dqMSZj\">sign up</a>, or look through this <a href=\"https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing\">spreadsheet</a> of all summaries that have ever been in the newsletter.</p><h2>Highlights</h2><p><strong><a href=\"https://www.alignmentforum.org/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas\">Three AI Safety Related Ideas</a> and <a href=\"https://www.alignmentforum.org/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety\">Two Neglected Problems in Human-AI Safety</a></strong> <em>(Wei Dai)</em>: If any particular human got a lot of power, or was able to think a lot faster, then they might do something that we would consider bad. Perhaps power corrupts them, or perhaps they get so excited about the potential technologies they can develop that they do so without thinking seriously about the consequences. We now have both an opportunity and an obligation to design AI systems that operate more cautiously, that aren&#x27;t prone to the same biases of reasoning and heuristics that we are, such that the future actually goes <em>better</em> than it would if we magically made humans more intelligent.</p><p>If it&#x27;s too hard to make AI systems in this way and we need to have them learn goals from humans, we could at least have them learn from <em>idealized</em> humans rather than real ones. Human values don&#x27;t extrapolate well -- just look at the myriad answers that people give to the various hypotheticals like the <a href=\"https://en.wikipedia.org/wiki/Trolley_problem\">trolley problem</a>. So, it&#x27;s better to learn from humans that are kept in safe, familiar environment with all their basic needs taken care of. These are our idealized humans. In practice the AI system would learn a lot from the preferences of real humans, since that should be a very good indicator of the preferences of idealized humans. But if the idealized humans begin to have different preferences from real humans, then the AI system should ignore the &quot;corrupted&quot; values of the real humans.</p><p>More generally, it seems important for our AI systems to help us figure out what we care about before we make drastic and irreversible changes to our environment, especially changes that prevent us from figuring out what we care about. For example, if we create a hedonic paradise where everyone is on side-effect-free recreational drugs all the time, it seems unlikely that we check whether this is actually what we wanted. This suggests that we need to work on AI systems that differentially advance our philosophical capabilities relative to other capabilities, such as technological ones.</p><p>One particular way that &quot;aligned&quot; AI systems could make things worse is if they accidentally &quot;corrupt&quot; our values, as in the hedonic paradise example before. A nearer-term example would be making more addictive video games or social media. They might also make very persuasive but wrong moral arguments.</p><p>This could also happen in a multipolar setting, where different groups have their own AIs that try to manipulate other humans into having values similar to theirs. The attack is easy, since you have a clear objective (whether or not the humans start behaving according to your values), but it seems hard to defend against, because it is hard to determine the difference between manipulation and useful information.</p><p><strong>Rohin&#x27;s opinion:</strong> (A more detailed discussion is available on <a href=\"https://www.alignmentforum.org/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas#WP9Tc2eTAwsPNw3cz\">these</a> <a href=\"https://www.alignmentforum.org/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety#FoWAZFTnG8D2rreSs\">threads</a>.) I&#x27;m glad these posts were written, they outline real problems that I think are neglected in the AI safety community and outline some angles of attack. The rest of this is going to be a bunch of disagreements I have, but these should be taken as disagreements on how to <em>solve</em> these problems, not a disagreement that the problems exist.</p><p>It seems quite difficult to me to build AI systems that are safe, <em>without</em> having them rely on humans making philosophical progress themselves. We&#x27;ve been trying to figure this out for thousands of years. I&#x27;m pessimistic about our chances at creating AI systems that can outperform this huge intellectual effort correctly on the first try without feedback from humans. Learning from idealized humans might address this to some extent, but in many circumstances I think I would trust the real humans with <a href=\"http://slatestarcodex.com/2016/05/14/skin-in-the-game/\">skin in the game</a> more than the idealized humans who must reason about those circumstances from afar (in their safe, familiar environment).</p><p>I do think we want to have a general approach where we try to figure out how AIs <em>and</em> humans should reason, such that the resulting system behaves well. On the human side, this might mean that the human needs to be more cautious for longer timescales, or to have more epistemic and moral humility. Idealized humans can be thought of an instance of this approach where rather than change the policy of real humans, we indirectly change their policy in a hypothetical by putting them in safer environments.</p><p>For the problem of intentionally corrupting values, this seems to me an instance of the general class of &quot;Competing aligned superintelligent AI systems could do bad things&quot;, in the same way that we have the risk of nuclear war today. I&#x27;m not sure why we&#x27;re focusing on value corruption in particular. In any case, my current preferred solution is not to get into this situation in the first place (though admittedly that seems very hard to do, and I&#x27;d love to see more thought put into this).</p><p>Overall, I&#x27;m hoping that we can solve &quot;human safety problems&quot; by training the humans supervising the AI to not have those problems, because it sure does make the technical problem of aligning AI seem a lot easier. I don&#x27;t have a great answer to the problem of competing aligned superintelligent AI systems.</p><p><strong><a href=\"https://arxiv.org/abs/1811.01267\">Legible Normativity for AI Alignment: The Value of Silly Rules</a></strong> <em>(Dylan Hadfield-Menell et al)</em>: One issue we might have with value learning is that our AI system might look at &quot;silly rules&quot; and infer that we care about them deeply. For example, we often enforce dress codes through social punishments. Given that dress codes do not have much functional purpose and yet we enforce them, should an AI system infer that we care about dress codes as much as we care about (say) property rights? This paper claims that these &quot;silly rules&quot; should be interpreted as a coordination mechanism that allows group members to learn whether or not the group rules will be enforced by neutral third parties. For example, if I violate the dress code, no one is significantly harmed but I would be punished anyway -- and this can give everyone confidence that if I were to break an important rule, such as stealing someone&#x27;s wallet, <em>bystanders</em> would punish me by reporting me to the police, even though they are not affected by my actions and it is a cost to them to report me.</p><p>They formalize this using a model with a pool of agents that can choose to be part of a group. Agents in the group play &quot;important&quot; games and &quot;silly&quot; games. In any game, there is a scofflaw, a victim, and a bystander. In an important game, if the bystander would punish any rule violations, then the scofflaw follows the rule and the victim gets +1 utility, but if the bystander would not punish the violation, the scofflaw breaks the rule and the victim gets -1 utility. Note that in order to signal that they would punish, bystanders must pay a cost of c. A silly game works the same way, except the victim always gets 0 utility. Given a set of important rules, the main quantity of interest is how many silly rules to add. The authors quantify this by considering the <em>proportion</em> of all games that are silly games, which they call the density. Since we are imagining <em>adding</em> silly rules, all outcomes are measured with respect to the number of <em>important</em> games. We can think of this as a proxy for time, and indeed the authors call the expected number of games till an important game a <em>timestep</em>.</p><p>Now, for important games the expected utility to the victim is positive if the probability that the bystander is a punisher is greater than 0.5. So, each of the agents cares about estimating this probability in order to decide whether or not to stay in the group. Now, if we only had important games, we would have a single game per timestep, and we would only learn whether one particular agent is a punisher. As we add more silly games, we get more games per timestep, and so we can learn much more quickly the proportion of punishers, which leads to more stable groups. However, the silly rules are not free. The authors prove that if they <em>are</em> free, then we keep adding silly rules and the density would approach 1. (More precisely, they show that as density goes to 1, the value of being told the true probability of punishment goes to 0, meaning that the agent already knows everything.)</p><p>They then show experimental results showing a few things. When the agents are relatively certain of the probability of an agent being a punisher, then silly rules are not very useful and the group is more likely to collapse (since the cost of enforcing the silly rules starts to be important). Second, as long as c is low (so it is easy to signal that you will enforce rules), then groups with more silly rules will be more resilient to shocks in individual&#x27;s beliefs about the proportion of punishers, since they will very quickly converge to the right belief. If there aren&#x27;t any silly rules it can take more time and your estimate might be incorrectly low enough that you decide to leave the group even though group membership is still net positive. Finally, if the proportion of punishers drops below 0.5, making group membership net negative, agents in groups with high density will learn this faster, and their groups will disband much sooner.</p><p><strong>Rohin&#x27;s opinion:</strong> I really like this paper, it&#x27;s a great concrete example of how systems of agents can have very different behavior than any one individual agent <em>even if</em> each of the agents have similar goals. The idea makes intuitive sense and I think the model captures its salient aspects. There are definitely many quibbles you could make with the model (though perhaps it is the standard model, I don&#x27;t know this field), but I don&#x27;t think they&#x27;re important. My perspective is that the model is a particularly clear and precise way of communicating the effect that the authors are describing, as opposed to something that is supposed to track reality closely.</p><h1>Technical AI alignment</h1><h3>Problems</h3><p><strong><a href=\"https://www.alignmentforum.org/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas\">Three AI Safety Related Ideas</a> and <a href=\"https://www.alignmentforum.org/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety\">Two Neglected Problems in Human-AI Safety</a></strong> <em>(Wei Dai)</em>: Summarized in the highlights!</p><h3>Technical agendas and prioritization</h3><p><a href=\"https://www.lesswrong.com/posts/3fkBWpE4f9nYbdf7E/multi-agent-minds-and-ai-alignment\">Multi-agent minds and AI alignment</a> <em>(Jan Kulveit)</em>: This post argues against the model of humans as optimizing some particular utility function, instead favoring a model based on predictive processing. This leads to several issues with the way standard value learning approaches like inverse reinforcement learning work. There are a few suggested areas for future research. First, we could understand how hierarchical models of the world work (presumably for better value learning). Second, we could try to invert game theory to learn objectives in multiagent settings. Third, we could learn preferences in multiagent settings, which might allow us to better infer norms that humans follow. Fourth, we could see what happens if we take a system of agents, infer a utility function, and then optimize it -- perhaps one of the agents&#x27; utility functions dominates? Finally, we can see what happens when we take a system of agents and give it more computation, to see how different parts scale. On the non-technical side, we can try to figure out how to get humans to be more self-aligned (i.e. there aren&#x27;t &quot;different parts pulling in different directions&quot;).</p><p><strong>Rohin&#x27;s opinion:</strong> I agree with the general point that figuring out a human utility function and then optimizing it is unlikely to work, but for different reasons (see the first chapter of the <a href=\"https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc\">Value Learning sequence</a>). I also agree that humans are complex and you can’t get away with modeling them as Boltzmann rational and optimizing some fixed utility function. I wouldn’t try to make the model more accurate (eg. a model of a bunch of interacting subagents, each with their own utility function), I would try to make the model less precise (eg. a single giant neural net), because that reduces the chance of model misspecification. However, given the <a href=\"https://arxiv.org/abs/1712.05812\">impossibility result</a> saying that you must make assumptions to make this work, we probably have to give up on having some nice formally specified meaning of “values”. I think this is probably fine -- for example, iterated amplification doesn’t have any explicit formal value function.</p><h3>Reward learning theory</h3><p><a href=\"https://www.alignmentforum.org/posts/YfQGZderiaGv3kBJ8/figuring-out-what-alice-wants-non-human-alice\">Figuring out what Alice wants: non-human Alice</a> <em>(Stuart Armstrong)</em>: We know that if we have a potentially irrational agent, then inferring their preferences is <a href=\"https://arxiv.org/abs/1712.05812\">impossible</a> without further assumptions. However, in practice we can infer preferences of humans quite well. This is because we have very specific and narrow models of how humans work: we tend to agree on our judgments of whether someone is angry, and what anger implies about their preferences. This is exactly what the theorem is meant to prohibit, which means that humans are making some strong assumptions about other humans. As a result, we can hope to solve the value learning problem by figuring out what assumptions humans are already making and using those assumptions.</p><p><strong>Rohin&#x27;s opinion:</strong> The fact that humans are quite good at inferring preferences should give us optimism about value learning. In the <a href=\"https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/h9DesGT3WT9u2k7Hr\">framework</a> of rationality with a mistake model, we are trying to infer the mistake model from the way that humans infer preferences about other humans. This sidesteps the impossibility result by focusing on the <em>structure</em> of the algorithm that generates the policy. However, it still seems like we have to make some assumption about how the structure of the algorithm leads to a mistake model, or a model for what values are. Though perhaps we can get an answer that is principled enough or intuitive enough that we believe it.</p><h3>Handling groups of agents</h3><p><strong><a href=\"https://arxiv.org/abs/1811.01267\">Legible Normativity for AI Alignment: The Value of Silly Rules</a></strong> <em>(Dylan Hadfield-Menell et al)</em>: Summarized in the highlights!</p><h3>Miscellaneous (Alignment)</h3><p><a href=\"https://www.alignmentforum.org/posts/95i5B78uhqyB3d6Xc/assuming-we-ve-solved-x-could-we-do-y\">Assuming we&#x27;ve solved X, could we do Y...</a> <em>(Stuart Armstrong)</em>: We often want to make assumptions that sound intuitive but that we can&#x27;t easily formalize, eg. &quot;assume we&#x27;ve solved the problem of determining human values&quot;. However, such assumptions can often be interpreted as being very weak or very strong, and depending on the interpretation we could be assuming away the entire problem, or the assumption doesn&#x27;t buy us anything. So, we should be more precise in our assumptions, or focus on only on some precise <em>properties</em> of an assumption.</p><p><strong>Rohin&#x27;s opinion:</strong> I think this argument applies well to the case where we are trying to <em>communicate</em>, but not so much to the case where I individually am thinking about a problem. (I&#x27;m making this claim about me specifically; I don&#x27;t know if it generalizes to other people.) Communication is hard and if the speaker uses some intuitive assumption, chances are the listener will interpret it differently from what the speaker intended, and so being very precise seems quite helpful. However, when I&#x27;m thinking through a problem myself and I make an assumption, I usually have a fairly detailed intuitive model of what I mean, such that if you ask me whether I&#x27;m assuming that problem X is solved by the assumption, I could answer that, even though I don&#x27;t have a precise formulation of the assumption. Making the assumption more precise would be quite a lot of work, and probably would not improve my thinking on the topic that much, so I tend not to do it until I think there&#x27;s some insight and want to make the argument more rigorous. It seems to me that this is how most research progress happens: by individual researchers having intuitions that they then make rigorous and precise.</p><h1>Near-term concerns</h1><h3>Fairness and bias</h3><p><a href=\"https://ai.googleblog.com/2018/12/providing-gender-specific-translations.html\">Providing Gender-Specific Translations in Google Translate</a> <em>(Melvin Johnson)</em></p><h3>Machine ethics</h3><p><a href=\"http://arxiv.org/abs/1812.02953\">Building Ethics into Artificial Intelligence</a> <em>(Han Yu et al)</em></p><p><a href=\"http://arxiv.org/abs/1812.03980\">Building Ethically Bounded AI</a> <em>(Francesca Rossi et al)</em></p><h1>Malicious use of AI</h1><p><a href=\"https://futureoflife.org/2018/12/11/fli-signs-safe-face-pledge/\">FLI Signs Safe Face Pledge</a> <em>(Ariel Conn)</em></p><h1>Other progress in AI</h1><h3>Reinforcement learning</h3><p><a href=\"http://arxiv.org/abs/1812.02900\">Off-Policy Deep Reinforcement Learning without Exploration</a> <em>(Scott Fujimoto et al)</em> (summarized by Richard): This paper discusses off-policy batch reinforcement learning, in which an agent is trying to learn a policy from data which is not based on its own policy, and without the opportunity to collect more data during training. The authors demonstrate that standard RL algorithms do badly in this setting because they give unseen state-action pairs unrealistically high values, and lack the opportunity to update them. They proposes to address this problem by only selecting actions from previously seen state-action pairs; they prove various optimality results for this algorithm in the MDP setting. To adapt this approach to the continuous control case, the authors train a generative model to produce likely actions (conditional on the state and the data batch) and then only select from the top n actions. Their batch-conditional q-learning algorithm (BCQ) consists of that generative model, a perturbation model to slightly alter the top actions, and a value network and critic to perform the selection. When n = 0, BCQ resembles behavioural cloning, and when n -&gt; ∞, it resembles Q-learning. BCQ with n=10 handily outperformed DQN and DDPG on some Mujoco experiments using batch data.</p><p><strong>Richard&#x27;s opinion:</strong> This is an interesting paper, with a good balance of intuitive motivations, theoretical proofs, and empirical results. While it&#x27;s not directly safety-related, the broad direction of combining imitation learning and reinforcement learning seems like it might have promise. Relatedly, I wish the authors had discussed in more depth what assumptions can or should be made about the source of batch data. For example, BCQ would presumably perform worse than DQN when data is collected from an expert trying to minimise reward, and (from the paper’s experiments) performs worse than behavioural cloning when data is collected from an expert trying to maximise reward. Most human data an advanced AI might learn from is presumably somewhere in between those two extremes, and so understanding how well algorithms like BCQ would work on it may be valuable.</p><p><a href=\"http://bair.berkeley.edu/blog/2018/12/14/sac/\">Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots</a> <em>(Tuomas Haarnoja et al)</em></p><h3>Deep learning</h3><p><a href=\"https://blog.openai.com/science-of-ai/\">How AI Training Scales</a> <em>(Sam McCandlish et al)</em>: OpenAI has done an empirical investigation into the performance of AI systems, and found that the maximum useful batch size for a particular task is strongly influenced by the noise in the gradient. (Here, the noise in the gradient comes from the fact that we are using <em>stochastic</em> gradient descent -- any difference in the gradients across batches counts as &quot;noise&quot;.) They also found some preliminary results showing the more powerful ML techniques tend to have more gradient noise, and even a single model tends to have increased gradient noise over time as they get better at the task.</p><p><strong>Rohin&#x27;s opinion:</strong> While OpenAI doesn&#x27;t speculate on why this relationship exists, it seems to me that as you get larger batch sizes, you are improving the gradient by reducing noise by averaging over a larger batch. This predicts the results well: as the task gets harder and the noise in the gradients gets larger, there&#x27;s more noise to get rid of by averaging over data points, and so there&#x27;s more opportunity to have <em>even larger</em> batch sizes.</p>",
    "user": {
      "username": "rohinmshah",
      "slug": "rohinmshah",
      "displayName": "Rohin Shah"
    }
  },
  {
    "_id": "Z3fJXxKrnQx8qYPtk",
    "title": "A simple approach to 5-and-10",
    "slug": "a-simple-approach-to-5-and-10",
    "pageUrl": "https://www.lesswrong.com/posts/Z3fJXxKrnQx8qYPtk/a-simple-approach-to-5-and-10",
    "postedAt": "2018-12-17T18:33:46.735Z",
    "baseScore": 5,
    "voteCount": 1,
    "commentCount": 10,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p>To recap, A is trying to deduce enough about U to maximize it:</p>\n<p>A := Find some f with a proof of \"U = f(A)\", then return argmax f.</p>\n<p>If U is A, A can fail by proving f = {(5,5),(10,0)}: If it could prove this, it would be true, for only 5 is checked. Then by Löb's theorem, it can prove this.</p>\n<p>I've thrown this problem at some people in my university and a fellow student's idea led to:</p>\n<p>A' := Find all f with a proof of \"U = f(A')\", then return argmax (their pointwise maximum).</p>\n<p>The true f is among the found f. Fooling ourselves into a suboptimal action would require us to believe it at least as good as the true maximum, which the single check refutes.</p>\n<p>Note that A' can't prove that some f is not found, due to Gödel. Therefore, it can never prove what it is going to do!</p>\n</body></html>",
    "user": {
      "username": "Gurkenglas",
      "slug": "gurkenglas",
      "displayName": "Gurkenglas"
    }
  },
  {
    "_id": "kPi7MNgPrR3P3Lzav",
    "title": "In Defense of Finance",
    "slug": "in-defense-of-finance",
    "pageUrl": "https://www.lesswrong.com/posts/kPi7MNgPrR3P3Lzav/in-defense-of-finance",
    "postedAt": "2018-12-17T16:59:11.149Z",
    "baseScore": 51,
    "voteCount": 12,
    "commentCount": 31,
    "meta": false,
    "question": false,
    "url": "https://putanumonit.com/2018/12/14/defense-of-finance/",
    "htmlBody": "<p></p><span><figure><img src=\"https://putanumonit.files.wordpress.com/2018/12/cosmic-suit-banker.png\" class=\"draft-image center\" style=\"\" /></figure></span><p><a href=\"https://putanumonit.com/2018/12/14/defense-of-finance/\">I&#x27;m sharing the link</a> instead of a full cross-post because this essay has:</p><ul><li>5,300 words</li><li>2 footnotes, with links that actually work</li><li>1 GIF, 2 charts, 5 expanding brains</li><li>30+ lively comments</li></ul><p></p><p>This is a &quot;much more than you wanted to know&quot; type post on the financial industry:</p><ul><li>Why everyone hates it, except for people in positions of power.</li><li>The value of finance in coordinating trade across space and time, told via the parable of Banksy and the corn cobs.</li><li>Are mortgage-backed securities a scam or a brilliant innovation?</li><li>The stupid arguments about bailouts, and the smarter arguments for/against bailouts.</li><li>Economists writing books about bank equity ratios without googling what those equity ratios actually are.</li><li>What <em>is </em>up with equity ratios? Is making banks hold 50% capital the solution to financial crises?</li><li>What banking regulations actually make banks do.</li><li>The inextricable romance between finance and government.</li><li>A final request for incrementalism and humility.</li></ul>",
    "user": {
      "username": "Jacobian",
      "slug": "jacob-falkovich",
      "displayName": "Jacob Falkovich"
    }
  },
  {
    "_id": "qAnHyJJEW4L9mDjw7",
    "title": "Fifteen Things I Learned From Watching a Game of Secret Hitler",
    "slug": "fifteen-things-i-learned-from-watching-a-game-of-secret",
    "pageUrl": "https://www.lesswrong.com/posts/qAnHyJJEW4L9mDjw7/fifteen-things-i-learned-from-watching-a-game-of-secret",
    "postedAt": "2018-12-17T13:40:01.047Z",
    "baseScore": 13,
    "voteCount": 9,
    "commentCount": 6,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Epistemic Status: Not likely to be <em>true </em>things. Right?</p>\n<ol>\n<li>Liberals know nothing, fascists know everything.</li>\n<li>Most of the policies democratic governments could pass are fascist policies that expand government power.</li>\n<li>The remaining policies are liberal policies. There is no such thing as a conservative policy.</li>\n<li>Liberal policies do nothing.</li>\n<li>If the liberals do nothing enough times, they win and can congratulate themselves, no matter how much more fascist things got in the meantime.</li>\n<li>Governments must always be passing new policies, and never take away old policies. Thus, government inevitably gets more powerful over time.</li>\n<li>The more liberal policies you pass, the more likely it is any future policy will be fascist.</li>\n<li>The more fascist policies you pass, the more likely it is any future policy will be fascist.</li>\n<li>When the time comes to pass a policy, the government will choose from whatever proposals are lying around, even if all of them are fascist and everyone choosing is a liberal. There is almost never an option to just not do that, as such bold action requires a mostly fascist policy already be in place.</li>\n<li>If the government fails to agree to pass one of the things lying around, that’s even worse, because it will then choose a new policy <em>completely </em>at random from what is lying around, which will probably be fascist.</li>\n<li>Liberals spend most of their time being paranoid over which people claiming to be liberals are secretly fascists, or even secretly actual literal Hitler, as opposed to attempting to write or choose good policies.</li>\n<li>Someone enacting liberal policies, but not in a position to assume dictatorial power, is providing strong evidence they are probably secretly Hitler.</li>\n<li>When good people often have no choice but to do bad things, but there is no way to verify this, the default is for no one who is good to have any idea who is good and who is bad.</li>\n<li>Despite this, good people <em>think </em>they know who is good and who is bad.</li>\n<li>Introducing a random element to the play is good for veteran players, because the ‘good guys’ are no longer able to (and thus forced to as in Resistance/Avalon) fall back purely on an announced, deterministic strategy, as the ‘bad guys’ could know the rules and game the system.</li>\n</ol>",
    "user": {
      "username": "Zvi",
      "slug": "zvi",
      "displayName": "Zvi"
    }
  },
  {
    "_id": "HTgakSs6JpnogD6c2",
    "title": "Two Neglected Problems in Human-AI Safety",
    "slug": "two-neglected-problems-in-human-ai-safety",
    "pageUrl": "https://www.lesswrong.com/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety",
    "postedAt": "2018-12-16T22:13:29.196Z",
    "baseScore": 102,
    "voteCount": 45,
    "commentCount": 25,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>In this post I describe a couple of <a href=\"https://www.lesswrong.com/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas\">human-AI safety problems</a> in more detail. These helped motivate my proposed <a href=\"https://www.lesswrong.com/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas#2__A_hybrid_approach_to_the_human_AI_safety_problem\">hybrid approach</a>, and I think need to be addressed by other AI safety approaches that currently do not take them into account.</p>\n<p><strong>1. How to prevent \"aligned\" AIs from unintentionally corrupting human values?</strong></p>\n<p>We know that ML systems tend to have problems with adversarial examples and distributional shifts in general. There seems to be no reason not to expect that human value functions have similar problems, which even \"aligned\" AIs could trigger unless they are somehow designed not to. For example, such AIs could give humans so much power so quickly or put them in such novel situations that their moral development can't keep up, and their value systems no longer apply or give essentially random answers. AIs could give us new options that are irresistible to some parts of our motivational systems, like more powerful versions of video game and social media addiction. In the course of trying to figure out what we most want or like, they could in effect be searching for adversarial examples on our value functions. At our own request or in a sincere attempt to help us, they could generate philosophical or moral arguments that are wrong but extremely persuasive.</p>\n<p>(Some of these issues, like the invention of new addictions and new technologies in general, would happen even without AI, but I think AIs would likely, by default, strongly exacerbate the problem by differentially accelerating such technologies faster than progress in understanding how to safely handle them.)</p>\n<p><strong>2. How to defend against intentional attempts by AIs to corrupt human values?</strong></p>\n<p>It looks like we may be headed towards a world of multiple AIs, some of which are either unaligned, or aligned to other owners or users. In such a world there's a strong incentive to use one's own AIs to manipulate other people's values in a direction that benefits oneself (even if the resulting loss to others are greater than gains to oneself).</p>\n<p>There is an apparent asymmetry between attack and defense in this arena, because manipulating a human is a straightforward optimization problem with an objective that is easy to test/measure (just check if the target has accepted the values you're trying to instill, or has started doing things that are more beneficial to you), and hence relatively easy for AIs to learn how to do, but teaching or programming an AI to help defend against such manipulation seems much harder, because it's unclear how to distinguish between manipulation and useful information or discussion. (One way to defend against such manipulation would be to cut off all outside contact, including from other humans because we don't know whether they are just being used as other AIs' mouthpieces, but that would be highly detrimental to one's own moral development.)</p>\n<p>There's also an asymmetry between AIs with simple utility functions (either unaligned or aligned to users who think they have simple values) and AIs aligned to users who have high value complexity and moral uncertainty. The former seem to be at a substantial advantage in a contest to manipulate others' values and protect one's own.</p>\n",
    "user": {
      "username": "Wei_Dai",
      "slug": "wei-dai",
      "displayName": "Wei Dai"
    }
  },
  {
    "_id": "RcfRjauKeDqv7ZzyA",
    "title": "Babble, Learning, and the Typical Mind Fallacy",
    "slug": "babble-learning-and-the-typical-mind-fallacy",
    "pageUrl": "https://www.lesswrong.com/posts/RcfRjauKeDqv7ZzyA/babble-learning-and-the-typical-mind-fallacy",
    "postedAt": "2018-12-16T16:51:53.827Z",
    "baseScore": 6,
    "voteCount": 4,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": "http://an1lam.github.io/post/2018-12-16-babble-learning/",
    "htmlBody": "<p>This is an essay I wrote originally just for myself about Babble and decided to post on my personal blog at a whim. The post is messy and digresses more than I&#x27;d like, but writing my thoughts down clarified my own thinking on the topic enough that I figured they might help someone else some day.</p>",
    "user": null
  },
  {
    "_id": "6N3SzS4xsruvfxyTG",
    "title": "What are some concrete problems about logical counterfactuals?",
    "slug": "what-are-some-concrete-problems-about-logical",
    "pageUrl": "https://www.lesswrong.com/posts/6N3SzS4xsruvfxyTG/what-are-some-concrete-problems-about-logical",
    "postedAt": "2018-12-16T10:20:26.618Z",
    "baseScore": 25,
    "voteCount": 6,
    "commentCount": 4,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>Logical counterfactuals are key to <a href=\"https://intelligence.org/2017/10/22/fdt/\">Functional Decision Theory</a> and last I heard still an unsolved problem. Unfortunately, I am still rather confused about what exactly we are trying to solve. The only concrete problem I know of in this space is the <a href=\"https://wiki.lesswrong.com/wiki/5-and-10\">5-and-10 problem</a>. But as far as I know, this is solved by writing programs that immediately cause a paradox if they ever discover their output. So presumably there are some unsolved concrete problems that relate to logical counterfactuals?</p><p><strong>Edit</strong>: I should mention my post on the <a href=\"https://www.lesswrong.com/posts/NcA3dMJoWWEN4BQet/logical-counterfactuals-and-the-cooperation-game\">Cooperation Game</a> as an example. Plus the further work section of this <a href=\"https://intelligence.org/wp-content/uploads/2015/05/Slepnev-Models-of-decision-making-based-on-logical-counterfactuals.pdf\">slideshow</a>.</p>",
    "user": {
      "username": "Chris_Leong",
      "slug": "chris_leong",
      "displayName": "Chris_Leong"
    }
  },
  {
    "_id": "ZdCztwnxXu3aC4kxZ",
    "title": "The E-Coli Test for AI Alignment",
    "slug": "the-e-coli-test-for-ai-alignment",
    "pageUrl": "https://www.lesswrong.com/posts/ZdCztwnxXu3aC4kxZ/the-e-coli-test-for-ai-alignment",
    "postedAt": "2018-12-16T08:10:50.502Z",
    "baseScore": 70,
    "voteCount": 27,
    "commentCount": 24,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Let’s say you have an idea in mind for how to align an AI with human values.</p><p>Go prep a slide with some e-coli, put it under a microscope, and zoom in until you can see four or five cells. Your mission: satisfy the values of those particular e-coli. In particular, walk through whatever method you have in mind for AI alignment. You get to play the role of the AI; with your sophisticated brain, massive computing power, and large-scale resources, hopefully you can satisfy the values of a few simple e-coli cells.</p><p>Perhaps you say “this is simple, they just want to maximize reproduction rate.” Ah, but that’s not quite right. That’s optimizing for the goals of the process of evolution, not optimizing for the goals of the <a href=\"https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter\">godshatter</a> itself. The e-coli has some frozen-in values which have evolved to approximate evolutionary fitness maximization in some environments; your job is optimize for the frozen-in approximation, even in <em>new</em> environments. After all, we don’t want a strong AI optimizing for the reproductive fitness of humans - we want it optimizing for humans’ own values.</p><p>On the other hand, perhaps you say “these cells don’t have any consistent values, they’re just executing a few simple hardcoded algorithms.” Well, you know what else doesn’t have consistent values? Humans. Better be able to deal with that somehow.</p><p>Perhaps you say “these cells are too simple, they can’t learn/reflect/etc.” Well, chances are humans will have the same issue once the computational burden gets large enough.</p><p>This is the problem of AI alignment: we need to both define and optimize for the values of things with limited computational resources and inconsistent values. To see the problem from the AI’s point of view, look through a microscope.</p>",
    "user": {
      "username": "johnswentworth",
      "slug": "johnswentworth",
      "displayName": "johnswentworth"
    }
  },
  {
    "_id": "3BM6J7fA4hMSjQLjF",
    "title": "on wellunderstoodness",
    "slug": "on-wellunderstoodness",
    "pageUrl": "https://www.lesswrong.com/posts/3BM6J7fA4hMSjQLjF/on-wellunderstoodness",
    "postedAt": "2018-12-16T07:22:19.250Z",
    "baseScore": 9,
    "voteCount": 6,
    "commentCount": 2,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head><style type=\"text/css\">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></head><body><p><em>epistemic status: nothing new, informal, may or may not provide a novel compression which gives at least one person more clarity. Also, not an alignment post, just uses arguments about alignment as a motivating example, but the higher-level takeaway is extremely similar to the Rocket Alignment dialogue and to the definition of \"deconfusion\" that MIRI offered recently (even though I'm making a very low-level point)</em></p>\n<p>notation: please read <code>a &lt;- x</code> as \"a replaced with x\", e.g. <code>(fx) x&lt;-5 =&gt; (mx+b) x&lt;-5 =&gt; m5+b</code>. (typically the sequents literature uses <code>fx[5/x]</code> but I find this much harder to read).</p>\n<h1>motivation</h1>\n<p>A younger version of me once said <em>\"What's the big deal with sorceror's apprentice / paperclip maximizer? you would obviously just give an <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\epsilon\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ϵ</span></span></span></span></span></span> and say that you want to be satisfied up to a threshold\"</em>. But ultimately it isn't hard to believe that <a href=\"https://xkcd.com/2030/\">software</a> developers don't want to be worrying about whether some hapless fool somewhere forgot to give the <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\epsilon\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ϵ</span></span></span></span></span></span>. Not to mention that we don't expect the need for such thresholds to be finitely enumerable. Not to mention the need to pick the right epsilons.</p>\n<p>The aspiration of value alignment just tells you \"we want to <em>really understand</em> goal description, we want to understand it <em>so hard</em> that we make epsilons (chasing after the perfectly calibrated thresholds) obsolete\". Well, is it clear to everyone what \"really understand\" even means? It's definitely not obvious, and I don't think everyone's on the same page about it.</p>\n<h1>core claim</h1>\n<p>I want to focus on the following idea;</p>\n<h3>to be mathematically understood is to be reduced to substitution instances.</h3>\n<p>Consider; linear algebra. If I want to teach someone the meaning and the importance of <code>ax+by+c=0</code> I only need to make sure (skipping over addition, multiplication, and equality) that they can distinguish between a scalar and a variable and then sculpt one form from another. Maybe it generalizes to arbitrary length <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a_i x_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span> and to arbitrary number of equations as nicely as it does because the universe rolled us some dumb luck, it's unclear to me, but I know that every time I see a flat thing in the world I can say \"grab the normal forms from the sacred texts and show me by substitution how this data is an instance of that\". Yes, if I had coffee I'd derive it from something more primitive and less sacred, but I've been switching to tea.</p>\n<p>Substitution isn't at the root of the \"<a href=\"https://math.stackexchange.com/questions/1700798/is-linear-algebra-more-fully-understood-than-other-maths-disciplines\">all the questions... that can be asked in an introductory course can be answered in an introductory course</a>\" property, but I view it as the only <em>mechanism</em> in a typical set of formal mechanisms that roots the possibility of particularization. By which I mean the clean, elegant, \"all  wrapped up, time for lunch\" kind.</p>\n<p>Substitutions can behave as a path from a more abstract level to a more concrete level, but if you're lucky they have <em>types</em> to tell us which things fit into other things.</p>\n<p>A fundamentalist monk of the deductionist faith may never succumb to the dark inferences, but they can <em>substitute</em> if they believe they are entitled to do so, and they can be told they are entitled by <em>types</em>. <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a_i : \\mathbb{N} \\vdash \\Sigma{a_i x_i} = 0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">N</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">⊢</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">Σ</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span> allows even our monk to say <code>a &lt;- 5</code> just as soon as they establish that <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"5 : \\mathbb{N}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">N</span></span></span></span></span></span></span></span>. Yes, without the temptation of the dark inferences they'd never have known that the data could be massaged into the form and ultimately resolved by substitution, but this is exactly why we're considering a monk who can't induct or abduct <em>in public</em>, rather than an automaton who can't induct or abduct <em>at all</em>. The closer you get to deductionist fundamentalism, the more you realize that the whole \"never succumb to the dark inferences\" thing is more of a guideline anyway.</p>\n<p>Problems in well-understood domains impose some inferential cost to figure out which forms they're an instance of. But that's it. You do that, and it's constant time from there, substitution only costs 2 inferenceBucks, maybe 3.</p>\n<p>Now compare that to ill-understood domains, like solutions to differential equations. No, not the space of \"practical\" DEs that humans do for economic reasons, I mean every solution for every DE, the space which would only be \"practical\" if armies from dozens of weird <a href=\"https://en.wikipedia.org/wiki/Multiverse#Level_II:_Universes_with_different_physical_constants\">Tegmark II</a>'s were attacking all at once. You can see the monk stepping back, \"woah woah woah, I didn't sign up for this. I can only do things I know how to do, I can't go poking and prodding in these volatile functionspaces, it's too dangerous!\" You'd need some shoot-from-the-hip engineers, the kind that maybe forget an epsilon here and there and scratch their head for an hour before they catch it, but they have to move at that pace! If you're not willing to risk missing, don't shoot from the hip.</p>\n<p>I will give two off-the-cuff \"theorems\", steamrolling/handwaving through two major problems; 1. variety in skill level or cognitive capacity; 2. domains don't really have borders, we don't really have a way to carve math into nations. But we won't let either of those discourage us!</p>\n<h4>\"Theorem\": in a well-understood domain, there is a strict upper bound on the inferential cost of solving an arbitrary problem.</h4>\n<p>There is at least some social component here; on well-tread paths you're far less likely to bother with the wrong sink/rabbit holes. Others in the community have provided wonderful compressions for you, so you don't have to be a big whizzkid.\nWe expect an upper bound because even a brute force search for <code>your problem</code> in <code>corpus of knowledge</code> would terminate in finite time, assuming the corpus is finite and the search doesn't attempt a faulty regex. And like I said, after that search (which is probably slightly better than brute force), substitution is a constant number of steps.</p>\n<h4>\"Followup theorem\": in an ill-understood domain, we really can't give an upper bound on the inferential cost of solving an arbitrary problem. To play it safe, we'll say that problems cost an infinite amount of inferenceBucks unless proven otherwise.</h4>\n<p>to close where I began, we know that sorcerer's apprentice (i.e., goal description as a mathematical domain) is ill-understood because \"fill the cauldron with water\" <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mapsto\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">↦</span></span></span></span></span></span> \"oh crap, i meant maximize p subject to the constraint <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p < 1-\\epsilon\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">ϵ</span></span></span></span></span></span> <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mapsto\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">↦</span></span></span></span></span></span> \"oh crap, one more, last one i swear... \" etc. is worst case an infinite trap.</p>\n<h2>I drew the core insight from conversation</h2>\n<p>At the end of the day, \"well-understood just means substitution\" was my best attempt at clustering the way people talked about different aspects of math in college.</p>\n<p>notes:</p>\n<ul>\n<li>\"inferenceBucks\" sounds almost like graph distance when each inference step is some edge but I was intending more like <a href=\"https://en.wikipedia.org/wiki/Operational_semantics#Structural_operational_semantics\">small-step semantics</a>.</li>\n</ul>\n</body></html>",
    "user": {
      "username": "quinn-dougherty",
      "slug": "quinn-dougherty",
      "displayName": "Quinn"
    }
  },
  {
    "_id": "T6w7DnmBqhR32nvEW",
    "title": "Sabine \"Bee\" Hossenfelder (and Robin Hanson) on How to fix Academia with Prediction Markets",
    "slug": "sabine-bee-hossenfelder-and-robin-hanson-on-how-to-fix",
    "pageUrl": "https://www.lesswrong.com/posts/T6w7DnmBqhR32nvEW/sabine-bee-hossenfelder-and-robin-hanson-on-how-to-fix",
    "postedAt": "2018-12-16T06:37:13.623Z",
    "baseScore": 12,
    "voteCount": 4,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": "http://backreaction.blogspot.com/2018/12/dont-ask-what-science-can-do-for-you.html",
    "htmlBody": "<p>&quot;Don’t ask what science can do for you.&quot; is the clever title. </p><p>As I understand it, the prediction markets in science would not be easily influenced by the scientists and they would take into account a group of people currently completely neglected in scientific evaluations: those who left academia dissatisfied. Not a new approach in the industry, but certainly not something that has been tried in a research-oriented environment, as far as I know.</p>",
    "user": {
      "username": "shminux",
      "slug": "shmi",
      "displayName": "Shmi"
    }
  },
  {
    "_id": "NjFgqv8bzjhXFaELP",
    "title": "New edition of \"Rationality: From AI to Zombies\"",
    "slug": "new-edition-of-rationality-from-ai-to-zombies",
    "pageUrl": "https://www.lesswrong.com/posts/NjFgqv8bzjhXFaELP/new-edition-of-rationality-from-ai-to-zombies",
    "postedAt": "2018-12-15T21:33:56.713Z",
    "baseScore": 84,
    "voteCount": 36,
    "commentCount": 27,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>MIRI is releasing a new edition of <em><strong>Rationality: From AI to Zombies</strong>,</em> including the first set of <em>R:AZ</em> print books. As of this morning, print versions of <em>Map and Territory</em> (volume 1) and <em>How to Actually Change Your Mind</em> (volume 2) are now available <strong>on Amazon </strong>(<a href=\"https://smile.amazon.com/dp/1939311233\">1</a>, <a href=\"https://smile.amazon.com/dp/1939311276/\">2</a>), and we&#x27;ll be rolling out the other four volumes of <em>R:AZ </em>over the coming months.</p><p><em>R:AZ </em>is a book version of Eliezer Yudkowsky&#x27;s <a href=\"https://wiki.lesswrong.com/wiki/Original_sequences\">original sequences</a>, collecting a bit under half of his <em>Overcoming Bias</em> and <em>LessWrong </em>writing <a href=\"https://wiki.lesswrong.com/wiki/Less_Wrong/All_articles\">from November 2006 to September 2009</a>. <em>Map and Territory</em> is the canonical place to start, but we&#x27;ve tried to make <em>How to Actually Change Your Mind</em> a good jumping-on point too, since some people might prefer to dive right into <em>HACYM</em>.</p><p>The price for the print books is $6.50 for <em><a href=\"https://smile.amazon.com/dp/1939311233\">Map and Territory</a></em>, and $8 for <em><a href=\"https://smile.amazon.com/dp/1939311276/\">How to Actually Change Your Mind</a></em>. The new edition is also available electronically (in EPUB, MOBI, and PDF versions) on a pay-what-you-want basis: <a href=\"http://gumroad.com/l/mapterritory\">1</a>, <a href=\"http://gumroad.com/l/howtoactuallychangeyourmind\">2</a>. The <em>HACYM</em> ebook is currently available for preorders, and should be delivered in the next day.</p><p>The previous edition of <em>R:AZ</em> was a single sprawling 1800-page ebook. I announced at the time that we were also going to release a paper version divided into six more manageable chunks; but this ended up taking a lot longer than I expected, and involved more substantive revisions to the text.</p><p>Changes going into the new edition include:</p><ul><li>The first sequence in <em>Map and Territory</em>, &quot;Predictably Wrong,&quot; has been heavily revised, with a goal of making it a much better experience for new readers.</li><li>More generally, <em>R:AZ</em> is now more optimized for new readers, and less focused on extreme fidelity to the original blog posts, since this was one of the biggest requests from LessWrongers in response to the previous edition of <em>R:AZ</em>. This isn&#x27;t a <em>huge</em> change, but it was an update about which option to pick in quite a few textual tradeoffs.</li><li>A bunch of posts have been added or removed. E.g., <a href=\"http://lesswrong.com/rationality/the-robbers-cave-experiment\">The Robbers Cave Experiment</a> was removed because while it&#x27;s still a cool and interesting study, the researchers&#x27; methods and motives have turned out to be <a href=\"https://www.theguardian.com/science/2018/apr/16/a-real-life-lord-of-the-flies-the-troubling-legacy-of-the-robbers-cave-experiment\">pretty bad</a>, and it isn&#x27;t particularly essential to <em>HACYM</em>.</li><li>The &quot;Against Doublethink&quot; sequence in <em>How to Actually Change Your Mind</em> has been removed, to reduce <em>HACYM</em>&#x27;s page count (and therefore its price and its potential to intimidate readers) and improve the book&#x27;s focus. The first post in &quot;Against Doublethink&quot; (<a href=\"https://lesswrong.com/rationality/singlethink\">Singlethink</a>) has been kept, and moved to a different sequence (&quot;Letting Go&quot;).</li><li>Important links and references are now written out rather than hidden behind Easter egg hyperlinks, so they&#x27;ll show up in print editions too. Easter egg links are kept around if they&#x27;re interesting enough to be worth retaining, but not important enough to deserve a footnote; so there will still be some digital-only content, but the goal is for this to be pretty minor.</li><li>A glossary has been added to the back of each book.</li></ul><p>Oliver and Ben also plan to post the digital versions of <em>M&amp;T</em> and <em>HACYM</em> to <a href=\"https://lesswrong.com/rationality\"><em>R:AZ</em> on LessWrong</a> — initially as new posts, though the URLs and comment sections of new and old versions may be merged in the future if LW adds a feature for toggling between post revisions.</p>",
    "user": {
      "username": "RobbBB",
      "slug": "robbbb",
      "displayName": "Rob Bensinger"
    }
  },
  {
    "_id": "sTboWTyf9MfERnsKp",
    "title": "Gwern about centaurs: there is no chance that any useful man+machine combination will work together for more than 10 years, as humans soon will be only a liability",
    "slug": "gwern-about-centaurs-there-is-no-chance-that-any-useful-man",
    "pageUrl": "https://www.lesswrong.com/posts/sTboWTyf9MfERnsKp/gwern-about-centaurs-there-is-no-chance-that-any-useful-man",
    "postedAt": "2018-12-15T21:32:55.180Z",
    "baseScore": 34,
    "voteCount": 15,
    "commentCount": 4,
    "meta": false,
    "question": false,
    "url": "https://www.reddit.com/r/reinforcementlearning/comments/a3rbm3/alphazero_shedding_new_light_on_the_grand_games/",
    "htmlBody": "<p>This is a quote from the discussion in Reddit (responding to another commenter):</p><blockquote>&quot;I remember and I kind of subscribe to this idea of &quot;man+machines as a future of work&quot;. I think Tyler Cowen implanted this idea into my head&quot;</blockquote><blockquote>Yes, Cowen was big on that too in stuff like Average is Over.</blockquote><blockquote>Very irresponsible of them to try to foster complacency like that. It should have been beyond obvious that there was no reason chess engines wouldn&#x27;t keep improving and that at some point very quickly, far from representing a new stable paradigm and a reason to not worry about technological unemployment, the &#x27;centaur&#x27; would be a net liability. As far as I can tell, in chess, the centaur era lasted barely a decade, and would&#x27;ve been shorter still had anyone been seriously researching computer chess rather than disbanding research after Deep Blue or the centaur tournaments kept running instead of stopping a while ago. In Go, it lasted a year at best (if we assume that world champs like Lee Sedol could spot &#x27;delusions&#x27; like made it lose a game to Lee Sedol and contribute at all, but then by the Ke Jie tournament with Master, between Master&#x27;s performance and the various match settings, it looked like humans were way far behind and liabilities when paired with Master, and even if we doubt that, Zero then came out and superseded Master entirely). Not very comforting precedents... The idea was nice but it doesn&#x27;t work.</blockquote>",
    "user": {
      "username": "avturchin",
      "slug": "avturchin",
      "displayName": "avturchin"
    }
  },
  {
    "_id": "n4ukoQzkgbAqpzqb5",
    "title": "Argue Politics* With Your Best Friends",
    "slug": "argue-politics-with-your-best-friends",
    "pageUrl": "https://www.lesswrong.com/posts/n4ukoQzkgbAqpzqb5/argue-politics-with-your-best-friends",
    "postedAt": "2018-12-15T19:00:00.549Z",
    "baseScore": 75,
    "voteCount": 31,
    "commentCount": 6,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Epistemic Status: I endorse this strongly but don’t think I’m being original or clever at all.</em></p>\n<p>Until recently — yesterday, in fact — I was seriously wrong about something.</p>\n<p>I thought that it was silly when I saw people spending lots of energy arguing with their <em>closest </em>friends who <em>almost completely agreed with them, but not quite.</em></p>\n<p>That’s some <a href=\"https://www.youtube.com/watch?v=WboggjN_G-4\">People’s Front Of Judaea </a>shit, I thought.  Don’t you know that guy you’re arguing with so vehemently is your friend?  He likes you!  He’s a pretty good guy!  He even shares your values and models, <em>almost </em>completely! He’s only wrong about this one, itty bitty, relatively abstract thing!</p>\n<p>Meanwhile, there are people out there in the world who <em>don’t </em>share your values. And there are people out there who are <em>actually evil and do awful things</em>.</p>\n<p>It’s like “ok, saying mean things about Muslims can be bad, but being a Muslim terrorist is a hell of a lot worse! Why do the people who are so quick to penalize Islamophobic speech never have anything bad to say about actual mass murder?  C’mon, get a sense of proportion!”</p>\n<p>I still think, obviously, that really bad actions are worse than slightly bad actions.</p>\n<p>But I was <em>seriously </em>misunderstanding why people argue with their close friends.</p>\n<p>Have you noticed my mistake yet?  Give it a moment.</p>\n<p>. . .</p>\n<p>. . .</p>\n<p>. . .</p>\n<p>Ok, here it is.</p>\n<p><em>Arguing is not a punishment.</em></p>\n<p>Again.</p>\n<p><em>Arguing is not a punishment.</em></p>\n<p>Sure, serious wrongdoing should be penalized, and socially disapproved of, more than mild wrongdoing.   (Murder is worse than prejudiced speech.)</p>\n<p>Also, fixing big problems should take priority over fixing little problems. (Saving money on rent is worth more of your attention than saving money on apples.)</p>\n<p>But let’s frame it differently.</p>\n<p><em>Cooperation </em>is really valuable. Stable cooperation, that is; when even in the future, when you know each other better, and you’ve had more time to think, you’ll <em>still </em>want to cooperate.</p>\n<p><em>Trust </em>is really valuable, and scarce.  <em>Justified </em>trust, that is; when you can rely on what somebody says to be true and base your decisions on information you get from them.</p>\n<p>Having “true friends” — people you can cooperate with and trust, stably, to a high degree — is valuable.</p>\n<p>Yeah, you can get along and even thrive in a low-trust environment if you have the right skills for it.  <a href=\"https://nikcorvus.wordpress.com/the-cowboy-havamal-full-text/\">Havamal,</a> the medieval Icelandic wisdom literature, attributed to the god Odin, is my favorite advice for how to be a savvy customer in a low-trust world. (Exercise for the reader: think about how it applies to the replication crisis in science.) But <em>especially </em>in a low-trust world, true friends are valuable, as <em>Havamal </em>will remind you again and again.</p>\n<p>How do you get more trust and cooperation with your friends?</p>\n<p>It’s a hard problem; I haven’t solved it or even really started trying yet, the following are just ideas at the conceptual level rather than things I’ve found successful.</p>\n<p>But communicating with them to get on the same page is <em>clearly </em>part of the puzzle.  Cooperation means “you and I agree to do X, and then we follow through and actually do X.”  The part about willingness to follow through is about loyalty, conscientiousness, motivation, integrity, all those kinds of virtues.  The part about agreeing to do X, though?  That’s not possible unless you both clearly understand <em>what X is</em>, which is much harder than it sounds!  It takes a lot of discussion, in my experience and from what I’ve heard, to get people on the same page about what exactly they’ve committed to doing.</p>\n<p>Moreover, if I don’t understand <em>why </em>X is so important to you, and I say “yeah, ok, sure, X”, and then I go home and back to my life, but X <em>still seems pointless to me</em>, then I’m going to be less motivated to do X.</p>\n<p>Because we didn’t have the <em>argument</em> about “is X pointless or not?”</p>\n<p>We didn’t resolve it. We let it drop, to be nice, because we’re friends and we like each other.  But we <em>didn’t get on the same page, </em>and now a ball got dropped and you’re unhappy with me.</p>\n<p>That getting-on-the-same-page process <em>is not a punishment.</em></p>\n<p>It’s something you’d <em>only </em>do with a friend close enough that you really might cooperate on work that you care about getting done.  (Mundane example: household chores.  Gotta get on the same page about who’s responsible for what!  Negotiating for fewer/different responsibilities is better than shirking!  That can be a really hard thing to internalize, though.)</p>\n<p>“I spend more time communicating and getting on the same page with my friends than I do on having discussions with people I hate” — frame it that way, and suddenly that doesn’t sound like pointless infighting, it sounds mature and practical, right?</p>\n<p>Of <em>course </em>you’d focus most on clarifying communication with your closest friends! They’re the people you’re most likely to be able to cooperate with!</p>\n<p>Ok, so <em>what kind </em>of agreement is most valuable and attainable?  After all, nobody, even your closest friends, agrees with you on <a href=\"https://srconstantin.wordpress.com/2018/12/05/playing-politics/\">everything.</a></p>\n<p>Short term, the answer is obvious: agreement on the details that are practical and relevant to the tasks you share.  Share an apartment?  Gotta come to agreement on chores, and share world-models relevant for those. (It’s no good if I agree to sweep but I don’t know where we keep the broom.)</p>\n<p>But how about the long-run and more meta problem of <em>living in a low-cooperation world</em> itself?</p>\n<p>Here’s one example: we’re in a real trade war with China now. Chinese investment in the US dropped <a href=\"https://www.cnbc.com/2018/06/20/chinese-investment-in-the-us-drops-90-percent-amid-political-pressure.html\">92 percent</a> in the first half of 2018!  I’ve tuned out financial markets for most of my life, but I’m essentially a professional fundraiser now, and let me tell you, <em>a drop in Chinese-US investment that drastic affects a US organization’s ability to raise capital</em>.  Trade wars, like real wars, can come along all of a sudden and destroy value. Cooperation in this sense is less about singing kumbaya and more about not taking a wrecking ball to your own house.  The Hobbesian war of all against all <em>ruins things that people were trying to build.</em></p>\n<p>You want collaborators on fixing <em>that </em>kind of a problem?</p>\n<p>The relevant things to agree (and disagree!) on are about the nature of <em>cooperation and trust themselves</em>. How are alliances and coalitions formed and maintained and broken?  How, and how well, do enforcement mechanisms and incentive strategies work?  You can think of these questions through the lenses of a number of fields:</p>\n<ul>\n<li>game theory</li>\n<li>evolutionary psychology</li>\n<li>some branches of economics (mechanism design, public choice, price theory in general)</li>\n<li>international relations (I know none of this)</li>\n<li>Marxism (I haven’t read Marx either, but I’ve heard that his class analysis can be seen as applied iterated game theory, where a “class” refers to a coalition)</li>\n</ul>\n<p>In all cases, the things to get on the same page about are <em>positive not normative </em>aspects of <em>fundamental </em><em>theory not immediate policy</em>.</p>\n<p>We want <em>long-term </em>cooperation, right? That means <em>fundamentals </em>need to be gotten right. Why? If you focus on object-level policy, it’s too easy for your friend to concur without agreeing (“I agree we should do X, but not with your reason for doing X”), which means that on the <em>next </em>policy question that comes up, your friend might not even concur!</p>\n<p>(I have a friend — a good guy! a smart guy! — who concurs with me on 100% of object-level political controversies, and in every case, he concurs for a reason I think is dumb.  You may know someone like that too.  For the purposes of building long-term cooperation, your friend Mr. Concur is <em>harder </em>to get on the same page with, and thus <em>lower </em>priority to have discussions with, than your friend Ms. Dissent, who starts with the same premises as you but takes them in a totally different direction.  This is counterintuitive, because often <em>you will initially get along better with Mr. Concur!</em> That is because the mechanism that produces “getting along with” and makes friendships closer or weaker is <em>itself a short-term, object-level policy! </em>For instance, people in the same political tribe are nicer to each other.)</p>\n<p>So, that’s why <em>fundamental principles, not immediate policy.</em></p>\n<p>Why positive and not normative?  <em>So you’ll avoid unnecessary hostility.</em></p>\n<p>Hostility, after all, in game-theory-land, is what it feels like from the inside to decide that your interests are opposed to someone else’s.  You can come to this conclusion mistakenly.  To avoid becoming hostile by mistake, <em>first try to clearly understand and communicate what the landscape of interests and incentives even looks like</em>.  That’s what professional negotiators <a href=\"https://en.wikipedia.org/wiki/Getting_to_Yes\">harp on</a> all the time — more often than most people assume, it’s in your interests to <em>keep asking clarifying questions until you understand wtf is going on, and stay cordial enough to keep talking until you understand wtf is going on, </em>because that increases the odds you’ll find a mutually agreeable deal, should one exist.  (Notwithstanding this, there are cases in which obfuscating your negotiating position <em>is </em>in your interest.  That’s less true, I expect, the more meta you go.  Another reason to start with foundations rather than policies.)</p>\n<p>Sticking around for a technical discussion is, itself, a gesture of trust. It invests resources.</p>\n<p>That’s why it’s hard to get this stuff started. As I write this, I haven’t washed up yet, I’m not cleaning the house or reading science papers or adding stuff to the <a href=\"https://thelri.org/blog/\">LRI blog</a>, and I’m ignoring my baby (who, luckily, is happily playing with his toys and smiling at me every so often.)  I’m of the opinion that laying these things out in writing is one of the better ways I have to start coordinated conversations, but, let’s be real, it does involve being a little…spendthrift.  Feeling like “sure, I can afford to do this.”  I’m also reading <a href=\"https://www.amazon.com/Laws-Order-What-Economics-Matters/dp/0691090092\">Law’s Order, </a>currently.  That’s also a resource investment into this whole maybe-doomed “understand the micro-foundations of politics” goal, and it also looks kinda like goofing off, and lookit, aren’t there already economists for this who do it better?  I’m in a remarkably privileged position at the moment when I <em>have </em>a bunch of time flexibility, and something tells me that this is one of the ways I want to be using it.  It is kind of the future of humanity, after all.  But <em>actually </em>spending hours chatting merrily — or furiously — with a friend about what is effectively politics for nerds — well, that’s what people usually call “wasting time”, isn’t it?</p>\n<p>It’s not a waste if you do it well.  But I get that there are a lot of incentives pushing against it.</p>\n<p>What friendly theory talk has going for it is the very long term — getting to be the future’s equivalent of Confucius or Boethius and their friends, or maybe even the <a href=\"https://en.wikipedia.org/wiki/Amoraim\">Amoraim</a>— and the very short term, in which it’s <em>fun </em>to hang out with your friends and talk about interesting things and have some sense that you’re getting somewhere.</p>\n<p>Example question to explore:</p>\n<p>The nitty-gritty of the  “forgiveness” part of “tit-for-tat-with-forgiveness” in iterated games.  There are a lot of slightly different variants of this, I know, which are viable enough to <a href=\"https://thezvi.wordpress.com/2017/11/15/the-darwin-game/\">see play</a>.  Algorithms for <em>recovery of cooperation after defection</em> — how do different ones work? Advantages or disadvantages?  Do any of them correspond to known human behaviors or historical/current institutions?  As a practical matter, what kind of heuristics do people use as to whether or how to revive relationships with friends that have grown distant, pitch to leads that have gone cold, collect debts that have gone unpaid for a long time, etc?</p>\n<p> </p>",
    "user": {
      "username": "sarahconstantin",
      "slug": "sarahconstantin",
      "displayName": "sarahconstantin"
    }
  },
  {
    "_id": "XAwY5czzfkNNQohDJ",
    "title": "Interpreting genetic testing",
    "slug": "interpreting-genetic-testing",
    "pageUrl": "https://www.lesswrong.com/posts/XAwY5czzfkNNQohDJ/interpreting-genetic-testing",
    "postedAt": "2018-12-15T15:56:57.339Z",
    "baseScore": 24,
    "voteCount": 8,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Several years ago I <a href=\"https://www.lesswrong.com/posts/DyZuewJMopqfEnQmo/\">participated in a study</a> where my DNA was sequenced, and while I ended up not getting the sequence data [1] I did get a file of 23andme-style SNP variant calls. I loaded it into Promethease, and excluded mutations with <a href=\"https://www.snpedia.com/index.php/Magnitude\">magnitude</a> below 2 (&quot;looks interesting enough to be worth reading&quot;). I saw 139 mutations marked as &quot;bad,&quot; 41 as &quot;good,&quot; and 26 as &quot;not set.&quot;</p><p>Initially I interpreted this to mean that I should be more pessimistic about my health than I was before getting the report, since more of the mutations are bad (2x risk of something) than good (0.5x risk of something else). To figure out how your beliefs should change, though, you need to know how many bad vs good mutations people typically have. For example, if someone might normally have 200 bad mutations and 10 good ones then my report is good news, but if instead normal is 100 bad mutations and 70 good ones then my report is bad news.</p><p>In general, I would expect most people to have more negative mutations than positive ones, simply because most mutations with an effect are negative. Randomly changing something is much more likely to break things than make them better.</p><p>This also applies when determining total risk of something. For example, lets say I have SNPs that individually give me 3x, 1.5x, 2x, and 0.5x risk for heart disease. I could naively multiply them together, ignoring that they don&#x27;t stack perfectly, [2] and conclude that I had 4.5x the risk of the general population. But most people will probably have <em>some</em> mutations that increase their risk of heart disease. I think the proper way to handle this is for each case where you have the normal value of a variant you count that as slightly improving your risk, and when you consider all of these tiny improvements you get back to the average person having the average risk. Alternatively, and probably more accurately, you could just naively compute each person&#x27;s risk, and then normalize.</p><p>Is this just a Promethease problem? Do other places that give health reports handle this better? Or do places just avoid giving consumer health information because this is both really hard to do well and highly regulated?</p><p>(It&#x27;s also definitely possible I&#x27;m misinterpreting Promethease, or not thinking well about how the stats work here.)</p><p></p><p>[1] This was really frustrating. They confirmed receipt of my sample in September 2012, and in March 2015 they said they had the full 26GB sequence data available for me to transfer. Unfortunately they only ever uploaded the first few 200MB chunks, and then stopped responding to my emails in mid-April. I wrote to them a few more times, and eventually gave up about a year later.</p><p>[2] This caveat about stacking is pretty serious, though. Imagine mutations A and B both give you a 3x risk of some condition. If they act completely independently then a &quot;stacked&quot; 9x risk from having both A and B is reasonable. But if instead A and B act exactly the same way, breaking something that has multiple ways to be rendered fully inoperable, then having them both is no worse than having just one. I don&#x27;t know which end of this is closer to how things usually work.</p>",
    "user": {
      "username": "jkaufman",
      "slug": "jkaufman",
      "displayName": "jefftk"
    }
  },
  {
    "_id": "bYScAyTebG452Ds8A",
    "title": "What is abstraction?",
    "slug": "what-is-abstraction",
    "pageUrl": "https://www.lesswrong.com/posts/bYScAyTebG452Ds8A/what-is-abstraction",
    "postedAt": "2018-12-15T08:36:01.089Z",
    "baseScore": 25,
    "voteCount": 9,
    "commentCount": 11,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>I&#x27;m not clear on what the term abstraction really means. It seems to mean different things in different contexts.</p><p>I&#x27;m a programmer, and when I think of abstraction, the first thing I think of is something that feels like <em>composition</em>. Suppose you are a parent and want to teach your child how to brush their teeth. To do that, you break the task &quot;brush your teeth&quot; into subtasks like 1) prepare your toothbrush, 2) actually brush your teeth, 3) rinse your mouth and 4) clean your toothbrush. In programming, you would break the <code>brushTeeth</code> method into helper methods like <code>prepareToothbrush</code>, <code>actuallyBrushTeeth</code>, <code>rinseMouth</code> and <code>cleanToothbrush</code>. And you would call <code>brushTeeth</code> an abstraction, because it is composed of subtasks. My understanding is that this doesn&#x27;t just apply to programming: in the real world, if you told your child &quot;brush your teeth&quot;, that would also be an abstraction for &quot;prepare your toothbrush, actually brush your teeth, rinse your mouth, and then clean your toothbrush&quot;.</p><p>But there&#x27;s something else that I think of when I think of the term &quot;abstraction&quot;. Suppose I have a program that has people, dogs, and cats. All three have heights, weights and names, so I create an <em>animal</em> class, and I say that people, dogs and cats are more specific versions of an animal. My understanding is that this too would be an abstraction. In programming, <em>and</em> in the real world. My understanding is that the idea of an animal is an abstraction over the idea of a human, dog or cat.</p><p>But there seems to be something very different about each of the two situations. In the first, we would say that the &quot;brush your teeth&quot; abstraction is <em>composed</em> of the subtasks, but we wouldn&#x27;t say that &quot;animal&quot; is composed of humans, dogs and cats in the second. And we would say that the idea of an animal is <em>a more general version</em> of the idea of a human, dog and cat, but we wouldn&#x27;t say that &quot;brush your teeth&quot; is a more general version of preparing your toothbrush or rinsing your mouth.</p><p>The first example seems to be about composition, and the second about generality, so I&#x27;m confused as to what <em>abstraction</em> really is.</p><p>And if we look at yet more contexts, there seems to be more inconsistency in how the term is used. <a href=\"https://en.wikipedia.org/wiki/Abstract_and_concrete\">In semantics and philosophy</a>, &quot;abstract&quot; means something that you can&#x27;t perceive with one of your five senses; the opposite of &quot;concrete&quot;. A tennis ball is concrete because you can see it and touch it, but &quot;tennis&quot; is an abstract concept that you can&#x27;t see or touch. But this use of the word doesn&#x27;t distinguish between <em>degrees</em> of abstraction. According to this use of the word, something either is abstract, or it isn&#x27;t. But with the first two examples, there&#x27;s clearly levels of abstraction.</p><p>And the semantics and philosophy version of abstraction seems like it matches the use of the term in the context of art. In the context of art, art is abstract when it doesn&#x27;t depict concrete things. Eg. a painting that tries to depict fear is abstract because fear is an abstract concept, not a concrete thing, whereas a portrait of Lisa Gherardini (the Mona Lisa) is not abstract art, because it depicts a concrete thing.</p><p>As for the term &quot;abstract&quot; in the context of academic papers, that seemings like it might just be a butchering of the term. It&#x27;s a summary. If you were to <em>summarize</em> the task of brushing your teeth, you wouldn&#x27;t say &quot;here&#x27;s a container that is composed of the subtasks&quot;. If you were to summarize the idea of an animal, that would just be a different thing than saying that it is a more general version of a human, dog and cat.</p><p>In the context of math, abstract math is a field of study. I don&#x27;t know much about it, but from the googling I&#x27;ve done, it seems to be about, well, dealing with questions that are more abstract. More abstract in the sense of generalness, like how an animal is more general than a human. For example, algebra is more abstract than arithmetic. Arithmetic says that <code>1 + 1 = 2</code>, <code>2 + 2 = 4</code> , and <code>3 + 3 = 6</code>. <em>Algebra</em>, says that <code>n + n = 2n</code>. <code>n</code> is more general than <code>1</code> in the same sense that &quot;animal&quot; is more general than &quot;human&quot;. Well, it seems that way at least.</p><p>Bret Victor has an essay called <a href=\"http://worrydream.com/LadderOfAbstraction/\">Up and Down the Ladder of Abstraction</a> where he talks about abstraction as moving away from the concrete by removing concrete properties. The more concrete properties you remove, the more abstract it becomes. He uses the example of a car moving along a curved road according to a certain algorithm. The concrete version of this is a given car on a given curved road pointed in a given direction. You can make it more abstract by looking at the system not at a given point in time, but at <em>all possible</em> points of time, and that will give you a line describing where the car would be at all points in time.</p><span><figure><img src=\"http://funkyimg.com/i/2Pday.png\" class=\"draft-image \" style=\"\" /></figure></span><p>This view of abstraction seems consistent with the idea of abstraction being about making something more general. We said &quot;let&#x27;s not look at the car being at this point in particular, let&#x27;s look at all possible points more generally&quot;. With the math example we said &quot;let&#x27;s not look at the number 1 in particular, let&#x27;s look at any possible number more generally&quot;. With the animal example, we said &quot;let&#x27;s not look at a humans in particular, let&#x27;s look at organisms with any possible [whatever] more generally&quot;.</p><p>These usages of the term &quot;abstraction&quot; seem to be about how general a thing is, where something that is more general has less properties specified, and something that is more specific has more properties specified. But if that is what abstraction is about, why call it abstraction? Why not call it generalization? And if it is indeed about specifying less properties, what word do we use to distinguish between concrete things in the physical world that we can detect with our senses, and things that we can&#x27;t detect with our senses, that we&#x27;d call abstract in the <a href=\"https://en.wikipedia.org/wiki/Abstract_and_concrete\">context of semantics and philosophy</a>?</p><p>I don&#x27;t mean to imply that abstraction actually <em>is </em>used inconsistently. Just that it seems that way to me, and I <em>definitely </em>notice I am confused.  </p>",
    "user": {
      "username": "adamzerner",
      "slug": "adamzerner",
      "displayName": "Adam Zerner"
    }
  },
  {
    "_id": "cGQtDf9arnvRuecW5",
    "title": "Introducing the Longevity Research Institute",
    "slug": "introducing-the-longevity-research-institute",
    "pageUrl": "https://www.lesswrong.com/posts/cGQtDf9arnvRuecW5/introducing-the-longevity-research-institute",
    "postedAt": "2018-12-14T20:20:00.532Z",
    "baseScore": 79,
    "voteCount": 28,
    "commentCount": 11,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>I’ve just founded a nonprofit, the Longevity Research Institute — you can check it out <a href=\"https://thelri.org/\">here.</a></p><p>The basic premise is: we know there are more than 50 compounds that have been reported to extend healthy lifespan in mammals, but most of these have never been tested independently, and in many cases the experimental methodology is poor.</p><p>In other words, there seems to be a <em>lot </em>of low-hanging fruit in aging.  There are many long-lived mutant strains of mice (and invertebrates), there are many candidate anti-aging drugs, but very few of these drugs have actually been tested rigorously.</p><p>Why?  It’s an incentives problem.  Lifespan studies for mice take 2-4 years, which don’t play well with the fast pace of publication that academics want; and the FDA doesn’t consider aging a disease, so testing lifespan isn’t on biotech companies’ critical path to getting a drug approved.  Mammalian lifespan studies are an underfunded area — which is where we come in.</p><p>We write grants to academic researchers and commission studies from contract research organizations.  Our first planned studies are on epitalon (a peptide derived from the pineal gland, which has been reported to extend life in mice, rats, and humans, but only in Russian studies) and C3 carboxyfullerene (yes, a modified buckyball, which prevents Parkinsonism in primate models and has been reported to extend life in mice).  I’m also working on a paper with <a href=\"http://www.vium.com/\">Vium</a> about some of their long-lived mice, and a quantitative network analysis of aging regulatory pathways that might turn up some drug targets.</p><p>We’re currently fundraising, so if this sounds interesting, please consider donating. The more studies that can be launched in parallel, the sooner we can get results.</p>",
    "user": {
      "username": "sarahconstantin",
      "slug": "sarahconstantin",
      "displayName": "sarahconstantin"
    }
  },
  {
    "_id": "fyGEP4mrpyWEAfyqj",
    "title": "Player vs. Character: A Two-Level Model of Ethics",
    "slug": "player-vs-character-a-two-level-model-of-ethics",
    "pageUrl": "https://www.lesswrong.com/posts/fyGEP4mrpyWEAfyqj/player-vs-character-a-two-level-model-of-ethics",
    "postedAt": "2018-12-14T19:40:00.520Z",
    "baseScore": 110,
    "voteCount": 43,
    "commentCount": 32,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Epistemic Status: Confident</em></p>\n<p>This idea is actually due to my husband, Andrew Rettek, but since he doesn’t blog, and I want to be able to refer to it later, I thought I’d write it up here.</p>\n<p>In many games, such as Magic: The Gathering, Hearthstone, or Dungeons and Dragons, there’s a two-phase process. First, the player constructs a <em>deck</em> or <em>character</em> from a very large sample space of possibilities.  This is a particular combination of strengths and weaknesses and capabilities for action, which the player thinks can be successful against other decks/characters or at winning in the game universe.  The choice of deck or character often determines the strategies that deck or character can use in the second phase, which is actual gameplay.  In gameplay, the character (or deck) can only use the affordances that it’s been previously set up with.  This means that there are two separate places where a player needs to get things right: first, in designing a strong character/deck, and second, in executing the optimal strategies for that character/deck during gameplay.</p>\n<p>(This is in contrast to games like chess or go, which are single-level; the capacities of black and white are set by the rules of the game, and the only problem is how to execute the optimal strategy. Obviously, even single-level games can already be complex!)</p>\n<p>The idea is that human behavior works very much like a two-level game.</p>\n<p>The “player” is the whole mind, choosing subconscious strategies.  The “<a href=\"https://en.wikipedia.org/wiki/The_Happiness_Hypothesis\">elephant</a>“, not the “rider.”  The player is very influenced by evolutionary pressure; it is built to direct behavior in ways that increases inclusive fitness.  The player directs what we perceive, do, think, and feel.</p>\n<p>The player <em>creates </em>what we experience as “personality”, fairly early in life; it notices what strategies and skills work for us and invests in those at the expense of others.  It builds our “character sheet”, so to speak.</p>\n<p>Note that even things that seem like “innate” talents, like the <a href=\"https://www.researchgate.net/profile/Isabelle_Soulieres/publication/7321285_Enhanced_Perceptual_Functioning_in_Autism_An_Update_and_Eight_Principles_of_Autistic_Perception/links/02bfe510fe89e400d7000000/Enhanced-Perceptual-Functioning-in-Autism-An-Update-and-Eight-Principles-of-Autistic-Perception.pdf\">savant skills or hyperacute senses</a> sometimes observed in autistic people, can be observed to be tightly linked to feedback loops in early childhood. In other words, savants <em>practice the thing they like and are good at</em>, and gain “superhuman” skill at it.  They “practice” along a faster and more hyperspecialized path than what we think of as a neurotypical “practicing hard,” but it’s still a learning process.  Savant skills are <em>more </em>rigidly fixed and seemingly “automatic” than non-savant skills, but they still change over time — e.g. <a href=\"https://www.stephenwiltshire.co.uk/\">Stephen Wiltshire</a>, a savant artist who manifested an ability to draw hyper-accurate perspective drawings in early childhood, has changed and adapted his art style as he grew up, and even acquired <em>new </em>savant talents in music.  If even savant talents are subject to learning and incentives/rewards, certainly ordinary strengths, weaknesses, and personality types are likely to be “strategic” or “evolved” in this sense.</p>\n<p>The player determines what we find rewarding or unrewarding.  The player determines what we notice and what we overlook; things come to our attention if it suits the player’s strategy, and not otherwise.  The player gives us emotions when it’s strategic to do so.  The player sets up our subconscious evaluations of what is good for us and bad for us, which we experience as “liking” or “disliking.”</p>\n<p>The character is what <em>executing the player’s strategies feels like from the inside</em>.  If the player has decided that a task is unimportant, the character will experience “forgetting” to do it.  If the player has decided that alliance with someone will be in our interests, the character will experience “liking” that person.  Sometimes the player will notice and seize opportunities in a very strategic way that feels to the character like “being lucky” or “being in the right place at the right time.”</p>\n<p>This is where confusion often sets in. People will often protest “but I <em>did </em>care about that thing, I just forgot” or “but I’m <em>not </em>that Machiavellian, I’m just doing what comes naturally.”  This is true, because when we talk about ourselves and our experiences, we’re speaking “in character”, as our character.  The strategy is not going on at a conscious level. In fact, I don’t believe we (characters) have direct access to the player; we can only <em>infer </em>what it’s doing, based on what patterns of behavior (or thought or emotion or perception) we observe in ourselves and others.</p>\n<p>Evolutionary psychology refers to the player’s strategy, not the character’s. (It’s unclear which animals even <em>have </em>characters in the way we do; some animals’ behavior may <em>all </em>be “subconscious”.)  So when someone speaking in an evolutionary-psychology mode says that babies are manipulating their parents to not have more children, for instance, that obviously doesn’t mean that my baby is a cynically manipulative evil genius.  To him, it probably just feels like “I want to nurse at night. I miss Mama.”  It’s perfectly innocent. But of course, this has the <em>effect </em>that I can’t have more children until I wean him, and that’s to his interest (or, at least, it was in the ancestral environment when food was more scarce.)</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Thomas_Szasz\">Szaszian</a> or <a href=\"http://slatestarcodex.com/2018/12/03/book-review-evolutionary-psychopathology/\">evolutionary</a> analysis of mental illness is absurd if you think of it as applying to the character — of course nobody wakes up in the morning and decides to have a mental illness. It’s not “strategic” in that sense. (If it were, we wouldn’t call it mental illness, we’d call it feigning.)  But at the <em>player </em>level, it can be fruitful to ask “what strategy could this behavior be serving the person?” or “what experiences could have made this behavior adaptive at one point in time?” or “what incentives are shaping this behavior?”  (And, of course, externally visible “behavior” isn’t the only thing the player produces: thoughts, feelings, and perceptions are <em>also </em>produced by the brain.)</p>\n<p>It may make more sense to frame it as “what strategy is <em>your brain </em>executing?” rather than “what strategy are <em>you </em>executing?” since people generally identify as their characters, not their players.</p>\n<p>Now, let’s talk morality.</p>\n<p>Our intuitions about praise and blame are driven by <a href=\"https://oll.libertyfund.org/titles/smith-the-theory-of-moral-sentiments-and-on-the-origins-of-languages-stewart-ed\">moral sentiments</a>. We have emotional responses of sympathy and antipathy, towards behavior of which we approve and disapprove. These are driven by the player, which creates incentives and strategic behavior patterns for our characters to play out in everyday life.  The character engages in coalition-building with other characters, forms and breaks alliances with other characters, honors and shames characters according to their behavior, signals to other characters, etc.</p>\n<p>When we, speaking as our characters, say “that person is good” or “that person is bad”, we are making one move in an overall <em>strategy</em> that our players have created.  That strategy is the <em>determination of when, in general, we will call things or people “good” or “bad”</em>.</p>\n<p>This is precisely what Nietzsche meant by “<a href=\"https://www.gutenberg.org/files/4363/4363-h/4363-h.htm\">beyond good and evil</a>.”  Our notions of “good” and “evil” are character-level notions, encoded by our players.</p>\n<p>Imagine that somewhere in our brains, the player has drawn two cartoons, marked “hero” and “villain”, that we consult whenever we want to check whether to call another person “good” or “evil.” (That’s an oversimplification, of course, it’s just for illustrative purposes.)  Now, is <em>the choice of cartoons itself</em> good or evil?  Well, the character checks… “Ok, is it more like the hero cartoon or the villain cartoon?”  The answer is “ummmm….type error.”</p>\n<p>The player is <em>not </em>like a hero or a villain. It is not like a person at all, in the usual (character-level) sense. Characters have feelings! Players don’t have feelings; they are beings of pure strategy that <em>create </em>feelings.  Characters can have virtues or vices! Players don’t; they <em>create </em>virtues or vices, strategically, when they build the “character sheet” of a character’s skills and motivations.  Characters can be <em>evaluated </em>according to moral standards; players <em>set</em><em> </em>those moral standards.  Players, compared to we characters, are hyperintelligent Lovecraftian creatures that we cannot relate to socially.  They are <em>beyond good and evil</em>.</p>\n<p>However! There is another, very different sense in which players <em>can </em>be evaluated as “moral agents”, even though our moral sentiments don’t apply to them.</p>\n<p>We can observe what various game-theoretic strategies <em>do </em>and how they perform.  Some, like “tit for tat”, perform well on the whole.  Tit-for-tat-playing agents cooperate with each other. They can survive pretty well even if there are different kinds of agents in the population; and a population composed entirely of tit-for-tat-ers is stable and well-off.</p>\n<p>While we can’t call cellular automata performing game strategies “good guys” or “bad guys” in a sentimental or socially-judgmental way (they’re <em>not people</em>), we can totally make objective claims about which strategies dominate others, or how strategies interact with one another. This is an empirical and theoretical field of science.</p>\n<p>And there is a kind of “”morality”” which I almost hesitate to call morality because it isn’t very much like social-sentiment-morality at all, but which is <em>very important</em>, which says simply: <em>the strategies that win in the long run are good, the ones that lose in the long run are bad.</em>  Not “like the hero cartoon” or “like the villain cartoon”, but simply “win” and “lose.”</p>\n<p>At this level you can say “look, objectively, people who <em>set up their tables of values </em>in this way, calling X good and Y evil, <em>are gonna die.</em>”  Or “this strategy is conducting a campaign of unsustainable exploitation, which will work well in the short run, but will flame out when it runs out of resources, <em>and so it’s gonna die</em>.”  Or “this strategy is going to lose to that strategy.”  Or “this strategy is fine in the best-case scenario, but it’s not robust to noise, and if there are any negative shocks to the system, it’s going to result in <em>everybody dying.</em>”</p>\n<p>“But what if a losing strategy is good?” Well, if you are in that value system, of course you’ll say it’s good.  Also, you will lose.</p>\n<p>Mother Teresa is a saint, in the literal sense: she was canonized by the Roman Catholic Church. Also, she provided <a href=\"https://en.wikipedia.org/wiki/Criticism_of_Mother_Teresa\">poor medical care </a>for the sick and destitute — unsterilized needles, no pain relief, conditions in which tuberculosis could and did spread.  Was she a good person? It depends on your value system, and, obviously, according to some value systems she was.  But, it seems, that a population that places Mother Teresa as its ideal (relative to, say, Florence Nightingale) will be a population with more deaths from illness, not fewer, and more pain, not less.  A strategy that says “showing care for the dying is better than promoting health” <em>will lose </em>to one that actually can reward actions that promote health.  That’s the “player-level” analysis of the situation.</p>\n<p>Some game-theoretic strategies (what Nietzsche would call “tables of values”) are more survival-promoting than others.  That’s the sense in which you can get from “is” to “ought.”  The Golden Rule (Hillel’s, Jesus’s, Confucius’s, etc) is a “law” of game theory, in the sense that <em>it is a universal, abstract fact, which even a Lovecraftian alien intelligence would recognize, that it’s an effective strategy</em>, which is why it keeps being rediscovered around the world.</p>\n<p>But you can’t adjudicate between character strategies just by <em>being a character playing your strategy</em>.  For instance, a Democrat usually can’t convert a Republican just by <em>being a Democrat at him</em>. To change a player’s strategy is more like “getting the bodymind to change its fundamental assessments of what is in its best interests.”  Which can happen, and can happen deliberately and with the guidance of the intellect! But not without some…what you might call, <em>wiggling things around</em>.</p>\n<p>The way I think the intellect plays into “metaprogramming” the player is indirect; you can <em>infer </em>what the player is doing, do some formal analysis about how that will play out, comprehend (again at the “merely” intellectual level) if there’s an error or something that’s no longer relevant/adaptive, plug that new understanding into <em>some </em>change that the intellect <em>can </em>affect (maybe “let’s try this experiment”), and maybe somewhere down the chain of causality the “player”‘s strategy changes. (Exposure therapy is a simple example, probably much simpler than most: add some experiences of the thing not being dangerous and the player determines it really isn’t dangerous and stops generating fear emotions.)</p>\n<p>You <em>don’t </em>get changes in player strategies just by executing social praise/blame algorithms though; those algorithms are for <em>interacting </em>with other characters.  Metaprogramming is… I want to say “cold” or “nonjudgmental” or “asocial” but none of those words are quite right, because they describe character traits or personalities or mental states and it’s <em>not a character-level thing at all</em>.  It’s a thing Lovecraftian intelligences can do to themselves, in their peculiar tentacled way.</p>\n<p> </p>",
    "user": {
      "username": "sarahconstantin",
      "slug": "sarahconstantin",
      "displayName": "sarahconstantin"
    }
  },
  {
    "_id": "WDXZr5agoEykohcnv",
    "title": "How to reset my password?",
    "slug": "how-to-reset-my-password",
    "pageUrl": "https://www.lesswrong.com/posts/WDXZr5agoEykohcnv/how-to-reset-my-password",
    "postedAt": "2018-12-14T16:18:02.936Z",
    "baseScore": 3,
    "voteCount": 1,
    "commentCount": 1,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>How do I change my password? On the account settings page there is a &quot;reset password&quot; button that emails me a link. I thought this would be additional security measure to get to a &quot;set new password&quot; page, but it only tells me that the password was reset. (To what???) However, my password seems to be unchanged.</p><p>Since I don&#x27;t use the site regularly, I would appreciate an email about the answer, if it&#x27;s not automatic. Thanks!</p>",
    "user": {
      "username": "hirvinen",
      "slug": "hirvinen",
      "displayName": "hirvinen"
    }
  },
  {
    "_id": "hPcCv8bmAkwEXBeuk",
    "title": "What podcasts does the community listen to?",
    "slug": "what-podcasts-does-the-community-listen-to",
    "pageUrl": "https://www.lesswrong.com/posts/hPcCv8bmAkwEXBeuk/what-podcasts-does-the-community-listen-to",
    "postedAt": "2018-12-14T15:40:31.414Z",
    "baseScore": 13,
    "voteCount": 4,
    "commentCount": 6,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>Hi everyone! Excited to hear what people in the community listen to. </p><p>I have been recently enjoying the 80,000 hours podcast and Tyler Cowen&#x27;s. </p><p>https://80000hours.org/podcast/episodes/</p><p>https://medium.com/conversations-with-tyler</p>",
    "user": {
      "username": "hristovassilev",
      "slug": "hristovassilev",
      "displayName": "hristovassilev"
    }
  },
  {
    "_id": "iPGpENE4ARKbzzQmt",
    "title": "Meditations on Momentum",
    "slug": "meditations-on-momentum",
    "pageUrl": "https://www.lesswrong.com/posts/iPGpENE4ARKbzzQmt/meditations-on-momentum",
    "postedAt": "2018-12-14T10:53:05.446Z",
    "baseScore": 107,
    "voteCount": 60,
    "commentCount": 32,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Cross-posted and lightly-edited from <a href=\"https://thedeepdish.org/meditations-on-momentum/\">The Deep Dish</a>.</p><p><strong>Epistemic status:</strong> describing a general phenomenon; may not be correct on every specific point. may have used scientific terms in an annoying metaphorical fashion. elements of pointing out the bleeding obvious, hopefully framed in a novel way.</p><p><strong>tl;dr:</strong><em> </em>positive feedback loops are a thing, thinking in systems/exponentially is hard, intersectionality is underrated.</p><hr class=\"dividerBlock\"/><blockquote><em>“For to everyone who has, more will be given, and he will have an abundance. But from the one who has not, even what he has will be taken away.”</em></blockquote><blockquote>—MATTHEW 25:29</blockquote><p><br/>In 2013, unknown author Robert Galbraith published his debut novel… to crickets<em>.</em></p><p>The first print run of <em>The Cuckoo’s Calling</em> was 1500 copies. It’s not clear how many actually sold. The book occupied 4709th place on Amazon’s bestseller charts.</p><p>And there, perhaps it would have stayed, if the cuckoo in the nest had remained undiscovered. The secret unravelled after a few months: ex-military security contractor Galbraith was a pseudonym for J.K. Rowling. As soon as the news broke, <em>The Cuckoo’s Calling</em> soared to the number one spot on Amazon. Sales increased by 150,000 per cent overnight. Copies from that first neglected print run are now worth thousands of dollars.</p><p>What’s the difference between Robert Galbraith and J.K. Rowling? Clearly, being a talented writer is necessary, but not sufficient. Rowling has <em>momentum</em> on her side. At this point, she could publish the contents of a bowl of alphabet soup, and it would still sell better than 99 per cent of novels by hopeful first-time authors.</p><p>This is a ‘no duh’ example, designed to get you nodding your head along. But momentum is <em>everywhere</em>, and it’s rarely in plain sight. Without being consciously aware of doing so, I’ve written about it in four domains:</p><h4><strong>1. Popularity</strong></h4><blockquote><em>Popular things often get their start through what amounts to good luck. The rapid ascent is driven by something even more powerful than rocket fuel: social contagion. Our opinions and preferences cluster together, but it’s not because we’ve carefully evaluated them on their merits. We just want to feel close to our fellow social apes, and have something to gossip about around the water cooler.</em></blockquote><blockquote><em>In other words, popularity is a lot like herpes. After catching a lucky initial break, it manages to spread to a few hosts, then rides the exponential growth curve until it has planted its gentle, blistery kiss on 60 per cent of the population.</em></blockquote><p><strong>—</strong><a href=\"https://thedeepdish.org/the-madness-of-crowds/\">The Madness of Crowds: Why It Pays to Go Where the Tourists Aren’t</a></p><h4><strong>2. Wealth</strong></h4><blockquote><em>With enough time on his side, [Fry’s] 93 cents transforms into $4.3 billion. If your gut instincts are screaming that this is staggeringly, ridiculously, wrong—well, you’re not alone.</em></blockquote><blockquote><em>As Mark Zuckerberg put it: “Humans don’t understand exponential growth. If you fold a paper 50 times, it goes to the moon and back.”</em></blockquote><blockquote><em>This is a delicious example, not only because the imagery is so jarring—whoa, a tiny sheet of paper can do that?—but because the Zuck himself got it wrong. If you fold a piece of paper 50 times over, it doesn’t make a paltry return trip to the moon—it goes all the way to the freakin’</em> <em>sun. Humans don’t understand exponential growth, indeed.</em></blockquote><p><strong>—</strong><a href=\"https://thedeepdish.org/futurama-compound-interest/\">Futurama Taught Me Everything I Know About Compound Interest</a></p><h4><strong>3. Entrepreneurship</strong></h4><blockquote><em>Something idea-based can be sold over and over again with almost no extra time or effort. It’s infinitely scalable. Your debut album might sell 10 copies (three of which your mum bought) or 10 million, but the amount of work that went into recording it was the same.</em></blockquote><blockquote><em>Scalable careers don’t follow a normal distribution, with a clear relationship between effort and reward. Instead, they produce grotesque inequalities. It doesn’t necessarily matter how good you are, or how hard you work: a select few people capture almost all the rewards, while everyone else gets next to nothing.</em></blockquote><p><strong>—</strong><a href=\"https://thedeepdish.org/barbell-strategy-starving-artist/\">The Barbell Strategy: Don’t Be a Starving Artist</a></p><h4><strong>4. Health</strong></h4><blockquote><em>What’s life like for a moderately fit and muscular person? Well, everything works in your favor. The wind is at your back. You’ve got momentum.</em></blockquote><blockquote><em>The fitter you are, the better your hormonal and metabolic health, the lower your bodyfat, the more relaxed you can be with your diet, the more fun life is, the more motivation you have to train, the cooler feats you can perform, the deeper the habit is ingrained, and so on, in an endless positive feedback loop.</em></blockquote><blockquote><em>In fact, it’s even better than that. Almost all these factors are mutually reinforcing. If you do screw up, and drunkenly devour an entire box of cereal, or take a week off from the gym to clock a new video game, it’s no biggie. Any one link can seize up for a while, and the cycle will keep on turning without it.</em></blockquote><p><em><strong>—</strong></em><a href=\"https://thedeepdish.org/fat-people-are-heroes/\">Fat People Are Heroes</a></p><p>…and a few more examples I’ve collected, but haven’t written about:</p><h4><strong>5. Academia</strong></h4><p>Sociologist Robert Merton coined the term ‘The Matthew Effect’, after the parable of the talents line quoted up top.</p><p>Merton noticed that famous scientists often get credited for discoveries made by lesser-known researchers or grad students toiling in obscurity. Similarly, the success of any given paper often depends on the prominence of the author, and how many early citations it happens to receive:</p><blockquote><em>So great is this problem that we are tempted to turn again to the Scriptures to designate the status-enhancement and status-suppression components of the Matthew effect. We can describe it as the Ecclesiasticus component, from the familiar injunction ‘Let us now praise famous men’.</em></blockquote><p><strong>—</strong><a href=\"http://www.garfield.library.upenn.edu/merton/matthew1.pdf\">The Matthew Effect in Science, Robert K. Merton</a></p><h4><strong>6. Reading</strong></h4><p>Psychologists have discovered the same effect in education. The longer it takes kids to learn how to read, the slower the development of their other cognitive skills and performance:</p><blockquote><em>The longer this developmental sequence is allowed to continue, the more generalized the deficits will become, seeping into more and more areas of cognition and behavior. Or to put it more simply – and sadly – in the words of a tearful nine-year-old, already falling frustratingly behind his peers in reading progress, “Reading affects everything you do.&quot;</em></blockquote><p>—<a href=\"https://books.google.co.th/books/about/Progress_in_Understanding_Reading.html?id=hoH9xMSIGr4C&redir_esc=y\">Progress in Understanding Reading, Keith Stanovich</a></p><h4><strong>7. Market prices</strong></h4><p>For most intents and purposes, the efficient markets hypothesis is correct. But even the Nobel prize-winning EMH creator, Eugene Fama, has admitted there is one major anomaly: <em>momentum</em>, which he describes as the “biggest embarrassment to the theory”.</p><p>Here’s Fama’s old advisor, Benoit Mandelbrot, on the long memory of market pricing:</p><blockquote><em>What a company does today—a merger, a spin-off, a critical product launch—shapes what the company will look like a decade hence; in the same way, its stock-price movements today will influence movements tomorrow.</em></blockquote><blockquote><em>…a bottom line emerges. Stock prices are not independent. Today’s action can, at least slightly, affect tomorrow’s action. The standard model is, again, wrong.</em></blockquote><p>—<a href=\"https://www.goodreads.com/book/show/665134.The_Mis_Behavior_of_Markets\">Benoit Mandelbrot, The (Mis)behavior of Markets</a></p><p></p><p>Had enough?</p><p>There’s also the <a href=\"https://www.researchgate.net/publication/289554313_Dominance_of_the_suppressed_Power-law_size_structure_in_tropical_forests\">height of trees</a>, the <a href=\"https://en.wikipedia.org/wiki/Initial_mass_function\">colour, brightness, and lifetime</a> of stars, the <a href=\"https://en.wikipedia.org/wiki/Cambrian_explosion\">proliferation of  species</a>, the <a href=\"https://en.wikipedia.org/wiki/Halo_effect\">halo</a> and <a href=\"https://en.wikipedia.org/wiki/Horn_effect\">horns</a> effect, <a href=\"https://wiki.lesswrong.com/wiki/Affective_death_spiral\">affective death spirals</a>, and the <a href=\"https://en.wikipedia.org/wiki/Entropy_and_life\">existence of life itself</a>.</p><p>The principle of cumulative advantage spans physics, biology, psychology, economics, and culture. It almost seems like some underlying feature of the universe. Here’s Mandelbrot again:</p><blockquote><em>“Can you seriously compare the wind to a financial market, a gale to a rally, a hurricane to a crash? In terms of the underlying causes, certainly not. But mathematically, yes. It is an extraordinary feature of science that the most diverse, seemingly unrelated, phenomena can be described with the same mathematical tools.”</em></blockquote><p>On the macro scale of the universe—the birth of stars, complex life bootstrapped from mud—momentum is kind of miraculous. For a brief candle-flicker, we get to resist the relentless march of entropy; create defiant bastions of order and beauty amongst the chaos.</p><p>On the micro scale of individual human affairs—wealth, waistlines, popularity, power—momentum is kind of terrifying. It makes us, and it breaks us. The 1 per cent control almost half of the world’s wealth, a small number of startups succeed astronomically, most books are sold by the J.K Rowlings of the world.</p><p>Momentum leaves behind a distinctive calling card, which looks something like this:</p><span><figure><img src=\"https://i2.wp.com/thedeepdish.org/wp-content/uploads/3-8-e1542951554654-700x411.png?resize=700%2C411&ssl=1\" class=\"draft-image \" style=\"width:682%\" /></figure><em>If this graph was drawn to scale, the tail would extend several kilometres off your computer screen. For self-published ebooks, it’s worse: the median number of sales is zero.</em></span><p>You will know these various patterns as the ‘80/20 rule’, power laws, long-tails, and Pareto distributions. The economist Vilfredo Pareto devoted years to the pattern which now bears his name. Surely <em>he</em> has some kind words to say about his curvy wife?</p><blockquote><em>At the bottom of the [curve], men and women starve and children die young. In the broad middle of the curve all is turmoil and motion: people rising and falling, climbing by talent or luck and falling by alcoholism, tuberculosis and other kinds of unfitness. At the very top sit the elite of the elite, who control wealth and power for a time — until they are unseated through revolution or upheaval by a new aristocratic class.</em></blockquote><p>Yikes!</p><p>If each instance of the Matthew effect stayed in its own lane, that would be unfair enough. But as Pareto points out, they’re all hopelessly entangled. Each of these domains – money, opportunity, health, education, talent, prestige – not only compounds on itself; but spills over into the other buckets too. Some interactions are obvious: a successful author will almost by definition make more money. Others are less so: a fit and healthy person might get promoted over an equally-qualified overweight person, for no good reason at all.</p><p>And that’s the <em>positive</em> side of the ledger…</p><p></p><h3><strong>The Downward Spiral</strong></h3><blockquote><em>My dear, here we must run as fast as we can, just to stay in place. And if you wish to go anywhere you must run twice as fast as that.</em></blockquote><blockquote>—Lewis Carroll<br/></blockquote><p>Momentum also works in reverse.</p><p>Imagine your partner breaks up with you.  You start drinking more. The drinking affects your work. You become isolated from friends and family. You stop exercising and looking after yourself. Eventually, you lose your job. Now you have money problems, on top of your declining physical and mental health, and total lack of support network. Things don’t tend to deteriorate in a linear fashion: you spiral downwards faster and faster, until you fall off a cliff.</p><p>The further down you slip, the harder it is to regain lost ground.</p><p>I had a little taste of this recently. A series of bad things came along in quick succession. Each of them would have been OK in isolation; together, they put me into a tailspin. I pride myself on being put-together, but I unravelled disturbingly quickly. Order begets order; chaos begets chaos. It was an uncomfortable reminder that everyone is always only a few strokes of misfortune away from the abyss: there but for the grace of God go I.</p><h3><strong>Further Down the Spiral</strong></h3><p>Just to make it explicit: the title of this post is an homage to Scott Alexander&#x27;s essay <em><a href=\"http://slatestarcodex.com/2014/07/30/meditations-on-moloch/\">Meditations on Moloch.</a></em> As Scott points out in his epic close-reading of an Allen Ginsberg poem, there are obvious things we could do to make the world a better place, but some invisible force stymies our efforts:</p><blockquote><em>If everyone hates the current system, who perpetuates it? And Ginsberg answers: “Moloch”. It’s powerful not because it’s correct – nobody literally thinks an ancient Carthaginian demon causes everything – but because thinking of the system as an agent throws into relief the degree to which the system</em> isn’t <em>an agent.</em></blockquote><p>The same alien ‘otherness’ applies to momentum. A handful of A-list actors are inundated with roles, when tens of thousands of talented hopefuls would jump at the chance to eat the scraps from their table. One per cent of everyone owns half the wealth, while billions of others are desperately poor.</p><p>In every area of life, the people who are least in need of further advantage are most likely to receive it.</p><p>Almost everyone is unhappy with this distribution of outcomes, but blaming ‘capitalism’ or ‘the government’ or whichever tribe you happen to hate might be missing the point. If there <em>is </em>some blind force of nature operating behind the scenes, then the exact same pattern will continue to persist (which might explain why socialist utopias don’t tend to go exactly as planned).</p><p>Back to Pareto, for more cheerful words of encouragement:</p><blockquote><em>“There is no progress in human history. Democracy is a fraud. Human nature is primitive, emotional, unyielding. The smarter, abler, stronger, and shrewder take the lion’s share. The weak starve, lest society become degenerate: One can compare the social body to the human body, which will promptly perish if prevented from eliminating toxins.”</em></blockquote><p>Assume we <em>are</em> dealing with some kind of all-pervasive force of nature. Moloch works tirelessly to destroy everything humans hold dear. The Matthew Effect/momentum is more like the <a href=\"https://www.lesswrong.com/posts/pLRogvJLPPg6Mrvg4/an-alien-god\">blind, alien god of evolution</a>—responsible for <em>creating</em> everything humans hold dear, but in the same mindless fashion, smites entire species into oblivion.</p><p>The universe is neither hostile nor benevolent; it’s utterly indifferent. What to do?</p><h3><strong>The Lord Giveth, and The Lord Taketh Away</strong></h3><p>The <a href=\"https://en.wikipedia.org/wiki/Parable_of_the_talents_or_minas\">parable of the talents</a> says: you better use it or lose it. Get some momentum behind you. Start saving money as early as possible. Reduce debt aggressively. Build behaviours that compound, and nip bad habits in the bud as soon as possible. Stay the hell away from the abyss.</p><p>Saving that first $100,000, as Charlie Munger put it, is a bitch. You have to be the little rocket trying to escape the Earth’s gravitational pull, with all your engines on full thrust. Then you can take your foot off the gas a little, but don’t get complacent. If you lose your momentum, you’ll drift back to earth, slowly at first, then faster and faster, until you slam into the ground at 200 kph.</p><p>You have to fight tooth and claw to get some momentum, and then stay up there just as long as you possibly can.</p><p>This moral sounds suspiciously demonic. But unlike Moloch’s favourite games, which are zero or negative-sum, climbing the pyramid doesn’t always involve stamping on the fingers of those below you.</p><p>Improving your own health and fitness doesn’t make anyone else sickly. Making a consistent habit of reading, or learning new skills, doesn’t make other people dumber. Contrary to popular belief, getting richer doesn’t necessarily make other people poorer. And of course, one of the best ways of getting rich in the first place is refusing to pay a premium for popular things that <em>are</em> <em>popular only because they are popular.</em></p><h3><strong>Extending a Helping Hand</strong></h3><p>If you help yourself without hurting anyone, that’s great, but it still leaves loads of people stuck at the bottom of the curve.</p><p>Three encouraging observations: First, even if the overall pattern never changes, at least the individual data-points can move around.</p><p>We know this happens, because even mighty empires topple. Generational wealth doesn’t last forever. Celebrities burn out or fade away. Trees get struck by lightning. Stars implode. In dynamic societies, everyone gets their turn at the top.</p><p>The second encouraging observation is that momentum reaches a point of diminishing returns.</p><p>Sometimes there are hard physical limits: a redwood can only grow so tall before it takes more energy to pump water up from its roots than its new needles can harvest through photosynthesis. After a certain point, a fit person has to train harder and harder to eke out smaller and smaller gains, and so on.</p><p>Even where there are no physical limits, there’s a rapid drop-off in marginal utility. A famous person receives more offers and opportunities than they know what to do with. The same goes for wealth. After the first couple million bucks, Bill Gates tells us, it’s the same hamburger.</p><p>If you take these two observations together, it makes a lot of sense to extend a helping hand up, rather than keep pushing for smaller and smaller gains. The pattern persists, but you create a lot more mobility up and down the curve.</p><h3><strong>Above and Beyond</strong></h3><p>Maybe Pareto was wrong.</p><p>The third encouraging observation is that mobility might be increasing, <em>without</em> a bloody revolution. [EDIT: had another look, I don&#x27;t think the data actually supports me here. damn!]</p><p>Human nature is primitive and emotional, but not unyielding. Even though we struggle to wrap our monkey-minds around <em>compound interest—</em>much less social contagion and non-linear causality<em>—</em>we’re getting less bad at it.</p><p>It’s pretty cool that J.K. Rowling deliberately tried to play life on hard-mode again. It’s much more exciting that more than 100 billionaires have <a href=\"https://en.wikipedia.org/wiki/The_Giving_Pledge\">pledged</a> to give away most (or all) of their fortunes. And that thousands of ordinary people have made <a href=\"https://www.givingwhatwecan.org/\">a lifetime commitment</a> to give at least 10 per cent of their income to the most effective charities.</p><p>The parable of the talents is pretty cut-throat. My guess is that it’s meant to be descriptive, not normative. And lots of people<em>—</em>even those at the top<em>—</em> <em>aren’t</em> OK with it.</p><p>Sure, it’s the natural order of things. But nature also gave us strychnine, parasitic wasps, and cuddly meerkats that systematically murder their infants. Nature is <a href=\"https://en.wikipedia.org/wiki/Appeal_to_nature\">not to be trusted</a>.</p><p>What’s the moral of the story? As far as I can see:</p><ol><li>work your butt off to get some momentum behind you,</li><li>keep a watchful eye out for any signs of entropy creeping in,</li><li>once you hit the point of diminishing returns, focus your efforts on helping other people up.</li></ol><p>John Wesley, the founder of Methodism, delivered a famous sermon on this topic in the 18th century. I think he summed it up more pithily:</p><blockquote><em>&quot;Having, First, gained all you can, and, Secondly saved all you can, Then give all you can.&quot;</em></blockquote>",
    "user": {
      "username": "richard-meadows-1",
      "slug": "richard-meadows-1",
      "displayName": "Richard Meadows"
    }
  },
  {
    "_id": "a7PmA7NX3zdAuhnoK",
    "title": "Can I use Less Wrong branding in youtube videos?",
    "slug": "can-i-use-less-wrong-branding-in-youtube-videos",
    "pageUrl": "https://www.lesswrong.com/posts/a7PmA7NX3zdAuhnoK/can-i-use-less-wrong-branding-in-youtube-videos",
    "postedAt": "2018-12-14T07:10:16.072Z",
    "baseScore": 3,
    "voteCount": 2,
    "commentCount": 5,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<html><head></head><body><p>I will be starting a youtube channel where I record my Street Epistemology encounters. I want to heavily plug our community, for the benefit of growing it, and also because it's just so darn appropriate (I am literally asking people what they think they know, and how they think they know it).</p>\n<p>I will not be claiming ownership of the branding (I'll be diverting people to the website, of course), but I would probably go so far as to use it in the name of the channel (for lack of a better pithy name, 'Less Wrong' is excellent).\nAre there any legal problems with this?</p>\n</body></html>",
    "user": {
      "username": "Senarin",
      "slug": "senarin",
      "displayName": "Bae's Theorem"
    }
  },
  {
    "_id": "vbtvgNXkufFRSrx4j",
    "title": "Three AI Safety Related Ideas",
    "slug": "three-ai-safety-related-ideas",
    "pageUrl": "https://www.lesswrong.com/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas",
    "postedAt": "2018-12-13T21:32:25.415Z",
    "baseScore": 70,
    "voteCount": 34,
    "commentCount": 38,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p>(I have a health problem that is acting up and making it hard to type for long periods of time, so I'm condensing three posts into one.)</p>\n<p><strong>1. AI design as opportunity and obligation to address human safety problems</strong></p>\n<p>Many AI safety problems are likely to have counterparts in humans. AI designers and safety researchers shouldn't start by assuming that humans are safe (and then try to inductively prove that increasingly powerful AI systems are safe when developed/trained by and added to a team of humans) or try to solve AI safety problems without considering whether their designs or safety approaches exacerbate human safety problems relative to other designs / safety approaches. At the same time, the development of AI may be a huge opportunity to address human safety problems, for example by transferring power from probably unsafe humans to de novo AIs that are designed from the ground up to be safe, or by assisting humans' built-in safety mechanisms (such as moral and philosophical reflection).</p>\n<p><strong>2. A hybrid approach to the human-AI safety problem</strong></p>\n<p>Idealized humans can be safer than actual humans. An example of idealized human is a human whole-brain emulation that is placed in a familiar, safe, and supportive virtual environment (along with other humans for socialization), so that they are not subject to problematic \"distributional shifts\" nor vulnerable to manipulation from other powerful agents in the physical world. One way to take advantage of this is to design an AI that is ultimately controlled by a group of idealized humans (for example, has a terminal goal that refers to the reflective equilibrium of the idealized humans), but this seems impractical due to computational constraints. An idea to get around this is to give the AI an advice or hint, that it can serve that terminal goal by learning from actual humans as an instrumental goal. This learning can include imitation learning, value learning, or other kinds of learning. Then, even if the actual humans become corrupted, the AI has a chance of becoming powerful enough to discard its dependence on actual humans and recompute its instrumental goals directly from its terminal goal. (Thanks to Vladimir Nesov for giving me a <a href=\"https://www.lesswrong.com/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior#5Gx787nr6ynpYBfZH\">hint</a> that led to this idea.)</p>\n<p><strong>3. Several approached to AI alignment will <a href=\"https://wiki.lesswrong.com/wiki/Differential_intellectual_progress\">differentially accelerate</a> intellectual progress that are <a href=\"https://arxiv.org/abs/1811.07871\">analogous</a> to solving problems that are low in the polynomial hierarchy.</strong></p>\n<p>This is bad if the \"good\" kind of intellectual progress (such as philosophical progress) is disproportionally high in the hierarchy or outside PH entirely, or if we just don't know how to formulate such progress as problems low in PH. I think this issue needs to be on the radar of more AI safety researchers.</p>\n<p>(A reader might ask, \"differentially accelerate relative to what?\" An \"aligned\" AI could accelerate progress in a bad direction relative to a world with no AI, but still in a good direction relative to a world with only unaligned AI. I'm referring to the former here.)</p>\n</body></html>",
    "user": {
      "username": "Wei_Dai",
      "slug": "wei-dai",
      "displayName": "Wei Dai"
    }
  },
  {
    "_id": "NdKqCrRJ9JXigYcjp",
    "title": "An Extensive Categorisation of Infinite Paradoxes",
    "slug": "an-extensive-categorisation-of-infinite-paradoxes",
    "pageUrl": "https://www.lesswrong.com/posts/NdKqCrRJ9JXigYcjp/an-extensive-categorisation-of-infinite-paradoxes",
    "postedAt": "2018-12-13T18:36:53.972Z",
    "baseScore": -14,
    "voteCount": 23,
    "commentCount": 48,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><strong>Todo: Yablo&apos;s paradox</strong></p><p>Infinities are one of the most complex and confounding topics in mathematics and they lead to an absurd number of paradoxes. However, many of the paradoxes turn out to be variations on the same theme once you dig into what is actually happening. I will provide informal hints on how surreal numbers could help us solve some of these paradoxes, although the focus on this post is primarily categorisation, so please don&apos;t mistake these for formal proofs. I&apos;m also aware that simply noting that a formalisation provides a satisfactory solution doesn&apos;t philosophically justify its use, but this is also not the focus of this post. I plan to update this post to include new infinite paradoxes that I learn about, including ones that are posted in the comments.</p><p><strong><u>Resolution Paradoxes</u></strong></p><p><strong><a href=\"https://nickbostrom.com/ethics/infinite.pdf\">Infinitarian Paralysis</a></strong>: Suppose there are an infinite number of people and they are happy so there is infinite utility. A man punches 100 people and destroys 1000 utility. He then argues that he hasn&apos;t done anything wrong as there was an infinite amount utility before and that there is still an infinite utility after. What is wrong with this argument?</p><div class=\"spoilers\"><p class=\"spoiler-v2\">If we use cardinal numbers, then we can&apos;t make such a distinction. However, if we use surreal numbers, then n+1 is different from n every when n is infinite.</p></div><p><strong><a href=\"https://www.wikiwand.com/en/Supertask\">Paradox of the Gods</a></strong>: &quot;A man walks a mile from a point &#x3B1;. But there is an infinity of gods each of whom, unknown to the others, intends to obstruct him. One of them will raise a barrier to stop his further advance if he reaches the half-mile point, a second if he reaches the quarter-mile point, a third if he goes one-eighth of a mile, and so on ad infinitum. So he cannot even get started, because however short a distance he travels he will already have been stopped by a barrier. But in that case no barrier will rise, so that there is nothing to stop him setting off. He has been forced to stay where he is by the mere unfulfilled intentions of the gods&quot;</p><div class=\"spoilers\"><p class=\"spoiler-v2\">Let n be a surreal number representing the number of Gods. The man can move: 1/2^n distance before he is stopped.</p></div><p><strong><a href=\"https://www.wikiwand.com/en/Two_envelopes_problem\">Two Envelopes Paradox</a></strong>: Suppose that there are two sealed envelopes with one having twice as much money in it as the other such that you can&apos;t see how much is in either. Having picked one, should you switch?</p><p>If your current envelope has x, then there is a 50% chance of receiving 2x and a 50% chance of receiving x/2, which then creates an expected value of 5x/4. But then according to this logic we should also want to switch again, but then we&apos;d get back to our original envelope.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">This paradox is understood to be due to treating a conditional probability as an unconditional probability. The expected value calculation should be 1/2[EV(B|A&gt;B) + EV(B|A&lt;B)] = 1/2[EV(A/2|A&gt;B) + EV(2A|A&lt;B)] = 1/4 EV(A|A&gt;B) + EV(A|A&lt;B)</p><p class=\"spoiler-v2\">However, if we have a uniform distribution over the positive reals then arguably EV(A|A&gt;B) = EV(A|A&lt;B) in which case the paradox reoccurs. Such a prior is typically called an improper prior as it has an infinite integral so it can&apos;t be normalised. However, even if we can&apos;t practically work with such a prior, these priors are still relevant as it can produce a useful posterior prior. For this reason, it seems worthwhile understanding how this paradox is produced in this case</p></div><p>Part 2:</p><div class=\"spoilers\"><p class=\"spoiler-v2\">Suppose we uniformly consider all numbers between 1/n and n for a surreal n and we double this to generate a second number between 2/n and 2n. We can then flip a coin to determine the order. This now creates a situation where EV(A|A&gt;B) = n+1/n and EV(A|A&lt;B) = (n+1/n)/2. So these expected values are unequal and we avoid the paradox.</p></div><p><strong><a href=\"https://philarchive.org/archive/CAIIUv1\">Sphere of Suffering</a></strong>: Suppose we have an infinite universe with people in a 3-dimensional grid. In world 1, everyone is initially suffering, except for a person at the origin and by time t the happiness spreads out to people within a distance of t from the origin. In world 2, everyone is initially happy, except for a person at the origin who is suffering and this spreads out. Which world is better? In the first world, no matter how much time passes, more people will be suffering than happy, but everyone becomes happy after some finite time and remains that way for the rest of time.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">With surreal numbers, we can use l  to represent how far the grid extents in any particular direction and q to represent how many time steps there are. We should then be able to figure out whether q is long enough that the majority of individual time slices are happy or suffering.</p></div><p><strong><u>Last-Half Paradoxes</u></strong></p><p>Suppose we have an infinite set of finite numbers (for example the natural numbers or the reals), for any finite number x, there exists another finite number 2x. Non-formally, we can say that each finite number is in the first half; indeed the first 1/x for any integer x. Similarly, for any number x, there&apos;s always another number x+n for arbitrary n. </p><p>The canonical example is the <strong><a href=\"https://www.wikiwand.com/en/Hilbert%27s_paradox_of_the_Grand_Hotel\">Hilbert Hotel</a></strong>. If we have an infinite number of rooms labelled 1, 2, 3... all of which are full, we fit an extra person in by shifting everyone up one room number. Similarly, we can fit an infinite number of additional people in the hotel by sending each person in room x to room 2x to free up all of the odd rooms. This kind of trick only works because any finite-indexed person isn&apos;t in the last n rooms or in the last-half of rooms.</p><div class=\"spoilers\"><p class=\"spoiler-v2\"><strong>If</strong> we model the sizes of sets with cardinal numbers, then all countable sets have the same size. In contrast, if we were to say that there were n rooms where n is a surreal number, then n would be the last room and anyone in a room &gt; n/2 would be in the last half of rooms. So by modelling these situations in this manner prevents last half paradoxes, but arguably insisting on the existence of rooms with infinite indexes changes the problem. After all, in the original Hilbert Hotel, all rooms have a finite natural number, but the total number of rooms are infinite.</p></div><p><strong><a href=\"https://www.wikiwand.com/en/Galileo%27s_paradox\">Galileo&apos;s Paradox</a></strong>: Similar to the Hilbert Hotel, but notes that every natural number has a square, but the that only some are squares, so the number of squares must be both at least the number of integers and less than the number of integers.</p><p><strong><a href=\"https://courses.edx.org/courses/course-v1:MITx+24.118x+1T2018/courseware/ffd777cae66444e8971be3a018220b90/192282bb485f413d8911fd4121d4eace/8?activate_block_id=block-v1%3AMITx%2B24.118x%2B1T2018%2Btype%40vertical%2Bblock%40c5b7890e301045f1b52f4816b3740d18\">Bacon&apos;s Puzzle</a></strong>: Suppose that there is an infinitely many people in a line each with either a black hat or a white hat. Each person can see the hats of everyone in front of them, but they can&apos;t see their own hat. Suppose that everyone was able to get together beforehand to co-ordinate on a strategy. How would they be able to ensure that only a finite number of people would guess the colour of their hat incorrectly? This possibility seems paradoxical as each person should only have a 50% chance of guessing their hat correctly.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">We can divide each infinite sequence into equivalence classes where two sequences are equivalent if they stop differing after a finite number of places. Let&apos;s suppose that we choose one representative from each equivalence class (this requires the axiom of choice). If each person guesses their hat as per this representative, after a finite number of places, each person&apos;s guess will be correct.</p><p class=\"spoiler-v2\">This paradox is a result of the representative never last differing from the actually chosen sequence in the last half of the line due to this index having to be finite.</p></div><p><strong><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.735.4486&amp;rep=rep1&amp;type=pdf\">Trumped</a></strong>: Donald Trump is repeatedly offered two days in heaven for one day in hell. Since heaven is as good as hell is bad, Trump decides that this is a good deal and accepts. After completing his first day in hell, God comes back and offers him the deal again. Each day he accepts, but the result is that Trump is never let into heaven.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">Let n be a surreal number representing how many future time steps there are. Trump should accept the deal as long as the day number is less than n/3</p></div><p><strong><a href=\"https://www.wikiwand.com/en/St._Petersburg_paradox\">St Petersberg Paradox</a>:</strong> A coin is tossed until it comes up heads and if it comes up heads on the nth toss, then you win 2^n dollars and the game ends. What is the fair price for playing this game? The expected value is infinite, but if you pay infinity, then it is impossible for you to win money. </p><div class=\"spoilers\"><p class=\"spoiler-v2\">If we model this with surreals, then simply stating that there is potentially an infinite number of tosses is undefined. Let&apos;s suppose first that there is a surreal  number n that bounds the maximum number of tosses. Then your expected value is $n and if we were to pay $n/2, then we&apos;d expect to make $n/2 profit. Now, setting a limit on the number of tosses, even an infinite limit is probably against the spirit of the problem, but the point remains, that paying infinity to play this game isn&apos;t unreasonable as it first sounds and the game effectively reduces to Pascal&apos;s Wager.</p></div><p><strong><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.735.4486&amp;rep=rep1&amp;type=pdf\">Trouble in St. Petersberg</a></strong>: Suppose we have a coin and we toss it until we receive a tails and then stop. We are offered the following deals:</p><ul><li>Offer 1: Lose $1 if it never lands on tails, gain $3 if it lands on tails on toss 1</li><li>Offer 2: Lose $4 if if lands on tails on toss 1, gain $9 if it lands on tails on toss 2</li><li>Offer 2: Lose $10 if if lands on tails on toss 2, gain $13 if it lands on tails on toss 3</li></ul><p>And so on, when we calculate the losses by doubling and adding two and the gains by doubling and adding 3.</p><p>Each deal has a positive expected value, so we should accept all the deals, but then we expect to lose in each possibility.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">In the finite case, there is a large amount of gain in the final offer and this isn&apos;t countered by a loss in a subsequence bet, which is how the EV ends up being positive despite being negative in every other case. The paradox occurs for infinities because there is no last n with an unmitigated gain, but if we say that we are offered n deals where n is a surreal number, then deal n will have this property.</p></div><p><strong><a href=\"https://pdfs.semanticscholar.org/cbe0/488bed039f0068002f53e463b51eb6c6729d.pdf\">Dice Room Murders</a>:</strong> Suppose that a serial killer takes a man hostage. They then roll a ten-sided dice and release them if it comes up 10, killing them otherwise. If they were killed, then the serial killer repeats taking twice as many hostages each time until they are released. It seems that each person taken hostage should have a 1/10 chance of surviving, but there are always more people who survive than die.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">There&apos;s an infinitesimal chance that the dice never comes up a 10. Including this possibility, approximately 50% of people should survive, excluding it the probability is 1/10.</p><p class=\"spoiler-v2\">This is a result of a biased sample. To see a finite version, imagine that there are two people. One tosses a coin. If it&apos;s not heads, the other tosses a coin too. The person-flip instances will be 50/50 heads/tails, but if you average over people before averaging over possibilities, you&apos;ll expect most of the flips to be heads. How you interpret this depends on your perspective on SSA and SIA. In any case, nothing to do with infinities.</p></div><p><strong><a href=\"https://www.wikiwand.com/en/Ross%E2%80%93Littlewood_paradox\">Ross-Littlewood Paradox</a></strong>: Suppose we have a line and at each time period we add 10 balls to the end and then remove the first ball remaining. How many balls remain at the end if we perform this an infinite number of times?</p><div class=\"spoilers\"><p class=\"spoiler-v2\">Our first intuition would be to answer infinite as 9 balls are added in each time period. On the other hand, the nth ball added is removed at time 10n, so arguably all balls are removed. Clearly, we can see that this paradox is a result of all balls being in the first 1/10.</p></div><p><strong><a href=\"https://drive.google.com/file/d/1XDlo-8zexjYDvP8ryJ4v3OquW4KdDlDk/view\">Soccer Teams</a></strong>: Suppose that there are two soccer teams. Suppose one team has players with abilities ...-3,-2,-1,0,1,2,3... Now lets suppose a second team started off equally as good, but all the players trained until they raised their ability levels by one. It seems that the second team should be better as each player has strictly improved, but it also seems like it should be equally good as both teams have the same distribution.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">With surreal numbers, suppose that the teams originally have players between -n and n in ability. After the players have trained, their abilities end up being between -n+1 and n+1. So the distribution ends up being different after all.</p></div><p><strong>Positive Soccer Teams</strong>: Imagine that the skill level of your soccer team is 1,2,3,4... You improve everyone&apos;s score by two which should improve the team, however your skill levels are now 3,4... Since your skill levels are now a subset of the original it could be argued that the original team is better since it has two addition (weakly positive) players assisting.</p><p><strong><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.735.4486&amp;rep=rep1&amp;type=pdf\">Can God Pick an Integer at Random?</a></strong> - Suppose that there are an infinite amount of planets labelled 1,2,3... and God has chosen exactly one. If you bet that God didn&apos;t pick that planet, you lose $2 if God actually chose it and you receive $1/2^n. On each planet, there is a 1/&#x221E; chance of it being chosen and a (&#x221E;-1)/&#x221E; chance of it not being chosen. So there is an infinitesimal expected loss and a finite expected gain, given a positive expected value. This suggests we should bet on each planet, but then we lose $2 on one planet win less than $1 from all the other planets.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">Let surreal n be the number of planets. We expect to win on each finite valued planet, but for surreal-valued planets, our expected gain becomes not only infinitesimal, but smaller than our expected loss. </p></div><p><strong><a href=\"https://people.math.umass.edu/~weston/oldpapers/banach.pdf\">Banach-Tarski Paradox</a></strong>: This paradox involves describing a way in which a ball can be divided up into five sets that can then be reassembled into two identical balls. How does this make any sense? This might not appear like an infinite paradox as first, but this becomes apparent once you dig into the details.</p><p>(Other variants include the <a href=\"https://www.wikiwand.com/en/Hausdorff_paradox\">Hausdorff paradox</a>, <a href=\"https://www.math.hmc.edu/funfacts/ffiles/30001.1-2-8.shtml\">Sierpinski-Mazurkiewicz Paradox</a> and <a href=\"https://www.wikiwand.com/en/Von_Neumann_paradox\">Von Neumann Paradox</a>)</p><div class=\"spoilers\"><p class=\"spoiler-v2\">Let&apos;s first explain how the proof of this paradox works. We divide up the sphere by using the free group of rank 2 to create equivalence classes of points. If you don&apos;t know group theory, you can simply think of this as combinations of <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a, b, a^{-1}, b^{-1}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span></span> where an element is not allowed to be next to its inverse, plus the special element 1. We can then think of this purely in terms of these sequences.</p><p class=\"spoiler-v2\">Let S represent all such sequences and S[a] represent a sequence starting with a. Then <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S=\\{1\\} \\cup S[a] \\cup S[b] \\cup S[a^{-1}] \\cup S[b^{-1}]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">{</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">}</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span> . </p><p class=\"spoiler-v2\">Further <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S[a]=\\{a\\} \\cup S[aa] \\cup S[ab] \\cup S[ab^{-1}]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">{</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">}</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span> </p><p class=\"spoiler-v2\">In other words, noting that there is a bijection between all sequences and all sequences starting with any symbol s, we can write <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S[a] \\equiv S[a] \\cup S[b] \\cup S[b^{-1}]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">&#x2261;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span> </p><p class=\"spoiler-v2\">Breaking down <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S[b]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span> similarly, we get: </p><p class=\"spoiler-v2\"> <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S \\equiv \\{1\\} \\cup (S[a] \\cup S[b] \\cup S[b^{-1}]) \\cup (S[a] \\cup S[b] \\cup S[a^{-1}]) \\cup S[a^{-1}] \\cup S[b^{-1}]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">&#x2261;</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">{</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">}</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">&#x222A;</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">&#x2212;</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span>  </p><p class=\"spoiler-v2\">In other words, almost precisely two copies of itself apart from 1.</p><p class=\"spoiler-v2\">However, instead of just considering the sequences of infinite length, it might be helpful to assign them a surreal length n. Then S[a] consists of &quot;a&quot; plus a string of n-1 characters. So S[aa] isn&apos;t actually congruent to S[a] as the former has n-2 addition characters and the later n-1. This time instead of the paradox relying on there being that no finite numbers are in the last half, it&apos;s relying on there being no finite length strings that can&apos;t have either &quot;a&quot; or &quot;b&quot; prepended in front of them which is practically equivalent if we think of these strings as representing numbers in quaternary.</p></div><p><strong><a href=\"https://philarchive.org/archive/CAIIUv1\">The Headache</a></strong>: Imagine that people live for 80 years. In one world each person has a headache for the first month of their life and are happy the rest, in the other, each person is happy for the first month, but has a headache after that. Further assume that the population triples at the end of each month. Which world is better? In the first, people live the majority of their life headache free, but in the second, the majority of people at any time are headache free.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">If we say that the world runs for t timesteps where t is a surreal number, then the people in the last timesteps don&apos;t get to live all of their lives, so it&apos;s better to choose the world where people only have a headache for the first month.</p></div><p><strong><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.735.4486&amp;rep=rep1&amp;type=pdf\">The Magic Dartboard</a></strong>: Imagine that we have a dartboard where each point is colored either black or white. It is possible to construct a dartboard where all but measure 0 of each vertical line is black and all but measure 0 of each horizontal line is white. This means that that we should expect any particular point to be black with probability 0 and white with probability 0, but it has to be some color.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">One way of constructing this situation is to first start with a bijection f from [0, 1] onto the countable ordinals which is known to exist. We let the black points be those ones where f(x) &lt; f(y). So given any f(y) there are only a countable number of  ordinals less than it, so only a countable number of x that are black. This means that the measure of black points in that line must be 0, and by symmetry we can get the same result for any horizontal line.</p><p class=\"spoiler-v2\">We only know that there will be a countable number of black x for each horizontal line because f(x) will always be in the first 1/n of the ordinals for arbitrary n. If on the other hand we allowed f(x) to be say in the last half of countable ordinals, then for that x we would get the majority of points being black. This is distinct from the other paradoxes in this section as for this argument to be correct, this theorem would have to be wrong. If we were to bite this bullet, it would suggest any other proof using similar techniques might also be wrong. I haven&apos;t investigated enough to conclude whether this would be a reasonable thing to do, but it could have all kinds of consequences.</p></div><p><strong><u>Parity Paradoxes</u></strong></p><p>The canonical example is <strong><a href=\"https://www.wikiwand.com/en/Thomson%27s_lamp\">Thomson&apos;s Lamp</a></strong>. Suppose we have a lamp that is turned on at t=-1, off at t=-1/2, on at t=1/4, ect. At t=0, will the lamp be on or off?</p><div class=\"spoilers\"><p class=\"spoiler-v2\"><strong>Wi</strong>th surreal numbers, this question will depend on whether the number of times that the switch is pressed is represented by an od<a href=\"https://math.stackexchange.com/a/49044/123\">d or even O</a>mrific number, which will depend in a relatively complex manner on how we define the reals.</p></div><p><a href=\"https://www.wikiwand.com/en/Grandi%27s_series\">Grandi&apos;s Series</a>: What is the sum of 1-1+1-1...? </p><div class=\"spoilers\"><p class=\"spoiler-v2\">Using surreal numbers, we can assign a length n to the series as merely saying that it is infinite lacks resolute. The sum then depends on whether n is even or odd.</p></div><p><strong><u>Boundary Paradoxes</u></strong></p><p>This is one class of paradoxes that surreal numbers don&apos;t help with as surreals don&apos;t have a largest finite number or a smallest infnity.</p><p><strong><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.735.4486&amp;rep=rep1&amp;type=pdf\">Satan&apos;s Apple</a></strong>: Satan has cut a delicious apple into infinitely many pieces. Eve can take as many pieces as she likes, but if she takes infinitely many pieces she will be kicked out of paradise and this will outweigh the apple. For any finite number i, it seems like she should take that apple piece, but then she will end up taking infinitely many pieces.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">I find this paradox the most troubling for attempts to formalise actual infinities. If we actually had infinitely many pieces, then we should be able to paint all finitely numbered pieces red and all infinitely numbered pieces blue, but any finite number plus one is finite and any infinite number minus one is infinite, so it doesn&apos;t seem like we can have a red and blue piece next to each other. But then, what does the boundary look like.</p></div><p><strong><u>Trivial Paradoxes</u></strong></p><p>These &quot;paradoxes&quot; may point to interesting mathematical phenomenon, but are so easily resolved that they hardly deserve to be called paradoxes.</p><p><strong><a href=\"https://www.wikiwand.com/en/Gabriel%27s_Horn\">Gabriel&apos;s Horn</a></strong>: Consider rotating 1/x around the x-axis. This can be proven to have finite volume, but infinite surface area. So it can&apos;t contain enough paint to paint its surface.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">It only can&apos;t paint its surface if we assume a fixed finite thickness of paint. As x approaches infinity the size of the cross-section of the horn approaches 0, so past a certain point, this would make the paint thicker than the horn.</p></div><p><strong><u>Miscellaneous</u></strong></p><p><strong><a href=\"https://www.wikiwand.com/en/Bertrand_paradox_(probability)\">Bertrand Paradox</a></strong>: Suppose we have an equilateral triangle inscribed in a circle. If we choose a chord at random, what is the probability that the length of chord is longer than a side of the triangle.</p><p>There are at least three different methods that give different results:</p><ul><li>Picking two random end points gives a probability of 1/3</li><li>Picking a random radius then a random point on that radius gives probability of 1/2</li><li>Picking a random point and using it as a midpoint gives 1/4</li></ul><p>Now some of these method will count diameters multiple times, but even after these are excluded, we still obtain the same probabilities.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">We need to bite the bullet here and say that all of these probabilities are valid. It&apos;s just that we can&apos;t just choose a &quot;random chord&quot; without specifying this more precisely. In other words, there isn&apos;t just a single set of chords, but multiple that can be defined in different ways.</p></div><p><strong><a href=\"https://www.wikiwand.com/en/Zeno%27s_paradoxes\">Zeno&apos;s Paradoxes</a></strong>: There are technically multiple paradoxes, but let&apos;s go with the Dichtomy Paradox. Before you can walk a distance, you must go half way. But before you can get halfway, you must get a quarter-way and before that an eight of the way. So moving a finite distance requires an infinite number of tasks to be complete which is impossible.</p><div class=\"spoilers\"><p class=\"spoiler-v2\">It&apos;s generally consider uncontroversial and boring these days that infinite sequences can converge. But more interesting, this paradox seems to be a result of claiming an infinite amount of time intervals to diverge, whilst allowing an infinite number of space intervals to converge, which is a major inconsistency.</p></div><p><strong><a href=\"https://plato.stanford.edu/entries/paradox-skolem/#2.1\">Skolem&apos;s Paradox</a></strong>: Any countable axiomisation of set theory has a countable model according to the L&#xF6;wenheim&#x2013;Skolem theorem, but Cantor&apos;s Theorem proves that there must be an uncountable set. It seems like this confusion arises from mixing up whether we want to know if there exists a set that contains uncountably many elements or if the set contains uncountably many elements in the model (the corresponding definition of membership in the model only refers to elements in the model). So at a high level, there doesn&apos;t seem to be very much interesting here, but I haven&apos;t dug enough into the philosophical discussion to verify that it isn&apos;t actually relevant.</p><p><em>This post was written with the support of the <a href=\"http://eahotel.org/\">EA Hotel</a></em></p>",
    "user": {
      "username": "Chris_Leong",
      "slug": "chris_leong",
      "displayName": "Chris_Leong"
    }
  },
  {
    "_id": "vk2yS8osapSch9Cz2",
    "title": "The Bat and Ball Problem Revisited",
    "slug": "the-bat-and-ball-problem-revisited",
    "pageUrl": "https://www.lesswrong.com/posts/vk2yS8osapSch9Cz2/the-bat-and-ball-problem-revisited",
    "postedAt": "2018-12-13T07:16:30.017Z",
    "baseScore": 70,
    "voteCount": 29,
    "commentCount": 30,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head><style type=\"text/css\">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></head><body><p><em>Cross posted from <a href=\"https://drossbucket.wordpress.com/2018/12/12/the-bat-and-ball-problem-revisited/\">my personal blog</a>.</em></p>\n<p><em>In this post, I'm going to assume you've come across the Cognitive Reflection Test before and know the answers. If you haven't, it's only three quick questions, <a href=\"https://mindyourdecisions.com/blog/2013/06/24/can-you-correctly-answer-the-cognitive-reflection-test-83-percent-of-people-miss-at-least-1-question/\">go and do it now</a>.</em></p>\n<p>One of the striking early examples in Kahneman's <em>Thinking, Fast and Slow</em> is the following problem:</p>\n<blockquote>\n<p>(1) A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball.</p>\n</blockquote>\n<blockquote>\n<p>How much does the ball cost? _____ cents</p>\n</blockquote>\n<p>This question first turns up informally in <a href=\"https://pdfs.semanticscholar.org/4069/615a36c33e61ca309b8ceaeb628a10d441b5.pdf\">a paper by Kahneman and Frederick</a>, who find that most people get it wrong:</p>\n<blockquote>\n<p>Almost everyone we ask reports an initial tendency to answer “10 cents” because the sum $1.10 separates naturally into $1 and 10 cents, and 10 cents is about the right magnitude. Many people yield to this immediate impulse. The surprisingly high rate of errors in this easy problem illustrates how lightly System 2 monitors the output of System 1: people are not accustomed to thinking hard, and are often content to trust a plausible judgment that quickly comes to mind.</p>\n</blockquote>\n<p>In <em>Thinking Fast and Slow</em>, the bat and ball problem is used as an introduction to the major theme of the book: the distinction between fluent, spontaneous, fast 'System 1' mental processes, and effortful, reflective and slow 'System 2' ones. The explicit moral is that we are too willing to lean on System 1, and this gets us into trouble:</p>\n<blockquote>\n<p>The bat-and-ball problem is our first encounter with an observation that will be a recurrent theme of this book: many people are overconfident, prone to place too much faith in their intuitions. They apparently find cognitive effort at least mildly unpleasant and avoid it as much as possible.</p>\n</blockquote>\n<p>This story is very compelling in the case of the bat and ball problem. I got this problem wrong myself when I first saw it, and still find the intuitive-but-wrong answer very plausible looking. I have to consciously remind myself to apply some extra effort and get the correct answer.</p>\n<p>However, this becomes more complicated when you start considering other tests of this fast-vs-slow distinction. Frederick later combined the bat and ball problem with two other questions to <a href=\"http://emilkirkegaard.dk/en/wp-content/uploads/Shane-Frederick-Cognitive-Re%EF%AC%82ection-and-Decision-Making.pdf\">create the Cognitive Reflection Test</a>:</p>\n<blockquote>\n<p>(2) If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets? _____ minutes</p>\n</blockquote>\n<blockquote>\n<p>(3) In a lake, there is a patch of lily pads. Every day, the patch doubles in size. If it takes 48 days for the patch to cover the entire lake, how long would it take for the patch to cover half of the lake? _____ days</p>\n</blockquote>\n<p>These are designed to also have an 'intuitive-but-wrong' answer (100 minutes, 24 days), and an 'effortful-but-right' answer (5 minutes, 47 days). But this time I <a href=\"https://drossbucket.wordpress.com/2017/02/14/stupid-bat-and-ball/\">seem to be immune to the wrong answers</a>, in a way that just doesn't happen with the bat and ball:</p>\n<blockquote>\n<p>I always have the same reaction, and I don’t know if it’s common or I’m just the lone idiot with this problem. The ‘obvious wrong answers’ for 2. and 3. are completely unappealing to me (I had to look up 3. to check what the obvious answer was supposed to be). Obviously the machine-widget ratio hasn’t changed, and obviously exponential growth works like exponential growth.</p>\n</blockquote>\n<blockquote>\n<p>When I see 1., however, I always think ‘oh it’s that bastard bat and ball question again, I know the correct answer but cannot see it’. And I have to stare at it for a minute or so to work it out, slowed down dramatically by the fact that Obvious Wrong Answer is jumping up and down trying to distract me.</p>\n</blockquote>\n<p>If this test was really testing my propensity for effortful thought over spontaneous intuition, I ought to score zero. I hate effortful thought! As it is, I score two out of three, because I've trained my intuitions nicely for ratios and exponential growth. The 'intuitive', 'System 1' answer that pops into my head is, in fact, the correct answer, and the supposedly 'intuitive-but-wrong' answers feel bad on a visceral level. (Why the hell would the lily pads take the same amount of time to cover the second half of the lake as the first half, when the rate of growth is increasing?)</p>\n<p>The bat and ball still gets me, though. My gut hasn't internalised anything useful, and it's super keen on shouting out the wrong answer in a distracting way. My dislike for effortful thought is definitely a problem here.</p>\n<p>I wanted to see if others had raised the same objection, so I started doing some research into the CRT. In the process I discovered a lot of follow-up work that makes the story much more complex and interesting.</p>\n<p>I've come nowhere near to doing a proper literature review. Frederick's original paper has been cited nearly 3000 times, and dredging through that for the good bits is a lot more work than I'm willing to put in. This is just a summary of the interesting stuff I found on my limited, partial dig through the literature.</p>\n<h1>Thinking, inherently fast and inherently slow</h1>\n<p>Frederick's original Cognitive Reflection Test paper describes the System 1/System 2 divide in the following way:</p>\n<blockquote>\n<p>Recognizing that the face of the person entering the classroom belongs to your math teacher involves System 1 processes — it occurs instantly and effortlessly and is unaffected by intellect, alertness, motivation or the difficulty of the math problem being attempted at the time. Conversely, finding <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\sqrt{19163}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msqrt\"><span class=\"mjx-box\" style=\"padding-top: 0.045em;\"><span class=\"mjx-surd\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">√</span></span><span class=\"mjx-box\" style=\"padding-top: 0.119em; border-top: 1px solid;\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">19163</span></span></span></span></span></span></span></span></span></span> to two decimal places without a calculator involves System 2 processes — mental operations requiring effort, motivation, concentration, and the execution of learned rules.</p>\n</blockquote>\n<p>I find it interesting that he frames mental processes as being <em>inherently</em> effortless or effortful, independent of the person doing the thinking. This is not quite true even for the examples he gives — faceblind people and calculating prodigies exist.</p>\n<p>This framing is important for interpreting the CRT. If the problem inherently has a wrong 'System 1 solution' and a correct 'System 2 solution', the CRT can work as intended, as an efficient tool to split people by their propensity to use one strategy or the other. If there are 'System 1' ways to get the correct answer, the whole thing gets much more muddled, and it's hard to disentangle natural propensity to reflection from prior exposure to the right mathematical concepts.</p>\n<p>My tentative guess is that the bat and ball problem <em>is</em> close to being this kind of efficient tool. Although in some ways it's the simplest of the three problems, solving it in a 'fast', 'intuitive' way relies on seeing the problem in a way that most people's education won't have provided. (I <em>think</em> this is true, anyway - I'll go into more detail later.) I suspect that this is less true the other two problems - ratios and exponential growth are topics that a mathematical or scientific education is more likely to build intuition for.</p>\n<p>(Aside: I'd like to know how these other two problems were chosen. The paper just states the following:</p>\n<blockquote>\n<p>Motivated by this result [the answers to the bat and ball question], two other problems found to yield impulsive erroneous responses were included with the “bat and ball” problem to form a simple, three-item “Cognitive Reflection Test” (CRT), shown in Figure 1.</p>\n</blockquote>\n<p>I have a vague suspicion that Frederick trawled through something like 'The Bumper Book of Annoying Riddles' to find some brainteasers that don't require too much in the way of mathematical prerequisites. The lilypads one has a family resemblance to the classic <a href=\"https://en.wikipedia.org/wiki/Wheat_and_chessboard_problem\">grains-of-wheat-on-a-chessboard puzzle</a>, for instance.)</p>\n<p>However, I haven't found any great evidence either way for this guess. The original paper doesn't break down participants' scores by question – it just gives mean scores on the test as a whole. I did however find <a href=\"https://mpra.ub.uni-muenchen.de/68049/1/MPRA_paper_68049.pdf\">this meta-analysis of 118 CRT studies</a>, which shows that the bat and ball question is the most difficult on average – only 32% of all participants get it right, compared with 40% for the widgets and 48% for the lilypads. It also has the biggest jump in success rate when comparing university students with non-students. That looks like better mathematical education does help on the bat and ball, but it doesn't clear up how it helps. It could improve participants' ability to intuitively see the answer. Or it could improve ability to come up with an 'unintuitive' solution, like solving the corresponding simultaneous equations by a rote method.</p>\n<p>What I'd really like is some insight into what individual people <em>actually do</em> when they try to solve the problems, rather than just this aggregate statistical information. I haven't found exactly what I wanted, but I did turn up a few interesting studies on the way.</p>\n<h1>No, seriously, the answer isn't ten cents</h1>\n<p>My favourite thing I found was <a href=\"https://law.yale.edu/system/files/area/workshop/leo/document/Frederick_Bat-BallProblem.pdf\">this (apparently unpublished) ‘extremely rough draft’</a> by Meyer, Spunt and Frederick from 2013, revisiting the bat and ball problem. The intuitive-but-wrong answer turns out to be <em>extremely</em> sticky, and the paper is basically a series of increasingly desperate attempts to get people to actually think about the question.</p>\n<p>One conjecture for what people are doing when they get this question wrong is the <em>attribute substitution hypothesis</em>. This was <a href=\"https://pdfs.semanticscholar.org/853c/2304f0b6455f27677023e19ffc30dc6ca683.pdf\">suggested early on by Kahneman and Frederick</a>, and is a fancy way of saying that they are instead solving the following simpler problem:</p>\n<blockquote>\n<p>(1) A bat and a ball cost $1.10 in total. The bat costs $1.00.</p>\n</blockquote>\n<blockquote>\n<p>How much does the ball cost? _____ cents</p>\n</blockquote>\n<p>Notice that this is missing the 'more than the ball' clause at the end, turning the question into a much simpler arithmetic problem. This simple problem <em>does</em> have 'ten cents' as the answer, so it's very plausible that people are getting confused by it.</p>\n<p>Meyer, Spunt and Frederick tested this hypothesis by getting respondents to recall the problem from memory. This showed a clear difference: 94% of 'five cent' respondents could recall the correct question, but only 61% of 'ten cent' respondents. It's possible that there is a different common cause of both the 'ten cent' response and misremembering the question, but it at least gives some support for the substitution hypothesis.</p>\n<p>However, getting people to actually answer the question correctly was a much more difficult problem. First they tried  bolding the words <strong>more than the ball</strong> to make this clause more salient. This made surprisingly little impact: 29% of respondents solved it, compared with 24% for the original problem. Printing both versions was slightly more successful, bumping up the correct response to 35%, but it was still a small effect.</p>\n<p>After this, they ditched subtlety and resorted to pasting these huge warnings above the question:</p>\n<p><img src=\"https://drossbucket.files.wordpress.com/2018/12/bat_and_ball.png\" alt=\"Computation warning: 'Be careful! Many people miss the following problem because they do not take the time to check their answer. Comprehension warning: 'Be careful! Many people miss the following problem because they read it too quickly and actually answer a different question than the one that was asked.'\"></p>\n<p>These were still only mildly effective, with a correct solution jumping to 50% from 45%. People just really like the answer 'ten cents', it seems.</p>\n<p>At this point they completely gave up and just flat out added “HINT: 10 cents is not the answer.” This worked reasonably well, though there was still a hard core of 13% who persisted in writing down 'ten cents'.</p>\n<p>That's where they left it. At this point there's not really any room to escalate beyond confiscating the respondents' pens and prefilling in the answer 'five cents', and I worry that somebody would still try and scratch in 'ten cents' in their own blood. The wrong answer is just incredibly compelling.</p>\n<h1>So, what <em>are</em> people doing when they solve this problem?</h1>\n<p>Unfortunately, it's hard to tell from the published literature (or at least what I found of it). What I'd really like is lots of transcripts of individuals talking through their problem solving process. The closest I found was <a href=\"https://www.tandfonline.com/doi/abs/10.1080/13546783.2017.1292954\">this paper</a> by Szaszi et al, who did carry out these sort of interview, but it doesn't include any examples of individual responses. Instead, it gives a aggregated overview of types of responses, which doesn't go into the kind of detail I'd like.</p>\n<p>Still, the examples given for their response categories give a few clues. The categories are:</p>\n<ul>\n<li>\n<p><strong>Correct answer, correct start.</strong> Example given: 'I see. This is an equation. Thus if the ball equals to x, the bat equals to x plus 1... '</p>\n</li>\n<li>\n<p><strong>Correct answer, incorrect start.</strong> Example: 'I would say 10 cents... But this cannot be true as it does not sum up to €1.10...'</p>\n</li>\n<li>\n<p><strong>Incorrect answer, reflective</strong>, i.e. some effort was made to reconsider the answer given, even if it was ultimately incorrect. Example: '... but I'm not sure... If together they cost €1.10, and the bat costs €1 more than the ball... the solution should be 10 cents. I'm done.'</p>\n</li>\n<li>\n<p><strong>No reflection.</strong> Example: 'Ok. I'm done.'</p>\n</li>\n</ul>\n<p>These demonstrate one way to reason your way to the correct answer (solve the simultaneous equations) and one way to be wrong (just blurt out the answer). They also demonstrate one way to recover from an incorrect solution (think about the answer you blurted out and see if it actually works). Still, it's all rather abstract and high level.</p>\n<h1>How To Solve It</h1>\n<p>However, I did manage to stumble onto another source of insight. While researching the problem I came across <a href=\"https://www.psychologicalscience.org/publications/observer/obsonline/a-new-twist-on-a-classic-puzzle.html\">this article</a> from the online magazine of the Association for Psychological Science, which discusses a variant 'Ford and Ferrari problem'. This is quite interesting in itself, but I was most excited by the comments section. Finally some examples of how the problem is solved in the wild!</p>\n<p>The simplest 'analytical', 'System 2' solution is to rewrite the problem as two simultaneous linear equations and plug-and-chug your way to the correct answer. For example, writing <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span> for the bat and <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"b\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span></span></span></span> for the ball, we get the two equations</p>\n<p><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B + b = 110\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">110</span></span></span></span></span></span>,\n<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B - b = 100\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">100</span></span></span></span></span></span>,</p>\n<p>which we could then solve in various standard ways, e.g.</p>\n<p><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"2B = 210\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">210</span></span></span></span></span></span>,\n<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B = 105\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">105</span></span></span></span></span></span>,</p>\n<p>which then gives</p>\n<p><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"b = 110 - B = 5\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">110</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">5</span></span></span></span></span></span>.</p>\n<p>There are a couple of variants of this explained in the comments. It's a very reliable way to tackle the problem: if you already know how to do this sort of rote method, there are no surprises. This sort of method would work for any similar problem involving linear equations.</p>\n<p>However, it's pretty obvious that a lot of people <em>won't</em> have access to this method. Plenty of people noped out of mathematics long before they got to simultaneous equations, so they won't be able to solve it this way. What might be less obvious, at least if you mostly live in a high-maths-ability bubble, is that these people may also be missing the sort of tacit mathematical background that would even allow them to frame the problem in a useful form in the first place.</p>\n<p>That sounds a bit abstract, so let's look at some responses (I'll paste all these straight in, so any typos are in the original). First, we have these two confused commenters:</p>\n<blockquote>\n<p>The thing is, why does the ball have to be $.05? It could have been .04 0r.03 and the bat would still cost more than $1.</p>\n</blockquote>\n<p>and</p>\n<blockquote>\n<p>This is exactly what bothers me and resulted in me wanting to look up the question online. On the quiz the other 2 questions were definitive. This one technically could have more than one answer so this is where phycologists actually mess up when trying to give us a trick question. The ball at .4 and the bat at 1.06 doesn’t break the rule either.</p>\n</blockquote>\n<p>These commenters don't automatically see two equations in two variables that together are enough to constrain the problem. Instead they seem to focus mainly on the first condition (adding up to $1.10) and just use the second one as a vague check at best ('the bat would still cost more than $1'). This means that they are unable to immediately tell that the problem has a unique solution.</p>\n<p>In response, another commenter, Tony, suggests a correct solution which is an interesting mix of writing the problem out formally and then figuring out the answer by trial and error:\\</p>\n<blockquote>\n<p>I hear your pain. I feel as though psychologists and psychiatrists get together every now and then to prove how stoopid I am. However, after more than a little head scratching I’ve gained an understanding of this puzzle. It can be expressed as two facts and a question A=100+B and A+B=110, so B=? If B=2 then the solution would be 100+2+2 and A+B would be 104. If B=6 then the solution would be 100+6+6 and A+B would be 112. But as be KNOW A+B=110 the only number for B on it’s own is 5.</p>\n</blockquote>\n<p>This suggests enough half-remembered mathematical knowledge to find a sensible abstract framing, but not enough to solve it the standard way.</p>\n<p>Finally, commenter Marlo Eugene provides an ingenious way of solving the problem without writing all the algebraic steps out:</p>\n<blockquote>\n<p>Linguistics makes all the difference. The conceptual emphasis seems to lie within the word MORE.</p>\n</blockquote>\n<blockquote>\n<p>X + Y = $1.10. If X = $1 MORE then that leaves $0.10 TO WORK WITH rather than automatically assign to Y</p>\n</blockquote>\n<blockquote>\n<p>So you divide the remainder equally (assuming negative values are disqualified) and get 0.05.</p>\n</blockquote>\n<p>So even this small sample of comments suggests a wide diversity of problem-solving methods leading to the two common answers. Further, these solutions don't all split neatly into 'System 1' 'intuitive' and 'System 2' 'analytic'. Marlo Eugene's solution, for instance, is a mixed solution of writing the equations down in a formal way, but then finding a clever way of just seeing the answer rather than solving them by rote.</p>\n<p>I'd still appreciate more detailed transcripts, including the time taken to solve the problem. My suspicion is still that very few people solve this problem with a fast intuitive response, in the way that I rapidly see the correct answer to the lilypad question. Even the more 'intuitive' responses, like Marlo Eugene's, seem to rely on some initial careful reflection and a good initial framing of the problem.</p>\n<p>If I'm correct about this lack of fast responses, my tentative guess for the reason is that it has something to do with the way most of us learn simultaneous equations in school. We generally learn arithmetic as young children in a fairly concrete way, with the formal numerical problems supplemented with lots of specific examples of adding up apples and bananas and so forth.</p>\n<p>But then, for some reason, this goes completely out of the window once the unknown quantity isn't sitting on its own on one side of the equals sign. This is instead hived off into its own separate subject, called 'algebra', and the rules are taught much later in a much more formalised style, without much attempt to build up intuition first.</p>\n<p>(One exception is the sort of puzzle sheets that are often given to young kids, where the unknowns are just empty boxes to be filled in. Sometimes you get 2+3=□, sometimes it's 2+□=5, but either way you go about the same process of using your wits to figure out the answer. Then, for some reason I'll never understand, the worksheets get put away and the poor kids don't see the subject again until years later, when the box is now called <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span></span></span></span> for some reason and you have to find the answer by defined rules. Anyway, this is a separate rant.)</p>\n<p>This lack of a rich background in puzzling out the answer to specific concrete problems means most of us lean hard on formal rules in this domain, even if we're relatively mathematically sophisticated. Only a few build up the necessary repertoire of tricks to solve the problem quickly by insight. I'm reminded of a story in Feynman's <em>The Pleasure of Finding Things Out</em>:</p>\n<blockquote>\n<p>Around that time my cousin, who was three years older, was in high school. He was having considerable difficulty with his algebra, so a tutor would come. I was allowed to sit in a corner while the tutor would try to teach my cousin algebra. I'd hear him talking about x.</p>\n</blockquote>\n<blockquote>\n<p>I said to my cousin, \"What are you trying to do?\"</p>\n</blockquote>\n<blockquote>\n<p>\"I'm trying to find out what x is, like in 2x + 7 = 15.\"</p>\n</blockquote>\n<blockquote>\n<p>I say, \"You mean 4.\"</p>\n</blockquote>\n<blockquote>\n<p>\"Yeah, but you did it by arithmetic. You have to do it by algebra.\"</p>\n</blockquote>\n<blockquote>\n<p>I learned algebra, fortunately, not by going to school, but by finding my aunt's old schoolbook in the attic, and understanding that the whole idea was to find out what <em>x</em> is - it doesn't make any difference how you do it.</p>\n</blockquote>\n<p>I think this reliance on formal methods might be somewhat less true for exponential growth and ratios, the subjects underpinning the lilypad and widget questions. Certainly I seem to have better intuition there, without having to resort to rote calculation. But I'm not sure how general this is.</p>\n<h1>How To Visualise It</h1>\n<p>If you wanted to solve the bat and ball problem without having to 'do it by algebra', how would you go about it?</p>\n<p>My <a href=\"https://drossbucket.wordpress.com/2017/02/14/stupid-bat-and-ball/\">original post</a> on the problem was a pretty quick, throwaway job, but over time it picked up some truly excellent comments by anders and Kyzentun, which really start to dig into the structure of the problem and suggest ways to 'just see' the answer. The thread with anders in particular goes into lots of other examples of how we think through solving various problems, and is well worth reading in full. I'll only summarise the bat-and-ball-related parts of the comments here.</p>\n<p>We all used some variant of the method suggested by Marlo Eugene in the comments above. Writing out the basic problem again, we have:</p>\n<p><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B + b = 110\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">110</span></span></span></span></span></span>,\n<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B - b = 100\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">100</span></span></span></span></span></span>.</p>\n<p>Now, instead of immediately jumping to the standard method of eliminating one of the variables, we can just look at what these two equations are saying and solve it directly 'by thinking'. We have a bat, <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span>. If you add the price of the ball, <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"b\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span></span></span></span>, you get 110 cents. If you instead remove the same quantity <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"b\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">b</span></span></span></span></span></span> you get 100 cents. So the bat's price must be exactly halfway between these two numbers, at 105 cents. That leaves five for the ball.</p>\n<p>Now that I'm thinking of the problem in this way, I directly see the equations as being 'about a bat that's halfway between 100 and 110 cents', and the answer is incredibly obvious.</p>\n<p>Kyzentun suggests a variant on the problem that is much less counterintuitive than the original:</p>\n<blockquote>\n<p>A centered piece of text and its margins are 110 columns wide. The text is 100 columns wide. How wide is one margin?</p>\n</blockquote>\n<blockquote>\n<p>Same numbers, same mathematical formula to reach the solution. But less misleading because you know there are two margins, and thus know to divide by two after subtracting.</p>\n</blockquote>\n<p>In the original problem, the 110 units and 100 units both refer to something abstract, the sum and difference of the bat and ball. In Kyzentun's version these become much more concrete objects, the width of the text and the total width of the margins. The work of seeing the equations as relating to something concrete has mostly been done for you.</p>\n<p>Similarly, anders works the problem by 'getting rid of the 100 cents', and splitting the remainder in half to get at the price of the ball:</p>\n<blockquote>\n<p>I just had an easy time with #1 which I haven’t before. What I did was take away the difference so that all the items are the same (subtract 100), evenly divide the remainder among the items (divide 10 by 2) and then add the residuals back on to get 105 and 5.</p>\n</blockquote>\n<blockquote>\n<p>The heuristic I seem to be using is to treat objects as made up of a value plus a residual. So when they gave me the residual my next thought was “now all the objects are the same, so whatever I do to one I do to all of them”.</p>\n</blockquote>\n<p>I think that after reasoning my way through all these perspectives, I'm finally at the point where I have a quick, 'intuitive' understanding of the problem. But it's surprising how much work it was for such a simple bit of algebra.</p>\n<h1>Final thoughts</h1>\n<p>Rather than making any big conclusions, the main thing I wanted to demonstrate in this post is how <em>complicated</em> the story gets when you look at one problem in detail. I've written about <a href=\"https://drossbucket.wordpress.com/2018/11/04/a-braindump-on-derrida-and-close-reading/\">close reading</a> recently, and this has been something like a close reading of the bat and ball problem.</p>\n<p>Frederick's original paper on the Cognitive Reflection Test is in that generic social science style where you define a new metric and then see how it correlates with a bunch of other macroscale factors (either big social categories like gender or education level, or the results of other statistical tests that try to measure factors like time preference or risk preference). There's a strange indifference to the details of the test itself – at no point does he discuss why he picked those specific three questions, and there's no attempt to model what was making the intuitive-but-wrong answer appealing.</p>\n<p>The later paper by Meyer, Spunt and Frederick is much more interesting to me, because it really starts to pick apart the specifics of the bat and ball problem. Is an easier question getting substituted? Can participants reproduce the correct question from memory?</p>\n<p>I learned the most from the individual responses, though. This is where you really get to see the variety of ways that people tackle the problem. Careful reflection definitely seems to improve the chance of a correct answer in general, but many of the responses don't really fit the neat 'fast vs slow' division of the original setup.</p>\n<h1>Questions</h1>\n<p>I'm interested in any comments on the post, but here are a few specific things I'd like to get your answers to:</p>\n<ul>\n<li>\n<p>My rapid, intuitive answer for the bat and ball question is wrong (at least until I retrained it by thinking about the problem way too much). However, for the other two I 'just see' the correct answer. Is this common for other people, or do you have a different split?</p>\n</li>\n<li>\n<p>If you're able to rapidly 'just see' the answer to the bat and ball question, how do you do it?</p>\n</li>\n<li>\n<p>How do people go about designing tests like these? This isn't at all my field and I'd be interested in any good sources. I'd kind of assumed that there'd be some kind of serious-business Test Creation Methodology, but for the CRT at least it looks like people just noticed they got surprising answers for the bat and ball question and looked around for similar questions. Is that unusual compared to other psychological tests?</p>\n</li>\n</ul>\n</body></html>",
    "user": {
      "username": "drossbucket",
      "slug": "drossbucket",
      "displayName": "drossbucket"
    }
  },
  {
    "_id": "3fkBWpE4f9nYbdf7E",
    "title": "Multi-agent predictive minds and AI alignment",
    "slug": "multi-agent-predictive-minds-and-ai-alignment",
    "pageUrl": "https://www.lesswrong.com/posts/3fkBWpE4f9nYbdf7E/multi-agent-predictive-minds-and-ai-alignment",
    "postedAt": "2018-12-12T23:48:03.155Z",
    "baseScore": 63,
    "voteCount": 27,
    "commentCount": 18,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Abstract: An attempt to map a best-guess model of how human values and motivations work to several more technical research questions. The mind-model is inspired by predictive processing / active inference framework and multi-agent models of the mind.</em></p><p>The text has slightly unusual epistemic structure:</p><p><strong>1st part:</strong> my current best-guess model of how human minds work. </p><p><strong>2nd part:</strong> explores various problems which such mind architecture would pose for some approaches to value learning. The argument is: if such a model seems at least plausible, we should probably extend the space of active research directions. </p><p><strong>3rd part:</strong> a list of specific research agendas, sometimes specific research questions, motivated by the previous. </p><p>I put more credence in the usefulness of research questions suggested in the third part than in the specifics of the model described the first part.  Also, you should be warned I have no formal training in cognitive neuroscience and similar fields, and it is completely possible I’m making some basic mistakes. Still, my feeling is even if the model described in the first part is wrong, something from the broad class of “motivational systems not naturally described by utility functions” is close to reality, and understanding problems from the 3rd part can be useful.</p><h1>How minds work</h1><p>As noted, this is a “best guess model”. I have large uncertainty about how human minds actually work. But if I could place just one bet, I would bet on this.</p><p>The model has two prerequisite ideas: <a href=\"http://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/\">predictive processing</a> and the active inference framework.  I&#x27;ll give brief summaries and links for elsewhere. </p><p>In the predictive processing / the active inference framework, brains constantly predict sensory inputs, in a hierarchical generative way. As a dual, action is also “generated” by the same machinery (changing environment to match “predicted” desirable inputs and generating action which can lead to them). The “currency” on which the whole system is running is prediction error (or something in style of <a href=\"https://en.wikipedia.org/wiki/Free_energy_principle\">free energy, in that language</a>).</p><p>Another important ingredient is <a href=\"https://en.wikipedia.org/wiki/Bounded_rationality\">bounded rationality,</a> i.e. a limited amount of resources being available for cognition. Indeed, the specifics of hierarchical modelling, neural architectures, principle of reusing and repurposing everything, all seem to be related to quite brutal optimization pressure, likely related to brain’s enormous energy consumption (It is unclear to me if this can be also reduced to the same “currency”. Karl Friston would probably answer &quot;yes&quot;).</p><p>Assuming this whole, how do motivations and “values” arise? The guess is, in many cases something like a “subprogram” is modelling/tracking some variable, “predicting” its desirable state, and creating the need for action by “signalling” prediction error. Note that such subprograms can work on variables on very different hierarchical layers of modelling - e.g. tracking a simple variable like “feeling hungry” vs. tracking a variable like “social status”. Such sub-systems can be large: for example tracking “social status” seems to require lot of computation. </p><p><figure><img src=\"https://lh5.googleusercontent.com/a9yC2OFMZWepWZxdJI0uGd7-4P8iaxtxTImuu-aEFmiqZtnEqKMx8pHOcahb1Jhmp42_IVrJWilvlPhMkAN4CiEYhM7ZuQ6kq0dn0fvhMsQYbqPcKAGyoLWiNaeDKPIpvI5UKKPi\" class=\"draft-image \" style=\"width:624%\" /></figure></p><p>How does this relate to emotions? Emotions could be quite complex processes, where some higher-level modelling (“I see a lion”) leads to a response in lower levels connected to body states, some chemicals are released, and this <a href=\"https://en.wikipedia.org/wiki/Interoception\">interoceptive</a> sensation is re-integrated in the higher levels in the form of emotional state, eventually reaching consciousness. Note that the emotional signal from the body is more similar to “sensory” data - the guess is body/low level responses are a way how genes insert a reward signal into the whole system. </p><p>How does this relate to our conscious experience, and stuff like <a href=\"https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\">Kahneman&#x27;s System 1/System 2</a>? It seems for most people the light of consciousness is illuminating only a tiny part of the computation, and most stuff is happening in the background. Also, S1 has much larger computing power. On the other hand it seems relatively easy to “spawn background processes” from the conscious part, and it seems possible to illuminate larger part of the background processing than is usually visible through specialized techniques and efforts (for example, some meditation techniques). </p><p><figure><img src=\"https://lh5.googleusercontent.com/cz-eMZothqPCvYT4E75dEwWiJkZ38mx4Ql3xasG1Se3U4n3ncjXDrOcziuNEgrT92oobf1ejRG3gCf-OaptZXMQN8TTibrFl4hFJ8kf3UXxy0cGvqwyEJzFvqQsGPsoPgzwNpHw1\" class=\"draft-image \" style=\"width:624%\" /></figure></p><p>Another ingredient is the observation that a big part of what the conscious self is doing is interacting with other people, and rationalizing our behaviour. (Cf. press secretary theory, <a href=\"https://www.lesswrong.com/posts/BgBrXpByCSmCLjpwr/book-review-the-elephant-in-the-brain\">elephant in the brain</a>.) It is also quite possible the relation between acting rationally and the ability to rationalize what we did is bidirectional, and significant part of motivation for some rational behaviour is that it is easy to rationalize it.</p><p>Also, it seems important to appreciate that the most important part of the human “environment” are other people, and what human minds are often doing is likely simulating other human minds (even simulating how other people would be simulating someone else!).</p><h3>Problems with prevailing value learning approaches</h3><p>While the above sketched picture is just a best guess, it seems to me at least compelling. At the same time, there are notable points of tension between it and at least some approaches to AI alignment.</p><h4>No clear distinction between goals and beliefs</h4><p>In this model, it is hardly possible to disentangle “beliefs” and “motivations” (or values). “Motivations” interface with the world only via a complex machinery of hierarchical generative models containing all other sorts of “beliefs”.<br/>To appreciate the problems for the value learning program, consider a case of someone who’s predictive/generative model strongly predicts failure and suffering. Such person may take actions which actually lead to this outcome, minimizing the prediction error. </p><p>Less extreme but also important problem is that extrapolating “values” outside of the area of validity of generative models is problematic and could be fundamentally ill-defined. (This is related to “ontological crisis”.)</p><p><strong>No clear self-alignment</strong></p><p>It seems plausible the common formalism of <a href=\"https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">agents with utility functions</a> is more adequate for describing the individual “subsystems” than the whole human minds. Decisions on the whole mind level are more like results of interactions between the sub-agents; results of multi-agent interaction are not in general an object which is naturally represented by utility function. For example, consider the sequence of game outcomes in repeated <a href=\"https://en.wikipedia.org/wiki/Prisoner%27s_dilemma\">PD game</a>. If you take the sequence of game outcomes (e.g. 1: defect-defect, 2:cooperate-defect, ... ) as a sequence of actions, the actions are not representing some well behaved preferences, and in general not maximizing some utility function. </p><p>Note: This is not to claim <a href=\"https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM rationality</a> is useless - it still has the normative power - and some types of interaction lead humans to approximate <a href=\"https://en.wikipedia.org/wiki/Subjective_expected_utility\">SEU</a> optimizing agents better. </p><p>One case is if mainly one specific subsystem (subagent) is in control, and the decision does not go via too complex generative modelling. So, we should expect more VNM-like behaviour in experiments in narrow domains than in cases where very different sub-agents are engaged and disagree. <br/>Another case is if sub-agents are able to do some “social welfare function” style aggregation, bargain, or trade - the result could be more VNM-like, at least in specific points of time, with the caveat that such “point” aggregate function may not be preserved in time.</p><p>On the contrary, cases where the resulting behaviour is very different from VNM-like may be caused by sub-agents locked in some non-cooperative Nash equilibria.  </p><h4>What we are aligning AI with</h4><p>Given this distinction between the whole mind and sub-agents, there are at least four somewhat different notions of what alignment can mean.</p><p><figure><img src=\"https://lh3.googleusercontent.com/O_ehN6wCFX0ExgylxIcmgmmkncPPoiJu9K-b7MMJCsBZq1Pt3YtXYxXIHl12PMgeZbl88aPkbCUlKWlSeW6wz3AtrPMvAnaTsXfIAYrj571GtMgIt_88KBvO0oA3d0n4VWvTEVhu\" class=\"draft-image \" style=\"width:624%\" /></figure></p><p>1. Alignment with the outputs of the generative models, without querying the human. This includes for example proposals centered around approval. In this case, generally only the output of the internal aggregation has some voice.</p><p><figure><img src=\"https://lh6.googleusercontent.com/5m6rzA0R_ackbDqV23JjikVk1_s3tP4FCbVoJgeg-WDFW2wS29qrFEyyO6zzjQ74_tnsYf_qVU5bxecCkXsvuOa18L7oeIDoBsbvaiL26H7LOTu2oMUKXMYeaSQqgzT0u5o6nEMs\" class=\"draft-image \" style=\"width:624%\" /></figure></p><p>2. Alignment with the outputs of the generative models, with querying the human. This includes for example <a href=\"https://arxiv.org/abs/1606.03137\">CIRL</a> and similar approaches. The problematic part of this is, by carefully crafted queries, it is possible to give voice to different sub-agenty systems (or with more nuance, give them very different power in the aggregation process). One problem with this is, if the internal human system is not self-aligned, the results could be quite arbitrary (and the AI agent has a lot of power to manipulate)</p><p><figure><img src=\"https://lh5.googleusercontent.com/5JG1CqYU9PgUMen4Qguh1tiuqP7F4A5rXhcq6NV46guUIenZTW1AmdyYB4Brj6NLtXG35LgcQMhw-1JDLbnbIB9Imb6Y5mvin9SncE1X2qWvIu-vj_9p6VquqLeRk98xY_aaaFPi\" class=\"draft-image \" style=\"width:624%\" /></figure></p><p>3. Alignment with the whole system, including the human aggregation process itself. This could include for example some deep NN based black-box trained on a large amount of human data, predicting what would the human want (or approve).<br/><br/>4. Adding layers of indirection to the question, such as defining alignment as a state where the<em> “A is trying to do what H wants it to do.”</em></p><p>In practice, options 1. and 2. can collapse into one, as far as there is some feedback loop between the AI agent actions and the human reward signal. (Even in case 1, the agent can take an action with the intention to elicit feedback from some subpart.)</p><p>We can construct a rich space of various meanings of &quot;alignment&quot; by combining basic directions.</p><p>Now, we can analyze how these options interact with various alignment research programs.</p><p>Probably the most interesting case is <a href=\"https://www.lesswrong.com/posts/dy6JHE7vzJS8SpSiu/iterated-distillation-and-amplification\">IDA</a>. IDA-like schemes can probably carry forward arbitrary properties to more powerful systems, as long as we are able to construct the individual step preserving the property. (I.e. one full cycle of distillation and amplification, which can be arbitrarily small).</p><p>Distilling and amplifying the alignment in sense #1 (what the human will actually approve) is conceptually easiest, but, unfortunately, brings some of the problems of potentially super-human system optimizing for manipulating the human for approval.</p><p>Alignment in sense #3 creates a very different set of problems. One obvious risk are mind-crimes. More subtle risk is related to the fact that as the implicit model of human “wants” scales (becomes less bounded), I. the parts may scale at different rates II. the outcome equilibria may change even if the sub-parts scale at the same rate.</p><p><figure><img src=\"https://lh5.googleusercontent.com/nlnX1hVn8U1g8YfjcpG-CGJd4T78-xE_GxjkX8hSvm0tv25wTnLQoJAomQ7dQDkdX08R_L0K0Yc1KLpp3tp3KZzrAIq1pWN8wFctOva2oOUizn3U_i0pSQnx7lx_N7-cOZtVN7v8\" class=\"draft-image \" style=\"width:624%\" /></figure></p><p>Alignment in sense #4 seems more vague, and moves the burden of understanding the problem in part to the side of the AI. We can imagine that at the end the AI will be aligned with some part of the human mind in a self-consistent way (the part will be a fixed point of the alignment structure). Unfortunately, it is <em>a priori</em> unclear if a unique fixed point exists. If not, the problems become similar to case #2. Also, it seems inevitable the AI will need to contain some structure representing what the human wants the AI to do, which may cause problems similar to #3.  </p><p>Also, in comparison with other meanings, it is much less clear to me how to even establish some system has this property.</p><h4>Rider-centric and meme-centric alignment</h4><p>Many alignment proposals seem to focus on interacting just with the conscious, narrating and rationalizing part of mind. If this is just a one part entangled in some complex interaction with other parts, there are specific reasons why this may be problematic. </p><p>One: if the “rider” (from the rider/elephant metaphor) is the part highly engaged with tracking societal rules, interactions and memes. It seems plausible the “values” learned from it will be mostly aligned with societal norms and interests of memeplexes, and not “fully human”. <br/><br/>This is worrisome: from a <a href=\"https://en.wikipedia.org/wiki/Memetics\">meme-centric</a> perspective, humans are just a substrate, and not necessarily the best one. Also - a more speculative problem may be - schemes learning human memetic landscape and “supercharging it” with superhuman performance may create some hard to predict evolutionary optimization processes.</p><h4>Metapreferences and multi-agent alignment</h4><p>Individual “preferences” can often in fact be mostly a meta-preference to have preferences compatible with other people, based on simulations of such people. <br/><br/>This may make it surprisingly hard to infer human values by trying to learn what individual humans want without the social context (necessitating inverting several layers of simulation). If this is the case, the whole approach of extracting individual preferences from a single human could be problematic. (This is probably more relevant to some “prosaic” alignment problems)</p><h4>Implications</h4><p>Some of the above mentioned points of disagreements point toward specific ways how some of the existing approaches to value alignment may fail. Several illustrative examples:</p><ul><li>Internal conflict may lead to inaction (also to not expressing approval or disapproval). While many existing approaches represent such situation only by the <em>outcome</em> of the conflict, the internal experience of the human seems to be quite different with and without the conflict</li><li>Difficulty with splitting “beliefs” and “motivations”. </li><li>Learning inadequate societal equilibria and optimizing on them. </li></ul><h2>Upside</h2><p>On the positive side, it could be expected the sub-agents still easily agree on things like “it is better not to die a horrible death”. </p><p>Also, the mind-model with bounded sub-agents which interact only with their local neighborhood and do not actually care about the world may be a viable design from the safety perspective.</p><h1>Suggested technical research directions</h1><p>While the previous parts are more in backward-chaining mode, here I attempt to point toward more concrete research agendas and questions where we can plausibly improve our understanding either by developing theory, or experimenting with toy models based on current ML techniques.</p><p>Often it may be the case that some research was already done on the topic, just not with AI alignment in mind, and a high value work could be “importing the knowledge” into safety community.</p><p><strong>Understanding hierarchical modelling.</strong></p><p>It seems plausible the human hierarchical models of the world optimize some &quot;boundedly rational&quot; function. (Remembering all details is too expensive, too much coarse-graining decreases usefulness. A good bounded rationality model can work as a principle for how to select models. In a similar way to the minimum description length principle, just taking some more “human” (energy?) costs as cost function.)</p><p><strong>Inverse Game Theory.</strong></p><p>Inverting agent motivations in MDPs is a different problem from inverting motivations in multi-agent situations where game-theory style interactions occur. This leads to the inverse game theory problem: observe the interactions, learn the objectives.</p><p><strong>Learning from multiple agents.</strong></p><p>Imagine a group of five closely interacting humans. Learning values just from person A may run into the problem that big part of A’s motivation is based on A simulating B,C,D,E (on the same “human” hardware, just incorporating individual differences). In that case, learning the “values” just from A’s actions could be in principle more difficult than observing the whole group, trying to learn some “human universals” and some “human specifics”. A different way of thinking about this could be by making a parallel with meta-learning algorithms (e.g. REPTILE) but in IRL frame.</p><p><strong>What happens if you put a system composed of sub-agents under optimization pressure?</strong></p><p>It is not clear to me what would happen if you, for example, successfully “learn” such a system of “motivations” from a human, and then put it inside of some optimization process selecting for VNM-like rational behaviour. </p><p>It seems plausible the somewhat messy system will be forced to get more internally aligned; for example, one way how it can happen is one of the sub-agent systems takes control and “wipes out the opposition”.</p><p><strong>What happens if you make a system composed of sub-agents less computationally bounded?</strong></p><p>It is not clear that the relative powers of sub-agents will scale the same with the whole system becoming less computationally bounded. (This is related to MIRI’s sub-agents agenda) </p><h1>Suggested non-technical research directions</h1><p><strong>Human self-alignment. </strong></p><p>All other things being equal, it seem safer to try to align AI with humans which are self-aligned.</p><h1>Notes &amp; Discussion </h1><p><strong>Motivations</strong></p><p>Part of my motivation for writing this was an annoyance: there is a plenty of reasons to believe the view </p><ul><li>human mind is a unified whole,</li><li>at first approximation optimizing some utility function,</li><li>this utility is over world-states,</li></ul><p>is neither a good model of humans, nor the best model how to think about AI. Yet, it is the paradigm shaping a lot of thoughts and research. I hope if the annoyance surfaced in the text, it is not too distractive.</p><p><strong>Multi-part minds in literature</strong></p><p>There are dozens of schemes describing mind as some sort of multi-part system, so there is nothing original about this claim. Based on a very shallow review, it seems the way how psychologists often conceptualize the sub-agents is as <a href=\"https://en.wikipedia.org/wiki/Subpersonality\">subpersonalities</a>, which are almost fully human. This seems to err on the side of sub-agents being too complex, and anthropomorphising instead of trying to describe formally. (Explaining humans as a composition of humans is not much useful for AI alignment). On the other hand, Minsky’s <a href=\"https://en.wikipedia.org/wiki/Society_of_Mind\">“<em>Society of Mind</em>”</a> has sub-agents which often seem to be too simple (e.g. similar in complexity to individual logic gates). If there is some literature having sub-agent complexity right, and sub-agents being inside predictive processing, I’d be really excited about it!</p><p><strong>Discussion</strong></p><p>When discussion the draft, several friends noted something along the line: “It is overdetermined that approaches like IRL are doomed. There are many reasons for that and the research community is aware of them”.  To some extent, I agree this is the case, on the other hand 1. the described model of mind may pose problems even for more sophisticated approaches 2. My impression is many people still have something like utility-maximizing agent as a the central example.</p><p>The complementary objection is that while interacting sub-agents may be a more precise model, it seems in practice it is often enough to think about humans as unified agents is good enough, and may be good enough even for the purpose of AI alignment. My intuitions on this is based on the connection of rationality to exploitability: it seems humans are usually more rational and less exploitable when thinking about narrow domains, but can be quite bad when vastly different subsystems are in in play (imagine on one side a person exchanging stock and money, on the other side some units of money, free time, friendship, etc.. In the second case, many people are willing to trade in different situations by very different rates)</p><p><em>I’d like to thank Linda Linsefors , Alexey Turchin, Tomáš Gavenčiak, Max Daniel, Ryan Carey, Rohin Shah, Owen Cotton-Barratt and others for helpful discussions. Part of this originated in the efforts of the “Hidden Assumptions” team on the 2nd AI safety camp, and my thoughts about how minds work are inspired by CFAR.</em></p>",
    "user": {
      "username": "Jan_Kulveit",
      "slug": "jan_kulveit",
      "displayName": "Jan_Kulveit"
    }
  },
  {
    "_id": "WsWGy7DPnFBD5AnMX",
    "title": "What went wrong in this interaction?",
    "slug": "what-went-wrong-in-this-interaction",
    "pageUrl": "https://www.lesswrong.com/posts/WsWGy7DPnFBD5AnMX/what-went-wrong-in-this-interaction",
    "postedAt": "2018-12-12T19:59:57.790Z",
    "baseScore": 1,
    "voteCount": 7,
    "commentCount": 8,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p> </p><p>I&#x27;m curious about an interaction I had a few weeks ago with someone in the rationality community, I was wondering if someone here can look at the conversation and evaluate what &#x27;went wrong&#x27; so to speak.</p><p>It began with some comments I made on a blog post, where I disagreed with the author that &#x27;metoo&#x27; was good, but rather than discuss the entire point I wanted just to address some counterexamples to something the author said about metoo never having gone too far. The post is here: <a href=\"http://benjaminrosshoffman.com/metoo-is-good/\">http://benjaminrosshoffman.com/metoo-is-good/</a></p><p>After a bit of back and forth, it seemed like I should try take it to private chat before it turned into a <a href=\"https://www.lesswrong.com/posts/BZtAavpsy9WtMYgEL/demon-threads\">demon thread</a>. This was the ensuing conversation: <a href=\"https://pastebin.com/epQmxZK2\">https://pastebin.com/epQmxZK2</a></p><p>It seems to me like my points were understood, and likewise I didn&#x27;t quite get what the author was trying to make me understand.</p><p> To me, it seems to me like my points were understood, and likewise I didn’t quite get what the author was trying to make me understand.</p><p>The author also seemed hostile and unwilling to engage, and how he disengaged from the conversation seemed like a personal attack that was unjustified. But I’m biased, so I was wondering if it was something about my comments or behavior or tone that I was missing that provoked that response, or if I misread the hostility at all.</p><p>And any thoughts about why the author had that kind of a reaction? It was not what I expected since I thought most rational community members would welcome a honest discussion like the one I was trying to start.</p>",
    "user": {
      "username": "t3tsubo",
      "slug": "t3tsubo",
      "displayName": "t3tsubo"
    }
  },
  {
    "_id": "TfBPQLzB5n98TrGQD",
    "title": "Internet Search Tips: how I use Google/Google Scholar/Libgen ",
    "slug": "internet-search-tips-how-i-use-google-google-scholar-libgen",
    "pageUrl": "https://www.lesswrong.com/posts/TfBPQLzB5n98TrGQD/internet-search-tips-how-i-use-google-google-scholar-libgen",
    "postedAt": "2018-12-12T14:50:30.970Z",
    "baseScore": 51,
    "voteCount": 13,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": "https://www.gwern.net/Search",
    "htmlBody": null,
    "user": {
      "username": "gwern",
      "slug": "gwern",
      "displayName": "gwern"
    }
  },
  {
    "_id": "zwa4APWNEALjpWFv6",
    "title": "Zagreb Rationality Meetup",
    "slug": "zagreb-rationality-meetup",
    "pageUrl": "https://www.lesswrong.com/events/zwa4APWNEALjpWFv6/zagreb-rationality-meetup",
    "postedAt": "2018-12-12T13:08:34.403Z",
    "baseScore": 1,
    "voteCount": 1,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": null,
    "user": {
      "username": "roko-jelavic",
      "slug": "roko-jelavic",
      "displayName": "Roko Jelavić"
    }
  },
  {
    "_id": "LRKXuxLrnxx3nSESv",
    "title": "Should ethicists be inside or outside a profession?",
    "slug": "should-ethicists-be-inside-or-outside-a-profession",
    "pageUrl": "https://www.lesswrong.com/posts/LRKXuxLrnxx3nSESv/should-ethicists-be-inside-or-outside-a-profession",
    "postedAt": "2018-12-12T01:40:13.298Z",
    "baseScore": 97,
    "voteCount": 33,
    "commentCount": 7,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Originally written in 2007.</em></p><hr class=\"dividerBlock\"/><p>Marvin Minsky in an interview with Danielle Egan for <em>New Scientist:</em></p><blockquote><strong>Minsky:</strong> The reason we have politicians is to prevent bad things from happening. It doesn’t make sense to ask a scientist to worry about the bad effects of their discoveries, because they’re no better at that than anyone else. Scientists are not particularly good at social policy.</blockquote><blockquote><strong>Egan:</strong> But shouldn’t they have an ethical responsibility for their inventions</blockquote><blockquote><strong>Minsky:</strong> No they shouldn’t have an ethical responsibility for their inventions. They should be able to do what they want. You shouldn’t have to ask them to have the same values as other people. Because then you won’t get them. They’ll make stupid decisions and not work on important things, because they see possible dangers. What you need is a separation of powers. It doesn’t make any sense to have the same person do both.</blockquote><p>The Singularity Institute was recently asked to comment on this interview - which by the time it made it through the editors at <em>New Scientist</em>, contained just the unvarnished <a href=\"http://www.newscientist.com/channel/opinion/death/mg19626251.800-death-special-the-plan-for-eternal-life.html\">quote</a> “Scientists shouldn’t have an ethical responsibility for their inventions. They should be able to do what they want. You shouldn’t have to ask them to have the same values as other people.” Nice one, <em>New Scientist.</em> Thanks to Egan for providing the original interview text.</p><p>This makes an interesting contrast with what I said in my “<a href=\"https://intelligence.org/files/CognitiveBiases.pdf\">Cognitive biases</a>” chapter for Bostrom’s <em>Global Catastrophic Risks:</em></p><blockquote>Someone on the physics-disaster committee should know what the term “<a href=\"http://www.nickbostrom.com/existential/risks.html\">existential risk</a>” means; should possess whatever skills the field of existential risk management has accumulated or borrowed. For maximum safety, that person should also be a physicist. The domain-specific expertise and the expertise pertaining to existential risks should combine in one person. I am skeptical that a scholar of heuristics and biases, unable to read physics equations, could check the work of physicists who knew nothing of heuristics and biases.</blockquote><p>Should ethicists be inside or outside a profession?</p><p>It seems to me that trying to separate ethics and engineering is like trying to separate the crafting of paintings into two independent specialties: a profession that’s in charge of pushing a paintbrush over a canvas, and a profession that’s in charge of artistic beauty but knows nothing about paint or optics.</p><p>The view of ethics as a separate profession is part of the <em>problem.</em> It arises, I think, from the same deeply flawed worldview that sees technology as something foreign and distant, something <em>opposed to</em> life and beauty. Technology is an expression of human intelligence, which is to say, an expression of human nature. Hunter-gatherers who crafted their own bows and arrows didn’t have cultural nightmares about bows and arrows being a mechanical death force, a blank-faced System. When you craft something with your own hands, it seems like a part of you. It’s the Industrial Revolution that enabled people to buy artifacts which they could not make or did not even understand.</p><p>Ethics, like engineering and art and mathematics, is a natural expression of human minds.</p><p>Anyone who gives a part of themselves to a profession discovers a sense of beauty in it. Writers discover that sentences can be beautiful. Programmers discover that code can be beautiful. Architects discover that house layouts can be beautiful. We all start out with a native sense of beauty, which already responds to rivers and flowers. But as we begin to <em>create</em> - sentences or code or house layouts or flint knives - our sense of beauty develops with use.</p><p>Like a sense of beauty, one’s native ethical sense must be continually used in order to develop further. If you’re just working at a job to make money, so that your real goal is to make the rent on your apartment, then neither your aesthetics nor your morals are likely to get much of a workout.</p><p>The way to develop a highly specialized sense of professional ethics is to do something, ethically, a whole bunch, until you get good at both the thing itself and the ethics part.</p><p>When you look at the “bioethics” fiasco, you discover bioethicists writing mainly for an audience of other bioethicists. Bioethicists aren’t writing to doctors or bioengineers, they’re writing to tenure committees and journalists and foundation directors. Worse, bioethicists are not <em>using</em> their ethical sense in bio-work, the way a doctor whose patient might have incurable cancer must choose how and what to tell the patient.</p><p>A doctor treating a patient should not try to be <em>academically original,</em> to come up with a brilliant new theory of bioethics. As I’ve written before, ethics is not <em>supposed</em> to be <a href=\"https://www.lesswrong.com/posts/Aud7CL7uhz55KL8jG/transhumanism-as-simplified-humanism\">counterintuitive</a>, and yet academic ethicists are biased to be just exactly counterintuitive enough that people won’t say, “Hey, I could have thought of that.” The purpose of ethics is to shape a well-lived life, not to be impressively complicated. Professional ethicists, to get paid, must transform ethics into something difficult enough to require professional ethicists.</p><p>It’s, like, a good idea to save lives? “Duh,” the foundation directors and the review boards and the tenure committee would say.</p><p>But there’s nothing <em>duh</em> about saving lives if you’re a doctor.</p><p>A book I once read about writing - I forget which one, alas - observed that there is a level of depth beneath which repetition ceases to be boring. Standardized phrases are called “cliches” (said the author of writing), but murder and love and revenge can be woven into a thousand plots without ever becoming old. “You should save people&#x27;s lives, mmkay?” won’t get you tenure - but as a theme of real life, it’s as old as thinking, and no more obsolete.</p><p>Boringly obvious ethics are just fine if you’re <em>using</em> them in your work rather than talking about them. <a href=\"http://www.overcomingbias.com/2007/10/outside-the-box.html\">The goal is to do it right, not to do it originally.</a> Do your best whether or not it is “original”, and originality comes in its own time; not every change is an improvement, but every improvement is necessarily a change.</p><p>At the Singularity Summit 2007, several speakers alleged we should “reach out” to artists and poets to encourage their participation in the Singularity dialogue. And then a woman went to a microphone and said: “I am an artist. I want to participate. What should I do?”</p><p>And there was a <a href=\"http://www.overcomingbias.com/2007/09/we-dont-really-.html\">long, delicious silence</a>.</p><p>What I would have said to a question like that, if someone had asked it of me in the conference lobby, was: “You are not an ‘artist’, you are a human being; art is only one facet in which you express your humanity. Your reactions to the Singularity should arise from your entire self, and it’s okay if you have a standard human reaction like ‘I’m afraid’ or ‘Where do I send the check?’, rather than some special ‘artist’ reaction. If your artistry has something to say, it will express itself naturally in your response as a human being, without needing a conscious effort to say something artist-like. I would feel patronized, like a dog commanded to perform a trick, if someone presented me with a painting and said ‘Say something mathematical!’”</p><p>Anyone who calls on “artists” to participate in the Singularity clearly thinks of artistry as a special function that is only performed in Art departments, an icing dumped onto cake from outside. But you can always pick up some <a href=\"http://www.overcomingbias.com/2007/09/applause-lights.html\">cheap applause</a> by calling for more icing on the cake.</p><p>Ethicists should be inside a profession, rather than outside, because ethics itself should be inside rather than outside. It should be a natural expression of yourself, like math or art or engineering. If you don’t like trudging up and down stairs you’ll build an escalator. If you don’t want people to get hurt, you’ll try to make sure the escalator doesn’t suddenly speed up and throw its riders into the ceiling. Both just natural expressions of desire.</p><p>There are opportunities for market distortions here, where people get paid more for installing an escalator than installing a safe escalator. If you don’t <em>use</em> your ethics, if you don’t wield them as part of your profession, they will grow no stronger. But if you want a safe escalator, by far the best way to get one - if you can manage it - is to find an engineer who naturally doesn’t want to hurt people. Then you’ve just got to keep the managers from demanding that the escalator ship immediately and without all those expensive safety gadgets.</p><p>The first iron-clad steamships were actually <em>much safer</em> than the <em>Titanic</em>; the first ironclads were built by engineers without much management supervision, who could design in safety features to their heart’s content.  The <em>Titanic</em> was built in an era of cutthroat price competition between ocean liners.  The grand fanfare about it being unsinkable was a marketing slogan like “World’s Greatest Laundry Detergent”, not a failure of engineering prediction.</p><p>Yes, safety inspectors, yes, design reviews; but these just <em>verify </em>that the engineer put forth an effort of ethical design intelligence. Safety-inspecting doesn’t build an elevator. Ethics, to be effective, must be part of the intelligence that expresses those ethics - you can’t add it in like icing on a cake.</p><p>Which leads into the question of the ethics of AI. “Ethics, to be effective, must be part of the intelligence that expresses those ethics - you can’t add it in like icing on a cake.” My goodness, I wonder how I could have learned such <a href=\"http://www.overcomingbias.com/2007/10/how-to-seem-and.html\">Deep Wisdom</a>?</p><p>Because I studied AI, and the art spoke to me.  Then I translated it back into English.</p><p>The truth is that I can’t inveigh properly on bioethics, because I am not myself a doctor or a bioengineer. If there is a special ethic of medicine, beyond the obvious, I do not know it. I have not worked enough healing for that art to speak to me.</p><p>What I do know a thing or two about, is AI. There I can testify definitely and from direct knowledge, that anyone who sets out to study “AI ethics” without a technical grasp of cognitive science, is <a href=\"http://www.overcomingbias.com/2007/10/inferential-dis.html\">absolutely doomed</a>.</p><p>It’s the technical knowledge of AI that forces you to <a href=\"http://www.overcomingbias.com/2007/05/think_like_real.html\">deal with the world in its own strange terms,</a> rather than the surface-level concepts of everyday life. In everyday life, you can take for granted that “people” are easy to identify; if you look at the modern world, the humans are easy to pick out, to categorize. An unusual boundary case, like Terri Schiavo, can throw a whole nation into a panic: Is she “alive” or “dead”? AI explodes the language that people are described of, unbundles the properties that are always together in human beings. Losing the standard view, throwing away the human conceptual language, forces you to <em>think for yourself</em> about ethics, rather than parroting back things that sound Deeply Wise.</p><p>All of this comes of studying the math, nor may it be divorced from the math. That’s not as comfortably egalitarian as my earlier statement that ethics isn’t meant to be complicated. But if you mate ethics to a highly technical profession, you’re going to get ethics expressed in a <em>conceptual language</em> that is highly technical.</p><p>The technical knowledge provides the conceptual language in which to express ethical problems, ethical options, ethical decisions. If politicians don’t understand the distinction between terminal value and instrumental value, or the difference between a utility function and a probability distribution, then some fundamental <em>problems</em> in Friendly AI are going to be complete gibberish to them - never mind the solutions. I’m sorry to be the one to say this, and I don’t like it either, but Lady Reality does not have the goal of making things easy for political idealists.</p><p>If it helps, the technical ethical thoughts I’ve had so far require only comparatively basic math like Bayesian decision theory, not high-falutin’ complicated damn math like real mathematicians do all day. Hopefully this condition does not hold merely because I am stupid.</p><p>Several of the responses to Minsky’s statement that politicians should be the ones to “prevent bad things from happening” were along the lines of “Politicans are not particularly good at this, but neither necessarily are most scientists.” I think it’s sad but true that modern industrial civilization, or even modern academia, imposes many shouting external demands within which the quieter internal voice of ethics is lost. It may even be that a majority of people are not particularly ethical to begin with; the thought seems to me uncomfortably elitist, but that doesn’t make it comfortably untrue.</p><p>It may even be true that most scientists, say in AI, haven’t really had a lot of opportunity to express their ethics and so the art hasn’t said anything in particular to them.</p><p>If you talk to some AI scientists about the Singularity / Intelligence Explosion they may say something cached like, “Well, who’s to say that humanity really ought to survive?” This doesn’t sound to me like someone whose art is speaking to them. But then artificial intelligence is not the same as artificial <em>general </em>intelligence; and, well, to be brutally honest, I think a lot of people who claim to be working in AGI haven’t really gotten all that far in their pursuit of the art.</p><p>So, if I listen to the voice of experience, rather to the voice of comfort, I find that most people are not very good at ethical thinking. Even most doctors - who ought properly to be confronting ethical questions in every day of their work - don’t go on to write famous memoirs about their ethical insights. The terrifying truth may be that Sturgeon’s Law applies to ethics as it applies to so many other human endeavors: “Ninety percent of everything is crap.”</p><p>So asking an engineer an ethical question is not a sure-fire way to get an especially ethical answer. I wish it were true, but it isn’t.</p><p>But what experience tells me, is that there is no way to obtain the ethics of a <em>technical </em>profession <em>except</em> by being ethical inside that profession. I’m skeptical enough of nondoctors who propose to tell doctors how to be ethical, but I <em>know </em>it’s not possible in AI. There are all sorts of AI-ethical questions that anyone should be able to answer, like “Is it good for a robot to kill people? No.” But if a dilemma requires more than this, the specialist ethical expertise will only come from someone who has practiced expressing their ethics from inside their profession.</p><p>This doesn’t mean that all AI people are on their own. It means that if you want to have specialists telling AI people how to be ethical, the “specialists” have to be AI people who express their ethics within their AI work, and <em>then </em>they can talk to other AI people about what the art said to them.</p><p>It may be that most AI people will not be above-average at AI ethics, but without technical knowledge of AI you don’t even get an <em>opportunity</em> to develop ethical expertise because you’re not thinking in the right language. That’s the way it is in my profession. Your mileage may vary.</p><p>In other words:  To get good AI ethics you need someone technically good at AI, but not all people technically good at AI are automatically good at AI ethics. The technical knowledge is <em>necessary</em> but not <em>sufficient</em> to ethics.</p><p>What if you think there are specialized ethical concepts, typically taught in philosophy classes, which AI ethicists will need? Then you need to make sure that at least some AI people take those philosophy classes. If there is such a thing as special ethical knowledge, it has to <em>combine in the same person</em> who has the technical knowledge.</p><p>Heuristics and biases are critically important knowledge relevant to ethics, in my humble opinion. But if you want that knowledge expressed in a profession, you’ll have to find a professional expressing their ethics and teach them about heuristics and biases - not pick a random cognitive psychologist off the street to add supervision, like so much icing slathered over a cake.</p><p>My nightmare here is people saying, “Aha! A randomly selected AI researcher is not guaranteed to be ethical!” So they turn the task over to professional “ethicists” who are <em>guaranteed</em> to fail: who will simultaneously try to sound counterintuitive enough to be worth paying for as specialists, while also making sure to not think up anything <em>really</em> technical that would scare off the foundation directors who approve their grants.</p><p>But even if professional “AI ethicists” fill the popular air with nonsense, all is not lost. AIfolk who express their ethics as a continuous, non-separate, non-special function of the same life-existence that expresses their AI work, will yet learn a thing or two about the special ethics pertaining to AI. They will not be able to avoid it. Thinking that ethics is a separate profession which judges engineers from above, is like thinking that math is a separate profession which judges engineers from above. If you’re doing ethics <em>right,</em> you can’t <em>separate </em>it from your profession.</p>",
    "user": {
      "username": "Eliezer_Yudkowsky",
      "slug": "eliezer_yudkowsky",
      "displayName": "Eliezer Yudkowsky"
    }
  },
  {
    "_id": "XFEJg9gxak5agyxJo",
    "title": "Alignment Newsletter #36",
    "slug": "alignment-newsletter-36",
    "pageUrl": "https://www.lesswrong.com/posts/XFEJg9gxak5agyxJo/alignment-newsletter-36",
    "postedAt": "2018-12-12T01:10:01.398Z",
    "baseScore": 21,
    "voteCount": 6,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Developing a theory of values to solve extrapolation issues, and an approach to train AI systems to reason well</em></p><p>Find all Alignment Newsletter resources <a href=\"http://rohinshah.com/alignment-newsletter/\">here</a>. In particular, you can <a href=\"http://eepurl.com/dqMSZj\">sign up</a>, or look through this <a href=\"https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing\">spreadsheet</a> of all summaries that have ever been in the newsletter.</p><h2>Highlights</h2><p><strong><a href=\"https://www.alignmentforum.org/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values\">Why we need a <em>theory</em> of human values</a></strong> <em>(Stuart Armstrong)</em>: There are many different sources of information for human values, such as behavior, speech, facial expressions/emotions, and extrapolations of what a human would do. These have all been honed to produce similar preferences in our current environment. However, the environment will change a lot due to the AI&#x27;s actions, inducing a distributional shift, after which we should no longer expect the values inferred from these different methods to agree with each other. In addition, we also have the problem that each method only applies in some circumstances -- for example, people are likely to misrepresent their values if asked in a courtroom. We could try to patch these problems by having a meta-method that chooses from the various methods of value learning, as well as predicting human judgments about when each method applies. However, then we&#x27;d likely have similar issues with the meta-method and predictions, which are also likely to be specific to the current environment. Instead, we should have a <em>theory</em> of human values, from which we can get principled approaches to resolve these problems.</p><p><strong>Rohin&#x27;s opinion:</strong> I strongly agree that due to the problems mentioned in this post, we shouldn&#x27;t be trying to mix and match value learning methods to infer a static notion of human preferences that is then optimized over the long term. I don&#x27;t personally work on building a theory of human values because it seems like I could apply the same critiques to the result: the theory is specialized to our current situation and won&#x27;t capture changes in the future. But of course this is hard to predict without knowing the theory. My preferred solution is not to build a system that is optimizing a goal-directed utility function over the long term, and to find other ways of making an AI system that are still just as useful.</p><h1>Technical AI alignment</h1><h3>Iterated amplification sequence</h3><p><a href=\"https://www.alignmentforum.org/posts/DFkGStzvj3jgXibFG/factored-cognition\">Factored Cognition</a> <em>(Andreas Stuhlmüller)</em>: This was previously summarized in <a href=\"https://mailchi.mp/bcb2c6f1d507/alignment-newsletter-12\">AN #12</a>. While this post describes a project from Ought, it explains many of the ideas behind iterated amplification (which is why it is in this sequence). There are two important and distinct topics addressed in this post: first, whether it is possible in principle to achieve good performance on tasks when you must use <em>explicit reasoning</em>, and second, how to use explicit reasoning to train an aligned AI system via iterated amplification.</p><p>Currently, most AI research is supervised based on <em>behavior</em>: we specify a domain and data or reward functions that identify good behavior, and hope that the AI develops a good decision-making system during training. We don&#x27;t provide training data for decision-making itself. In contrast, Factored Cognition aims to make the decision-making algorithm itself explicit, in a form that could be used to train an AI system, which we might call internal supervision. Note that this is <em>not</em> a required feature of iterated amplification, which can work with external (i.e. behavioral) supervision, as in <a href=\"https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84\">recursive reward modeling</a> (<a href=\"https://mailchi.mp/f1947668b183/alignment-newsletter-34\">AN #34</a>). However, we might want to use internal supervision because then we get to control the decision-making process itself, and that is what determines whether the AI system is aligned or not.</p><p>Research currently does not use internal supervision because it requires training data on <em>how</em> to solve the problem, whereas a reward function only needs to tell <em>whether</em> the problem is solved. However, for sufficiently general AI systems, we could provide training data on general problem-solving, which could then be applied to many tasks. This could be competitive with the alternative of providing training data or reward functions for each task separately.</p><p>How might we make general problem-solving explicit enough that we could train AI systems to replicate it? Factored Cognition hypothesizes that we could do this by providing <em>decompositions</em> of tasks into simpler subtasks. While this is not how humans solve tasks (for example, expert Go players use a lot of intuition), it seems plausible that we could get similar or better performance using decomposition as long as we were willing to wait a long time (for example, by implementing minimax using decomposition to solve Go). Ought wants to test this hypothesis, by studying whether humans are in fact capable of providing decompositions in challenging problem domains, such as math and programming puzzles, textbook problems, fact-checking, interactive dialog, and task prioritization. (There will likely be no ML here, just humans decomposing problems for other humans to then solve.)</p><p>The post makes this concrete by considering a particular way in which humans might provide decompositions. They develop a &quot;cognitive workspace&quot; where a human can take &quot;actions&quot; such as editing the text in the workspace, each of which can only involve a small amount of cognitive work. There are a lot of details that I won&#x27;t get into, but broadly it seems to me like what you would get if you tried to create a functional programming language where the primitive forms are things that a human can do. Note that the examples in the post use task-specific decompositions for clarity but in the long term we would need general decomposition strategies.</p><p>We could train an AI system to mimic this explicit reasoning. It seems like this would achieve human performance but no more, since after all we are imitating human decision-making algorithms. However, now we can use the iterated amplification trick. If a human got to use explicit reasoning over a long time, we would expect significantly better decisions. So, we can first distill an agent A that mimics a human using explicit reasoning over a short time period, then amplify it to get H[A], where a human performs a decomposition into subquestions that are answered by A, then distill that into A&#x27;, etc. The post has a nice visualization of how this allows you to mimic an implicit exponentially large tree of decompositions in polynomial time. As before, distillation could be done using imitation learning, or RL with some good inferred reward.</p><p><strong>Rohin&#x27;s opinion:</strong> It seems a lot easier to generalize well to novel situations if you&#x27;ve learned the right decision-making algorithms. Since it&#x27;s way easier to learn something if you have direct supervision of it, internal supervision seems particularly interesting as a method of generalizing well. For example, in <a href=\"https://arxiv.org/abs/1704.06611\">Making Neural Programming Architectures Generalize via Recursion</a>, the learned programs generalize perfectly, because they learn from execution traces, which show <em>how</em> to solve a task, and the learned algorithm is forced to use a recursive structure. (Typical approaches to neural program induction only use input-output examples, and often fail to generalize to inputs longer than those seen during training.)</p><p>I also like the point that iterated amplification allows you to mimic an implicit exponential-time computation. This is a key reason for my optimism: it seems like well-honed human intuition beats short explicit reasoning almost always. In fact, I think it is reasonable to view human intuition as the result of iterated amplification (see <a href=\"https://www.lesswrong.com/posts/XHMCvvhb7zTZcQAgA/argument-intuition-and-recursion\">these</a> <a href=\"https://terrytao.wordpress.com/career-advice/theres-more-to-mathematics-than-rigour-and-proofs/\">posts</a>).</p><h3>Learning human intent</h3><p><strong><a href=\"https://www.alignmentforum.org/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values\">Why we need a <em>theory</em> of human values</a></strong> <em>(Stuart Armstrong)</em>: Summarized in the highlights!</p><p><a href=\"https://arxiv.org/abs/1811.08549\">Reinforcement learning and inverse reinforcement learning with system 1 and system 2</a> <em>(Alexander Peysakhovich)</em></p><h3>Interpretability</h3><p><a href=\"http://arxiv.org/abs/1811.09722\">Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Agent Behavior</a> <em>(Tathagata Chakraborti et al)</em></p><h3>Adversarial examples</h3><p><a href=\"https://arxiv.org/abs/1811.00525\">On the Geometry of Adversarial Examples</a> <em>(Marc Khoury et al)</em>: This paper analyzes adversarial examples based off a key idea: even if the data of interest forms a low-dimensional manifold, as we often assume, the \u000f\u000fϵ-tube <em>around</em> the manifold is still high-dimensional, and so accuracy in an ϵ-ball around true data points will be hard to learn.</p><p>For a given L_p norm, we can define the optimal decision boundary to be the one that maximizes the margin from the true data manifold. If there exists some classifier that is adversarially robust, then the optimal decision boundary is as well. Their first result is that the optimal decision boundary can change dramatically if you change p. In particular, for concentric spheres, the optimal L_inf decision boundary provides an L_2 robustness guarantee √d times smaller than the optimal L_2 decision boundary, where d is the dimensionality of the input. This explains why a classifier that is adversarially trained on L_inf adversarial examples does so poorly on L_2 adversarial examples.</p><p>I&#x27;m not sure I understand the point of the next section, but I&#x27;ll give it a try. They show that a nearest neighbors classifier can achieve perfect robustness if the underlying manifold is sampled sufficiently densely (requiring samples exponential in k, the dimensionality of the manifold). However, a learning algorithm with a particular property that they formalize would require exponentially more samples in at least some cases in order to have the same guarantee. I don&#x27;t know why they chose the particular property they did -- my best guess is that the property is meant to represent what we get when we train a neural net on L_p adversarial examples. If so, then their theorem suggests that we would need exponentially more training points to achieve perfect robustness with adversarial training compared to a nearest neighbor classifier.</p><p>They next turn to the fact that the ϵ-tube around the manifold is d-dimensional instead of k-dimensional. If we consider ϵ-balls around the training set X, this covers a very small fraction of the ϵ-tube, approaching 0 as d becomes much larger than k, even if the training set X covers the k-dimensional manifold sufficiently well.</p><p>Another issue is that if we require adversarial robustness, then we severely restrict the number of possible decision boundaries, and so we may need significantly more expressive models to get one of these decision boundaries. In particular, since feedforward neural nets with Relu activations have piecewise linear decision boundaries, it is hard for them to separate concentric spheres. Suppose that the spheres are separated by a distance d. Then for accuracy on the manifold, we only need the decision boundary to lie entirely in the shell of width d. However, for ϵ-tube adversarial robustness, the decision boundary must lie in a shell of width d - 2ϵ. They prove a lower bound on the number of linear regions for the decision boundary that grows as τ^(-d), where τ is the width of the shell, suggesting that adversarial robustness would require more parameters in the model.</p><p>Their experiments show that for simple learning problems (spheres and planes), adversarial examples tend to be in directions orthogonal to the manifold. In addition, if the true manifold has high codimension, then the learned model has poor robustness.</p><p><strong>Rohin&#x27;s opinion:</strong> I think this paper has given me a significantly better understanding of how L_p norm balls work in high dimensions. I&#x27;m more fuzzy on how this applies to adversarial examples, in the sense of any confident misclassification by the model on an example that humans agree is obvious. Should we be giving up on L_p robustness since it forms a d-dimensional manifold, whereas we can only hope to learn the smaller k-dimensional manifold? Surely though a small enough perturbation shouldn&#x27;t change anything? On the other hand, even humans have <em>some</em> decision boundary, and the points near the decision boundary have some small perturbation which would change their classification (though possibly to &quot;I don&#x27;t know&quot; rather than some other class).</p><p>There is a phenomenon where if you train on L_inf adversarial examples, the resulting classifier fails on L_2 adversarial examples, which has previously been described as &quot;overfitting to L_inf&quot;. The authors interpret their first theorem as contradicting this statement, since the optimal decision boundaries are very different for L_inf and L_2. I don&#x27;t see this as a contradiction. The L_p norms are simply a method of label propagation, which augments the set of data points for which we know labels. Ultimately, we want the classifier to reproduce the labels that we would assign to data points, and L_p propagation captures some of that. So, we can think of there as being many different ways that we can augment the set of training points until it matches human classification, and the L_p norm balls are such methods. Then an algorithm is more robust as it works with more of these augmentation methods. Simply doing L_inf training means that by default the learned model only works on one of the methods (L_inf norm balls) and not all of them as we wanted, and we can think of this as &quot;overfitting&quot; to the imperfect L_inf notion of adversarial robustness. The meaning of &quot;overfitting&quot; here is that the learned model is too optimized for L_inf, at the cost of other notions of robustness like L_2 -- and their theorem says basically the same thing, that optimizing for L_inf comes at the cost of L_2 robustness.</p><p><a href=\"https://arxiv.org/abs/1802.01421\">Adversarial Vulnerability of Neural Networks Increases With Input Dimension</a> <em>(Carl-Johann Simon-Gabriel et al)</em>: The key idea of this paper is that imperceptible adversarial vulnerability happens when small changes in the input lead to large changes in the output, suggesting that the gradient is large. They first recommend choosing ϵ_p to be proportional to d^(1/p). Intuitively, this is because larger values of p behave more like maxing instead of summing, and so using the same value of ϵ across values of p would lead to more points being considered for larger p. They show a link between adversarial robustness and regularization, which makes sense since both of these techniques aim for better generalization.</p><p>Their main point is that the norm of the gradient increases with the input dimension d. In particular, a typical initialization scheme will set the variance of the weights to be inversely proportional to d, which means the absolute value of each weight is inversely proportional to √d. For a single-layer neural net (that is, a perceptron), the gradient is exactly the weights. For L_inf adversarial robustness, the relevant norm for the gradient is the L_1 norm. This gives the sum of the d weights, which will be proportional to √d. For L_p adversarial robustness, the corresponding gradient is L_q with q larger than 1, which decreases the size of the gradient. However, this is exactly offset by the increase in the size of ϵ_p that they proposed. Thus, in this simple case the adversarial vulnerability increases with input dimension. They then prove theorems that show that this generalizes to other neural nets, including CNNS (albeit still only at initialization, not after training). They also perform experiments showing that their result also holds after training.</p><p><strong>Rohin&#x27;s opinion:</strong> I suspect that there is some sort of connection between the explanation given in this paper and the explanation that there are many different perturbation directions in high-dimensional space which means that there are lots of potential adversarial examples, which increases the chance that you can find one. Their theoretical result comes primarily from the fact that weights are initialized with variance inversely proportional to d. We could eliminate this by having the variance be inversely proportional to d^2, in which case their result would say that adversarial vulnerability is constant with input dimension. However, in this case the variance of the activations would be inversely proportional to d, making it hard to learn. It seems like adversarial vulnerability should be the product of &quot;number of directions&quot;, and &quot;amount you can search in a direction&quot;, where the latter is related to the variance of the activations, making the connection to this paper.</p><p><a href=\"http://arxiv.org/abs/1811.03571\">Intrinsic Geometric Vulnerability of High-Dimensional Artificial Intelligence</a> <em>(Luca Bortolussi et al)</em></p><h1>Other progress in AI</h1><h3>Reinforcement learning</h3><p><a href=\"https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/\">AlphaZero: Shedding new light on the grand games of chess, shogi and Go</a> <em>(David Silver et al)</em>: If you didn&#x27;t already believe that AlphaZero is excellent at Go, Chess and Shogi, this post and the associated paper show it more clearly with a detailed evaluation. A few highlights:</p><p>- AlphaZero can beat Stockfish starting from common human openings, suggesting that it generalizes well</p><p>- The amount of computation given to AlphaZero to choose a move has a larger effect on the win probability than I was expecting</p><p>- I always wondered why they use MCTS and not alpha-beta search. They speculate that alpha-beta search with a neural net evaluation function succumbs to the <a href=\"https://www.investopedia.com/terms/w/winnerscurse.asp\">winner&#x27;s curse</a> since alpha-beta involves a lot of maxes and mins, whereas MCTS averages over evaluations and so is more robust. In contrast, evaluation functions designed by humans are much more likely to generalize well, and alpha-beta outperforms MCTS.</p><p><a href=\"https://bair.berkeley.edu/blog/2018/11/30/visual-rl/\">Visual Model-Based Reinforcement Learning as a Path towards Generalist Robots</a> <em>(Frederik Ebert, Chelsea Finn et al)</em>: How can we get general robots that can perform a diverse array of tasks? We could collect a lot of data from robots acting randomly, train a dynamics model on pixels, and then use model-predictive control to plan. The dynamics model is a neural net trained to predict the next image given the current image and action. It helps to use temporal skip connections, because this allows the robot to get some object permanence since it can now &quot;remember&quot; objects it saw in the past that are currently blocked by something else. Model predictive control then samples sequences of actions (called plans), predicts the final image achieved, chooses the plan that best achieves the goal, and takes the first action of that plan. This is then repeated to choose subsequent actions. (Their method is slightly more sophisticated but this is the basic idea.) We can specify the goal by choosing a particular pixel and asking that the object at that pixel be moved to some other pixel. Alternatively, <a href=\"https://arxiv.org/abs/1810.00482\">Few-Shot Goal Inference for Visuomotor Learning and Planning</a> (<a href=\"https://mailchi.mp/0212425e5544/alignment-newsletter-27\">AN #27</a>) trains a classifier that can take a few demonstrations and output a goal.</p><p><strong>Rohin&#x27;s opinion:</strong> This is probably the easiest way to get a robot to do interesting things, since you just need it to collect experience autonomously with very little human involvement, you don&#x27;t need to have good object detection, and in many cases goal specification can be done without too much effort. I&#x27;m surprised that using random actions is enough -- how does the robot get enough examples of picking up an object with random actions? Maybe the robot&#x27;s random strategy is actually coded up in such a way that it is particularly likely to do interesting things like picking up an object.</p><p>It does seem like this approach will need something else in order to scale to more advanced capabilities, especially hierarchical tasks -- for example, you&#x27;ll never have an example of picking up a napkin, getting it wet, and wiping down a table. But perhaps we can iterate the process, where after we learn how to grasp and push, we start collecting data again using grasping and pushing instead of random low-level actions. Safe exploration would become more of a concern here.</p><p><a href=\"https://arxiv.org/abs/1811.12560\">An Introduction to Deep Reinforcement Learning</a> <em>(Vincent Francois-Lavet et al)</em></p><p><a href=\"https://blog.openai.com/quantifying-generalization-in-reinforcement-learning/\">Quantifying Generalization in Reinforcement Learning</a> <em>(Karl Cobbe)</em></p><h3>Applications</h3><p><a href=\"https://deepmind.com/blog/alphafold/\">AlphaFold: Using AI for scientific discovery</a> <em>(Andrew Senior et al)</em>: This post briefly describes AlphaFold, a system that does well at the protein folding problem. They train neural networks that can be used to evaluate how good a particular proposed protein structure is. This can be used to guide an evolutionary search that repeatedly replaces pieces of a protein structure with new protein fragments from a generative model. Alternatively, it can be used to construct a loss function for entire proteins, which allows us to use gradient descent to optimize the protein structure.</p><p><strong>Rohin&#x27;s opinion:</strong> The approach here is to learn heuristics that can guide a top-level search algorithm, which is the sort of thing that I think deep learning is particularly well poised to improve right now. Note that gradient descent is a top-level search algorithm here, because a separate loss function is constructed <em>for every protein</em>, rather than having a single loss function that is used to train a network that works on all proteins. However, unlike other applications such as SMT solvers, the top-level search algorithm does not have some sort of &quot;correctness&quot; guarantee.</p><p><em>Copyright © 2018 Rohin Shah, All rights reserved.</em></p>",
    "user": {
      "username": "rohinmshah",
      "slug": "rohinmshah",
      "displayName": "Rohin Shah"
    }
  },
  {
    "_id": "mPy4pmFSuPScNqSop",
    "title": "A hundred Shakespeares",
    "slug": "a-hundred-shakespeares",
    "pageUrl": "https://www.lesswrong.com/posts/mPy4pmFSuPScNqSop/a-hundred-shakespeares",
    "postedAt": "2018-12-11T23:11:48.668Z",
    "baseScore": 29,
    "voteCount": 12,
    "commentCount": 5,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p>In his <a href=\"https://www.lesswrong.com/posts/v7c47vjta3mavY3QC/is-science-slowing-down\">post on science slowing down</a>, Scott said:</p>\n<ul>\n<li>\"Are there a hundred Shakespeare-equivalents around today? This is a harder problem than it seems – Shakespeare has become so venerable with historical hindsight that maybe nobody would acknowledge a Shakespeare-level master today even if they existed – but still, a hundred Shakespeares?\"</li>\n</ul>\n<p>I'd argue that there are way more than a hundred Shakespeares around today, and there were several in Shakespeare's time. By Shakespeares, I mean authors who could have produced works of comparable quality to Shakespeare, by some <a href=\"http://slatestarcodex.com/2013/05/05/ambijectivity/\">reasonable measure of quality</a>.</p>\n<p>This seems surprising; there do not seem to be hundred living authors that are almost universally agreed to be must-reads in the same way that Shakespeare was.</p>\n<p>But this lack hints at a resolution of the paradox: we just don't have space for a hundred authors with the same fervour as we make space for Shakespeare. Neither as individuals nor as cultures can we fit these in. Shakespeare was a literary superstar. And superstars are rare, due to <a href=\"https://en.wikipedia.org/wiki/Network_effect\">network effects</a> and the <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200196\">power law of fame</a>.</p>\n<p>So my thesis would be that:</p>\n<ul>\n<li>There are many non-superstars who could plausibly have become superstars, and if they had done, they would produce works of comparable quality to the superstars.</li>\n</ul>\n<p>Part of this is the <a href=\"https://en.wikipedia.org/wiki/Halo_effect\">halo effect</a>: superstars just get judged as better than anyone else.</p>\n<p>Also, just by being famous, the interpretation of their work is altered. Bits of Shakespeare have permeated popular culture, and many articles and theories have been created about him. When we watch a Shakespeare play, we don't just see the words; we see the layers of cultural meaning and interpretation that have accumulated on it.</p>\n<p>I'd argue that, just by knowing that a play is by Shakespeare, we assume that it's deep and meaningful, and read in deeper interpretations and symbolism than we would otherwise. If we rediscovered two old plays, and they were word for word identical, but one was believed to be by Shakespeare and the other by some forgotten minor playwright, I'd expect that the first one would be a better play, just by what the audience would bring to it.</p>\n<p>Apart from those effects, superstars have the unique ability to focus more on their own vision. They have great self-confidence, and they can afford to trust that their audiences will have the patience to follow them where they want to go - rather than expecting immediate literary gratification. This would tend to result in works that are better than the average work of someone of equivalent skill, and more likely to be \"deep\", \"insightful\", or \"timeless\". This effect might be even more obvious with bloggers than with authors.</p>\n<p>So, though the number of superstars is severely limited, the number of potential superstars of equivalent skill can and most likely does increase with population.</p>\n<h1>Superstars in science</h1>\n<p>I'd argue that there's also a superstar effect in science. But here it combines with Scott's explanation 3: low hanging fruit. Newton did not come up with general relativity; Einstein didn't find quantum field theory; Tesla didn't invent the laser. You can't develop an idea until certain pre-requisites are met.</p>\n<p>And, unlike those solitary geniuses, most of science and technology is collaborative. Superstars get to be part of the best teams, interact with the best other scientists, and are more free to focus on the biggest, sexiest problems. I expect that there are many non-superstars who would have developed a certain part of theory, if a superstar hadn't got there first. It seems plausible to me that a single scientific superstar could have done the equivalent of derailing a hundred promising careers, just by getting to the key insight faster - without necessarily being much smarter (if at all) than the ones they preempted.</p>\n<p>Then, as discoveries pour in from superstars, and the far less productive non-superstars, the domain of science changes, and new avenues of discovery open up. And these new avenues are going to be claimed by the next generation of superstars, who will get there first. I expect that if we removed every single superstar of science in the last two hundred years, that we'd get roughly comparable scientific progress, with alternate superstars rising to the fore.</p>\n</body></html>",
    "user": {
      "username": "Stuart_Armstrong",
      "slug": "stuart_armstrong",
      "displayName": "Stuart_Armstrong"
    }
  },
  {
    "_id": "vHSrtmr3EBohcw6t8",
    "title": "Norms of Membership for Voluntary Groups",
    "slug": "norms-of-membership-for-voluntary-groups",
    "pageUrl": "https://www.lesswrong.com/posts/vHSrtmr3EBohcw6t8/norms-of-membership-for-voluntary-groups",
    "postedAt": "2018-12-11T22:10:00.975Z",
    "baseScore": 192,
    "voteCount": 80,
    "commentCount": 10,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Epistemic Status: Idea Generation</em></p>\n<p>One feature of the internet that we haven’t fully adapted to yet is that it’s trivial to create voluntary groups for discussion.  It’s as easy as making a mailing list, group chat, Facebook group, Discord server, Slack channel, etc.</p>\n<p>What we <em>don’t </em>seem to have is a good practical language for talking about <em>norms </em>on these mini-groups — what kind of moderation do we use, how do we admit and expel members, what kinds of governance structures do we create.</p>\n<p>Maybe this is a minor thing to talk about, but I suspect it has broader impact. In past decades voluntary membership in organizations has <a href=\"http://bowlingalone.com/\">declined</a> in the US — we’re less likely to be members of the Elks or of churches or bowling leagues — so lots of people who don’t have any experience in founding or participating in traditional types of voluntary organizations are now finding themselves engaged in governance <em>without even knowing that’s what they’re doing</em>.</p>\n<p>When we do this badly, we get “internet drama.”  When we do it <em>really </em>badly, we get harassment campaigns and calls for regulation/moderation at the corporate or even governmental level.  And <em>that </em>makes the news.  It’s not inconceivable that Twitter moderation norms affect international relations, for instance.</p>\n<p>It’s a traditional observation about 19th century America that Americans were eager joiners of voluntary groups, and that these groups were practice for democratic participation.  Political wonks today lament the lack of civic participation and loss of trust in our national and democratic institutions. Now, maybe you’ve moved on; maybe you’re a creature of the 21st century and you’re not hoping to restore trust in the institutions of the 20th. But what <em>will </em>be the institutions of the future?  That may well be affected by what formats and frames for group membership people are used to at the small scale.</p>\n<p>It’s also relevant for the future of freedom.  It’s starting to be a common claim that “give people absolute ‘free speech’ and the results are awful; therefore we need regulation/governance at the corporate or national level.”  If you’re not satisfied with that solution (as I’m not), <em>you have work to do</em> — there are a lot of questions to unpack like “what kind of ‘freedom’, with what implementational details, is the valuable kind?”, “if small-scale voluntary organizations can handle some of the functions of the state, how exactly will they work?”, “how does one prevent the outcomes that people consider so awful that they want large institutions to step in to govern smaller groups?”</p>\n<p>Thinking about, and working on, governance for voluntary organizations (and micro-organizations like online discussion groups) is a laboratory for figuring this stuff out in real time, with fairly low resource investment and risk. That’s why I find this stuff fascinating and wish more people did.</p>\n<p>The other place to start, of course, is <em>history</em>, which I’m not very knowledgeable about, but intend to learn a bit.  <a href=\"http://www.daviddfriedman.com/\">David Friedman</a> is the historian I’m familiar with who’s studied historical governance and legal systems with an eye to potential applicability to building voluntary governance systems today; I’m interested in hearing about others. (Commenters?)</p>\n<p>In the meantime, I want to start generating a (non-exhaustive list) of <em>types of norms </em>for group membership, to illustrate the diversity of how groups work and what forms “expectations for members” can take.</p>\n<p>We found organizations based on formats and norms that we’ve seen before.  It’s useful to have an idea of the <em>range </em>of formats that we might encounter, so we don’t get anchored on the first format that comes to mind.  It’s also good to have a vocabulary so we can have higher-quality disagreements about the purpose &amp; nature of the groups we belong to; often disagreements seem to be about policy details but are really about the <em>overall type </em>of what we want the group to be.</p>\n<p><strong>Civic/Public Norms</strong></p>\n<ul>\n<li>Roughly everybody is welcome to join, and free to do as they like in the space, so long as they obey a fairly minimalist set of ground rules &amp; behavioral expectations that apply to everyone.</li>\n<li>We expect it to be <em>easy </em>for most people to follow the ground rules; you have to be <em>deviant </em>(really unusually antisocial) to do something egregious enough to get you kicked out or penalized.</li>\n<li>If you dislike someone’s behavior but it isn’t against the ground rules, you can grumble a bit about it, but you’re expected to tolerate it. You’ll have to admit things like “well, he has a right to do that.”</li>\n<li>Penalties are expected to be predictable, enforced the same way towards all people, and “impartial” (not based on personal relationships). If penalties are enforced unfairly, you’re <em>not </em>expected to tolerate it — you can question why you’re being penalized, and kick up a public stink, and it’s even praiseworthy to do so.</li>\n<li>Examples: “rule of law”, public parks and libraries, stores and coffeeshops open to the public, town hall meetings</li>\n</ul>\n<p><strong>Guest Norms</strong></p>\n<ul>\n<li>The host can invite, or not invite, anyone she chooses, based on her preference.  She doesn’t have to justify her preferences to anyone.  Nobody is entitled to an invitation, and it’s very rude to complain about not being invited.</li>\n<li>Guests can also choose to attend or not attend, based on <em>their </em>preferences, and they don’t have to justify their preferences to anyone either; it’s rude to complain or ask for justification when someone declines an invitation.</li>\n<li>Personal relationships and subjective feelings, in particular, are totally legitimate reasons to include or exclude someone.</li>\n<li>The atmosphere within the group is expected to be pleasant for everyone.  If you don’t want to be asked to leave, you shouldn’t do things that will predictably bother people.</li>\n<li>Hosts are expected to be kind and generous to guests; guests are expected to be kind and generous to the host and each other; the host is responsible for enforcing boundaries.</li>\n<li>Criticizing other people at the gathering itself is taboo. You’re expected to do your critical/judgmental pruning <em>outside </em>the gathering, by deciding whom you will invite or whether you’ll attend.</li>\n<li>We don’t expect that everyone will be invited to be a guest at every gathering, or that everyone will attend everything they’re invited to. It can be prestigious to be invited to some gatherings, and embarrassing to be asked to leave or passed over when you expected an invitation, but it’s normal to just <em>not be invited </em>to some things.</li>\n<li>Examples: private parties, invitation-only events, consent ethics for sex</li>\n</ul>\n<p><strong>Kaizen Norms</strong></p>\n<ul>\n<li>Members of the group are expected to be committed to an <em>ideal</em> of some kind of excellence and to continually strive to reach it.</li>\n<li>Feedback or critique on people’s performance is continuous, normal, and not considered inherently rude. It’s considered praiseworthy to give high-quality feedback and to accept feedback willingly.</li>\n<li>Kaizen groups may have very specific norms about the <em>style </em>or <em>format </em>of critique/feedback that’s welcome, and it may well be considered rude to give feedback in the wrong style.</li>\n<li>Receiving <em>some </em>negative feedback or penalties is normal and not considered a sign of failure or shame.  What <em>is </em>shameful is responding defensively to negative feedback.</li>\n<li>You can lose membership in the group by getting <em>too much </em>negative feedback (in other words, failing to live up to the minimum standards of the group’s ideal.)  It’s <em>not </em>expected to be easy for most people to meet these standards; they’re challenging by design.  The group isn’t expected to be “for everyone.”</li>\n<li>The feedback and incentive processes are supposed to correlate tightly to the ideal. It’s acceptable and even praiseworthy to criticize those processes if they reward and punish people for things unrelated to the ideal.</li>\n<li>Conflict about things unrelated to the ideal isn’t taboo, but it’s somewhat discouraged as “off-topic” or a “distraction.”</li>\n<li>Examples: competitive/meritocratic school and work environments, sports teams, specialized religious communities (e.g. monasteries, rabbinical schools)</li>\n</ul>\n<p><strong>Coalition Norms</strong></p>\n<ul>\n<li>The degree to which one is “welcome” in the coalition is the degree to which one is loyal, i.e. contributes resources to the coalition.  (Either by committing one’s own resources or by driving others to contribute their resources.   The latter tends to be more efficient, and hence makes you more “welcome.”)</li>\n<li>Membership is a matter of degree, not a hard-and-fast boundary.  The more solidly loyal a member you are, the more of the coalition’s resources you’re entitled to.  (Yes, this means membership is defined recursively, like PageRank.)</li>\n<li>People can be penalized or expelled for not contributing enough, or for doing things that have the effect of preventing the coalition gaining resources (like making it harder to recruit new members.)</li>\n<li>Conflict, complaint, and criticism over <em>the growth of the coalition</em> (and whether people are contributing enough, or whether they’re taking more than their fair share) is acceptable and even praiseworthy; criticisms about other things are discouraged, because they make people less willing to contribute resources or pressure others to do so.</li>\n<li>Membership in the coalition is considered praiseworthy.  Non-membership is considered shameful.</li>\n<li>Examples: political coalitions, proselytizing religions</li>\n</ul>\n<p><strong>Tribal Norms</strong></p>\n<ul>\n<li>Membership in the group is defined by an immutable, unchosen characteristic, like sex or heredity (or, to a lesser extent, geographic location.)  It is difficult to join, leave, or be expelled from the group; you <em>are </em>a member as a matter of fact, regardless of what you want or how you behave.</li>\n<li>It’s not considered shameful not to be a member of the group; after all, it isn’t up to you.</li>\n<li>Since expulsion is difficult, behavioral norms for the group are maintained primarily by persuasion/framing, reward, and punishment, so these play a larger role than they do in voluntary groups.  Important norms are framed as <em>commandments </em>or simply <em>how things are</em>.</li>\n<li>Examples: families, public schools, governments, traditional cultures</li>\n</ul>\n<p>Some comparisons-and-contrasts:</p>\n<p><em>Honor and Shame</em></p>\n<p>Kaizen and Guest group norms say that being a member of the group is an honor and comes with high expectations, but that <em>not </em>being a member is normal and not especially shameful.</p>\n<p>Civic norms say that being a member of the group is normal and easy to attain, but <em>not </em>being a member is shameful, because it indicates egregiously bad behavior.</p>\n<p>Coalition norms say that being a member is an honor and comes with high expectations <em>and </em>that not being a member is shameful.  This means that most people will have something to be ashamed of.</p>\n<p>Tribal norms say that being a member is not an honor (though it may be a privilege), and that not being a member is no shame.</p>\n<p><em>Protest</em></p>\n<p>Civic and Kaizen norms say that it’s okay to protest “unfair” treatment by the governing body.  In a Civic context, “fair” means “it’s possible for everyone to stay out of trouble by following the rules” — it’s okay for rules to be arbitrary, but they should be clear and consistent and not so onerous that most people can’t follow them.  In a Kaizen context, “fair” means “corresponding to the ideal” — it’s okay to “not do things by the book”  if that gets you better performance, but it’s not okay if you’re rewarding bad performance and punishing good.</p>\n<p>Guest and Coalition norms say that it’s not okay to protest “unfair” treatment; if you get kicked out, arguing can’t help you get back in.  Offering the decisionmakers something they value might work, though.</p>\n<p>In Tribal norms, protest and argument can be either licit or taboo; it depends on the specific tribe and its norms.</p>\n<p><em>Examples of debates that are about what type of group you want to be in:</em></p>\n<p>Asking for “inclusiveness” is usually a bid to make the group more Civic or Coalitional.</p>\n<p>Making accusations of “favoritism” is usually a bid to make the group more Civic or Kaizen.</p>\n<p>Complaining about “problem members” is usually a bid to make the group more Coalitional, Guest, or Kaizen.</p>\n<p><strong>Not A Taxonomy</strong></p>\n<p>I don’t think these are <em>the </em>definitive types of groups. The idea is to illustrate how you can have different starting assumptions about what kind of thing the group is for. (Is it for achieving a noble goal? For providing a public forum or service open to all? For meeting the needs of its members?)</p>\n<p>I suspect these kinds of <em>aims </em>are prior to <em>mechanisms </em>(things like “what is a bannable offense” or “what incentive systems do we set up”?)  Before diving into the technical stuff about the rules of the game, you want to ask what kinds of <em>outcomes or group dynamics </em>you want the “game structure” to achieve.</p>\n<p> </p>",
    "user": {
      "username": "sarahconstantin",
      "slug": "sarahconstantin",
      "displayName": "sarahconstantin"
    }
  },
  {
    "_id": "5TYnquzAQPENyZXDa",
    "title": "Quantum immortality: Is decline of measure compensated by merging timelines?",
    "slug": "quantum-immortality-is-decline-of-measure-compensated-by",
    "pageUrl": "https://www.lesswrong.com/posts/5TYnquzAQPENyZXDa/quantum-immortality-is-decline-of-measure-compensated-by",
    "postedAt": "2018-12-11T19:39:28.534Z",
    "baseScore": 9,
    "voteCount": 9,
    "commentCount": 8,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>I wrote an <a href=\"https://docs.google.com/document/d/1Y8uvR4rxR8LMqtIjO0Eho5JzCMNR9KU39Yo00en09gM/edit?usp=sharing\">article</a> about the quantum immortality which, I know, is a controversial topic, and I would like to get comments on it.  The interesting twist, suggested in the article, is the idea of measure increase  which could compensate declining measure in quantum immortality. (There are other topics in the article, like the history of QM, its relation to the multiverse immortality, the utility of cryonics, impossibility of euthanasia and the relation of QI to different decision theories.)</p><p>The standard argument against quantum immortality in MWI runs as following. One should calculate the expected utility by multiplying the expected gain on the measure of existence (roughly equal to the one&#x27;s share of the world’s timelines).  In that case, if someone expects to win 10.000 USD in the  Quantum suicide lottery with 0.01 chance of survival, her actual expected utility is 100 USD (ignoring negutility of death).  So, the rule of thumb is that the measure declines very quickly after series of quantum suicide experiments, and thus this improbable timeline should be ignored. The following equation could be used for U(total) = mU, where m is measure and U is expected win in the lottery.  </p><p>However, if everything possible exists in the multiverse, there are many my pseudo-copies, which differ from me in a few bits, for example, they have a different phone number or different random child memory. The difference is small but just enough for not regard them as my copies. </p><p>Imagine that this different child memory is 1kb (if compressed) size. Now, one morning both me and all my pseudo-copies forget this memory, and all we become exactly the same copies. In some sense, our timelines merged. This could be interpreted as a jump in my measure, which will as high as 2power1024 = (roughly) 10E300. If I use the equation U(total) = mU I can get an extreme jump of my utility. For example, I have 100 USD and now my measure increased trillion of trillion of times, I supposedly get the same utility as if I become mega-multi-trillioner.  </p><p>As a result of this absurd conclusion, I can spend the evening hitting my head with a stone and thus losing more and more memories, and getting higher and higher measure, which is obviously absurd behaviour for a human being - but could be a failure mode for an AI, which uses the equation to calculate the expected utility.  </p><p>In case of the Quantum suicide experiment, I can add to the bomb, which kills me with 0.5 probability, also a laser, which kills just one neuron in my brain (if I survive), which - let&#x27;s assume it - is equal to forgetting 1 bit of information. In that case, QS reduces my measure in half, but forgetting one bit increases it in half.  Obviously, if I play the game for too long, I will damage my brain by the laser, but anyway, brain cells are dying so often in aging brain (millions  a day), that it will be completely non-observable.</p><p>BTW, Pereira suggested the similar idea as an anthropic argument against existence of any superintelligence <a href=\"https://arxiv.org/abs/1705.03078\">https://arxiv.org/abs/1705.03078</a> </p><p></p><p></p>",
    "user": {
      "username": "avturchin",
      "slug": "avturchin",
      "displayName": "avturchin"
    }
  },
  {
    "_id": "DuPjCTeW9oRZzi27M",
    "title": "Bounded rationality abounds in models, not explicitly defined",
    "slug": "bounded-rationality-abounds-in-models-not-explicitly-defined",
    "pageUrl": "https://www.lesswrong.com/posts/DuPjCTeW9oRZzi27M/bounded-rationality-abounds-in-models-not-explicitly-defined",
    "postedAt": "2018-12-11T19:34:17.476Z",
    "baseScore": 15,
    "voteCount": 9,
    "commentCount": 9,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Last night, I did not register a patent to cure all forms of cancer. Even though it’s probably possible to figure such a cure out, from basic physics and maybe a download of easily available biology research papers.</p>\n<p>Can we then conclude that I don’t want cancer to be cured – or, alternatively, that I am pathologically modest and shy, and thus don’t want the money and fame that would accrue?</p>\n<p>No. The correct and obvious answer is that I am boundedly rational. And though an unboundedly rational agent – and maybe a superintelligence – could figure out a cure for cancer from first principles, poor limited me certainly can’t.</p>\n<p>Modelling bounded rationality is tricky, and it is often accomplished by artificially limiting the action set/action space. Many economic models use revealed preferences, and feature agents that are assumed to be fully rational, but who are restricted to choosing between a tiny set of possible goods or lotteries. They don’t have the options of developing new technologies, rousing the population to rebellion, going online and fishing around for functional substitutes, founding new political movements, begging, befriending people who already have the desired goods, setting up GoFundMe pages, and so on.</p>\n<p>There’s nothing wrong with modelling bounded rationality via action set restriction, as long as we’re aware of what we’re doing. In particular, we can’t naively conclude that because a such a model fits with observation, that therefore humans actually are fully rational agents. In particular, though economists are right that humans are more rational than we might naively suppose, thinking of us as rational, or “mostly rational”, is a colossally erroneous way of thinking. In terms of achieving our goals, as compared with a rational agent, we are barely above agents acting randomly.</p>\n<p>Another problem with using small action sets, is that it may lead us to think that an AI might be similarly restricted. That is unlikely to be the case; an intelligent robot walking around would certainly have access to actions that no human would, and possibly ones we couldn’t easily imagine.</p>\n<p>Finally, though action set reduction can work well in toy models, it is wrong about the world and about humans. So as we make more and more sophisticated models, there will come a time when we have to discard it, and tackle head-on the difficult issue of defining bounded rationality properly. And it’s mainly for this last point I’m writing this post; we’ll never see the necessity of better ways of defining bounded rationality, unless we realise that modelling it via action set restriction is a) common, b) useful, and c) wrong.</p>\n",
    "user": {
      "username": "Stuart_Armstrong",
      "slug": "stuart_armstrong",
      "displayName": "Stuart_Armstrong"
    }
  },
  {
    "_id": "YfQGZderiaGv3kBJ8",
    "title": "Figuring out what Alice wants: non-human Alice",
    "slug": "figuring-out-what-alice-wants-non-human-alice",
    "pageUrl": "https://www.lesswrong.com/posts/YfQGZderiaGv3kBJ8/figuring-out-what-alice-wants-non-human-alice",
    "postedAt": "2018-12-11T19:31:13.830Z",
    "baseScore": 16,
    "voteCount": 6,
    "commentCount": 17,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head><style type=\"text/css\">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></head><body><p>I’ve shown that we <a href=\"https://arxiv.org/abs/1712.05812\">cannot deduce the preferences</a> of a potentially irrational agent. Even simplicity priors don’t help. We need to make extra <a href=\"https://www.lesswrong.com/posts/Fg83cD3M7dSpSaNFg/normative-assumptions-regret\">‘normative’ assumptions</a> in order to be able to say anything about these preferences.</p>\n<p>I then presented a more <a href=\"https://www.lesswrong.com/posts/rcXaY3FgoobMkH2jc/figuring-out-what-alice-wants-part-ii\">intuitive example</a>, in which Alice was <a href=\"https://www.lesswrong.com/posts/b3TLbcwfJK5rmcwXc/poker-example-not-deducing-someone-s-preferences\">playing poker</a>, and had two possible beliefs about Bob’s hand, and two possible preferences: wanting money, or wanting Bob (which, in that situations, translated into wanting to lose to Bob).</p>\n<p>That example illustrated the impossibility result, <em>within the narrow confines of that situation</em> – if Alice calls, she could be a money-maximiser expecting to win, or a love-maximiser expecting to lose.</p>\n<p>As has been pointed out, this uncertainty doesn’t really persist if we move beyond the initial situation. If Alice was motivated by love or money, we would expect to be able to tell which one, by seeing what she does in other situations – how does she respond to Bob’s flirtations, what does she confess to her closest friends, how does she act if she catches a peek of Bob’s cards, etc…</p>\n<p>So if we look at her more general behaviour, it seems that we have two possible versions of Alice. First, <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_m\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span>, who clearly wants money, and <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_\\heartsuit\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">♡</span></span></span></span></span></span></span></span>, who clearly wants Bob. The actions of these two agents match up in the specific case I described, but not in general. Doesn’t this undermine my claim that we can’t tell the preferences of an agent from their actions?</p>\n<p>What’s actually happening here is that we’re <em>already</em> making a lot of extra assumptions when we’re interpreting <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_m\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span> or <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_\\heartsuit\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">♡</span></span></span></span></span></span></span></span>’s actions. We model other humans in very specific and narrow ways, and other humans do the same – and their models are very similar to ours (consider how often humans agree that another human is angry, or that being drunk impairs rationality). The agreement isn’t perfect, but is much better than random.</p>\n<p>If we set those assumptions aside, then we can see what the theorem implies. There is a possible agent <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_m'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.157em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span></span>, whose preference is for love, but that nevertheless acts identically to <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_m\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span> (and the reverse for money-loving <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_\\heartsuit'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.35em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">♡</span></span></span></span></span></span></span></span></span> versus <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_\\heartsuit\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">♡</span></span></span></span></span></span></span></span>). <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_m'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.157em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span></span></span> and <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A_\\heartsuit'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.35em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">♡</span></span></span></span></span></span></span></span></span> are perfectly plausible agents – they just aren’t ‘human’ according to our models of what being human means.</p>\n<p>It’s because of this that I’m somewhat optimistic we can solve the value learning problem, and why I often say the problem is “impossible in theory, but doable in practice”. Humans make a whole host of assumptions that allow them to interpret the preferences of other humans (and of themselves). And these assumptions are quite similar from human to human. So we don’t need to solve the value learning problem in some principled way, nor figure out the necessary assumptions abstractly. Instead, we just need to extract the normative assumptions <em>that humans are already making</em> and use these in the value learning process (and then resolve all the contradictions within human values, but that seems <a href=\"https://www.lesswrong.com/posts/Y2LhX3925RodndwpC/resolving-human-values-completely-and-adequately\">doable if messy</a>).</p>\n</body></html>",
    "user": {
      "username": "Stuart_Armstrong",
      "slug": "stuart_armstrong",
      "displayName": "Stuart_Armstrong"
    }
  },
  {
    "_id": "95i5B78uhqyB3d6Xc",
    "title": "Assuming we've solved X, could we do Y...",
    "slug": "assuming-we-ve-solved-x-could-we-do-y",
    "pageUrl": "https://www.lesswrong.com/posts/95i5B78uhqyB3d6Xc/assuming-we-ve-solved-x-could-we-do-y",
    "postedAt": "2018-12-11T18:13:56.021Z",
    "baseScore": 31,
    "voteCount": 14,
    "commentCount": 16,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p>The year is 1933. <a href=\"https://en.wikipedia.org/wiki/Leo_Szilard\">Leó Szilárd</a> has just hypothised the <a href=\"https://en.wikipedia.org/wiki/Nuclear_chain_reaction\">nuclear chain reaction</a>. Worried researchers from proto-MIRI or proto-FHI ask themselves \"assuming we've solved the issue of nuclear chain reactions in practice, could we build a nuclear bomb out of it\"?</p>\n<p>Well, what do we mean by \"assuming we've solved the issue of nuclear chain reactions\"? Does it mean that \"we have some detailed <a href=\"https://en.wikipedia.org/wiki/Nuclear_weapon#/media/File:Fission_bomb_assembly_methods.svg\">plans</a> for viable nuclear bombs, including all the calculations needed to make them work, and everything in the plans is doable by a rich industrial state\"? In that case, the answer to \"could we build a nuclear bomb out of it?\" is a simple and trivial <strong>yes</strong>.</p>\n<p>Alternatively, are we simply assuming \"there exists a collection of matter that supports a chain reaction\"? In which case, note that the assumption is (almost) completely useless. In order to figure out whether a nuclear bomb is buildable, we still need to figure out all the details of chain reactions - that assumption has bought us nothing.</p>\n<h2>Assuming human values...</h2>\n<p>At the recent <a href=\"https://aisafetyunconference.info/\">AI safety unconference</a>, David Krueger wanted to test, empirically, whether <a href=\"https://blog.openai.com/debate/\">debate methods</a> could be used for creating aligned AIs. At some point in the discussion, he said \"let's assume the question of defining human values is solved\", wanting to move on to whether a debate-based AI could then safely implement it.</p>\n<p>But as above, when we assume that an underdefined definition problem (<a href=\"https://www.lesswrong.com/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values\">human values</a>) is solved, we have to be very careful what we mean - the assumption might be useless, or might be too strong, and end up solving the implementation problem entirely.</p>\n<p>In the conversation with David, we were imagining a definition of human values related to what humans would answer if we could reflexively ponder specific questions for thousands of years. One could object to that definition on the grounds that people can be coerced or <a href=\"https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search\">tricked</a> into giving the answers that the AI might want - hence the circumstances of that pondering is critical.</p>\n<p>If we assume X=\"human values are defined in this way\", could an AI safely implement X via debate methods? Well, what about coercion and trickery by the AI during the debate process? It could be that X doesn't help at all, because we still have to resolve all of the same issues.</p>\n<p>Or, conversely, X might be too strong - it might define what trickery is, which solves a lot of the implementation problem for free. Or, in the extreme case, maybe X is expressed in computer code, and solve all the contradictions within humans, dealing with ontology issues, population changes, what an agent is, and all other subtleties. Then the question \"given X, could an AI safely implement it?\" reduces to \"can the AI run code?\"</p>\n<p>In summary, when the issue is underdefined, the boundary between definition and implementation is very unclear, and assuming that one of them is solved is very unclear.</p>\n<h2>How to assume (for the good of all of us)</h2>\n<p>The obvious way around this issue is to be careful and precise in what we're assuming. So, for example, we might assume \"we have an algorithm A, if run for a decade, would compute what humans would decide after a thousand years of debate\". Then we have two practical and well defined subproblems to work on: can we approximate the output of A within reasonable time, and is \"what humans would decide after a thousand years of debate\" a good <a href=\"https://www.lesswrong.com/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values\">definition of human values</a>?</p>\n<p>Another option, when we lack a full definition, is to focus on some of the <em>properties</em> of that definition that we feel are certain or likely. For example, we can assume that \"the total extinction of the all intelligent beings throughout the cosmos\" is not a desirable feature according to most human values, and argue whether debate methods will lead to that outcome. Or, at smaller scale, we might assume that telling us informative truths is compatible with our values, and check whether the debate AI would do that.</p>\n</body></html>",
    "user": {
      "username": "Stuart_Armstrong",
      "slug": "stuart_armstrong",
      "displayName": "Stuart_Armstrong"
    }
  },
  {
    "_id": "4aG7DjPQrzeA8K9hy",
    "title": "Who's welcome to our LessWrong meetups?",
    "slug": "who-s-welcome-to-our-lesswrong-meetups",
    "pageUrl": "https://www.lesswrong.com/posts/4aG7DjPQrzeA8K9hy/who-s-welcome-to-our-lesswrong-meetups",
    "postedAt": "2018-12-10T13:31:17.357Z",
    "baseScore": 19,
    "voteCount": 9,
    "commentCount": 5,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p> </p><p>As  part of announcing meetups publically, it&#x27;s good to write in the meetup  description about what kind of people would likely be a good match for  the meetup. I still haven&#x27;t gotten a good description myself.</p><p>How would you describe the kind of people we are in words that are clear to outsiders?</p>",
    "user": {
      "username": "ChristianKl",
      "slug": "christiankl",
      "displayName": "ChristianKl"
    }
  },
  {
    "_id": "8EqTiMPbadFRqYHqp",
    "title": "How Old is Smallpox?",
    "slug": "how-old-is-smallpox",
    "pageUrl": "https://www.lesswrong.com/posts/8EqTiMPbadFRqYHqp/how-old-is-smallpox",
    "postedAt": "2018-12-10T10:50:33.960Z",
    "baseScore": 44,
    "voteCount": 15,
    "commentCount": 5,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>The conventional view is that smallpox has been around since antiquity, but more recent evidence has suggested it&#x27;s actually only around 500 years old. </p><p>So I have a research/rationality question: how conclusive is the &quot;500 years old hypothesis&quot;? I don&#x27;t really have the expertise to evaluate it.</p><p>The wikipedia entry briefly notes the new findings, but doesn&#x27;t seem to have rewritten the overall history section:</p><blockquote>The earliest credible clinical evidence of smallpox is found in the smallpox-like disease in medical writings from ancient India (as early as 1500 BC),<a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-Shchelkunov2011-54\">[54]</a><a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-55\">[55]</a><a href=\"https://en.wikipedia.org/wiki/Ancient_Egypt\">Egyptian</a> <a href=\"https://en.wikipedia.org/wiki/Mummy\">mummy</a> of <a href=\"https://en.wikipedia.org/wiki/Ramses_V\">Ramses V</a> who died more than 3000 years ago (1145 BC)<a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-Ramses-56\">[56]</a> and China (1122 BC).<a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-Hopkins_2002-57\">[57]</a> It has been speculated that Egyptian traders brought smallpox to India during the 1st millennium BC, where it remained as an endemic human disease for at least 2000 years. Smallpox was probably introduced into China during the 1st century AD from the southwest, and in the 6th century was carried from China to Japan.<a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-Fenner1-26\">[26]</a> In Japan, the <a href=\"https://en.wikipedia.org/wiki/735%E2%80%93737_Japanese_smallpox_epidemic\">epidemic of 735–737</a> is believed to have killed as much as one-third of the population.<a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-Hays2005-14\">[14]</a><a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-58\">[58]</a> At least seven religious deities have been specifically dedicated to smallpox, such as the god <a href=\"https://en.wikipedia.org/wiki/Sopona\">Sopona</a> in the <a href=\"https://en.wikipedia.org/wiki/Yoruba_religion\">Yoruba religion</a>. In India, the Hindu goddess of smallpox, <a href=\"https://en.wikipedia.org/wiki/Shitala_Devi\">Sitala Mata</a>, was worshiped in temples throughout the country.<a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-59\">[59]</a></blockquote><blockquote>A different viewpoint is that smallpox emerged 1588 AD and the earlier reported cases were incorrectly identified as smallpox.<a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-60\">[60]</a><a href=\"https://en.wikipedia.org/wiki/Smallpox#cite_note-61\">[61]</a></blockquote><h2>Paper: 17th Century Variola Virus Reveals the Recent History of Smallpox</h2><p>The <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0\">paper arguing the 500 years hypothesis is here</a>. </p><p><strong>Highlights:</strong></p><blockquote>• Variola virus genome was reconstructed from a 17th century mummified child</blockquote><blockquote>• The archival strain is basal to all 20th century strains, with same gene degradation</blockquote><blockquote>• Molecular-clock analyses show that much of variola virus evolution occurred recently</blockquote><p><strong>Abstract</strong></p><blockquote>Smallpox holds a unique position in the history of medicine. It was the first disease for which a vaccine was developed and remains the only human disease eradicated by vaccination. Although there have been claims of smallpox in Egypt, India, and China dating back millennia [<a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">1</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">2</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">3</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">4</a>], the timescale of emergence of the causative agent, variola virus (VARV), and how it evolved in the context of increasingly widespread immunization, have proven controversial [<a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">4</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">5</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">6</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">7</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">8</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">9</a>]. </blockquote><blockquote>In particular, some molecular-clock-based studies have suggested that key events in VARV evolution only occurred during the last two centuries [<a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">4</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">5</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">6</a>] and hence in apparent conflict with anecdotal historical reports, although it is difficult to distinguish smallpox from other pustular rashes by description alone. </blockquote><blockquote>To address these issues, we captured, sequenced, and reconstructed a draft genome of an ancient strain of VARV, sampled from a Lithuanian child mummy dating between 1643 and 1665 and close to the time of several documented European epidemics [<a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">1</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">2</a>, <a href=\"https://www.cell.com/current-biology/fulltext/S0960-9822(16)31324-0#\">10</a>]. When compared to vaccinia virus, this archival strain contained the same pattern of gene degradation as 20th century VARVs, indicating that such loss of gene function had occurred before ca. 1650. </blockquote><blockquote>Strikingly, the mummy sequence fell basal to all currently sequenced strains of VARV on phylogenetic trees. Molecular-clock analyses revealed a strong clock-like structure and that the timescale of smallpox evolution is more recent than often supposed, with the diversification of major viral lineages only occurring within the 18th and 19th centuries, concomitant with the development of modern vaccination.</blockquote>",
    "user": {
      "username": "Raemon",
      "slug": "raemon",
      "displayName": "Raemon"
    }
  },
  {
    "_id": "RRkpx9Nc5QE5D2vrx",
    "title": "LessWrong Tel Aviv: Civilizational Collapse",
    "slug": "lesswrong-tel-aviv-civilizational-collapse",
    "pageUrl": "https://www.lesswrong.com/events/RRkpx9Nc5QE5D2vrx/lesswrong-tel-aviv-civilizational-collapse",
    "postedAt": "2018-12-10T07:26:46.272Z",
    "baseScore": 7,
    "voteCount": 1,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p></p><p>LessWrong Israel  holds &quot;Pub Nights&quot; and &quot;Game Nights&quot; most Tuesdays. You can follow them <a href=\"https://www.facebook.com/groups/480249315379682/\">here</a>.<br/><br/>On Dec. 18, LessWrong Israel presents <a href=\"https://www.facebook.com/events/2206046162787783/\">Daniel Armak on civilizational collapse</a> at Google, 98 Yigal Alon, Floor 27 (<em>not &quot;</em>Google Campus&quot;), Tel Aviv. <br/></p><p>Get together at 19:00, start at 19:30.<br/></p><p>Abstract:</p><p>It&#x27;s easy to imagine the end of the world as we know it brought about by war, technology, disease, or climate change. But such external impulses are not necessary for civilization to collapse and technology to greatly regress. Such collapses have occurred in the past for purely internal, &#x27;social&#x27; reasons. We should remember these events and make them emotionally salient when considering future possibilities.<br/></p><p>I will focus on the two biggest and best-known recent European &#x27;falls of civilization&#x27;: one at the end of the Western Roman Empire (4th-5th centuries), another at the end of the Bronze Age (12th c. BC), and briefly mention some others. I will not dwell on their causes (this requires a much longer discussion), but will try to describe what is agreed to have resulted.</p><p></p>",
    "user": {
      "username": "JoshuaFox",
      "slug": "joshuafox",
      "displayName": "JoshuaFox"
    }
  },
  {
    "_id": "me9Bc8JGkXTovWJCs",
    "title": "Boston Secular Solstice",
    "slug": "boston-secular-solstice",
    "pageUrl": "https://www.lesswrong.com/events/me9Bc8JGkXTovWJCs/boston-secular-solstice",
    "postedAt": "2018-12-10T01:59:24.756Z",
    "baseScore": 10,
    "voteCount": 3,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>I previously made a <a href=\"https://www.lesswrong.com/posts/ERboWueanAyqwKbiQ/boston-solstice-2018\">LessWrong Post</a> but since it wasn&#x27;t an event I&#x27;m also making an event entry.</p><p>If you can <a href=\"https://www.facebook.com/events/177150809894419/\">RSVP  on Facebook</a> or email me (jeff@jefftk.com) if you&#x27;re coming it will be easier to have the right amount of seating.</p>",
    "user": {
      "username": "jkaufman",
      "slug": "jkaufman",
      "displayName": "jefftk"
    }
  },
  {
    "_id": "45oMPv7cjp9FRkyKy",
    "title": "Why should EA care about rationality (and vice-versa)?",
    "slug": "why-should-ea-care-about-rationality-and-vice-versa",
    "pageUrl": "https://www.lesswrong.com/posts/45oMPv7cjp9FRkyKy/why-should-ea-care-about-rationality-and-vice-versa",
    "postedAt": "2018-12-09T22:03:58.158Z",
    "baseScore": 14,
    "voteCount": 3,
    "commentCount": 13,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>There&#x27;s a lot of overlap between the effective altruism movement and the LessWrong rationality movement in terms of their membership, but each also has many people who are part of one group and not the other. For those in the overlap, why should EA care about rationality and rationality care about EA?</p>",
    "user": {
      "username": "gworley",
      "slug": "gordon-seidoh-worley",
      "displayName": "Gordon Seidoh Worley"
    }
  },
  {
    "_id": "oMcQPMcMcmNgQ9jvp",
    "title": "Measly Meditation Measurements",
    "slug": "measly-meditation-measurements",
    "pageUrl": "https://www.lesswrong.com/posts/oMcQPMcMcmNgQ9jvp/measly-meditation-measurements",
    "postedAt": "2018-12-09T20:54:46.781Z",
    "baseScore": 63,
    "voteCount": 23,
    "commentCount": 19,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p>A few months ago, I decided to start meditating regularly, around an hour a day. It seemed like a good opportunity to measure possible effects, so <a href=\"https://www.lesswrong.com/posts/EPndkemxaiu25yXJa/an-invitation-to-measure-meditation\">I asked for advice on what to measure</a>. This post summarizes the results. In short, while the <em>subjective</em> effects of meditation were strong, the <em>measurements</em> didn't show anything. This is a fine place to stop reading; I'm mostly posting this because I promised to.</p>\n<p>I did mindfulness meditation, as guided by <a href=\"https://www.amazon.com/Mind-Illuminated-Meditation-Integrating-Mindfulness/dp/1501156985\">The Mind Illuminated</a>. My object of focus was typically my breath (while sitting), or my steps (as hiking).</p>\n<h2>What I Measured</h2>\n<ul>\n<li>About once a week, I did some online tasks either before or after meditating. These were the Go/NoGo and CuedAttention tasks on <a href=\"http://www.quantified-mind.com/experiment/meditation\">quantified-mind.com</a>, and <a href=\"http://sleepdisordersflorida.com/pvt1.html\">this psychomotor vigilance task</a>.</li>\n<li>I set up roughly once or twice daily pings from <a href=\"http://messymatters.com/tagtime/\">TagTime</a> for experience sampling.</li>\n</ul>\n<h2>What I Measured</h2>\n<ul>\n<li>My performance on the tasks looked entirely random. It wasn't better or worse after meditating, and it didn't get better or worse over time.</li>\n<li>I have no idea how to do experience sampling. I understand that some people have moods. I'm almost always in a neutral mood, and so wasn't sure what to put most of the time. Also, I'm apparently often away from my phone, and missed many (most?) pings.</li>\n</ul>\n<h2>What I Learned</h2>\n<ul>\n<li><a href=\"https://www.amazon.com/Mind-Illuminated-Meditation-Integrating-Mindfulness/dp/1501156985\">The Mind Illuminated</a> is as good of a guide as I hoped it would be.</li>\n<li>A few measly months of meditation isn't going to change anything like your performance on reaction-time-like tasks.</li>\n<li>A few measly months of meditation <em>will</em> give you a fascinating look into your own mind. It's not what you think. I'd say more, but I'm deeply confused and don't have a good model.</li>\n<li>Meditation retreats are great. I went on a two-day one, whose format wasn't particularly well-suited for me, and even this had a large effect on my practice.</li>\n</ul>\n</body></html>",
    "user": {
      "username": "justinpombrio",
      "slug": "justinpombrio",
      "displayName": "justinpombrio"
    }
  },
  {
    "_id": "MEimkSwwuRgiPaZaY",
    "title": "Review: Slay the Spire",
    "slug": "review-slay-the-spire",
    "pageUrl": "https://www.lesswrong.com/posts/MEimkSwwuRgiPaZaY/review-slay-the-spire",
    "postedAt": "2018-12-09T20:40:01.616Z",
    "baseScore": 22,
    "voteCount": 10,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Epistemic Status: Many hours played</p>\n<p>Spoiler-Free Bottom Line: Slay the Spire is an amazing single-player roguelike deckbuilding game. When I wrote that Artifact was the most fun I’ve had gaming in a long time, the only alternative to give me pause was Slay the Spire. Each game, you work your way up the spire, with each room an opportunity to improve your deck, either with rewards from battle or other opportunities. Each turn of each battle, you see what the enemy is going to do, and by default you have three energy to spend on any combination of five drawn cards, to prepare to block their attacks while dealing damage back. If you die, that’s it, time to start over. </p>\n<p>Early plays ideally involve discovery of what cards are out there, what decks are possible to assemble, what enemies there are and what they do, and everything else the spire has to offer. As you gain in skill and experience, you play it on additional levels and in new ways.</p>\n<p>I highly recommend playing the game, and I highly recommend <i>not </i>learning more or reading further before doing so. Figuring the game out is half the fun.</p>\n<p></p>\n<p>My Mostly-Spoiler-Free Journey Through the Spire</p>\n<p>I started off knowing the basics above, but nothing else. The game was in (earlier) early access, so a bunch of the details were different, but aside from missing the third class (The Defect) the game was largely the same as it is now. </p>\n<p>I played my first few games as The Ironclad. At first things were tough, but a little experience went a long way. My first run ended on the Act I boss. My second run ended on the Act II boss. In my third run, I managed to get all the way through and win. </p>\n<p>That surprised me quite a bit. Rogelike games are supposed to be way harder than that! I put it up to a lot of luck and a lot of deckbuilding game experience, and moved on to the second character class, The Outcast. </p>\n<p>Once again, there was a learning curve, but once again it didn’t seem that hard, and on my second try I got all the way through. I assumed I was fortunate to win so fast, but it seemed powerful things would come my way reasonably often.</p>\n<p>At that point, I stopped playing. What more was there to do? I saw some talk of trying to win *more consistently*, and there was the option to use ‘Ascension’ to make the game harder, but I did not see the appeal in either approach. When The Defect became available, I tried it and won on the third try. So after eight games, I had won with all three classes. It had been fun. At $20 I felt I’d had more than my money’s worth, but I figured that was it. </p>\n<p>Later articles on the wesbite Rock Paper Shotgun, which I use as my main source of computer gaming news, convinced me to give the daily climb a shot. In each daily climb, all players are given the same random seed, which contains the contents of the spire and a bunch of modifications to spice things up. Then you compete for the high score, as determined by whether you made it the whole way but also by how elegantly you did it. You get rewards for killing extra elite monsters, for not taking damage, for building a bigger deck and so forth. With points to maximize, there’s a constant balance between going for more points, strengthening yourself for later on, and not dying. I spent a few weeks playing the daily climb each day, but after a while that too started to feel repetitive, and once again I was ready to move on.</p>\n<p>Then, a few weeks ago, the game released the ending. Five games later I had won with each of the three characters again, and it was time to start gathering keys on my climb to the final boss. On my second try, I reached the fourth and final act… and promptly got completely destroyed. I’d brought a relatively poor deck that was fortunate to get that far, so I tried again. Two games later I was back with a much stronger deck… and I got completely destroyed again. </p>\n<p>Finally, we had a challenge I could get behind. If you came with a relatively normal deck, it was clear you were going to have a bad time. </p>\n<p>Further games were not about the first three acts. The first three acts contained checkpoints, and ways you could die if you got too aggressive, but they were not the point. The point was to win that final fight. A third try did a little better, but was still not close. A fourth had a lot going for it, I thought I had it, and then I had to use one card too many on the last turn, couldn’t find what I needed, and died to exact damage the turn before I was going to win. </p>\n<p> </p>\n<p>Damn.</p>\n<p> </p>\n<p>Several tries later, and after several important lessons learned, the plan came together and the heart died in a barrage of Static Lightning. </p>\n<p> </p>\n<p>Two attempts later, The Outcast too was victorious, thanks to a truly absurd amount of poison damage.</p>\n<p> </p>\n<p>I still haven’t quite won with the Ironclad. I actually should have, but I forgot that the heart had an artifact, chose the wrong attack, and came up exactly two points short on the last turn. The Ironclad has the toughest path, but there are doubtless still ways.</p>\n<p> </p>\n<p>Perhaps I’ll try some games in ascension mode. Interestingly, the first ascension level is more likely to kill you, but arguably makes it <i>easier </i>to kill the heart, since you end up with extra relics.</p>\n<p>What Makes Slay the Spire Work</p>\n<p>The player has all the fun. </p>\n<p>Even when you are first discovering the game, it is easy to understand what is about to happen and why. You get a steady stream of meaningful choices. If you choose wisely, you get to do lots of cool things.</p>\n<p>Slay the Spire’s central innovation is enemy intents. Giving the player all the fun is its genius. Each turn, you can see what each enemy is planning to do – attack you for some amount, defend, use a buff, inflict a status. At first actions other than attacking and blocking can be mysterious, but you still have a general idea of what is happening, and in time you learn the patterns of each enemy and how they tick.</p>\n<p>At first, I thought lack of enemy diversity was a fatal flaw. There were only so many fights, so I would quickly tire of them. Later, I came around to this lack of diversity being actively good. Consider the difference between planning for a wide-open Magic metagame, where you could face anything at all, and planning for a particular metagame with a handful of opponents. Both are interesting in their own way. You get to enjoy both, with a wide open and unknown metagame early on, then a known set of enemies to target later on. </p>\n<p>Slay the Spire offers the same. In your first explorations anything can happen, then later on you are planning for the exact enemies and patterns you will face. Your own deck is constantly changing, it is good, once you have enough experience to use the information, to know exactly what you are up against and must do. That is why the game shows you, at the start of each act, which final boss you will face at the end of that act, to allow you to plan and prepare. Later plays of Slay the Spire are all about having a plan, getting what you need to face down exactly the challenges coming your way, and pushing yourself as far as you can but no farther. In my recent playthroughs, there was be a laser focus on what my deck must do to claim victory in the final fight, knowing exactly the attack patterns and challenges I will face.</p>\n<p>Another huge advantage of Slay the Spire is simplicity. The game could be simpler, but not without sacrifice. Every bit of complexity counts. You draw five cards a turn, you can play three energy worth of cards (most cost one, some zero or two, a few cost more or scale with what you spend), they mostly do damage or stop the enemy from doing damage, and the complexity is added slowly from there by the cards and relics. </p>\n<p>Slay the Spire also lets you do tons of good, powerful things all the time. You start with a basic deck, and every move makes you stronger. Relics give you special abilities and advantages, cards are upgraded at forges, you get a new card after each battle and so on, and there is no attempt whatsoever to balance those cards. The good cards are already a welcome relief compared to the starting cards. The great cards are fantastic. </p>\n<p>You get a mostly random set of relics, and can choose what path to take and which of a few options or cards to take at other junctures. You have enough customization to have a ton of influence over how your deck develops, but you are also at the mercy of events and forced to make the most of what you are offered. Again, there is zero attempt to balance things other than to make them fun, so often you’ll face a choice between the more powerful thing and the thing you actually want. Other times, you’ll be handed a huge gift, and other times still you’ll have no use for the relics and cards you’ll find and even sometimes intentionally pass them up (which you are allowed to do). In one recent playthrough I chose not to take a boss relic from the Act 2 boss, which is a huge kick in the nuts, but that’s the way it goes. Building your deck around the relics you are granted is a huge part of what keeps Slay the Spire interesting and fresh.</p>\n<p>This general idea of ‘give you lots of choices, each a randomized multiple choice’ pays big dividends. You get a choice of three cards, or a dozen things for sale at the merchant, or which of your fifteen cards to upgrade. Random events usually give you two or three choices. The story of the sum of these random choices becomes the story of the climb. So is the general story of figuring out how to get super powerful things out of your deck when you get the chance. </p>\n<p>Hearthstone’s Arena pioneered a similar simplified form of drafting, giving only three choices at a time and not forcing you to adjust to what others around you are doing. It lacks the richness of Magic booster drafts or Artifact drafts, but is much richer and more interesting than it first appears. There is likely much room to enrich such formats while retaining this simple essential nature. Even in Magic booster draft, you still are choosing one from up to fifteen options, so the difference there is mostly in degree – the lack of dynamic opponents is the bigger fundamental distinction. </p>\n<p>Slay the Spire also does a great job giving you lots of goals each climb. If you’re not sure if you will beat the Act 3 boss, that’s the goal. If you know you can’t, you can try to get as high as you can. If you know you’ll beat the Act 3 boss, you can try to score more points, or later to set up for the finale. It’s up to you and I found all the goals and the battles satisfying. More than anything, the game does a great job of making the battles <i>fun, </i>and not giving you too many with any one deck or against one type of opponent, before the game ends.</p>\n<p>Conclusion</p>\n<p>Slay the Spire is highly recommended. It shows how to use simple choices and abilities that combine in unique ways to create varied, interesting and fun puzzles. Its emphasis on letting the player have all the fun, and ensuring there is lots of fun to be had, is even more central than I had previously realized. Slay the Spire offers lots of lessons and innovations that can be used by other games, including multiplayer customizable card games. This is especially true in their limited formats, and for the creation of unique and interesting leagues and special events. </p>\n<p>I have strategic thoughts on the game as well, but have cut them from this review. I may or may not choose to write them out at another time.</p>",
    "user": {
      "username": "Zvi",
      "slug": "zvi",
      "displayName": "Zvi"
    }
  },
  {
    "_id": "8jFfEye4jvjp7XEYd",
    "title": "Instead of using the high-level languages, programmers will start using programming of more high-level or human language level programming?",
    "slug": "instead-of-using-the-high-level-languages-programmers-will",
    "pageUrl": "https://www.lesswrong.com/posts/8jFfEye4jvjp7XEYd/instead-of-using-the-high-level-languages-programmers-will",
    "postedAt": "2018-12-09T18:54:07.626Z",
    "baseScore": -24,
    "voteCount": 9,
    "commentCount": 7,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<html><head></head><body><p>Hi,</p>\n<p>It is written in book Java How to Program ninth edition that instead of using the strings of numbers that computers could directly understand, programmers began using English-like abbreviations to represent elementary\noperations:</p>\n<p>1.5 Machine Languages, Assembly Languages and High-Level Languages</p>\n<p>Programmers write instructions in various programming languages, some directly understandable by computers and others requiring intermediate translation steps. Hundreds of such languages are in use today. These may be divided into three general types:</p>\n<ol>\n<li>Machine languages</li>\n<li>Assembly languages</li>\n<li>High-level languages</li>\n</ol>\n<p>Any computer can directly understand only its own machine language, defined by its hardware design. Machine languages generally consist of strings of numbers (ultimately reduced to 1s and 0s) that instruct computers to perform their most elementary operations one at a time. Machine languages are machine dependent (a particular machine language can be used on only one type of computer). Such languages are cumbersome for humans. For example, here’s a section of an early machine-language program that adds overtime pay to base pay and stores the result in gross pay:</p>\n<p>+1300042774\n+1400593419\n+1200274027</p>\n<p>Programming in machine language was simply too slow and tedious for most programmers. Instead of using the strings of numbers that computers could directly understand, programmers began using English-like abbreviations to represent elementary\noperations. These abbreviations formed the basis of assembly languages. Translator programs called assemblers were developed to convert early assembly-language programs to machine language at computer speeds. The following section of an assembly-language program also adds overtime pay to base pay and stores the result in gross pay:</p>\n<p>load basepay\nadd overpay\nstore grosspay</p>\n<p>Although such code is clearer to humans, it’s incomprehensible to computers until translated to machine language. Computer usage increased rapidly with the advent of assembly languages, but programmers still had to use many instructions to accomplish even the simplest tasks. To speed the programming process, high-level languages were developed in which single statements could be written to accomplish substantial tasks. Translator programs called compilers convert high-level language programs into machine language. High-level languages allow you to write instructions that look almost like everyday English and contain commonly used mathematical notations. A payroll program written in a high-level language might contain a single statement such as</p>\n<p>grossPay = basePay + overTimePay</p>\n<p>Instead of using the high-level languages, programmers will start using programming of more high-level or human language level programming?</p>\n</body></html>",
    "user": {
      "username": "manhobby",
      "slug": "manhobby",
      "displayName": "manhobby"
    }
  },
  {
    "_id": "ha7Mxxmbx9QXKrrvB",
    "title": "  Kindergarten in NYC: Much More than You Wanted to Know",
    "slug": "kindergarten-in-nyc-much-more-than-you-wanted-to-know",
    "pageUrl": "https://www.lesswrong.com/posts/ha7Mxxmbx9QXKrrvB/kindergarten-in-nyc-much-more-than-you-wanted-to-know",
    "postedAt": "2018-12-09T15:36:37.441Z",
    "baseScore": 36,
    "voteCount": 18,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>  <strong>Kindergarten in NYC: Much More than You Wanted to Know</strong></p><p>My son is turning five next year, which means one of the most important transitions in his childhood and potentially his life: starting Kindergarten. I always thought New York City moms who obsessed over this were clearly crazy.</p><p>Now I am one of those moms. </p><p>Why do we do this to ourselves? It’s not the one year of kindergarten. It’s securing that spot in the school where you want them to stay until middle school and potentially high school, and probably send your other kids to as well. It’s all of the social and class insecurities that come with choosing a school and its associated peer group. It’s the fear that if you choose poorly, your child will age 100 years and his face will melt off in front of you.</p><p>Not quite that severe. Still, you worry you’ll mess up their life and they’ll become drug addled sociopaths living on your couch until you kick them out when they bring back that prostitute.</p><p>Maybe going overboard again. They’ll go to State College, move to the suburbs, and work in retail.</p><p>Wo wo wo, lets not be unrealistic.  Retail won&#x27;t be around in 10 years. Your kid will be horribly miserable for the next 14 years, go through depressive episodes, and blame you for all of it. That’s what I’m actually worried about. Both my husband and I had horrible elementary school experiences. We still carry scars. We don’t want that for our sons.</p><p>So why not home school? All the cool kids are doing it. We have personal reasons why this would not work for our family. Our son has some social deficits, but is extremely bright. Literally everyone we’ve spoken to who knows our son agrees that he would do better in a structured environment with peers. We have observed his profound social-emotional growth upon starting the school year. We saw back-sliding over the summer when he lacked structure or regular peer interactions. He will not listen to us when we teach him. He is a different child in the school setting, soaking up knowledge. </p><p>People can rant all they like about how horrible school is philosophically, but that does not negate what we’ve personally witnessed in our own child. Philosophy aside, home-schooling is a lot of work and coordination. We both work full-time. While we would pick home-school over the horrid elementary school experiences we had, we hope we can do better and find a school where he will be happy.</p><p>That is much easier said than done. Especially for unique children. Our son has done well in a private preschool with 15 children and 3 teachers. A public kindergarten in NYC has a class of 26 children and one teacher. This goes up to as high as 32 in first grade. That is a lot of kids in a small space. It presents two options. Either you get a very noisy and unruly class, or a strictly controlled group which conforms precisely with everyone sitting quietly and doing the same thing at the same time. We have seen both. Neither is pretty. Our son has sensory issues, and will not tolerate a very noisy classroom. We expect he also would not tolerate a conformist one. Him tolerating it would scare us even more.</p><p>If he went to public school, we might well be pressured to put him into a resource room, with children much worse off than himself. Children with emotional disturbance, severe autism, retardation and other severe problems. My mother has worked in such classrooms and what she describes is unacceptable. Those are her stories to tell, but I would not put him there. Ever. </p><p>So what can we do? Sue the city! That’s what everyone told us to do. Say the public schools can’t meet your kid’s needs, since they clearly cannot do so. Find a nice, private special needs school, and sue for tuition. </p><p>So we saw some special needs schools. Like public schools, they varied a fair bit and we liked some more than others. What they all had in common was a severely impaired peer group. He would be one of the most functional students in the class. We don’t want that for him. We want him to be challenged and learn from peers who can be models for him.</p><p>So what next? Private school! Private schools also vary a lot, but have one thing in common. They are expensive. </p><p>I’m not sure you understand how bad this situation is. I spent time looking around. The average private elementary school charges about $45,000 per year.</p><p>Yup. You saw that right, $45,000. That’s more than most students&#x27; college tuition. Before aid or loans. And it’s post-tax income. And we have more than one child.</p><p>With two (and perhaps more) children, that would be most if not all of my post-tax income as a psychiatrist. </p><p>People have the audacity to say “But you can afford it.” Don’t get Zvi started on that phrase. </p><p>Even if you want to send your kid to private school, you have to apply and be accepted. Most good private schools are selective. Most do not want to deal with a child with special needs.</p><p>We have been lucky to find one nearby private school that charges considerably less (though still far from cheap) and happens to have an educational philosophy we think would suit our son. It’s a Waldorf school. It emphasizes practical skills such as cooking, gardening, carpentry, foreign language, and trade. Since we believe our son is gifted academically, being less academic does not concern us. He will learn that stuff at home whether we want him to or not. Thus, we wait with baited breath for his trial period there to see if they’ll accept him. We don’t have a back-up option that comes close at present.</p><p>What’s been really interesting to me through this process is how vastly schools differ from each other. Often people speak about ‘school’ as if it is one thing. Either you agree with sending kids to ‘school’ or you don’t. This is not the case. One reason New York City moms go berserk over this is that there are *vast* differences between schools even a few blocks away from each other. Within the public schools, class is everything. Most children go to their ‘zoned&#x27; school, and so people will pay higher rents near the ‘good’ schools to get their kids in. One of the public schools we saw looked and felt like a prison, had no music or art program, and only let the kids outside for 20 minutes a day. Another 10 blocks north in the neighboring district collected $500K/yr from the PTA and had full music and art programs, book fairs, a large library, and extra in-classroom assistants. </p><p>We live in a district which has weird rules about admissions. Instead of having a zoned school, you make a rank-list of schools in the district and apply to all of them. In an attempt to ingrate the schools more, the city has imposed rules about who can be admitted by class. The schools are required to accept 67% of ‘diversity’ applicants who qualify either for low income, English as second language, or living in shelters (i.e. homeless). There is a lot of evidence supporting that peer group is a major factor in child development and life outcome. Political incorrectness aside, this is not a wonderful peer group. It also far reduces the chances that your child will get into the particular school you want them to go to. Since priority is first given to siblings, the ‘nice’ school in this district (that we would have previously been zoned for) now only has four ‘non-diversity’ spots open for admission this year. Even if we were willing to send him there, he probably wouldn’t get in. Because of this, many better-off families are moving out of the district entirely. This is reflected in the rents within our community – rent jumps considerably right at the district line. People respond to incentives. If we sent our kids to public school we would be forced to do the same. If you have any money at all, you go to the district where the PTA funds the nice art program, not the one with the metal detector in the lobby.</p><p>Going private for education hopefully means you avoid true disaster, and the peer group is relatively wealthy and educated. But even private schools differ vastly in their philosophy towards education. Some are super academic, drilling kids to get high SAT scores and become doctors and lawyers. Some are more laid back. Some hardly seem to teach anything at all. There are small schools with one class per grade, others that are much larger. Religious and secular schools. Science schools and arts schools. If you’re willing to pay for it odds are there is some school that you would like. That’s a big if though.</p><p>My practical advice:  If your only option is public school, move to an area that has a nice school at least one full school year before you intend to apply. You can tour schools just by saying you have a kid in the district, and they don’t force you to prove it. Once you find a school you like, you can move to that school’s zone, and you will have a high chance of admission. To be safe, you should make sure there are 1-2 back up schools you find acceptable in the district. If you cannot afford to live any places with reasonable public schools, you should seriously consider leaving the city. I am told of reasonable schools in NJ… <br/> </p><p>If you can’t stand public school, because at the end of the day they all follow common core, take those tests, and have 32 kids in a class, then you have to consider what you can afford. Home school has no tuition, but will require all-day child care, any educational materials/classes you want to use, and a large coordination effort on your part. If you’re a stay at home parent this might appeal to you anyway.  For the most part the people who choose to do it are happy with it. </p><p>Private school is expensive, but requires less advance planning, since they don’t care what district you’re in as long as you can pay. You might still need to consider moving for private school if you don’t want your child to have an infinitely long commute. The city will pay for busing to private schools for bus routes which are 0.25 – 2.0 miles. Keep in mind that they are measuring distance along bus routes and not geographically. Even if you are physically within 2 miles of the school, the bus route might be over 2 miles and you will be out of luck. To be fair, if you’re willing to spend $50,000/year on a school, then what’s another $40/day to hire someone to take them to school?</p><p>I am now going to write some school reviews. I will leave out specific names, but if you are interested you can message me privately, and I will let you know which is which. Zvi saw some schools I did not, which I haven’t written about, and we still have some tours planned at local public schools.</p><p><strong>Public Schools:</strong></p><p><strong>District 1</strong> (our district – the one with the integration)</p><p><strong>Public School A:</strong></p><p>I was pleasantly surprised by this school’s philosophy of education. They were laid back and progressive. Kids sit at tables instead of desks. Group conversations and creative expression was encouraged. No mandatory homework. Starting in 1st grade, kids learn chess and have the opportunity in 3rd and 4th grade to compete in tournaments. In 3rd grade the kids learn basic computer programming. There is a year of free music lessons. They have a theater and a roof-top garden. Gym is non-competitive until 4th grade. 45 minutes of daily outdoor time. I really liked everything they *said* and the principal was super cool. However, the actual classrooms were tiny and crammed full of students. It was loud. I felt claustrophobic there, and I don’t have sensory issues in general. Plus, the district just implemented the diversity criteria this year, so the students I was seeing are not the peer group my son is going to have if he went. And, of course, they only have four non-sibling, non-diversity spots available. </p><p><strong>Public School B:</strong></p><p>This place is a prison. There is an angry security guard at the entrance to the grime-encrusted orange walls. Multiple signs above the guard state ‘theft is a crime.’ The slit-like windows at the top of the rooms let in thin beams of daylight to an otherwise flickering-fluorescent landscape. This is hell. There is no music or art program – no room in the budget. So ‘we do that within our lessons’. 20 minutes of yard time a day. Everything is centered around standardized tests. The only white faces were part of a special program. No one with any choice would ever let their kid set foot in this place unless they were in the special program. Not worth it. It’s social control of minorities. Straight up. If SJWs want a cause, here’s one for you. And no, forcing white or wealthy children to go there is not going to work. They won’t.</p><p><strong>District 2 (the nice one)</strong></p><p><strong>Public School C:</strong></p><p>The platonic ideal of school. When you think school, you think this school. The people who designed it thought ‘what is school?’ and then based the design off of every trope and meme about school, ever. Charts of everything on the walls. ‘Task leaders.’ Bulletin boards. Window decals. Those weird cartoon people you only see in school ever. Worksheets, worksheets, worksheets. Chalk boards. White boards. This place has it all! The place felt nice. Larger rooms, more light. Nice enrichment activities. A music and art program. A nice library and computer lab. Several outdoor spaces and playground equipment. The place gets $500k/yr from the PTA to keep the place great. Mostly white faces sitting quietly in circles while the teacher spoke to them in exaggerated tones with big faces while pointing to a white board. </p><p>Looked like the children of the corn. Completely conformist. But conformists at least a year ahead academically. It is disturbing to see kindergarteners completing reading worksheets and pushing papers around, but they were able to do it. This is the place for upper-middle class white people who move into the ‘good’ part of the neighborhood.</p><p><strong>Private Schools: </strong></p><p><strong>Private School A:</strong> Preparatory School</p><p>EXPENSIVE. Beautiful school and facility. It is a ‘Quaker’ school, but mostly secular. Has a beautiful chapel where kids have ‘community assembly and quiet time’ once a week. Other parents were very well dressed – a lot of suits and jewelry. Academically rigorous without being oppressively conformist. Perhaps because the class size is 20 instead of 30, so there is more room to maneuver. A fine school as schools go, but not that much of an upgrade from PSC given the price. Also difficult to get into and unwilling to accommodate special needs.</p><p><strong>Private School B:</strong> Jewish School</p><p>I loved this school! I really did. It’s a progressive, laid-back atmosphere that is still academically oriented. It is very Jewish. The boys wear keepas and the curriculum is fully bilingual with one teacher speaking English and the other speaking Hebrew. They have all the usual stuff such as music and art. They go outside for 1 hr/day. They are willing to work with special needs. They know how to work with gifted and talented kids and make special assignments for children who are ahead. LOVE IT. Problem was, it is about 1 hour away by bus and it’s a 7.5 hr day. Not doing that to my kid. Not willing to move close enough to make it work. At least not this coming year.</p><p><strong>Private School C:</strong> Waldorf School</p><p>This is a very unique nearby school that happens to be less expensive than the others. It has a unique education philosophy (a Waldorf school) which emphasizes embodiment and practical skills over academic ones. The curriculum includes foreign languages, cooking, washing, gardening, carpentry, and trade. The kindergarten is entirely non-academic and includes copious time for free play and an hour of outdoor activity. The later grades teach traditional academics, but do so in somewhat unusual ways, which I don’t have a strong opinion on at present. Since the main reason we are sending our son to school is for socialization, and since he’s already brilliant, I’m less worried about academics, especially in the younger grades. The school requested a drastic reduction in our child’s screen time, which at first freaked me out (who are they to tell me what to do in my own home), but I kind of understand. It’s a very small school (only 1 class per grade) and they are currently considering whether or not they can accommodate his needs. This is our top choice at present.</p><p><strong>Special Needs Schools:</strong></p><p><strong>SNS A:</strong> Social Justice Away!</p><p>This school is an ‘integrated’ private school – meaning it’s a private school for regular kids which also accepts children with learning disabilities and has services for them. This means you can get the tuition paid by the city, unlike regular private schools, with a relatively normal peer group. It’s a great idea. The school itself is beautiful and has All The Things.</p><p>However there is a catch. The school has an agenda. It’s a social justice school. In the sense that other schools are reading and math schools. They call themselves ‘Advocates for Social Justice’ in their opening lines. I wouldn’t have thought this mattered for elementary age children. Sure, loving each other is wonderful! Accepting your neighbors is wonderful! But this is not where they draw the line. Social Justice is taught in every aspect of the curriculum. There are 7 year olds discussing their ‘identities’, an 8 year old talking about how his hero is Colin Kaepernick, that guy who keeled for the national anthem. The teachers then praise his &#x27;activism&#x27; for writing about it. The other sample lesson is on how Christopher Columbus was a white colonialist oppressor. And the children absorb this. The school is accepts all kinds – unless you happen to be a *gasp* Republican. No diversity of thinking. If you don’t fully swallow the SJW philosophy in all its forms, or don&#x27;t want them forced down your child&#x27;s throat, this is not the place for you.</p><p><strong>SNS B:</strong> Soothing Gardens…</p><p>Beautiful place. Therapeutic environment. Has the things. Didn’t want us to see the children – which was strange. When we peaked in at them, they were, well, very special. Seems like a great place for very special kids. If I have one that needed all that, I’d consider sending him there. </p><p><strong>SNS C:</strong> Jews with learning problems</p><p>While not specifically a Jewish school, there were clearly a lot of Jewish children and teachers. I actually liked this place a lot. It was very laid back and gave the kids a lot of lee-way to be who they are. It didn’t feel at all oppressive. They group kids into separate reading and math groups not by age, but by reading and math level, which I liked. The kids seemed less special than at SNS B, but still clearly special. The school didn’t have its own outdoor space and so kids only go outside twice week with a bunch of parent-volunteers, since they want one adult per kid when crossing the streets. What was particularly disappointing was that they were clearly quite academically behind. The classes were so laid back that there didn’t seem to be a challenge, and the teachers were fine with whatever they produced. I can imagine certain children this would be very good for. I have vastly higher hopes for our son.</p>",
    "user": {
      "username": "Lara_Foster",
      "slug": "laura_b",
      "displayName": "Laura B"
    }
  },
  {
    "_id": "oCPPAoTFFHk9SGfrc",
    "title": "New Ratfic: Nyssa in the Realm of Possibility",
    "slug": "new-ratfic-nyssa-in-the-realm-of-possibility",
    "pageUrl": "https://www.lesswrong.com/posts/oCPPAoTFFHk9SGfrc/new-ratfic-nyssa-in-the-realm-of-possibility",
    "postedAt": "2018-12-09T05:00:41.749Z",
    "baseScore": 40,
    "voteCount": 14,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>For NaNoWriMo, I decided to do a rationality themed pastiche of the Phantom Tollbooth.  It is complete and serializing at <a href=\"http://nyssa.elcenia.com\">http://nyssa.elcenia.com</a> on Saturdays and Wednesdays.  There are three chapters up as of this posting.</p>",
    "user": {
      "username": "Alicorn",
      "slug": "alicorn",
      "displayName": "Alicorn"
    }
  },
  {
    "_id": "jiWSMP3dknkpXEmfQ",
    "title": "Near-term Worse than Existential Threats",
    "slug": "near-term-worse-than-existential-threats",
    "pageUrl": "https://www.lesswrong.com/posts/jiWSMP3dknkpXEmfQ/near-term-worse-than-existential-threats",
    "postedAt": "2018-12-09T03:10:31.297Z",
    "baseScore": -15,
    "voteCount": 7,
    "commentCount": 3,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>How does Rationalism as a program square itself with the empirical reality that religious cognition is significantly inheritable, that religious people are out-breeding rationalists, that every large-scale improvement in technology increases the accessibility of that technology, and that many religious people think rationalists should be tortured indefinitely in fire?  Given the possibility of worse-than-existential threat from human-value disalignment, why are you comparatively more worried about paperclips?</p>",
    "user": {
      "username": "Alephywr",
      "slug": "alephywr",
      "displayName": "Alephywr"
    }
  },
  {
    "_id": "L3Zvpad7mjHv4ydR2",
    "title": "What precisely do we mean by AI alignment?",
    "slug": "what-precisely-do-we-mean-by-ai-alignment",
    "pageUrl": "https://www.lesswrong.com/posts/L3Zvpad7mjHv4ydR2/what-precisely-do-we-mean-by-ai-alignment",
    "postedAt": "2018-12-09T02:23:28.809Z",
    "baseScore": 30,
    "voteCount": 10,
    "commentCount": 8,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>We sometimes phrase AI alignment as the problem of aligning the behavior or values of AI with what humanity wants or humanity&#x27;s values or humanity&#x27;s intent, but this leaves open the questions of just what precisely it means for an AI to be &quot;aligned&quot; with just what precisely we mean by &quot;wants,&quot; &quot;values,&quot; or &quot;intent&quot;. So when we say we want to build aligned AI, what precisely do we mean to accomplish beyond vaguely building an AI that does-what-I-mean-not-what-I-say?</p>",
    "user": {
      "username": "gworley",
      "slug": "gordon-seidoh-worley",
      "displayName": "Gordon Seidoh Worley"
    }
  },
  {
    "_id": "WeyBovJq4qny6aM3o",
    "title": "LW Update 2018-12-06 – All Posts Page, Questions Page, Posts Item rework",
    "slug": "lw-update-2018-12-06-all-posts-page-questions-page-posts",
    "pageUrl": "https://www.lesswrong.com/posts/WeyBovJq4qny6aM3o/lw-update-2018-12-06-all-posts-page-questions-page-posts",
    "postedAt": "2018-12-08T21:30:13.874Z",
    "baseScore": 18,
    "voteCount": 3,
    "commentCount": 1,
    "meta": true,
    "question": false,
    "url": null,
    "htmlBody": "<p>First, an FYI – the LessWrong Team will be starting various holiday trips fairly soon. We&#x27;ll probably still be doing some work and answering intercom, but may be more delayed than usual over the next few weeks.</p><p>But, we also have a few more updates:</p><ul><li>There is now a <a href=\"https://www.lesswrong.com/questions\">questions page</a>, if you&#x27;d like to just view the latest question posts. (this uses a new backend view called &quot;questions&quot; for those interested in playing around with the API)</li><li>There is now a bare-bones <a href=\"https://www.lesswrong.com/allPosts\">All Posts page</a>. We&#x27;ll probably give it more features (i.e. sorting, filtering) at some point. For now it just loads the most recent 100 posts in chronological order. It doesn&#x27;t have all the anti-spam filters that we&#x27;ve added to other sections yet, so, um, watch out for that I guess.</li><li>Post Items have been reworked a bit so it&#x27;s easier to skim them for karma (while still trying to avoid making karma <em>too</em> visible). Also added some tooltips for vote-counts, alignment forum and word-count.</li></ul>",
    "user": {
      "username": "Raemon",
      "slug": "raemon",
      "displayName": "Raemon"
    }
  },
  {
    "_id": "LJiGhpq8w4Badr5KJ",
    "title": "GraphQL tutorial for LessWrong and Effective Altruism Forum",
    "slug": "graphql-tutorial-for-lesswrong-and-effective-altruism-forum",
    "pageUrl": "https://www.lesswrong.com/posts/LJiGhpq8w4Badr5KJ/graphql-tutorial-for-lesswrong-and-effective-altruism-forum",
    "postedAt": "2018-12-08T19:51:59.514Z",
    "baseScore": 95,
    "voteCount": 30,
    "commentCount": 5,
    "meta": true,
    "question": false,
    "url": null,
    "htmlBody": "<html><head><style type=\"text/css\">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></head><body><p>This post is a tutorial on using GraphQL to query for information about\nLessWrong and the Effective Altruism Forum. It's mostly intended for people who\nhave wanted to explore LW/EA Forum data but have found GraphQL intimidating\n(this was the case for myself until several weeks ago).</p>\n<h1>General steps for writing a query</h1>\n<p>(This section will make more sense if you have seen some example queries; see\nnext section.)</p>\n<p>For the queries that I know how to do, here is the general outline of steps:</p>\n<ol>\n<li>\n<p>Go to <a href=\"https://www.lesswrong.com/graphiql\">https://www.lesswrong.com/graphiql</a> or\n<a href=\"https://forum.effectivealtruism.org/graphiql\">https://forum.effectivealtruism.org/graphiql</a> depending on which forum you\nwant to query data for.</p>\n</li>\n<li>\n<p>Figure out what the output type should be (e.g. <code>comments</code>, <code>comment</code>,\n<code>posts</code>, <code>post</code>).</p>\n</li>\n<li>\n<p>Type <code>{output_type(input)}</code> into GraphiQL and hover over <code>input</code>.</p>\n<p>Here is what it looks like for the <code>comment</code> output type:</p>\n<p><a href=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comment-input-hover.png\"><img src=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comment-input-hover.png\" alt=\"\"></a></p>\n<p>Here is what it looks like for the <code>comments</code> output type:</p>\n<p><a href=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comments-input-hover.png\"><img src=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comments-input-hover.png\" alt=\"\"></a></p>\n</li>\n<li>\n<p>Click\non the type that appears after <code>input</code> (e.g. <code>MultiCommentInput</code>, <code>SingleCommentInput</code>).\nA column on the right should appear (if it was not there already).\nDepending on the fields listed in that column, there will now be two ways to proceed.\n(Generally, it seems like singular output types (e.g. <code>comment</code>) will have\n<code>selector</code> and plural output types (e.g. <code>comments</code>) will have <code>terms</code>.)</p>\n<p>Here is what it looks like for the <code>comment</code> output type. In the image, I have\nalready clicked on <code>SingleCommentInput</code> so you can see <code>selector</code> under the\ndocumentation (rightmost) column.</p>\n<p><a href=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comment-SingleCommentInput.png\"><img src=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comment-SingleCommentInput.png\" alt=\"\"></a></p>\n<p>Here is what it looks like for the <code>comments</code> output type. Again, in this image,\nI have already clicked on <code>MultiCommentInput</code> so you can see <code>terms</code> under the\ndocumentation (rightmost) column.</p>\n<p><a href=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comments-MultiCommentInput.png\"><img src=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comments-MultiCommentInput.png\" alt=\"\"></a></p>\n<p>In the fields listed, if there is <code>selector</code> (e.g. for <code>comment</code>):</p>\n<ul>\n<li>\n<p>Click on the selector type (e.g. <code>CommentSelectorUniqueInput</code>). Use one of\nthe fields (e.g. <code>_id</code>) to pick out the specific item you want.</p>\n<p>Here is what you should click on:</p>\n<p><a href=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comment-CommentSelectorUniqueInput.png\"><img src=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comment-CommentSelectorUniqueInput.png\" alt=\"\"></a></p>\n<p>What it looks like after you have clicked:</p>\n<p><a href=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comment-fields.png\"><img src=\"https://raw.githubusercontent.com/riceissa/ea-forum-reader/master/tutorial/comment-fields.png\" alt=\"\"></a></p>\n</li>\n</ul>\n<p>If there is <code>terms</code> (e.g. <code>comments</code>):</p>\n<ul>\n<li>\n<p>Go to the\n<a href=\"https://github.com/LessWrong2/Lesswrong2/tree/devel/packages/lesswrong/lib/collections\">collections</a>\ndirectory in the LessWrong 2.0 codebase, and find the <code>views.js</code> file for\nyour output type. For example, if your output type is <code>comments</code>, then the\ncorresponding <code>views.js</code> file is located at\n<a href=\"https://github.com/LessWrong2/Lesswrong2/blob/devel/packages/lesswrong/lib/collections/comments/views.js\"><code>collections/comments/views.js</code></a>.</p>\n</li>\n<li>\n<p>Look through the various \"views\" in the <code>views.js</code> file to see if there is\na relevant view. (There is also a default view if you don't select any\nview.) The main things to pay attention to are the <code>selector</code> block (which\ncontrols how the results will be filtered) and the <code>options</code> block (which\nmainly controls how the results are sorted).</p>\n</li>\n<li>\n<p>Pass in parameters for that view using keys in the <code>terms</code> block.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Start a <code>results</code> block, and select the fields you want to see for this output type.\n(If you don't select any fields, it will default to all fields, so you can\ndo that once and delete the fields you don't need.)</p>\n</li>\n</ol>\n<h1>Examples</h1>\n<p>I've built a sample interface for both LessWrong and EA Forum that allows an\neasy way to access the queries used to generate pages:</p>\n<ul>\n<li><a href=\"https://lw2.issarice.com/\">https://lw2.issarice.com/</a></li>\n<li><a href=\"https://eaforum.issarice.com/\">https://eaforum.issarice.com/</a></li>\n</ul>\n<p>By passing <code>format=queries</code> in the URL to any page, you can view the GraphQL\nqueries that were made to generate that page. Rather than showing many examples\nin this post, I will just show one example in this post, and let you explore\nthe reader.</p>\n<p>As an example, consider the page\n<a href=\"https://eaforum.issarice.com/?view=top\">https://eaforum.issarice.com/?view=top</a>.\nClicking on \"Queries\" at the top of the page takes you to the page\n<a href=\"https://eaforum.issarice.com/?view=top&amp;offset=0&amp;before=&amp;after=&amp;format=queries\">https://eaforum.issarice.com/?view=top&amp;offset=0&amp;before=&amp;after=&amp;format=queries</a>\nHere you will see the following:</p>\n<pre><code>    {\n      posts(input: {\n        terms: {\n          view: \"top\"\n          limit: 50\n          meta: null  # this seems to get both meta and non-meta posts\n\n\n\n        }\n      }) {\n        results {\n          _id\n          title\n          slug\n          pageUrl\n          postedAt\n          baseScore\n          voteCount\n          commentsCount\n          meta\n          question\n          url\n          user {\n            username\n            slug\n          }\n        }\n      }\n    }\n\nRun this query\n\n\n\n    {\n      comments(input: {\n        terms: {\n          view: \"recentComments\"\n          limit: 10\n        }\n      }) {\n        results {\n          _id\n          post {\n            _id\n            title\n            slug\n          }\n          user {\n            _id\n            slug\n          }\n          plaintextExcerpt\n          htmlHighlight\n          postId\n          pageUrl\n        }\n      }\n    }\n\nRun this query\n</code></pre>\n<p>Clicking on \"Run this query\" (not linked in this tutorial, but linked in the\nactual page) below each query will take you to the GraphiQL page with the query\npreloaded. There, you can click on the \"Execute Query\" button (which looks like\na play button) to actually run the query and see the result.</p>\n<p>I should note that my reader implementation is optimized for my own (probably\nunusual) consumption and learning. For article-reading and commenting purposes\n(i.e. not for learning how to use GraphQL), most users will probably prefer to\nuse the official versions of the forums or the GreaterWrong counterparts.</p>\n<h1>Tips</h1>\n<ul>\n<li>In GraphiQL, hovering over some words like <code>input</code> and <code>results</code> and then\nclicking on the resulting tooltip will show the parameters that can be passed\nto that block.</li>\n<li>Forum search is <em>not</em> done via GraphQL. Rather, a separate API (the Algolia\nsearch API) is used. Use of the search API is outside the scope of this\ntutorial. This is also why the search results page on my reader\n(<a href=\"https://eaforum.issarice.com/search.php?q=hpmor\">example</a>) has no \"Queries\"\nlink (<a href=\"https://github.com/riceissa/ea-forum-reader/issues/8\">for now</a>).</li>\n<li>For queries that use a <code>terms</code> block: even though a \"view\" is just a\n<a href=\"http://docs.vulcanjs.org/terms-parameters.html\">shorthand</a> for a\nselector/options pair, it is not possible to pass in arbitrary\nselector/options pairs (due to the way security is handled by Vulcan).\nIf you don't use a view, the default view is selected.\nThe main consequence of this is that you won't be able to make some queries\nthat you might want to make.</li>\n<li>Some queries are hard/impossible to do. Examples: (1) getting comments of a\nuser by placing conditions on the parent comment or post (e.g. finding all\ncomments by user 1 where they are replying to user 2); (2) querying and\nsorting posts by a function of arbitrary fields (e.g. as a function of\n<code>baseScore</code> and <code>voteCount</code>); (3) finding the highest-karma users looking\nonly at the past <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"N\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;\">N</span></span></span></span></span></span> days of activity.</li>\n<li>GraphQL vs GraphiQL: <code>/graphiql</code> seems to be endpoint for the interactive\nexplorer for GraphQL, whereas <code>/graphql</code> is the endpoint for the actual API.\nSo when you are actually querying the API (via a program you write) I think\nyou want to be using <a href=\"https://www.lesswrong.com/graphql\">https://www.lesswrong.com/graphql</a> and\n<a href=\"https://forum.effectivealtruism.org/graphql\">https://forum.effectivealtruism.org/graphql</a> (or at least, that is what I am\ndoing and it works).</li>\n</ul>\n<h1>Acknowledgments</h1>\n<p>Thanks to:</p>\n<ul>\n<li>Louis Francini for helping me with some GraphQL queries and for feedback on\nthe post and the reader.</li>\n<li>Oliver Habryka for answering some questions I had about GraphQL.</li>\n<li>Vipul Naik for funding my work on this post and some of my work on the\nreader.</li>\n</ul>\n</body></html>",
    "user": {
      "username": "riceissa",
      "slug": "riceissa",
      "displayName": "riceissa"
    }
  },
  {
    "_id": "j2mcSRxhjRyhyLJEs",
    "title": "What is \"Social Reality?\"",
    "slug": "what-is-social-reality",
    "pageUrl": "https://www.lesswrong.com/posts/j2mcSRxhjRyhyLJEs/what-is-social-reality",
    "postedAt": "2018-12-08T17:41:33.775Z",
    "baseScore": 38,
    "voteCount": 10,
    "commentCount": 17,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>Eliezer&#x27;s sequences <a href=\"https://www.lesswrong.com/posts/CEGnJBHmkcwPTysb7/lonely-dissent\">touch upon this concept</a> but I&#x27;m not sure they actually use the phrase. Much of my understanding of it came from in-person conversations. Various comments and posts have discussed it but to my knowledge there isn&#x27;t a clear online writeup.</p>",
    "user": {
      "username": "Raemon",
      "slug": "raemon",
      "displayName": "Raemon"
    }
  },
  {
    "_id": "k286sEwyuY7SiQjcs",
    "title": "Prediction Markets Are About Being Right",
    "slug": "prediction-markets-are-about-being-right",
    "pageUrl": "https://www.lesswrong.com/posts/k286sEwyuY7SiQjcs/prediction-markets-are-about-being-right",
    "postedAt": "2018-12-08T14:00:00.281Z",
    "baseScore": 83,
    "voteCount": 29,
    "commentCount": 7,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Response To (Marginal Revolution): <a href=\"https://marginalrevolution.com/marginalrevolution/2018/12/love-prediction-markets-love-art-world.html\">If you love prediction markets you should love the art world.</a></p>\n<p>Previously on prediction markets: <a href=\"https://thezvi.wordpress.com/2018/07/26/prediction-markets-when-do-they-work/\">Prediction Markets: When Do They Work?</a>, <a href=\"https://thezvi.wordpress.com/2018/08/17/subsidizing-prediction-markets/\">Subsidizing Prediction Markets</a></p>\n<p>I’ll quote the original in full, as it is short, and I found it interestingly and importantly wrong. By asking the question of <em>why </em>this perspective is wrong, we see what is so special about prediction markets versus other markets.</p>\n<blockquote><p>Think of art markets, and art collecting, as an ongoing debate over what is beautiful and also what is culturally important.  But unlike most debates, you have a very direct chance to “put your money where your mouth is,” namely by buying art (it is very difficult to sell art short, however).  In this regard, debates over artistic value may be among the most efficient debates in the world.  At least if you are persuaded by the basic virtues of prediction markets.  The prices of various art works really do aggregate information about their perceived values.</p>\n<p>I have, however, noted a correlation, how necessary or contingent I am not sure.  The “white male nerd types” who are enamored of prediction markets tend to be especially skeptical of the market judgments of particular art works, most of all for conceptual and contemporary art.</p>\n<p>In my view, discussions about the value of art, as they occur in the off-the-record, proprietary sphere, are indeed of high value and they deserve to be studied more closely.  Imagine a bunch of people competing to make “objects that are interesting but not interesting for reasons related to their practical value.”  And then we debate who has succeeded, or not.  And those debates reflect many broader social, political, and economic issues.  And it is all done with very real money on the line.  The money concerns not just the value of individual art works, but also the prestige and social capital value that arises from having assembled a prestigious and insightful collection.</p></blockquote>\n<p>That’s <em>exactly</em> why (almost) everyone who loves prediction markets hates the high-end, expensive art markets, even if they love art and artists and buy original paintings to hang on their walls. This goes beyond ‘skepticism of the market judgments.’ Expensive art markets are not fundamentally markets. They are fundamentally a political status game.</p>\n<p></p>\n<p>Consider three (non-exhaustive) types of markets: Consumption markets, commercial markets and prediction markets.</p>\n<p>Consumption markets are where the buyer is buying the item in order to use it.</p>\n<p>The buyer who pays more than necessary is sad in one sense, and the one who got the best deal is happy in that sense. But that sense isn’t the important one for the buyer. If you are ‘right’ it is because you indeed got good use of the item that justified the purchase. If you are ‘wrong’ it is because you didn’t.</p>\n<p>Thus, we can point to a ‘naive’ participant who doesn’t ‘play the game’ of that market, and say ‘look how much they could have saved’, or did ‘save’, but that doesn’t actually impact them.</p>\n<p>Liquid commercial markets are where the buyer plans to sell the item to someone else.</p>\n<p>Middle men, arbitrage, investment, greater fools, that sort of thing. Buy low, sell high.</p>\n<p>If you buy a stock, or a commodity piece of art, or inventory for your store, or a cryptocurrency, and others want to buy it for more, it goes up in price and you make money. If they want to sell it for less, it goes down in price and you lose money.</p>\n<p>The buyer who pays more than necessary is sad, and loses money, in the only sense that matters. If the price goes down, that too makes the buyer sad. Paying a locally good price, or having the price go up, makes the buyer happy. The key is to buy before others buy, so they drive the price up.</p>\n<p>You might reply, no, the bigger key is to buy what is cheap and sell what is expensive, based on fundamentals, and that will bear out over time.</p>\n<p>Well, maybe.</p>\n<p>Yes, often buyers and sellers are driven by fundamentals. But in an important sense, that is a coincidence. What is actually good news is often considered bad news, and vice versa. Prices are often largely driven by who is thinking about what and the emotional state and financial needs of participants. The market can stay insane longer than you can stay solvent. The people who say such non-fundamental movements are random, are mostly saying they aren’t good enough to understand and predict them in this case.</p>\n<p>Yes, <em>eventually </em>fundamentals <em>might</em> take over.  Or they might not. Low prices cause damage or make items impossible to justify storing or stocking. High prices trigger media attention and create opportunity. Low prices trigger margin calls, gets the company bought out or its employees and partners to quit. High prices trigger short squeezes and make everyone want to work with and for you. And so on. Momentum trading <em>works, </em>damn it (like everything else on this blog, not investment advice!).</p>\n<p>Ideally the commercial market is anchored by connection to a consumption market – someone wants the goods, or is willing to collect the profits from the stock, or what not. The stronger that anchor versus speculative factors, the more accurate the prices.</p>\n<p>Prediction markets have elements of both.</p>\n<p>Prediction market traders can choose to mostly act like traders. If you think that others will think that the Patriots will win next week, you can bet on the Patriots now and then bet against them later when the odds change, and make money. You can be a market maker, or a block trader, or any other traditional market role.</p>\n<p>In doing this, a trader cares about <em>future social reality. </em>They are people predicting what others will, in the future, predict that others will predict that others will predict, and so on. World events can help or hurt them, as they change perception, but they care about that perception and not the reality. By the time reality sets in, who knows what positions the trader will have?</p>\n<p>In prediction markets there is another option. You can care about <em>future reality. </em>The market predicts a future outcome, and importantly <em>you can stay solvent longer than the market can stay insane. </em>Either the Patriots will win next week, or they will not. You can do better by using your commercial market tactics to grab the best possible price on the Patriots winning or losing, but the important thing is that <em>you win if you are right about the concrete thing, and you lose if you are wrong. </em></p>\n<p>This works because there is an <em>objective outcome, </em>and it occurs <em>quickly. </em>Thus it functions in its own way like a consumption market.</p>\n<p><em>Truth matters. </em></p>\n<p>If you choose, <em>only truth matters. </em><em>I don’t have to care what other people think. They don’t determine if I win or lose.</em></p>\n<p>That’s what I love, more than anything, about prediction markets. That’s the reason behind <a href=\"https://thezvi.wordpress.com/2018/07/26/prediction-markets-when-do-they-work/\">many of the requirements of well-functioning prediction markets</a>: They enable this sole reliance on truth, without imposing virtual taxes via long lock-up periods. This also enables prediction markets to output accurate predictions.</p>\n<p>That’s also a lot of what I love about trading. With a sufficiently deep and liquid market, <em>you win if and only if you are right. </em>No one gets to take that away from you and decide who gets the credit and the money. Only your skill mattered, and you reap what you deserve.</p>\n<p>I <em>strongly encourage </em>the type of people who read this blog to strive to identify and work in such realms. <em>Be where being right, rather than being approved of, is rewarded. </em></p>\n<p>The world mostly does not work like this.</p>\n<p>The world mostly <em>hates </em>prediction markets, <em>because they predict concrete consequences and outcomes accurately without taking into account what those in power, with high social status, want to be the prediction. </em></p>\n<p>Mostly, winners and losers are determined by social processes, status, coalitions, power, money and so on.</p>\n<p>Credit and compensation mostly isn’t based on who knew the truth and predicted accurately, or who did the work or created the value, or even what was stated in the contract. It is based on who has power and what they decide, based on what is good for them. History, along with everything else, gets decided by the winners.</p>\n<p>That’s life.</p>\n<p>That’s also expensive art, and expensive art markets, of the type Tyler speaks of. Only more so.</p>\n<p>As I understand it (from, mostly, following Marginal Revolution links and posts) a small group determines who succeeds and fails, and buys art from each other, and manipulates the social reality of the art world and its prices to suit its fancies. Its fancies are mostly about the pursuit of conspicuous consumption, high social status and its associated rewards, wealth storage, money laundering and tax evasion, plus suckering outsiders and scamming them out of their money. Artistic merit, or aesthetics, are mostly a minor consideration.</p>\n<p>Recall Tyler’s description:</p>\n<blockquote><p>Imagine a bunch of people competing to make “objects that are interesting but not interesting for reasons related to their practical value.”  And then we debate who has succeeded, or not.  And those debates reflect many broader social, political, and economic issues.  And it is all done with very real money on the line.  The money concerns not just the value of individual art works, but also the prestige and social capital value that arises from having assembled a prestigious and insightful collection.</p></blockquote>\n<p>In this context, what does it mean for an object to ‘be interesting’? It means having a high price, but mostly it means being judged as interesting by a high social status cabal that is primarily designed as an alliance of the high status connected people against everyone else. This need not be explicit at all – it is how such people instinctively operate, and you either learn those instincts or you never make it into the club.</p>\n<p>There is no reason think any of this will ever “return to fundamentals” in any sense. The system sustains itself. There is (almost) no there, there. There never will be.</p>\n<p>Thus, if I buy art, and people don’t like me, they will find ways to charge me a lot more then they’d have charged an insider, and then they say therefore my art is not so valuable. Because I was buying it, and now I own it.</p>\n<p>If I <em>hadn’t </em>bought that piece, would it have become valuable? We’ll never know. Was it valuable before I bought it? Also impossible to say.</p>\n<p>That game is rigged, man. The only way to win is not to play.</p>\n<p>If I think those people are wrong, I can <em>consume </em>the art by displaying it in my house and admiring it. If I want to spend a few hundred or thousand dollars on something I love, by all means I should go for it, but have zero illusions about the work becoming ‘valuable.’</p>\n<p>What I cannot do is <em>predict </em>that they are wrong, and wait for events to prove me right. There is no judgment day. No profit stream. No right. No wrong.</p>\n<p>There are only cliques who watch each other to see if they are favoring the others in the clique, and use this to exploit others, because that’s what winners and clique members with power and money do. It’s sort of a market, like everything else. But in important senses, it is badly named, and something people like me despise. It is our failure mode and our doom, the way that prediction markets are our success mode and our hope.</p>\n<p>Thus, if you love art markets you likely despise prediction markets, at least outside of their designated safe areas like sports and elections. And if you love prediction markets, you likely despise art markets whether or not you find them informative and fascinating in their own way.</p>\n<p>What <em>none </em>of the people, whether they love or hate either market type, should be fooled by, is in accepting in a non-skeptical fashion the ‘market prices’ of ‘art’ in the art market. That is flat out not what is going on, at all. Such trades are not about the exchange of cash value for art value. Trying to use them to value the artwork misses the point entirely.</p>\n<p>Are these art-market games worth understanding for what they can teach us about the world and how people work? Absolutely. Such shadowy practices do not get the light shined on them, that they deserve. Scams and exploitation and manipulation should be exposed. Political games as well. To blame and ideally punish those responsible, to protect people against them <em>and against having to play such games to succeed. </em>But more than that, to educate us about <em>how people, and how such systems, work. </em>Mostly, those who do understand how such things work only understand them from the inside, and do so in a non-intellectual fashion. With exposure, and as they see such actions succeed, they adopt their actions, views, instincts and very identity towards perpetuating such systems through imitation, usually without ever understanding what is going on in either themselves or the system at large.</p>\n<p>Actually understanding how such things work might be the first step towards containing or overcoming such systems, or at least minimizing the damage they inflict on our lives, our status, our wealth and our souls.</p>\n<p>It is also possible that such systems are in fact how anything actually gets done at all, and the exposure of more and more hypocritical and exploitative systems is making society unable to function, which would be far worse.</p>\n<p>That’s a risk I am willing to take.</p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>",
    "user": {
      "username": "Zvi",
      "slug": "zvi",
      "displayName": "Zvi"
    }
  },
  {
    "_id": "PXNBxHS79rbBkaE9y",
    "title": "Why should I care about rationality?",
    "slug": "why-should-i-care-about-rationality",
    "pageUrl": "https://www.lesswrong.com/posts/PXNBxHS79rbBkaE9y/why-should-i-care-about-rationality",
    "postedAt": "2018-12-08T03:49:29.451Z",
    "baseScore": 24,
    "voteCount": 6,
    "commentCount": 5,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>  </p>",
    "user": {
      "username": "TurnTrout",
      "slug": "turntrout",
      "displayName": "TurnTrout"
    }
  },
  {
    "_id": "hjf8MGLTegiAyHjWB",
    "title": "Book review: Artificial Intelligence Safety and Security",
    "slug": "book-review-artificial-intelligence-safety-and-security",
    "pageUrl": "https://www.lesswrong.com/posts/hjf8MGLTegiAyHjWB/book-review-artificial-intelligence-safety-and-security",
    "postedAt": "2018-12-08T03:47:17.098Z",
    "baseScore": 27,
    "voteCount": 9,
    "commentCount": 3,
    "meta": false,
    "question": false,
    "url": "http://www.bayesianinvestor.com/blog/index.php/2018/12/06/artificial-intelligence-safety-and-security/",
    "htmlBody": "<html><head></head><body><p>Book review: Artificial Intelligence Safety and Security, by Roman V.\nYampolskiy.</p>\n<p>This is a collection of papers, with highly varying topics, quality, and\nimportance.</p>\n<p>Many of the papers focus on risks that are specific to\nsuperintelligence, some assuming that a single AI will take over the\nworld, and some assuming that there will be many AIs of roughly equal\npower. Others focus on problems that are associated with current AI\nprograms.</p>\n<p>I've tried to arrange my comments on individual papers in roughly\ndescending order of how important the papers look for addressing the\nlargest AI-related risks, while also sometimes putting similar topics in\none group. The result feels a little more organized than the book, but I\nworry that the papers are too dissimilar to be usefully grouped. I've\nignored some of the less important papers.</p>\n<p>The book's attempt at organizing the papers consists of dividing them\ninto \"Concerns of Luminaries\" and \"Responses of Scholars\". Alas, I see\nfew signs that many of the authors are even aware of what the other\nauthors have written, much less that the later papers are attempts at\nresponding to the earlier papers. It looks like the papers are mainly\narranged in order of when they were written. There's a modest cluster of\nauthors who agree enough with Bostrom to constitute a single <a href=\"https://en.wikipedia.org/wiki/Paradigm#Scientific_paradigm\">scientific\nparadigm</a>,\nbut half the papers demonstrate about as much of a consensus on what\ntopic they're discussing as I would expect to get from asking medieval\npeasants about airplane safety.</p>\n<p><strong>Drexler</strong>'s paper is likely the most important paper here, but it's\nfrustratingly cryptic.</p>\n<p>He hints at how we might build powerful AI systems out of components\nthat are fairly specialized.</p>\n<p>A key criterion is whether an AI has \"strong agency\". When I first read\nthis paper a couple of years ago, I found that confusing. Since then\nI've read some more of Drexler's writings (not yet published) which are\nmuch clearer about this. Drexler, hurry up and publish those writings!</p>\n<p>Many discussions of AI risks assume that any powerful AI will have goals\nthat extend over large amounts of time and space. Whereas it's fairly\nnatural for today's AI systems to have goals which are defined only over\nthe immediate output of the system. E.g. Google translate only cares\nabout the next sentence that it's going to create, and normal\nengineering practices aren't pushing toward having it look much further\ninto the future. That seems to be a key difference between a system that\nisn't much of an agent, versus one with strong enough agency to have the\ninstrumentally convergent goals that Omohundro (below) describes.</p>\n<p>Drexler outlines intuitions which suggest we could build a\nsuperintelligent system without strong agency. I expect that a number of\nAI safety researchers will deny that such a system will be <a href=\"https://medium.com/@eliezeryudkowsky/this-deserves-a-long-response-but-i-only-wrote-a-middle-sized-one-so-sorry-about-that-c9ae2b517ad1\">sufficiently\npowerful</a>.\nBut it seems quite valuable to try, even if it only has a 50% chance of\nproducing human-level AI, because it has little risk.</p>\n<p>This distinction between types of agency serves as an important\nthreshold to distinguish fairly safe AIs from AIs that want to remake\nthe universe into something we may or may not want. And it looks\nrelatively easy to persuade AI researchers to stay on the safe side of\nthat threshold, a long as they see signs that such an approach will be\ncompetitive.</p>\n<p>By \"relatively easy\", I mean something like \"requires slightly less than\nheroic effort\", maybe a bit harder than it has been to avoid nuclear war\nor contain Ebola. There are plenty of ways to make money while staying\non the safe side of that threshold. But some applications (Alexa? Siri?\n<a href=\"https://en.wikipedia.org/wiki/Non-player_character\">NPC</a>s?) where there\nare moderate incentives to have the system learn about the user in ways\nthat would blur the threshold.</p>\n<p>Note that Drexler isn't describing a permanent solution to the main AI\nsafety risks, he's only describing a strategy that would allow people to\nuse superintelligence in developing a more permanent solution.</p>\n<p><strong>Omohundro</strong>'s <strong>The Basic AI Drives</strong> paper has become a classic paper\nin AI safety, explaining why a broad category of utility functions will\ngenerate strategies such as self-preservation, resource acquisition,\netc.</p>\n<p>Rereading this after reading Drexler's AI safety writings, I now see\nsigns that Omohundro has anthropomorphised intelligence a bit, and has\nimplicitly assumed that all powerful AIs will have broader utility\nfunctions than Drexler considers wise.</p>\n<p>Also, Paul Christiano disagrees with one of Omohundro's assumptions in\n<a href=\"https://www.lesswrong.com/posts/fkLYhTQteAu5SinAc/corrigibility\">this discussion of\ncorrigibility</a>.</p>\n<p>Still, it seems nearly certain that someday we will get AIs for which\nOmohundro's warnings are important.</p>\n<h3>Focus on a singleton AI's value alignment</h3>\n<p><strong>Bostrom and Yudkowsky</strong> give a succinct summary of the ideas in\nSuperintelligence (which I <a href=\"http://www.bayesianinvestor.com/blog/index.php/2014/07/28/superintelligence/\">reviewed\nhere</a>),\nexplaining why we should worry about ethical problems associated with a\npowerful AI.</p>\n<p><strong>Soares</strong> discusses why it looks infeasible to just encode human values\nin an AI (e.g. <a href=\"http://lukemuehlhauser.com/wiener-on-the-ai-control-problem-in-1960/\">Sorcerer's\nApprentice</a>\nproblems), and gives some hints about how to get around that by\nindirectly specifying how the AI can learn human values. That includes\nextrapolating what we would want if we had the AI's knowledge.</p>\n<p>He describes an ontology crisis: a goal as simple as \"make diamond\" runs\ninto problems if \"diamond\" is described in terms of carbon atoms, but\nthe AI switches to using a nuclear model that sees\nprotons/neutrons/electrons - how do we know whether it will identify\nthose particles as carbon?</p>\n<p><strong>Tegmark</strong> provides a slightly different way of describing problems\nwith encoding goals into AI's: What happens if an AI is programmed to\nmaximize the number of human souls that go to heaven, and ends up\ndeciding that souls don't exist? This specific scenario seems unlikely,\nbut human values seem complex enough that any attempt to encode them\ninto an AI risks similar results.</p>\n<p><strong>Olle Häggström</strong> tries to analyze how we could use a malicious <a href=\"https://wiki.lesswrong.com/wiki/Oracle_AI\">Oracle\nAI</a> while keeping it from\nescaping its box.</p>\n<p>He starts with highly pessimistic assumptions (e.g. implicitly assuming\nDrexler's approach doesn't work, and worrying that the AI might decide\nthat hedonic utilitarianism is the objectively correct morality, and\nthat maximizing hedons per kilogram of brain produces something that\nisn't human).</p>\n<p>Something seems unrealistic here. Häggström focuses too much on whether\nthe AI can conceal a dangerous message in its answers.</p>\n<p>There are plenty of ways to minimize the risk of humans being persuaded\nby such messages. Häggström shows little interest in them.</p>\n<p>It's better to have a security mindset than not, but focusing too much\non mathematically provable security can cause researchers to lose sight\nof whether they're addressing the most important questions.</p>\n<p><strong>Herd at al</strong> talk about how value drift and wireheading problems are\naffected by different ways of specifying values.</p>\n<p>They raise some vaguely plausible concerns about trade-offs between\nefficiency and reliability.</p>\n<p>I'm not too concerned about value drift - if we get the AI(s) to\ninitially handle this approximately right (with maybe some risks due to\nontological crises), the AI will use its increasing wisdom to ensure\nthat subsequent changes are done more safely (for reasons that resemble\nPaul Christiano's intuitions about the <a href=\"https://www.lesswrong.com/posts/fkLYhTQteAu5SinAc/corrigibility\">robustness of\ncorrigibility</a>).</p>\n<h3>Concerns about paths to AI</h3>\n<p><strong>Bostrom</strong>'s <strong>Strategic Implications of Openness in AI Development</strong>\nthoughtfully describes what considerations should influence the\ndisclosure of AI research progress.</p>\n<p><strong>Sotola</strong> describes a variety of scenarios under which an AI or\ncollections of AIs could cause catastrophe. It's somewhat useful at\nexplaining why AI safety is likely to be hard. It's likely to persuade\npeople who are currently uncertain to be more uncertain, but unlikely to\naddress the beliefs that lead some people to be confident about safety.</p>\n<p><strong>Turchin and Denkenberger</strong> discuss the dangers of arms races between\nAI developers, and between AIs. The basic ideas behind this paper are\ngood reminders of some things that could go wrong.</p>\n<p>I was somewhat put off by the sloppy writing (e.g. \"it is typically\nassumed that the first superintelligent AI will be infinitely stronger\nthan any of its rivals\", followed by a citation to Eliezer Yudkowsky,\nwho has expressed\n<a href=\"https://wiki.lesswrong.com/wiki/Infinite_set_atheism\">doubts</a> about\nusing infinity to describe real-world phenomena).</p>\n<p><strong>Chessen</strong> worries about the risks of AI-driven disinformation, which\nmight destabilize democracies.</p>\n<p>Coincidentally, SlateStarCodex published <a href=\"http://slatestarcodex.com/2018/10/30/sort-by-controversial/\">Sort by\nControversial</a>\n(a more eloquent version of this) around the time when I read this\npaper.</p>\n<p>This seems much less than an extinction risk by itself, but it might\nmake some important governments short-sighted at key times when the more\npermanent risks need to be resolved.</p>\n<p>His policy advice seems uninspired, e.g. suggesting privacy laws that\nsound like the\n<a href=\"https://en.wikipedia.org/wiki/General_Data_Protection_Regulation\">GDPR</a>.</p>\n<p>And \"Americans must choose to <strong>pay for news</strong> again.\" This seems quite\nwrong to me. I presume Chessen means we should return to paying for news\nvia subscriptions instead of via ads.</p>\n<p>But the tv news of 1960s was financed by ads, and was about as\nresponsible as anything we might hope to return to. My view is that\nincreased click-bait is due mainly to having more choices of news\norganizations. Back before cable tv, our daily news choices were\ntypically one or two local newspapers, and two to five tv channels.\nThose gravitated toward a single point of view.</p>\n<p>Cable tv enabled modest ideological polarization, and a modest increase\nin channel surfing, which caused a modest increase in sensationalism.\nInternet enabled massive competition, triggering a big increase in\nsensationalism.</p>\n<p>Note that the changes from broadcast tv to cable tv to internet\nmultimedia involved switching from ad-based to subscription to ad-based\nmodels, with a steady trend away from a focus on fact-checking (although\nthat fact-checking may have been mostly checking whether the facts fit\nthe views of some elite?).</p>\n<p>Wikipedia is an example which shows that not paying for news can\ngenerate <a href=\"https://en.wikipedia.org/wiki/Portal:Current_events\">more responsible\nnews</a> - at the cost\nof entertainment.</p>\n<p>There's still plenty of room for responsible people to create new news\ninstitutions (e.g. bloggers such as Nate Silver), and it doesn't seem\nparticularly hard for us to distinguish them from disinformation\nsources.</p>\n<p>The main problem seems to be that people remain addicted to sources as\nthey become click-baity, and continue to treat them as news sources even\nafter noticing alternatives such as Nate Silver.</p>\n<p>I expect the only effective solution will involve most of us agreeing to\nassign low status to people who treat click-baity sources as anything\nmore than entertainment.</p>\n<h3>Miscellaneous other approaches</h3>\n<p><strong>Torres</strong> says the world needs to be ruled by a Friendly AI. (See a\nshorter version of the paper\n<a href=\"https://www.lesswrong.com/posts/XdgyJ8CchteDLYgPz/the-post-singularity-social-contract-and-bostrom-s\">here</a>.)</p>\n<p>His reasoning is loosely based on <a href=\"https://www.azquotes.com/quote/819025\">Moore's Law of Mad\nScience</a>: Every eighteen months,\nthe minimum IQ necessary to destroy the world drops by one point. But\nwhile Eliezer intended that to mostly focus on the risk of the wrong AI\ntaking over the world, Torres extends that to a broad set of weapons\nthat could enable one person to cause human extinction (e.g.\nbioweapons).</p>\n<p>He presents evidence that some people want to destroy the world. I\nsuspect that some of the people who worry him are thinking too locally\nto be a global danger, but there's likely enough variation that we\nshould have some concern that there are people who seriously want to\nkill all humans.</p>\n<p>He asks how low we need to get risk of any one such malicious person\ngetting such a weapon in order to avoid human extinction. But his answer\nseems misleading - he calculates as if the risks were independent for\neach person. That appears to drastically overstate the risks.</p>\n<p>Oh, and he also wants to stop space colonization. It creates\n<a href=\"https://docs.wixstatic.com/ugd/d9aaad_5c9b881731054ee8bca5fd30699e7df9.pdf\">risks</a>\nof large-scale war. But even if that argument convinced me that space\ncolonization was bad, I'd still expect it to happen. Mostly, he doesn't\nseem to be trying very hard to find good ways to minimize interstellar\nwar.</p>\n<p>If we're going to colonize space fairly soon, then his argument is\nweakened a good deal, and it would then imply that there's a short\nwindow of danger, after which it would take more unusual weapons to\ncause human extinction.</p>\n<p>What's this got to do with AI? Oh, right. A god-like AI will solve\nextinction risks via means that we probably can't yet distinguish from\nmagic (probably involving mass surveillance).</p>\n<p>Note that a singleton can <a href=\"http://www.overcomingbias.com/2018/11/world-government-risks-collective-suicide.html\">create extinction\nrisks</a>.\nTorres imagines a singleton that would be sufficiently wise and stable\nthat it would be much safer than the risks that worry him, but we should\ndoubt how well a singleton would match his stereotype.</p>\n<p>Torres is correct to point out that we live in an unsafe century, but he\nseems wrong about important details, and the AI-relevant parts of this\npaper are better explained by the Bostrom/Yudkowsky paper.</p>\n<p>Bostrom has recently published a <a href=\"https://nickbostrom.com/papers/vulnerable.pdf\">better\nversion</a> of this idea.</p>\n<p><strong>Miller</strong> wants to build addiction into an AI's utility function. That\nmight help, but it looks to me like that would only be important given\nsome fairly bizarre assumptions about what we can and can't influence\nabout the utility function.</p>\n<p><strong>Bekdash</strong> proposes adopting the kind of rules that have enabled humans\nto use checks and balances to keep us safe from other humans.</p>\n<p>The most important rules would limit AI's span of control - AI must have\nlimited influence on the world, and must be programmed to die.</p>\n<p>Bekdash proposes that all AIs (and ems) go to an artificial heaven after\nthey die. Sigh. That looks relevant only for implausibly\nanthropomorphised AI.</p>\n<p>Bekdash want to prevent AIs from using novel forms of communication (\"it\nis easier to monitor and scrutinize AI communication than that of\nhumans.\") - that seems to be clear evidence that Bekdash has no\nexperience at scrutinizing communications between ordinary computer\nprograms.</p>\n<p>He also wants to require diversity among AIs.</p>\n<p>Bekdash proposes that global law ensure obedience to those rules. Either\nBekdash is carefully downplaying the difficulties of enforcing such\nlaws, or (more likely) he's depending on any illegal AIs being weak\nenough that they can be stopped after they've had a good deal of time to\nenhance themselves. In either case, his optimism is unsettling.</p>\n<h3>Why do these papers belong in this book?</h3>\n<p><strong>Prasad</strong> talks about how to aggregate opinions, given the constraints\nthat opinions are expressed only through a voting procedure, and that\n<a href=\"http://www.gametheory.net/dictionary/ParetoDominated.html\">Pareto\ndominant</a>\nalternatives are rare.</p>\n<p>I expect a superintelligent AI to aggregate opinions via evidence that's\nmore powerful than voting for political candidates or complex\nlegislation (e.g. something close to estimating how much utility each\nperson gets from each option).</p>\n<p>I also expect a superintelligent AI to arrange something close to Pareto\ndominant deals often enough that it will be normal for 95+% of people to\nconsent to decisions, and pretty rare for us to need to fall back on\nvoting. And even if we do occasionally need voting, I'm optimistic that\na superintelligence can usually come up with the kind of binary choice\nwhere <a href=\"https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem\">Arrow's impossibility\ntheorem</a>\ndoesn't apply.</p>\n<p>So my impression is that Prasad doesn't have a clear reason for applying\nvoting theories to superintelligence. He is at very least assuming\nimplausibly little change in how politics works. Maybe there will be\nsome situations where a superintelligence needs to resort to something\nequivalent to our current democracy, but he doesn't convince me that he\nknows that. So this paper seems out of place in an AI safety book.</p>\n<p><strong>Portugal et el</strong> note that the leading robot operating system isn't\ndesigned to prevent unauthorized access to robots. They talk about how\nto add an ordinary amount of security to it. They're more concerned with\nminimizing the performance cost of the security than they are with how\nsecure the result is. So I'm guessing they're only trying to handle\nfairly routine risks, not the risks associated with human-level AI.</p>\n</body></html>",
    "user": {
      "username": "PeterMcCluskey",
      "slug": "petermccluskey",
      "displayName": "PeterMcCluskey"
    }
  },
  {
    "_id": "3xnkw6JkQdwc8Cfcf",
    "title": "Is the human brain a valid choice for the Universal Turing Machine in Solomonoff Induction?",
    "slug": "is-the-human-brain-a-valid-choice-for-the-universal-turing",
    "pageUrl": "https://www.lesswrong.com/posts/3xnkw6JkQdwc8Cfcf/is-the-human-brain-a-valid-choice-for-the-universal-turing",
    "postedAt": "2018-12-08T01:49:56.073Z",
    "baseScore": 22,
    "voteCount": 7,
    "commentCount": 13,
    "meta": false,
    "question": true,
    "url": null,
    "htmlBody": "<p>I&#x27;ve recently been thinking about Solomonoff induction, and in particular the <a href=\"https://www.lesswrong.com/posts/fC248GwrWLT4Dkjf6/open-problems-related-to-solomonoff-induction\">free choice of Universal Turing Machine</a>.</p><p>One variable that seems like a potential choice here is a human brain (my brain for example). It&#x27;s obviously a bit of a weird choice, but I don&#x27;t see any reason to disprefer it over a Python interpreter, given that the whole point of Solomonoff induction is to define a prior and so my knowledge of physics or atoms shouldn&#x27;t really come into play when choosing a UTM. </p><p>Concretely, the UTM would be a human simulated in an empty room with an infinitely large notebook symbolizing the tape. It&#x27;s output would be an encoding of sensory data, and it&#x27;s input would be a string of instructions in english.</p><p>If we do that, then the sentence <a href=\"https://www.lesswrong.com/posts/f4txACqDWithRi7hs/occam-s-razor\">&quot;the woman at the end of the street is a witch, she did it&quot;</a> suddenly becomes one of the simplest hypotheses that are available to us. Since the english sentence is so short, we now basically just need to give an encoding of who that woman is and what the action in question is, which is probably also going to be a lot shorter in human language than machine language (since our UTM already understands basic physics, society, etc.), and then our simulated human (which since Solomonoff induction doesn&#x27;t have runtime constraints can take as much time as they want) should be able to produce a prediction of historical sensory input quite well, with relatively little input. </p><p>I feel like I must be missing something in my understanding of Solomonoff induction. I have a lot more thoughts, but maybe someone else has already thought of this and can help me understand this. Some thoughts that come to mind: </p><ul><li>I don&#x27;t know how to build a human brain, but I know how to build a machine that runs a Python interpreter. In that sense I understand a Python interpreter a lot better than I do a human brain, and using it as the basis of Solomonoff induction is more enlightening</li><li>There is a weird circularity about choosing a human brain (or your own brain in particular) as the UTC in Solomonoff induction that I can&#x27;t quite put my finger on</li><li>Maybe I am misunderstanding the Solomonoff induction formalism so that this whole construction doesn&#x27;t make any sense</li></ul>",
    "user": {
      "username": "habryka4",
      "slug": "habryka4",
      "displayName": "habryka"
    }
  },
  {
    "_id": "mrGeJ4Wt66PxN9RQh",
    "title": "LW Update 2018-12-06 – Table of Contents and Q&A",
    "slug": "lw-update-2018-12-06-table-of-contents-and-q-and-a",
    "pageUrl": "https://www.lesswrong.com/posts/mrGeJ4Wt66PxN9RQh/lw-update-2018-12-06-table-of-contents-and-q-and-a",
    "postedAt": "2018-12-08T00:47:09.267Z",
    "baseScore": 55,
    "voteCount": 14,
    "commentCount": 28,
    "meta": true,
    "question": false,
    "url": null,
    "htmlBody": "<p>After a couple months of work, we&#x27;re ready for a significant update to LessWrong:</p><ul><li>Post Page Redesign</li><li>Open Questions (aka Q&amp;A)</li><li>Table of Contents (aka ToC)</li><li>Comment Guidelines</li></ul><h1>Post Page Redesign</h1><p>The first thing you probably noticed is that we redesigned the post page. This was more of an incidental change, which turned out to be necessary in order for both Table of Contents and Question posts to work nicely.</p><h1>Table of Contents</h1><p>We wanted a Table of Contents that not only helped you orient at the beginning of a post, but provided a frame of reference while reading a post, that would help make longer, more complicated posts more skim-able. </p><p>Features:</p><ul><li>Right now the ToC displays to the left of posts on desktop screens, for posts that have 3 or more headings.</li><li>An element that is entirely bold counts as a heading</li><li>On small screens, the Table of Contents is available when you click on the site-navigation icon in the upper left. (The icon will change to indicate a ToC)</li></ul><p>There are some additional nice-to-have features we plan to add soonish, such as the ability to collapse the ToC, and the ability to preview it while drafting a post.</p><p>In general we&#x27;ll probably experiment a bit more with the format. </p><p>Props to GreaterWrong for implementing a Table of Contents quite a while ago. :)</p><h1>Open Questions</h1><p>I wrote a couple weeks ago about our new <a href=\"https://www.lesswrong.com/posts/8XXjxjED6poddys9y/upcoming-open-questions\">Questions and Answers</a> system. We&#x27;re now ready to deploy the minimum-viable-version of this.</p><h2>Current Features</h2><ul><li><strong>Ask a Question.</strong> In your user menu (in the upper right corner of the screen) there is now an option for questions to ask a question, which will create a post with the question-flag. For now, it&#x27;ll appear normally in lists of recent posts (including the home page, daily and your personal profile)</li><li><strong>Answer a Question. </strong>This is similar to a comment, but the formatting is different to highlight that this is meant to have a different feel than commenting. Answers should aspire to resolve a question as accurately and thoroughly as possible, such that if you just read the question-followed-by-single-answer you&#x27;d have a pretty complete understanding of the issue.</li><li><strong>Comment on an Answer. </strong>By default, only the top 3 comments will be displayed, but if you want to dig into the discussion of a given answer you can expand them.</li><li><strong>Comment on a Question. </strong>You can comment on an overall question, without answering. This is for if you&#x27;re still trying to understand the question, or you think it&#x27;s making a conceptual mistake, or you just have some thoughts that don&#x27;t neatly fall into the &quot;answer&quot; format.</li><li><strong>Specialized Table of Contents.</strong> Questions with at least one answer automatically have a Table of Contents, even if there are no headings, to help users orient on a fairly complicated page.</li></ul><p>For the time being, questions are subject to the same frontpage guidelines as anything else. If a question seems productive, well-specified and not-too-political, we&#x27;ll promote them to frontpage, and otherwise they&#x27;ll be visible in the All Posts or Daily views.</p><h2>Upcoming Goals</h2><p>There are many additional features we expect to be necessary for Q&amp;A to flourish. </p><ul><li><strong>Marking questions as answered</strong>. The author of a post can choose an answer as a successful answer to this question. (There&#x27;s some debate about whether this is right approach, but for now it seems like the best approach is to let an author determine the frame of a question, and upvoting/downvoting of questions to provide information on well-framed the community thinks the question is)</li><li><strong>Related Questions. </strong>Many questions are interconnected. We&#x27;re interested in allowing for subquestions, related questions, and perhaps &quot;research agendas&quot; that are sort of like sequences for unsolved questions.</li><li><strong>Question Section on the home page. </strong>Our home page is already a bit cluttered, so adding a question section will require a significant rework. But we&#x27;d like a system that shows new questions, and highlights when they&#x27;ve received satisfactory answers.</li></ul><h2>What Makes a Good Question?</h2><p>We want people to feel comfortable asking a range of questions, from &quot;What caused the scientific revolution?&quot; to &quot;wtf is Moloch?&quot;</p><p>Some concrete examples include:</p><ul><li>How do I use Bayes&#x27; Theorem when trying to figure out which job offer to take?</li><li>Why do some people care about existential risk above most everything else? </li><li>What were the main changes to the human condition that occurred after the agricultural revolution and industrial revolution? </li><li>What is a hamming problem?</li><li>Why does assigning probability 1 to mathematical statements not make sense?</li><li>Why has nobody built a real prediction market?</li><li>Why should I care about the causes of my beliefs in double crux? Surely I should just care about the best arguments?</li><li> Why does Eliezer emphasize noticing confusion so much in the sequences?</li></ul><p>You can also add more detail about your epistemic state in the post body. Examples:</p><ul><li>I&#x27;ve read Bostrom&#x27;s initial paper but didn&#x27;t get &lt;thing&gt;</li><li>In particular I currently believe that &lt;stuff&gt;.</li><li>In particular I&#x27;m confused about &lt;detail&gt;.</li></ul><h1>Comment Guidelines</h1><p>Finally, we&#x27;ve also added some new features to your personal moderation guidelines. Previously, we gave users with 2000 karma the ability to moderate posts (whether on their personal blog or on frontpage). Along with that was the ability to set their moderation guidelines.</p><p>We also want people to be able to use their personal blog section roughly the way they would any other blogsite, which includes setting moderation policies. So, if you have 50 or more karma, you will now be able to set and enforce moderation guidelines <em>on personal blogposts.</em></p><p>If your post guidelines are compatible with the frontpage guidelines, we may promote it to frontpage. There, you won&#x27;t be able to moderate it yourself (if you have less than 2000 karma) but the normal site admins will attempt to enforce the spirit of your intended rules.</p><p>In the future we will build out some features to make this process smoother and clearer to new users, and to give them options about which posts they want to move to frontpage. For now, send us a PM if we&#x27;ve moved a post to frontpage that you want to retain direct moderator control of, and we&#x27;ll move set it back to personal-blog.</p><p>You can also use the &#x27;report comment&#x27; tool if you think we missed a particular comment that should be moderated while leaving it on frontpage. (We&#x27;ll be using our judgment about which cases are actually compatible with the frontpage guidelines)</p><p><em>Note: Due to a quirk of codebase, users won&#x27;t receive their new moderation permissions until they receive an vote. So, if you have 50+ karma now, it&#x27;ll kick in when a comment or post is voted on. Send us a PM if you really want to get going right away.</em></p><h2>How to Setup Moderation Guidelines</h2><ul><li>Go to to your <a href=\"https://www.lesswrong.com/account\">user account.</a></li><li>Find the <em>Moderation &amp; Moderation Guidelines </em>section.</li><li>Select a <em>moderation style </em>(either &#x27;Easy Going&#x27;, &#x27;Norm Enforcing&#x27; or &#x27;Reign of Terror&#x27;, to roughly communicate how heavily you&#x27;ll be moderating) [1]</li><li>If you like, you can spell out additional notes for what sort of norms you&#x27;d like to cultivate on your personal blog, which will be the default guidelines on posts you create.</li></ul><p>After having done that, you&#x27;ll also have the option to set the moderation guidelines for <em>individual posts.</em> By default, they will be the guidelines you set in your user profile, but you can change them if you&#x27;d like a particular discussion to go a certain way.</p><p>[1] Note, &quot;Easy Going&quot;, &quot;Reign of Terror&quot;, etc, are just rough suggestions. They don&#x27;t have a direct impact on what you&#x27;re allowed to do as a moderator.</p><h1>Go Forth and Be Curious!</h1><p>We&#x27;re looking forward to people asking questions and cultivating new types of conversations. Let us know if you have any thoughts or feedback. :)</p>",
    "user": {
      "username": "Raemon",
      "slug": "raemon",
      "displayName": "Raemon"
    }
  },
  {
    "_id": "cq4DsXzGRXJBmYuyB",
    "title": "Transhumanists Don't Need Special Dispositions",
    "slug": "transhumanists-don-t-need-special-dispositions",
    "pageUrl": "https://www.lesswrong.com/posts/cq4DsXzGRXJBmYuyB/transhumanists-don-t-need-special-dispositions",
    "postedAt": "2018-12-07T22:24:17.072Z",
    "baseScore": 98,
    "voteCount": 43,
    "commentCount": 18,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>This essay was originally posted in 2007.</em></p><hr class=\"dividerBlock\"/><p>I have <a href=\"https://www.lesswrong.com/posts/Aud7CL7uhz55KL8jG/transhumanism-as-simplified-humanism\">claimed</a> that transhumanism arises strictly from love of life.  A bioconservative humanist says that it is good to save someone&#x27;s life or cure them of debilitating syndromes if they are young, but once they are &quot;too old&quot; (the exact threshold is rarely specified) we should stop trying to keep them alive and healthy.  A transhumanist says unconditionally:  &quot;Life is good, death is bad; health is good, death is bad.&quot;  Whether you&#x27;re 5, 50, or 500, life is good, why die?  Nothing more is required.</p><p>Then why is there a widespread misunderstanding that transhumanism involves a special fetish for technology, or an unusually strong fear of death, or some other abnormal personal disposition?</p><p>I offer an analogy:  Rationality is often thought to be about cynicism.  The one comes to us and says, &quot;Fairies make the rainbow; I believe this because it makes me feel warm and fuzzy inside.&quot;  And you say, &quot;No.&quot;  And the one reasons, &quot;I believe in fairies because I enjoy feeling warm and fuzzy.  If I imagine that there are no fairies, I feel a sensation of deadly existential emptiness.  Rationalists say there are no fairies.  So they must enjoy sensations of deadly existential emptiness.&quot;  Actually, rationality follows a completely different rule - examine the rainbow very closely and see how it actually works.  If we find fairies, we accept that, and if we don&#x27;t find fairies, we accept that too.  The look-and-see rule makes no mention of our personal feelings about fairies, and it fully determines the rational answer.  So you cannot infer that a competent rationalist hates fairies, or likes feelings of deadly existential emptiness, by looking at what they believe about rainbows.</p><p>But this rule - the notion of actually <em>looking</em> at things - is not widely understood.  The more common belief is that rationalists make up stories about boring old math equations, instead of pretty little fairies, because rationalists have a math fetish instead of a fairy fetish.  A personal taste, and an odd one at that, but how else would you explain rationalists&#x27; strange and unusual beliefs?</p><p>Similarly, love of life is not commonly understood as a motive for saying that, if someone is sick, and we can cure them using medical nanotech, we really ought to do that.  Instead people suppose that transhumanists have a taste for technology, a futurism fetish, that we just love those pictures of little roving nanobots.  A personal taste, and an odd one at that, but how else would you explain transhumanists&#x27; strange and unusual moral judgments?</p><p>Of course I&#x27;m not claiming that transhumanists take no joy in technology.  That would be like saying a rationalist should take no joy in math.  <em>Homo sapiens </em>is the tool-making species; a complete human being should take joy in a contrivance of special cleverness, just as we take joy in music or storytelling.  It is likewise incorrect to say that the aesthetic beauty of a technology is a distinct good from its beneficial use - their sum is not merely additive, there is a harmonious combination.  The equations underlying a rainbow are all the more beautiful for being true, rather than just made up.  But the esthetic of transhumanism is very strict about positive outcomes taking precedence over how cool the technology looks.  If the choice is between using an elegant technology to save a million lives and using an ugly technology to save a million and one lives, you choose the latter.  Otherwise the harmonious combination vanishes like a soap bubble popping.  It would be like preferring a more elegant theory of rainbows that was not actually true.</p><p>In social psychology, the &quot;<a href=\"http://en.wikipedia.org/wiki/Fundamental_attribution_error\">correspondence bias</a>&quot; is that we see far too direct a correspondence between others&#x27; actions and their personalities.  As <a href=\"http://www.danielgilbert.com/Gilbert%20&%20Malone%20%28CORRESPONDENCE%20BIAS%29.pdf\">Gilbert and Malone</a> put it, we &quot;draw inferences about a person&#x27;s unique and enduring dispositions from behaviors that can be entirely explained by the situations in which they occur.&quot;  For example, subjects listen to speakers giving speeches for and against abortion.  The subjects are explicitly told that the speakers are reading prepared speeches assigned by coin toss - and yet the subjects still believe the pro-abortion speakers are personally in favor of abortion.</p><p>When we see someone else kick a vending machine for no visible reason, we assume he is &quot;an angry person&quot;.  But if you yourself kick the vending machine, you will tend to see your actions as caused by your situation, not your disposition.  The bus was late, the train was early, your report is overdue, and now the damned vending machine has eaten your money twice in a row.  But others will not see this; they cannot see your situation trailing behind you in the air, and so they will attribute your behavior to your disposition.</p><p>But, really, most of the people in the world are not mutants - are probably not exceptional in any given facet of their emotional makeup.  A key to understanding human nature is to realize that the vast majority of people see themselves as behaving normally, given their situations.  If you wish to understand people&#x27;s behaviors, then don&#x27;t ask after mutant dispositions; rather, ask what situation they might believe themselves to be in.</p><p>Suppose I gave you a control with two buttons, a red button and a green button.  The red button destroys the world, and the green button stops the red button from being pressed.  Which button would you press?  The green one.  This response is <em>perfectly normal.</em> No special world-saving disposition is required, let alone a special preference for the color green.  Most people would choose to press the green button and save the world, <em>if they saw their situation in those terms.</em></p><p>And yet people sometimes ask me why I want to <a href=\"https://intelligence.org/files/AIPosNegFactor.pdf\">save the world</a>. <em>Why?</em> They want to know <em>why</em> someone would want to save the world?  Like you have to be traumatized in childhood or something?  <em>Give</em> me a <em>break</em>.</p><p>We all seem normal to ourselves.  One must understand this to understand all those strange other people.</p><p>Correspondence bias can also be seen as essentialist reasoning, like explaining rain by water spirits, or explaining fire by phlogiston.  If you kick a vending machine, why, it must be because you have a vending-machine-kicking disposition.</p><p>So the transhumanist says, &quot;Let us use this technology to cure aging.&quot;  And the reporter thinks, <em>How strange!  He must have been born with an unusual technology-loving disposition</em>.  Or, <em>How strange!  He must have an unusual horror of aging!</em></p><p>Technology means many things to many people.  So too, death, aging, sickness have different implications to different personal philosophies.  Thus, different people incorrectly attribute transhumanism to different mutant dispositions.</p><p>If someone prides themselves on being cynical of all Madison Avenue marketing, and the meaning of technology unto them is Madison Avenue marketing, they will see transhumanists as shills for The Man, trying to get us to spend money on expensive but ultimately meaningless toys.</p><p>If someone has been fed Deep Wisdom about how death is part of the Natural Cycle of Life ordained by heaven as a transition to beyond the flesh, etc., then they will see transhumanists as Minions of Industry, Agents of the Anti-Life Death Force that is Science.</p><p>If someone has a postmodern ironic attitude toward technology, then they&#x27;ll see transhumanists as being on a mission to make the world even stranger, more impersonal, than it already is - with the word &quot;Singularity&quot; obviously referring to complete disconnection and incomprehensibility.</p><p>If someone sees computers and virtual reality as an escape from the real world, opposed to sweat under the warm sun and the scent of real flowers, they will think that transhumanists must surely hate the body; that they want to escape the scent of flowers into a grayscale virtual world.</p><p>If someone associates technology with Gosh-Wow-Gee-Whiz-So-Cool flying cars and jetpacks, they&#x27;ll think transhumanists have gone overboard on youthful enthusiasm for toys.</p><p>If someone associates the future with scary hyperbole from Wired magazine - <em>humans will merge with their machines and become indistinguishable from them</em> - they&#x27;ll think that transhumanists yearn for the cold embrace of metal tentacles, that we want to <em>lose our identity and be eaten by the Machine</em> or some other dystopian nightmare of the month.</p><p>In all cases they make the same mistake - drawing a one-to-one correspondence between the way in which the behavior strikes them as strange, and a mutant mental essence that exactly fits the behavior.  This is an unnecessarily burdensome explanation for why someone would advocate healing the sick by any means available, including advanced technology.</p>",
    "user": {
      "username": "Eliezer_Yudkowsky",
      "slug": "eliezer_yudkowsky",
      "displayName": "Eliezer Yudkowsky"
    }
  },
  {
    "_id": "KMvgXcrntHoNaPj7i",
    "title": "Is cognitive load a factor in community decline?",
    "slug": "is-cognitive-load-a-factor-in-community-decline",
    "pageUrl": "https://www.lesswrong.com/posts/KMvgXcrntHoNaPj7i/is-cognitive-load-a-factor-in-community-decline",
    "postedAt": "2018-12-07T15:45:20.605Z",
    "baseScore": 18,
    "voteCount": 7,
    "commentCount": 6,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>This is speculation; I had the thought and then ran in to trouble disentangling the question I am trying to answer from other research on a different question, and also the sources I know about are not conveniently available to me. Ideally I can either get a swift negation or a line on the right kind of research to look at from here.</p><p>From <a href=\"https://pseudoerasmus.com/2017/10/02/ijd/\">here</a>  I get the notion that more effort is required per hour of work than was  the case in the past. It&#x27;s very long, but here&#x27;s the part that piqued my interest:</p><blockquote>So labour productivity growth in textiles  came from a  combination of “speed-up” and “stretch-out”, which is equivalent to “labour intensification” — making each worker exert more e<em>ffort </em>for every hour of work.</blockquote><blockquote>Clark (1<a href=\"https://ideas.repec.org/a/cup/jechis/v47y1987i01p141-173_04.html\">987)</a>   notes that over the course of the 19th century the average Lancashire   operative roughly doubled the number of machines tended, even as the   speed of machines also increased. This higher workload makes it “unsafe  to infer that the increase in output per worker resulted solely from  technical progress”.</blockquote><blockquote>That view is powerfully supported by Bessen (2<a href=\"https://ideas.repec.org/a/cup/jechis/v72y2012i01p44-74_00.html\">012)</a>,   who estimates approximately 1/4 of the 50-fold increase in cloth  output  per worker-hour between 1800 and 1900 was due to each weaver  simply  operating more looms than they had done initially. That’s r<em>eally big.</em> But if you cut off the initial quantum leap from the h<em>and </em>  loom (1800) to the power loom (1819) and consider only the mechanised   era after 1819, the share of  the productivity growth due to greater   exertion of effort is even bigger <em>— </em><strong>more than 60% </strong>!</blockquote><p>From this <a href=\"https://www.youtube.com/watch?v=FKTxC9pl-WM\">Kathy Sierra</a>  talk I saw some months ago, I get the notion of total cognitive  resources used during work. Combining these two suggests to me that the  total cognitive resources used on the job have increased over time.</p><p>Finally I have been wondering about the decline of community in the United States these last few weeks. Referring to <a href=\"http://xroads.virginia.edu/~HYPER/DETOC/assoc/strange.html\">Putnam</a>, it seems this has been pretty consistent since ~1965.</p><p>So what I am wondering is: did we cross some threshold around 1965 where the demands of work ate up the all cognitive resources we had available, so none were left for working in/on our community?</p><p>In pseudoerasmus&#x27; post the term &quot;labor intensification&quot; is used, but when I search for variations on labor intensity/ification, mostly what I get is the ratio of labor expenditures to capital expenditures. I also don&#x27;t have access to Benson&#x27;s paper, and while I am prepared to go around that lack of access I wanted to see if there was a publicly available body of work to check first.</p>",
    "user": {
      "username": "ryan_b",
      "slug": "ryan_b",
      "displayName": "ryan_b"
    }
  },
  {
    "_id": "pDcQKJXBLJTHBDbhK",
    "title": "Worth keeping",
    "slug": "worth-keeping",
    "pageUrl": "https://www.lesswrong.com/posts/pDcQKJXBLJTHBDbhK/worth-keeping",
    "postedAt": "2018-12-07T04:50:01.210Z",
    "baseScore": 51,
    "voteCount": 26,
    "commentCount": 9,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>(Epistemic status: quick speculation which matches my intuitions about how social things go, but which I hadn’t explicitly described before, and haven’t checked.)</em></p>\n<p>If your car gets damaged, should you invest more or less in it going forward? It could go either way. The car needs more investment to be in good condition, so maybe you do that. But the car is worse than you thought, so maybe you start considering a new car, or putting your dollars into Uber instead.</p>\n<p>If you are writing an essay and run into difficulty describing something, you can put in additional effort to find the right words, or you can suspect that this is not going to be a great essay, and either give up, or prepare to get it out quickly and imperfectly, worrying less about the other parts that don’t quite work.</p>\n<p>When something has a problem, you always choose whether to double down with it or to back away.</p>\n<p>(Or in the middle, to do a bit of both: to fix the car this time, but start to look around for other cars.)</p>\n<p>I’m interested in this as it pertains to people. When a friend fails, do you move toward them—to hold them, talk to them, pick them up at your own expense—or do you edge away? It probably depends on the friend (and the problem). If someone embarrasses themselves in public, do you sully your own reputation to stand up for their worth? Or do you silently hope not to be associated with them? If they are dying, do you hold their hand, even if it destroys you? Or do you hope that someone else is doing that, and become someone they know less well?</p>\n<p>Where a person fits on this line would seem to radically change their incentives around you. Someone firmly in your ‘worth keeping’ zone does better to let you see their problems than to hide them. Because you probably won’t give up on them, and you might help. Since everyone has problems, and they take effort to hide, this person is just a lot freer around you. If instead every problem hastens a person’s replacement, they should probably not only hide their problems, but also many of their other details, which are somehow entwined with problems.</p>\n<p>(A related question is when you should let people know where they stand with you. <em>Prima facie</em>, it seems good to make sure people know when they are safe. But that means it also being clearer when a person is not safe, which has downsides.)</p>\n<p>If there are better replacements in general, then you will be inclined to replace things more readily. If you can press a button to have a great new car appear, then you won’t have the same car for long.</p>\n<p>The social analog is that in a community where friends are more replaceable—for instance, because everyone is extremely well selected to be similar on important axes—it should be harder to be close to anyone, or to feel safe and accepted. Even while everyone is unusually much on the same team, and unusually well suited to one another.</p>",
    "user": {
      "username": "KatjaGrace",
      "slug": "katjagrace",
      "displayName": "KatjaGrace"
    }
  },
  {
    "_id": "9hYaPDPJFSqfBv67y",
    "title": "Trivial Inconvenience Day (December 9th at 12 Noon PST)",
    "slug": "trivial-inconvenience-day-december-9th-at-12-noon-pst",
    "pageUrl": "https://www.lesswrong.com/posts/9hYaPDPJFSqfBv67y/trivial-inconvenience-day-december-9th-at-12-noon-pst",
    "postedAt": "2018-12-07T01:26:22.870Z",
    "baseScore": 32,
    "voteCount": 13,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p>Hi everybody, it's been 9 months since <a href=\"https://www.lesswrong.com/posts/9GX49j5DMtasxTbwd/trivial-inconvenience-day-retrospective\">I wrote my report on Trivial Inconvenience Day</a>. I wasn't quite sure when to run a sequel event. I wanted to give people an opportunity for deferred tasks to pile up again. With three seasons under the bridge I think it's safe to say that the procrastination coffers have since refilled. This post is your invitation to join us this Sunday, December 9th, to cross nagging tasks off your list.</p>\n<h2>Event Information</h2>\n<blockquote>\n<p>Shortly after reading Scott Alexander’s LessWrong Crypto Autopsy I found myself agreeing with the point so strongly I was brainstorming ways that its dismal outcome could have been prevented. Peoples personal accounts of why they didn’t buy bitcoin seemed to converge on a central theme: Buying Bitcoin was a trivial inconvenience. Pondering what might be done in light of this, I was reminded of Boston Rat’s Bureaucracy Day. The Bureaucracy Day is essentially a designated day for people to beat the ugh field effect by getting together and going through the whole mess of annoying tasks, paperwork, and other trivially inconvenient things people have been putting off. Having been impressed by the concept the first time I read about it, Scott’s dire analysis convinced me to try something like it in the hope that it would be a useful tool against this sort of thing happening again.</p>\n</blockquote>\n<p>Multi-hour session of not letting small emotional or logistical barriers get in the way of a better life, state tasks you'd like to do and do them. No minimum commitment or significance requirements, show up for 15 minutes to do something and leave if you like. Prefer most participants show up at designated time to maintain critical mass throughout event.</p>\n<h3>How do I join?</h3>\n<p><strong>Join <a href=\"https://discord.gg/sB4YYXZ\">this Discord server</a> on or before Sunday, December 9th at 12:00 (Noon) PST. No account or signup is required.</strong></p>\n<p>You can also join while things are ongoing, but it works best if most of us are there at the same time to start.</p>\n<h3>Why join?</h3>\n<blockquote>\n<p>From the outset I was aiming for the event to have a particular sort of feel. I wanted it to be serious, but also sort of cheesy. In real life we’re used to doing slog-y, tedious work and having nothing to show for it afterwards besides hours passed on the clock. It’s not a very rewarding experience. Therefore to help counteract this I wanted the experience to be chock full of artificial rewards. The fact of the matter is that these mundane necessities of life give us nowhere near the level of reward we feel we deserve for the effort. That is after all why they’re undone in the first place. Keeping this in mind I wanted the atmosphere to be high energy, exuberantly enthusiastic.</p>\n</blockquote>\n<p>The idea behind making it an event is to provide both peer pressure and an exciting atmosphere which would otherwise never be present while you're slogging through obtuse tasks. As well as to set aside designated time for these things to happen in.</p>\n<h3>How Should I Prepare?</h3>\n<p>Make a list of tasks that you need to do which have fallen off the wagon in the course of daily life. Also think about things you would like to do but haven't gotten around to.</p>\n<p>The inspiration for this event was <a href=\"https://www.lesswrong.com/posts/MajyZJrsf8fAywWgY/a-lesswrong-crypto-autopsy\">Scott's crypto autopsy</a>. So I would definitely encourage you to take a moment to think about what potentially <em>high impact</em> things you're putting off. For example in the previous event someone signed up for Vanguard.</p>\n<p>As an example, this is my tentative list:</p>\n<ul>\n<li>Identify consistently dysfunctional parts of my room and make them functional</li>\n<li>Figure out how on earth I'm going to make all the statistical graphs to show the results of my <a href=\"https://www.lesswrong.com/posts/yew3SfWAt7MCmzR6d/survey-help-us-research-coordination-problems-in-the\">Coordination Survey</a>. (Yes I know a lot of you have been asking for these for a while)</li>\n<li>Make a list of my backups &amp; data by year, where it's stored, etc to make sure that I have it all and am not at risk of losing anything</li>\n<li>Sort out the music on my MP3 player</li>\n<li>Order some books</li>\n<li>Make a list of websites the designer of Whistling Lobsters 2.0 can use as inspiration for the site design</li>\n</ul>\n<p>These are mostly things which fall into the short-term-annoyance but long-term-benefit bucket.</p>\n<p><strong>You will be sharing this list</strong>, so if a task is particularly embarrassing or impolite in mixed company, you may be best off listing it under a polite euphemism or dummy description.</p>\n<h3>How does it work?</h3>\n<p>I'll provide detailed instructions the day of the event, but the short version is:</p>\n<ul>\n<li>You put your list into the server</li>\n<li>You do items on the list</li>\n<li>As you check them off, you announce their completion</li>\n<li>Everyone reacts with encouraging emoji and warm expressions</li>\n</ul>\n<h3>How long does it last?</h3>\n<p><strong>As long as you want it to.</strong> If you show up, do one thing that's been bugging you that takes ten minutes and leave I'll consider that a success.\nRealistically it probably ends when almost everyone has done their list or given up.</p>\n</body></html>",
    "user": {
      "username": "ingres",
      "slug": "ingres",
      "displayName": "namespace"
    }
  },
  {
    "_id": "4MLpRxz7ZoX8YXSY3",
    "title": "COEDT Equilibria in Games",
    "slug": "coedt-equilibria-in-games",
    "pageUrl": "https://www.lesswrong.com/posts/4MLpRxz7ZoX8YXSY3/coedt-equilibria-in-games",
    "postedAt": "2018-12-06T18:00:08.442Z",
    "baseScore": 15,
    "voteCount": 5,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>From Jessica&#x27;s <a href=\"https://www.alignmentforum.org/posts/Rcwv6SPsmhkgzfkDw/edt-solves-5-and-10-with-conditional-oracles\">earlier post about conditional oracles</a>, the question was asked: what is the equilibrium concept for games with more than one COEDT agent?</p><p>This post will (partially) answer that question, and provide a link to a tool to visualize 2-player 2-move COEDT equilibria sets.</p><p></p><p>To begin with, we will review how conditional oracles work. A conditional oracle is a mapping from <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}^4\\to\\{0,1\\}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.646em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">4</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">→</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">{</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">}</span></span></span></span></span></span> (where <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mathcal{M}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-cal-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">M</span></span></span></span></span></span></span></span> is some set of finitely many probabilistic oracle machines). A conditional oracle distribution is a probability distribution over conditional oracles. A conditional oracle distribution <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> weakly leads to <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span></span></span></span></span>, if</p><p> <span><span class=\"mjx-full-width mjx-chtml\" style=\"min-width: 27.832em;\"><span class=\"mjx-math\" style=\"width: 100%;\" aria-label=\"P_Q(m_1^O=1\\wedge n_1^O)P_Q(n_2^O=1)>P_Q(m_2^O=1\\wedge n_2^O)P_Q(n_1^O=1)\\to\\\\\n P_{Q'}(O(m_1, n_1, m_2, n_2)=1)=1\"><span class=\"mjx-mrow\" style=\"width: 100%;\" aria-hidden=\"true\"><span class=\"mjx-stack\" style=\"width: 100%; vertical-align: -1.55em;\"><span class=\"mjx-block\"><span class=\"mjx-box\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">∧</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&gt;</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">∧</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">→</span></span></span></span><span class=\"mjx-block\" style=\"padding-top: 0.442em;\"><span class=\"mjx-box\"><span class=\"mjx-mspace\" style=\"width: 0px; height: 0px;\"></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.276em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span> </p><p> <span><span class=\"mjx-full-width mjx-chtml\" style=\"min-width: 27.832em;\"><span class=\"mjx-math\" style=\"width: 100%;\" aria-label=\"P_Q(m_1^O=1\\wedge n_1^O)P_Q(n_2^O=1)<P_Q(m_2^O=1\\wedge n_2^O)P_Q(n_1^O=1)\\to\\\\\n P_{Q'}(O(m_1, n_1, m_2, n_2)=0)=1\"><span class=\"mjx-mrow\" style=\"width: 100%;\" aria-hidden=\"true\"><span class=\"mjx-stack\" style=\"width: 100%; vertical-align: -1.55em;\"><span class=\"mjx-block\"><span class=\"mjx-box\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">∧</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.372em;\">∧</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.229em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-stack\" style=\"vertical-align: -0.315em;\"><span class=\"mjx-sup\" style=\"font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">→</span></span></span></span><span class=\"mjx-block\" style=\"padding-top: 0.442em;\"><span class=\"mjx-box\"><span class=\"mjx-mspace\" style=\"width: 0px; height: 0px;\"></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.276em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span></span></span></span> </p><p>From the last post, all reflective conditional oracle distributions weakly lead to themselves.</p><p>Two things must be noted. First, the space that the equilibrium set lies in is actually correlated strategy space (a probability distribution over all joint outcomes, which in a 2-player 2-move game, is a tetrahedron). This is because the same oracle is selected from the oracle distribution for all the algorithms to use, which opens the door to correlated outcomes, instead of both players independently drawing an oracle from the oracle distribution. Second, the full concept of an equilibrium produced by a reflective COD (in the sense of a necessary and sufficient condition) is difficult to characterize, because it involves a bunch of fiddly details of the utilities of various probability-zero actions and whether it is possible to limit to them in the appropriate way. Therefore, the following definition doesn&#x27;t characterize <em>all</em> COEDT equilibria, just the fully-mixed ones that lie in the interior of the space instead of on a boundary (because that corresponds to having no outcomes occur with probability zero).</p><p>Let <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span> be the <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span>&#x27;th player, <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"U_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span> be the utility function of the <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span>&#x27;th player, <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.032em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span> be the strategy set of the <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span>&#x27;th player, <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S=\\prod_{i\\in I}S_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-munderover MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size1-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">∏</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">∈</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;\">I</span></span></span></span></span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.032em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span> be the set of outcomes, <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"s_i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span></span></span> be the move the <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span>&#x27;th player plays as dictated by some <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"s\\in S\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">∈</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span></span></span></span>, and <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Delta\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Δ</span></span></span></span></span></span> be a probability distribution over <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"S\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span></span></span></span>. </p><p><strong>Definition 1:</strong> A <em>fully-mixed conditional equilibria</em> is a probability distribution <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Delta\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Δ</span></span></span></span></span></span> with nonzero probability over all <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"s\\in S\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">∈</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span></span></span></span>, s.t.</p><p> <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\forall i\\forall j,j'\\in S_i: \\mathbb{E}_{s\\sim\\Delta}(U_i|s_i=j)=\\mathbb{E}_{s\\sim\\Delta}(U_i|s_i=j')\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">∀</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">∀</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">∈</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\" style=\"margin-right: -0.032em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">E</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">∼</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Δ</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">E</span></span></span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">∼</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Δ</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">s</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-msup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span> </p><p>In short, all players are indifferent between all of the moves that they could play.</p><p><strong>Theorem 1:</strong> <em>All fully mixed conditional equilibria in a game have a reflective conditional oracle distribution which results in the equilibrium being played.</em></p><p>The proof will be deferred to the end, along with Theorem 2:</p><p><strong>Theorem 2:</strong> <em>All conditional oracle distributions which result in nonzero probability for all outcomes, when used, which aren&#x27;t fully-mixed conditional equilibria, aren&#x27;t reflective.</em></p><p>Due to these theorems, in the 2-player 2-move case, all conditional equilibria except for some points on the boundary may be found by plotting an &quot;indifference surface&quot; (where a player is indifferent between both moves) for both players in the unit tetrahedron, and the intersection of the indifference surfaces in the interior of the tetrahedron is the set of fully-mixed conditional equilibria. </p><p>Adam Scherlis has kindly coded a tool to do this, using the <a href=\"https://www.wolfram.com/cdf-player/\">Wolfram CDF player</a>. The .cdf file  is <a href=\"https://cdn.discordapp.com/attachments/350837629182279680/514293055927353347/indifference-tetrahedron.cdf\">here</a>. </p><p>Now for pictures.</p><span><figure><img src=\"https://i.imgur.com/4h5NRGs.png\" class=\"draft-image \" style=\"\" /></figure></span><p>This is Prisoner&#x27;s Dilemma. Both D,D and C,C are possible equilibria.</p><p></p><span><figure><img src=\"https://i.imgur.com/jhlstII.png\" class=\"draft-image \" style=\"\" /></figure></span><p></p><p>This is Chicken. There is a line of possible equilibria where both players get 4 utility in expectation. The two swerve/straight outcomes are also possible equilibria, because there are points arbitrarily close to them where the players pick swerve and straight, so there is a sequence of oracles which limit to swerve/straight, so the distribution <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> which corresponds to swerve/straight lies in <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\text{StepClosure}(Q)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">StepClosure</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>. This is an instance of disconnected points on the boundary which are produced by reflective conditional oracle distributions, which aren&#x27;t fully-mixed, so they aren&#x27;t an instance of the equilibrium concept that we defined. Note that the intuitively desirable outcome where both players go straight with 10% probability to get an expected utility of 4.05 is not an equilibrium, despite being the optimal strategy if you know your opponent will select the same probability distribution over moves as you, because it incentivizes both players to go straight if they know that the other player plays that strategy, and so isn&#x27;t stable.</p><p></p><span><figure><img src=\"https://i.imgur.com/3FImcGu.png\" class=\"draft-image \" style=\"\" /></figure></span><p></p><p>This is Stag Hunt. Again, there are many equilibria where both players earn 1 utility in expectation, and a disconnected equilibrium point where both players cooperate on hunting the stag.</p><p></p><p><strong>Theorem 1 Proof:</strong></p><p>For all outcomes, there is an oracle <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"O\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span></span></span></span></span> that produces it when used, by mapping the appropriate queries made by all the players to <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span> or <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span>, respectively. Therefore, all fully-mixed distributions over outcomes have fully-mixed oracle distributions which produce that distribution when used. Given a fully-mixed distribution <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Delta\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Δ</span></span></span></span></span></span>, let <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> be an arbitrary fully-mixed oracle distribution which produces it. Assume the implementation of COEDT where 1 means to take an action <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span></span></span> and 0 means to defer to a COEDT algorithm that chooses among the action set without action <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a</span></span></span></span></span></span>. <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"COEDT_{i,j}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span></span></span></span></span> is the <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"j\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span>&#x27;th instance in this chain, for player <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"i\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span></span></span>.(to implement choice among more than two actions)</p><p> Because all oracle queries are of the form <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"(U_i, COEDT_{i,j}, U_i, 1-COEDT_{i,j})\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>, by starting with the final COEDT algorithm in the chain, because expected utility is the same for both actions (by <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Delta\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Δ</span></span></span></span></span></span> being a fully-mixed conditional equilibrium), there is no constraint on the oracle distribution. Working backward from the maximum <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"j\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span> to <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span>, for all the COEDT algorithms, because expected utility is the same for taking an action and deferring, there is no constraint on the oracle distribution produced.</p><p>Because <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> produces no constraints on what <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span></span></span></span></span> must be, <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> weakly leads to all <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span></span></span></span></span>, and in particular, <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> weakly leads to <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span>. By the definition of <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\" \\text{StepClosure}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">StepClosure</span></span></span></span></span></span> from the previous post, and <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> being fully-mixed, <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\\in\\text{StepClosure}(Q)\\subseteq \\text{StepHull}(Q)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">∈</span></span><span class=\"mjx-mtext MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">StepClosure</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.446em;\">⊆</span></span><span class=\"mjx-mtext MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">StepHull</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>, so <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> is a reflective conditional oracle distribution.</p><p><strong>Theorem 2 Proof:</strong></p><p>As before, given a fully-mixed distribution <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Delta\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Δ</span></span></span></span></span></span>, select an arbitrary <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> which produces it. By the same reasoning as before (backwards induction), when we get to the first action with higher or lower expected utility than the other actions that have been seen so far, due to the difference in expected utility, we get a constraint on <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q'\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span></span></span></span></span>, namely that <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P_{Q'}(O(U_i, COEDT_{i,j}, U_i, 1-COEDT_{i,j})=1)=1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.276em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span>, or <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P_{Q'}(O(U_i, COEDT_{i,j}, U_i, 1-COEDT_{i,j})=0)=1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.109em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.276em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-msup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span><span class=\"mjx-sup\" style=\"font-size: 83.3%; vertical-align: 0.347em; padding-left: 0px; padding-right: 0.06em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.298em;\">′</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\" style=\"margin-right: -0.084em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;\">U</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.12em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;\">T</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">i</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">j</span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span></span></span>. Because <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> has some of its probability mass composed of oracles which permit later actions to be taken (because <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\Delta\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Δ</span></span></span></span></span></span> is fully-mixed) , it violates this condition, so <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> does not weakly lead to itself. All reflective <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> weakly lead to themselves, from the previous post, so <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em;\">Q</span></span></span></span></span></span> cannot be reflective.</p><p></p><p></p><p></p><p></p><p></p>",
    "user": {
      "username": "Diffractor",
      "slug": "diffractor",
      "displayName": "Diffractor"
    }
  },
  {
    "_id": "pPssXmTGEin9Ri3An",
    "title": "Hell Must Be Destroyed",
    "slug": "hell-must-be-destroyed",
    "pageUrl": "https://www.lesswrong.com/posts/pPssXmTGEin9Ri3An/hell-must-be-destroyed",
    "postedAt": "2018-12-06T04:11:19.417Z",
    "baseScore": 33,
    "voteCount": 22,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>[Originally published in <a href=\"https://qualiacomputing.com/2018/12/03/hell-must-be-destroyed/\">Qualia Computing</a>]</p><p><em>Singer called the movement that grew up around him “effective altruism”, and its rallying cry was that one ought to spend every ounce of one’s energy doing whatever most relieves human suffering, most likely either feeding the poor or curing various tropical diseases. Again, something his opponents rejected as impossible, unworkable, another example of liberal fanaticism. Really? Every ounce of your energy? Again, they could have just read their Bibles. Deuteronomy 6:5: “And you must love the Lord your God with all your heart, and with all your soul, and with all your strength.”</em> </p><p></p><p><em>Then Singer changed his tune. In the 1970s, after the sky cracked and the world changed, he announced that charity was useless, that feeding the poor was useless, that curing tropical diseases was useless. There was only one cause to which a truly rational, truly good human being could devote his or her life.</em> </p><p></p><p><em>Hell must be destroyed.</em> </p><p></p><p><em>The idea of billions of human beings suffering unbearable pain for all eternity so outweighed our little earthly problems that the latter didn’t even register. He began meeting with his disciples in secret, teaching them hidden Names he said had been vouchsafed to him by angels. Thamiel put a price on his life – quite a high price actually. Heedless of his own safety, Singer traveled what remained of the civilized world, making converts wherever he went, telling them to be perfect as God was perfect, and every speech ended the same way. Hell must be destroyed.</em><br/></p><p>–  <a href=\"http://unsongbook.com/interlude-%D7%92-cantors-and-singers/\">INTERLUDE ג: CANTORS AND SINGERS</a> from <a href=\"http://unsongbook.com/\">UNSONG</a></p><p></p><hr class=\"dividerBlock\"/><p></p><p>An angel appears on Earth. This genderless being connected to God shows up on every screen on Earth at once and asks us if we are interested in drastically improving life on Earth. A large enough portion of those who hear the message (which gets a coverage of 80%+ of people worldwide) see into their souls and find the willingness to make life better, and then they see into their hearts and see the warmth of hope, and so they resolve to agree to do whatever is necessary to help the angel improve life on Earth. And thus the angel says “thanks to the collective desire to make it so, I shall change some things about how the planet is programed, and you will see a 99% reduction in suffering and a 20% increase in overall happiness.”</p><p></p><p>And so the angel gets to work.</p><p></p><p>A year passes, and nobody can really tell the difference from before. Most people’s day to day experience is perhaps even slightly more tedious and slightly more boring. What happened? After a few years it is clear that no major change has happened, and indeed affective psychologists report a mild but very generalized decrease in people’s engagement with their day to day activities and increases in feelings of being a bit disoriented. Did the <a href=\"https://qualiacomputing.com/2017/03/08/memetic-vaccine-against-interdimensional-aliens-infestation/\">angel scam us</a>? Or did people fail to do their part? Or why are there no improvements? A large enough mass of people asked this question that the angel felt the need to provide an update. He comes back down and appears in all of the planet’s screens and says:</p><p></p><p>“Everything went according to plan. It is just that your society hasn’t reached the point of scientific development where you are able to measure the quality of experience of sentient beings. You aren’t quantifying pain very well.”</p><p></p><p>“Here is what I did. Above of all, I focused my energies on trying to prevent some of the worst experiences, which in aggregate happened to be an ethical catastrophe. I managed to reduce how bad these experiences were by about 99.99%.”</p><p></p><p>“I started by reducing how bad cluster headaches feel. They are now only about 44,000 dolors per second (d/s). They used to be around 450,000,000 d/s. You see, when most people get a fleeting headache, we are talking about headaches that range from 0.5 to 1d/s. You know, the type of headache that people are willing to wait out, and perhaps some people will ask for a little aspirin or some placebo of some sort and then get on with it. Most headaches are of this kind. But even if you bundle all of them together we are talking about a rounding error relative to the suffering caused by other types of headaches, the bad ones. Migraine, for example, tends to get to about 1,000 dolors per second, and sufferers have a hard time communicating the fact that it is not just a lot worse, it is a thousand times more painful than the “normal” ones. But even then that does not register relative to one of the really really bad ones, like cluster headaches, which as I said can spiral up to values close to a billion d/s. As it happens, on your planet there are simple chemical tricks to reduce that particular type of pain (e.g. <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/16801660\">LSD</a>), so I just went ahead and got rid of the bulk of it very easily. It’s still super painful by human standards, but not by my standards, like it was before. To have a cluster headache now is just as “indescribably bad” as before, meaning it goes beyond people’s ability to imagine and make sense of. But that doesn’t challenge the fact that the 99.99% improvement I did is an ethical victory of civilizational magnitude.”</p><p></p><p>“Next I went on to reducing how bad it feels to have kidney stones, bone pain, and various kinds of particularly bad neuropathies in people with schizophrenia. By the time I had taken care of about the dozen or so worst kinds of pain, I had already overdelivered by an order of magnitude and was starting to run into diminishing returns. So I decided to go on to helping other planets in my quest to prevent as much suffering as possible.”</p><p></p><p>“I apologize I used about 0.13 hedons per second (h/s) from mundane experiences to implement one of those cosmic pain diminishing plans. In order to increase the amount of happiness in the world as I promised I made the experience of showering about 50% more enjoyable and the experience of listening to music about twice as good. As you can see, the bathing industry did take off, but not many thought much of it. And the musicians were able to tell that music was awesome again and wondered why, but most people seem to have attributed their increased musical enjoyment to what they imagined had been their own hidden musical talents all along.”</p><p></p><p>“Thank you, and keep enjoying your drastically improved planet.”</p><p></p><p>Thus, people realized that the world was indeed a lot better. Well, some did. And others complained, but it was ok.</p><hr class=\"dividerBlock\"/><p><em>Thanks to</em> <em><a href=\"http://www.miqel.com/\">Michael Aaron Coleman</a> and</em> <em><a href=\"http://www.jonathanleighton.org/opis\">Jonathan Leighton</a> for inspiring this piece. Michael suffers from cluster headaches and has described their phenomenology in gruesome detail. He says that in a 0 to 10 scale, cluster headaches are solid 10/10. But he also says you really need a different scale to make sense of this monster. He once used the phrase “minus one million hedonic tone”. He says that morphine makes the pain go from 10/10 to 9/10, if at all, maybe more like 9.5/10. Thankfully, LSD in small doses (~25 micrograms) makes it go to 1/10. DMT also works, but 5-MeO-DMT does not (and yet it still</em> <em><a href=\"https://www.reddit.com/r/philosophy/comments/a1f2jk/the_pseudotime_arrow_explaining_phenomenal_time/\">expands time</a>, so not a good idea). Jonathan is the Executive Director of the Organization for the Prevention of Intense Suffering (<a href=\"http://www.preventsuffering.org/\">OPIS</a>). He works on identifying cases where intense suffering can be prevented on a massive scale and doing what has to be done. I recommend getting in touch with him if this is a particular interest of yours.</em></p>",
    "user": {
      "username": "algekalipso",
      "slug": "algekalipso",
      "displayName": "algekalipso"
    }
  },
  {
    "_id": "CGkZEQdeBZZXbBT7o",
    "title": "On Rationalist Solstice and Epistemic Caution",
    "slug": "on-rationalist-solstice-and-epistemic-caution",
    "pageUrl": "https://www.lesswrong.com/posts/CGkZEQdeBZZXbBT7o/on-rationalist-solstice-and-epistemic-caution",
    "postedAt": "2018-12-05T20:39:34.687Z",
    "baseScore": 68,
    "voteCount": 29,
    "commentCount": 6,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Since 2011, some LessWrong folk have observed the winter solstice, as a holiday ritual celebrating human achievement in the face of a confusing, often terrifying world.</p><p>I've written in the past about the <a href=\"https://www.lesswrong.com/posts/rijoxTpkSPXcTXRbN/the-value-and-danger-of-ritual\">potential value, and danger</a>, of ritual. Over the past years my opinion shifted somewhat, but is still essentially summarized as: \"I think ritual is <i>less </i>epistemically fraught than generally exposing yourself to the beliefs of a peer group (something that pretty much everyone does by default), and meanwhile has many benefits. We should be cautious of it, but it's a fairly important human experience we shouldn't discard simply because it pattern-matches to woo.\"</p><p>Still, I think the practice of rational ritual should still involve a lot of epistemic care, on the part of both organizers and participants.</p><p>Since 2012, in the various editions of the Solstice Book of Traditions, I've included a disclaimer at the beginning, and I think it'd be valuable to have that more publicly accessible so that people going into a Solstice can be properly informed.</p><p>Individual Solstice celebrations vary, and I can't promise that this document will accurately reflect all organizers' intentions. But it reflects my own goals and hopefully provides a reasonable starting context.</p><hr><h2>What is a Ritual?</h2><p>I think most people would agree a ritual is a symbolic action with some kind of emotional power. I personally find it useful to think about them with a more opinionated definition: \"A ritual is a symbolic action that transforms you in some way.\"&nbsp;</p><p>Ritual experience cannot be coerced - only entered willingly by those that believe in them. A ritual that you don’t believe in may feel hollow, or alienating.</p><p>I do not believe ritual and rationality are <i>inherently</i> contradictory. The human brain seems designed <i>badly</i>. It is hard to truly accept certain facts about the world, even when you have empirical evidence - especially for facts involving large numbers, or unspeakable horrors.</p><p>It can even be hard for your brain to accept truths like <i>“You are not alone, and you can do this.</i>”</p><p>Rituals can be useful, to internalize those facts.</p><p>They can also be useful to help<i> make it</i> <i>true, that you are not alone, and you can do this</i>.</p><p>Nonetheless, with power comes responsibility. If you are considering participating in the Rationalist Solstice, first consider as carefully as you can, in the light of day with your clear-thinking prefrontal cortex, whether the concepts herein seem true and good - the sort of things you’d want to employ emotional tricks and a ritual journey to cement. Or, if you are uncertain, that you nonetheless trust that a ritual invoking these principles is a good thing to experience, for whatever your reasons.</p><p>If you are an organizer, each year you should reflect upon the principles here and the specific content of the Solstice. A rationalist holiday doesn’t just need people to preserve one set of traditions –&nbsp;<a href=\"https://www.huffingtonpost.com/entry/can-the-secular-solstice-become-the-post-ironic-celebration_us_58511263e4b0a464fad3e530\">it needs cultural stewards to actively pursue truth</a>, who work to develop songs and stories that reflect our deepening understanding of the nature of reality.</p><h3>Principle Underpinnings</h3><p><i><strong>First,</strong></i> that rational inquiry and empirical evidence are the best tools to make sense of the world.</p><p><i><strong>Second,</strong></i> that our world is a harshly <i>neutral</i> world, with physics indifferent to our suffering.</p><p><i><strong>Third,</strong></i> more subjectively, that <i>it is right and good </i>that we look upon the world and have opinions about how to change it. That it is <i>wrong</i> that millions struggle in poverty, or die of malaria, or are trapped by systems <u>we built ourselves </u>that are indifferent to our struggles.</p><p><i><strong>Fourth,</strong></i> that you<i> </i>have the potential to help. Perhaps not now - maybe you must ensure your own life is flourishing before you are ready to help others or change the broader world. But you would, if you could, and that you would like a night to remember that possibility.</p><p><i><strong>Fifth</strong>, </i>some oddly <i>specific</i> things. These assumptions are not <i>intrinsic </i>to the solstice ceremony, but they permeate many of the songs and stories and it seems best to make them explicit:</p><p><i>Scientifically</i> - That the modern astronomical understanding of the big bang, star formation, and evolution are more or less correct. That the natural world is often dangerous and human civilization could potentially be destroyed. That artificial intelligence is quite possible, and will probably dramatically shape our future, sooner or later, one way or another.</p><p>The more specific claims get less specific story and song lyrics, to avoid overcommitting epistemically. Any specific empirical claim is something we should be prepared to discard, no matter how pretty a song lyric.</p><p><i>Philosophically</i> - Well, ethics is confusing, once you begin expanding your circle of concern beyond tribes of 150, and evolution-honed intuitions break down. But it seems to me:</p><p>That pointless suffering is bad. That the default state of nature – creatures, at least some sentient, eating each other alive, populations kept in check by starvation and disease - isn’t okay.</p><p>That love and excitement and curiosity and creativity are good. This is arbitrary and human-chauvinistic, but that’s fine. It’s what we have. It is good when people build things together, when they come to understand the world more deeply, when they become more self aware. It is good that we relate to and love each other. It is good that sometimes we laugh and joke and screw around.</p><p>That death is bad. Every time a conscious being which knows itself and doesn’t want to die is snuffed out of the world… that is a tragedy.</p><p><i>Strategically</i> - that compassion is good, but <i>not sufficient.</i> That changing the world requires deep thinking and innovation that often feels strange at first glance.</p><p><i><strong>And finally, sixth:</strong></i> that the neutral universe <i>does not begrudge our dreams</i>.</p><p>It does not fume at the death of smallpox or reduced scarcity or non-reproductive sex. We can choose as best we can what is right, and work to bring about the best world we can.</p><p>We may not agree on the specifics of what that means. The rest of the year, we may argue about what <i>exactly</i> is right and good and how to best achieve it. But tonight, we remember the visions we share. That in the space of all possible dreams, ours are incredibly aligned. That we share the meta-dream: we can work together to refine our visions as we strive to make them real.</p><p>We can cooperate, and help one another along the way.</p>",
    "user": {
      "username": "Raemon",
      "slug": "raemon",
      "displayName": "Raemon"
    }
  },
  {
    "_id": "Aud7CL7uhz55KL8jG",
    "title": "Transhumanism as Simplified Humanism",
    "slug": "transhumanism-as-simplified-humanism",
    "pageUrl": "https://www.lesswrong.com/posts/Aud7CL7uhz55KL8jG/transhumanism-as-simplified-humanism",
    "postedAt": "2018-12-05T20:12:13.114Z",
    "baseScore": 181,
    "voteCount": 89,
    "commentCount": 34,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>This essay was originally posted in 2007.</em></p><hr class=\"dividerBlock\"/><p><u><a href=\"http://www.robertboynton.com/?art_id=119\">Frank Sulloway</a></u> once said: “Ninety-nine per cent of what Darwinian theory says about human behavior is so obviously true that we don’t give Darwin credit for it. Ironically, psychoanalysis has it over Darwinism precisely because its predictions are so outlandish and its explanations are so counterintuitive that we think, <em>Is that really true? How radical!</em> Freud’s ideas are so intriguing that people are willing to pay for them, while one of the great disadvantages of Darwinism is that we feel we know it already, because, in a sense, we do.”</p><p>Suppose you find an unconscious six-year-old girl lying on the train tracks of an active railroad. What, morally speaking, ought you to do in this situation? Would it be better to leave her there to get run over, or to try to save her? How about if a 45-year-old man has a debilitating but nonfatal illness that will severely reduce his quality of life – is it better to cure him, or not cure him?</p><p>Oh, and by the way: This is not a trick question.</p><p>I answer that I would save them if I had the power to do so – both the six-year-old on the train tracks, and the sick 45-year-old. The obvious answer isn’t <em>always</em> the best choice, but sometimes it <em>is.</em></p><p>I won’t be lauded as a brilliant ethicist for my judgments in these two ethical dilemmas. My answers are not surprising enough that people would pay me for them. If you go around proclaiming “What does two plus two equal? Four!” you will not gain a reputation as a deep thinker. But it is still the correct answer.</p><p>If a young child falls on the train tracks, it is good to save them, and if a 45-year-old suffers from a debilitating disease, it is good to cure them. If you have a logical turn of mind, you are bound to ask whether this is a special case of a general ethical principle which says “Life is good, death is bad; health is good, sickness is bad.” If so – and here we enter into controversial territory – we can follow this general principle to a surprising new conclusion: If a 95-year-old is threatened by death from old age, it would be good to drag them from those train tracks, if possible. And if a 120-year-old is starting to feel slightly sickly, it would be good to restore them to full vigor, if possible. With current technology it is <em>not</em> possible. But if the technology became available in some future year – given sufficiently advanced medical nanotechnology, or such other contrivances as future minds may devise – would you judge it a good thing, to save that life, and stay that debility?</p><p>The important thing to remember, which I think all too many people forget, is that <em>it is not a trick question.</em></p><p>Transhumanism is simpler – requires fewer bits to specify – because it has no special cases. If you believe professional bioethicists (people who get paid to explain ethical judgments) then the rule “Life is good, death is bad; health is good, sickness is bad” holds only until some critical age, and then flips polarity. Why should it flip? Why not just keep on with life-is-good? It would seem that it is good to save a six-year-old girl, but bad to extend the life and health of a 150-year-old. Then at what <em>exact</em> age does the term in the utility function go from positive to negative? Why?</p><p>As far as a transhumanist is concerned, if you see someone in danger of dying, you should save them; if you can improve someone’s health, you should. There, you’re done. No special cases. You don’t have to ask anyone’s age.</p><p>You also don’t ask whether the remedy will involve only “primitive” technologies (like a stretcher to lift the six-year-old off the railroad tracks); or technologies invented less than a hundred years ago (like penicillin) which nonetheless seem ordinary because they were around when you were a kid; or technologies that seem scary and sexy and futuristic (like gene therapy) because they were invented after you turned 18; or technologies that seem absurd and implausible and sacrilegious (like nanotech) because they haven’t been invented yet. Your ethical dilemma report form doesn’t have a line where you write down the invention year of the technology. Can you save lives? Yes? Okay, go ahead. There, you’re done.</p><p>Suppose a boy of 9 years, who has tested at IQ 120 on the Wechsler-Bellvue, is threatened by a lead-heavy environment or a brain disease which will, if unchecked, gradually reduce his IQ to 110. I reply that it is a good thing to save him from this threat. If you have a logical turn of mind, you are bound to ask whether this is a special case of a general ethical principle saying that intelligence is precious. Now the boy’s sister, as it happens, currently has an IQ of 110. If the technology were available to gradually raise her IQ to 120, without negative side effects, would you judge it good to do so?</p><p>Well, of course. Why not? It’s not a trick question. Either it’s better to have an IQ of 110 than 120, in which case we should strive to decrease IQs of 120 to 110. Or it’s better to have an IQ of 120 than 110, in which case we should raise the sister’s IQ if possible. As far as I can see, the obvious answer is the correct one.</p><p>But – you ask – <em>where does it end?</em> It may seem well and good to talk about extending life and health out to 150 years – but what about 200 years, or 300 years, or 500 years, or more? What about when – in the course of properly integrating all these new life experiences and expanding one’s mind accordingly over time – the equivalent of IQ must go to 140, or 180, or beyond human ranges?</p><p>Where does it end? It doesn’t. Why should it? Life is good, health is good, beauty and happiness and fun and laughter and challenge and learning are good. This does not change for arbitrarily large amounts of life and beauty. If there were an upper bound, it would be a special case, and that would be inelegant.</p><p>Ultimate physical limits may or may not permit a lifespan of at least length X for some X – just as the medical technology of a particular century may or may not permit it. But physical limitations are questions of simple fact, to be settled strictly by experiment. Transhumanism, as a moral philosophy, deals only with the question of whether a healthy lifespan of length X is desirable <em>if</em> it is physically possible. Transhumanism answers yes for all X. Because, you see, it’s not a trick question.</p><p>So that is “transhumanism” – loving life without special exceptions and without upper bound.</p><p>Can transhumanism really be that simple? Doesn’t that make the philosophy trivial, if it has no extra ingredients, just common sense? Yes, in the same way that the scientific method is nothing but common sense.</p><p>Then why have a complicated special name like “transhumanism” ? For the same reason that “scientific method” or “secular humanism” have complicated special names. If you take common sense and rigorously apply it, through multiple inferential steps, to areas outside everyday experience, successfully avoiding many possible distractions and tempting mistakes along the way, then it often ends up as a minority position and people give it a special name.</p><p>But a moral philosophy should not <em>have</em> special ingredients. The purpose of a moral philosophy is not to look delightfully strange and counterintuitive, or to provide employment to bioethicists. The purpose is to guide our choices toward life, health, beauty, happiness, fun, laughter, challenge, and learning. If the judgments are simple, that is no black mark against them – morality doesn’t always have to be complicated.</p><p>There is nothing in transhumanism but the same common sense that underlies standard humanism, rigorously applied to cases outside our modern-day experience. A million-year lifespan? If it’s possible, why not? The prospect may seem very foreign and strange, relative to our current everyday experience. It may create a sensation of future shock. And yet – is life a <em>bad</em> thing?</p><p>Could the moral question really be just that simple?</p><p>Yes.</p>",
    "user": {
      "username": "Eliezer_Yudkowsky",
      "slug": "eliezer_yudkowsky",
      "displayName": "Eliezer Yudkowsky"
    }
  },
  {
    "_id": "zvrZi95EHqJPxdgps",
    "title": "Why we need a *theory* of human values",
    "slug": "why-we-need-a-theory-of-human-values",
    "pageUrl": "https://www.lesswrong.com/posts/zvrZi95EHqJPxdgps/why-we-need-a-theory-of-human-values",
    "postedAt": "2018-12-05T16:00:13.711Z",
    "baseScore": 66,
    "voteCount": 30,
    "commentCount": 15,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head></head><body><p>There have been multiple <em>practical</em> suggestions for methods about how we should extract the values of a given human. Here are four common classes of such methods:</p>\n<ul>\n<li>Methods that put high weight on human (bounded) quasi-rationality, or revealed preferences. For example, we can assume the Kasparov was actually trying to win against DeepBlue, not trying desperately to lose while inadvertently playing excellent chess.</li>\n<li>Methods that pay attention to our explicitly stated values.</li>\n<li>Methods that use <a href=\"https://www.lesswrong.com/posts/Fg83cD3M7dSpSaNFg/normative-assumptions-regret\">regret</a>, surprise, joy, or similar emotions, to estimate what humans actually want. This could be seen as a form of human <a href=\"https://en.wikipedia.org/wiki/Temporal_difference_learning\">TD learning</a>.</li>\n<li>Methods based on an explicit procedure for constructing the values, such as <a href=\"https://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">CEV</a> and Paul's <a href=\"https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/\">indirect normativity</a>.</li>\n</ul>\n<h2>Divergent methods</h2>\n<p>The first question is why we would expect these methods to point even vaguely in the same direction. They all take very different approaches - why do we think they're measuring the same thing?</p>\n<p>The answer is that they roughly match up in situations we encounter everyday. In such typical situations, people who feel regret are likely to act to avoid that situation again, to express displeasure about the situation, etc.</p>\n<p>By analogy, consider a town where there are only two weather events: bright sunny days and snow storms. In that town there is a strong correlation between barometric pressure, wind speed, cloud cover, and temperature. All four indicators track different things, but, in this town, they are basically interchangeable.</p>\n<p>But if the weather grows more diverse, this correlation can <a href=\"https://www.lesswrong.com/posts/ix3KdfJxjo9GQFkCo/web-of-connotations-bleggs-rubes-thermostats-and-beliefs\">break down</a>. Rain storms, cloudy days, meteor impacts: all these can disrupt the alignment of the different indicators.</p>\n<p>Similarly, we expect that an AI could remove us from typical situations and put us into extreme situations - at least \"extreme\" from the perspective of the everyday world where we forged the intuitions that those methods of extracting values roughly match up. Not only do we expect this, but we desire this: a world without absolute poverty, for example, is the kind of world we would want the AI to move us into, if it could.</p>\n<p>In those extreme and unprecedented situations, we could end up with revealed preferences pointing one way, stated preferences another, while regret and CEV point in different directions entierly. In that case, we might be tempted to ask \"should we follow regret or stated preferences?\" But that would be the wrong question to ask: our methods no longer correlated with each other, let alone with some fundamental measure of human values.</p>\n<p>We are thus in an undefined state; in order to continue, we need a meta-method that decides between the different methods. But what criteria could such meta-method use for deciding (note that simply getting human feedback is <a href=\"https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search\">not generically an option</a>)? Well, it would have to select the method which best matches up with human values in this extreme situation. <strong>To do that, it needs a definition - a theory - of what human values actually are</strong>.</p>\n<h2>Underdefined methods</h2>\n<p>The previous section understates the problems with purely practical ways of assessing human values. It pointed out divergences between the methods in \"extreme situations\". Perhaps we were imagining these extreme situations as the equivalent of a meteor impact on weather system: bizarre edge cases where reasonable methods finally break down.</p>\n<p>But all those actually methods fail in typical situations as well. If we interpret the methods naively, they fail often. For example, in 1919, some of the Chicago White Sox baseball team <a href=\"https://en.wikipedia.org/wiki/Black_Sox_Scandal\">were actually trying to lose</a>. If we ask someone their stated values in a political debate or a courtroom, we don't expect an honest answer. Emotion based approaches fail in situations where humans deliberately expose themselves to nostalgia, or fear, or other \"negative\" emotions (eg through scary movies). And there are <a href=\"https://www.lesswrong.com/posts/vgFvnr7FefZ3s3tHp/mahatma-armstrong-ceved-to-death\">failure</a> <a href=\"https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search\">modes</a> for the explicit procedures, too.</p>\n<p>This is true if we interpret the methods naively. If we were more \"reasonable\" or \"sophisticated\", we would point out that don't expect those methods to be valid in every typical situation. In fact, we can do better than that: we have a good intuitive understanding of when the methods succeed and when they fail, and different people have similar intuitions (we all understand that people are more honest in relaxed private settings that stressful public ones, for example). It's as if we lived in a town with either sunny days or snow storms <em>except on weekends</em>. Then everyone could agree that the different indicators correlate during the week. So the more sophisticated methods would include something like \"ignore the data if it's Saturday or Sunday\".</p>\n<p>But there are problems with this analogy. Unlike for the weather, there are no clear principle for deciding when it's the equivalent of the weekend. Yes, we have an <em>intuitive</em> grasp of when stated preferences fail, for instance. But as <a href=\"https://en.wikipedia.org/wiki/Moravec%27s_paradox\">Moravec's paradox</a> shows, an intuitive understanding doesn't translate into an explicit, formal definition - and it's that kind of formal definition that we need if we want to code up those methods. Even worse, we <strong>don't</strong> all agree as to when the methods fail. For example, some economists <a href=\"http://econfaculty.gmu.edu/bcaplan/pdfs/szasz.pdf\">deny the very existence of mental illness</a>, while psychiatrists (and most laypeople) <a href=\"http://slatestarcodex.com/2015/10/07/contra-caplan-on-mental-illness/\">very much feel these exist</a>.</p>\n<h2>Human judgement and machine patching</h2>\n<p>So figuring out whether the methods apply is an exercise in human judgement. Figuring out whether the methods have gone wrong is a similar exercise (see the <a href=\"https://intelligence.org/files/CEV.pdf\">Last Judge</a> in CEV).\nAnd figuring out what to do when they don't apply is also an exercise in human judgement - if we judge that someone is lying about their stated preferences, we could just reverse their statement to get their true values.</p>\n<p>So we need to patch the methods using our human judgement. And probably <a href=\"https://arbital.com/p/patch_resistant/\">patch the patches</a> and so on. Not only is the patching process a terrible and incomplete way of constructing a safe goal for the AI, but human judgements are not consistent - we can be swayed in things as basic as whether a <a href=\"https://faculty.washington.edu/jmiyamot/p466/pprs/slovic%20who%20accepts%20savages%20axiom.pdf\">behaviour is rational</a>, let alone <a href=\"https://en.wikipedia.org/wiki/Availability_heuristic\">all</a> <a href=\"https://en.wikipedia.org/wiki/Belief_bias\">the</a> <a href=\"https://en.wikipedia.org/wiki/Framing_effect_(psychology)\">situational</a> <a href=\"https://en.wikipedia.org/wiki/Social_desirability_bias\">biases</a> that cloud our assessments of more complicated issues.</p>\n<p>So obviously, the solution to these problems is to figure out which human is best in their judgements, and then to see under what circumstances these judgements can be least biased, and how to present the information to them in the most impartial way and then automate that judgement...</p>\n<p><a href=\"https://www.youtube.com/watch?v=es4Yq7jP03w\">Stop that. It's silly.</a>. The correct solution is not to assess the rationality of human judgements of methods of extracting human values. The correct solution is to come up with a better theoretical definition of what human values are. Armed with such a theory, we can resolve or ignore the above issues in a direct and principled way.</p>\n<h1>Building a theory of human values</h1>\n<p>Just because we need a theory of human values, doesn't mean that it's easy to find one - the universe is cruel like that.</p>\n<p>A big part of my current approach is to build such a theory. I will present an overview of my theory in a subsequent post, though most of the pieces have appeared in past posts already. My approach uses three key components:</p>\n<ol>\n<li>A way of defining the basic preferences (and basic meta-preferences) of a given human, even if these are under-defined or situational.</li>\n<li>A method for synthesising such basic preferences into a single utility function or similar object.</li>\n<li>A guarantee we won't end up in a terrible place, due to noise or different choices in the two definitions above.</li>\n</ol>\n</body></html>",
    "user": {
      "username": "Stuart_Armstrong",
      "slug": "stuart_armstrong",
      "displayName": "Stuart_Armstrong"
    }
  },
  {
    "_id": "DFkGStzvj3jgXibFG",
    "title": "Factored Cognition",
    "slug": "factored-cognition",
    "pageUrl": "https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition",
    "postedAt": "2018-12-05T01:01:43.544Z",
    "baseScore": 45,
    "voteCount": 21,
    "commentCount": 6,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Note: This post (originally published <a href=\"https://ought.org/presentations/factored-cognition-2018-05\">here</a>) is the transcript of a presentation about a project worked on at the non-profit Ought. It is included in the sequence because it contains a very clear explanation of some of the key ideas behind iterated amplification.</em></p><hr class=\"dividerBlock\"/><p>The presentation below motivates our <a href=\"https://ought.org/projects/factored-cognition\">Factored Cognition</a> project from an AI alignment angle and describes the state of our work as of May 2018. Andreas gave versions of this presentation at CHAI (4/25), a Deepmind-FHI seminar (5/24) and FHI (5/25).</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/6be39959fb4e5d5b3e50647e3cde07f7f8e784d6/f8019/images/presentations/2018-factored-cognition/2018-factored-cognition.001.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>I&#x27;ll talk about <em>Factored Cognition</em>, our current main project at Ought. This is joint work with Ozzie Gooen, Ben Rachbach, Andrew Schreiber, Ben Weinstein-Raun, and (as board members) Paul Christiano and Owain Evans.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/fb2b85f4a20a4a0c3c08a7917aca28d4f2f4025c/bd685/images/presentations/2018-factored-cognition/2018-factored-cognition.002.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Before I get into the details of the project, I want to talk about the broader research program that it is part of. And to do that, I want to talk about research programs for AGI more generally.</p><p>Right now, the dominant paradigm for researchers who explicitly work towards AGI is what you could call &quot;scalable learning and planning in complex environments&quot;. This paradigm substantially relies on training agents in simulated physical environments to solve tasks that are similar to the sorts of tasks animals and humans can solve, sometimes in isolation and sometimes in competitive multi-agent settings.</p><p>To be clear, not all tasks are physical tasks. There&#x27;s also interest in more abstract environments as in the case of playing Go, proving theorems, or participating in goal-based dialog.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/ea7928e125aaa8313e39238cafbb616ee6b62267/10bc6/images/presentations/2018-factored-cognition/2018-factored-cognition.003.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>For our purposes, the key characteristic of this research paradigm is that agents are optimized for success at particular tasks. To the extent that they learn particular decision-making strategies, those are learned implicitly. We only provide external supervision, and it wouldn&#x27;t be entirely wrong to call this sort of approach &quot;recapitulating evolution&quot;, even if this isn&#x27;t exactly what is going on most of the time.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/74df49dad1cd55212b04e8804a4786e9653f00d8/af287/images/presentations/2018-factored-cognition/2018-factored-cognition.004.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>As many people have pointed out, it could be difficult to become confident that a system produced through this sort of process is aligned - that is, that all its cognitive work is actually directed towards solving the tasks it is intended to help with. The reason for this is that alignment is a property of the decision-making process (what the system is &quot;trying to do&quot;), but that is unobserved and only implicitly controlled.</p><p>Aside: Could more advanced approaches to transparency and interpretability help here? They&#x27;d certainly be useful in diagnosing failures, but unless we can also leverage them for training, we might still be stuck with architectures that are difficult to align.</p><p>What&#x27;s the alternative? It is what we could call <em>internal supervision</em> - supervising not just input-output behavior, but also cognitive processes. There is some prior work, with <a href=\"https://arxiv.org/abs/1511.06279?context=cs\">Neural Programmer-Interpreters</a> perhaps being the most notable instance of that class. However, depending on how you look at it, there is currently much less interest in such approaches than in end-to-end training, which isn&#x27;t surprising: A big part of the appeal of AI over traditional programming is that you don&#x27;t need to specify how exactly problems are solved.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/54a3df8b543ae0bbd61706b21ef5b83d02d5f636/81a6b/images/presentations/2018-factored-cognition/2018-factored-cognition.005.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>In this talk, I&#x27;ll discuss an alternative research program for AGI based on internal supervision. This program is based on imitating human reasoning and meta-reasoning, and will be much less developed than the one based on external supervision and training in complex environments.</p><p>The goal for this alternative program is to codify reasoning that people consider &quot;good&quot; (&quot;helpful&quot;, &quot;corrigible&quot;, &quot;conservative&quot;). This could include some principles of good reasoning that we know how to formalize (such as probability theory and expected value calculations), but could also include heuristics and sanity checks that are only locally valid.</p><p>For a system built this way, it could be substantially easier to become confident that it is aligned. Any bad outcomes would need to be produced through a sequence of human-endorsed reasoning steps. This is far from a guarantee that the resulting behavior is good, but seems like a much better starting point. (See e.g. <a href=\"http://effective-altruism.com/ea/1ca/my_current_thoughts_on_miris_highly_reliable/#s6\">Dewey 2017</a>.)</p><p>The hope would be to (wherever possible) punt on solving hard problems such as what decision theory agents should use, and how to approach epistemology and value learning, and instead to build AI systems that inherit our epistemic situation, i.e. that are uncertain about those topics to the extent that we are uncertain.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/454457c86217a6038ebf38417319eb6fd8958c8d/0680c/images/presentations/2018-factored-cognition/2018-factored-cognition.006.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>I&#x27;ve described external and internal supervision as different approaches, but in reality there is a spectrum, and it is likely that practical systems will combine both.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/c7660f4fdddb6c50b22b407010d0c99b6e3f4847/93365/images/presentations/2018-factored-cognition/2018-factored-cognition.007.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>However, the right end of the spectrum - and especially approaches based on learning to reason from humans - are more neglected right now. Ought aims to specifically make progress on automating human-like or human-endorsed deliberation.</p><p>A key challenge for these approaches is scalability: Even if we could learn to imitate how humans solve particular cognitive tasks, that wouldn&#x27;t be enough. In most cases where we figured out how to automate cognition, we didn&#x27;t just match human ability, but exceeded it, sometimes by a large margin. Therefore, one of the features we&#x27;d want an approach to AI based on imitating human metareasoning to have is a story for how we could use that approach to eventually exceed human ability.</p><p>Aside: Usually, I fold &quot;aligned&quot; into the definition of <a href=\"https://ought.org/projects/factored-cognition/scalability\">&quot;scalable&quot;</a> and describe <a href=\"https://ought.org/mission\">Ought&#x27;s mission</a> as &quot;finding scalable ways to leverage ML for deliberation&quot;.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/508946135c71bd71409fe0b0c0a2f148936c4576/e056f/images/presentations/2018-factored-cognition/2018-factored-cognition.008.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>What does it mean to &quot;automate deliberation&quot;? Unlike in more concrete settings such as playing a game of Go, this is not immediately clear.</p><p>For Go, there&#x27;s a clear task (choose moves based on a game state), there&#x27;s relevant data (recorded human games), and there&#x27;s an obvious objective (to win the game). For deliberation, none of these are obvious.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/1e153c11ced790d841d13b117e5f3ed01f8f7333/f35c0/images/presentations/2018-factored-cognition/2018-factored-cognition.009.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>As a task, we&#x27;ll choose question-answering. This encompasses basically all other tasks, at least if questions are allowed to be big (i.e. can point to external data).</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/9c6ab911c223b1284fa5b56c12caadb19de61ab5/c6952/images/presentations/2018-factored-cognition/2018-factored-cognition.010.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>The data we&#x27;ll train on will be recorded human actions in <em>cognitive workspaces</em>. I&#x27;ll show an example in a couple of slides. The basic idea is to make thinking explicit by requiring people to break it down into small reasoning steps, to limit contextual information, and to record what information is available at each step.</p><p>An important point here is that our goal is not to capture human reasoning exactly as it is in day-to-day life, but to capture <em>a</em> way of reasoning that people would endorse. This is important, because the strategies we need to use to make thinking explicit will necessarily change how people think.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/cd9b160c93fca0dcf0140b788baa5bc2c3cf3aa0/775a5/images/presentations/2018-factored-cognition/2018-factored-cognition.011.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Finally, the objective will be to choose cognitive actions that people would endorse after deliberation.</p><p>Note the weird loop - since our task is automating deliberation, the objective is partially defined in terms of the behavior that we are aiming to improve throughout the training process. This suggests that we might be able to set up training dynamics where the supervision signal always stays a step ahead of the current best system, analogous to GANs and self-play.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/c7b679dee25b3774306cd83b21afe44787f4f865/3f2fa/images/presentations/2018-factored-cognition/2018-factored-cognition.012.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>We can decompose the problem of automating deliberation into two parts:</p><ol><li>How can we make deliberation sufficiently explicit that we could in principle replicate it using machine learning? In other words, how do we generate the appropriate kind of training data?</li><li>How do we actually automate it?</li></ol><p>In case you&#x27;re familiar with <a href=\"https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616\">Iterated Distillation and Amplification</a>: The two parts roughly correspond to amplification (first part) and distillation (second part).</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/0a54b6d496a266cba3c8c52977e44cf1d13e03b8/afb49/images/presentations/2018-factored-cognition/2018-factored-cognition.013.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>The core concept behind our approach is that of a <em>cognitive workspace</em>. A workspace is associated with a question and a human user is tasked with making progress on thinking through that question. To do so, they have multiple actions available:</p><ul><li>They can reply to the question.</li><li>They can edit a scratchpad, writing down notes about intermediate results and ideas on how to make progress on this question.</li><li>They can ask sub-questions that help them answer the overall question.</li></ul><p>Sub-questions are answered in the same way, each by a different user. This gives rise to a tree of questions and answers. The size of this tree is controlled by a budget that is associated with each workspace and that the corresponding user can distribute over sub-questions.</p><p>The approach we&#x27;ll take to automating cognition is based on recording and imitating actions in such workspaces. Apart from information passed in through the question and through answers to sub-questions, each workspace is isolated from the others. If we show each workspace to a different user and limit the total time for each workspace to be short, e.g. 15 minutes, we <em>factor</em> the problem-solving process in a way that guarantees that there is no unobserved latent state that is accumulated over time.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/303430ecf4a5e40955d9ff2fce01f027007ec0a1/db669/images/presentations/2018-factored-cognition/2018-factored-cognition.014.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>There are a few more technicalities that are important to making this work in practice.</p><p>The most important one is probably the use of <em>pointers</em>. If we can only ask plain-text questions and sub-questions, the bandwidth of the question-answering system is severely limited. For example, we can&#x27;t ask &quot;Why did the protagonist crash the car in book X&quot; because the book X would be too large to pass in as a literal question. Similarly, we can&#x27;t delegate &quot;Write an inspiring essay about architecture&quot;, because the essay would be too large to pass back.</p><p>We can lift this restriction by allowing users to create and pass around pointers to datastructures. A simple approach for doing this is to replace plain text everywhere with <em>messages</em> that consist of text interspersed with references to other messages.</p><p>The combination of pointers and short per-workspace time limits leads to a system where many problems are best tackled in an algorithmic manner. For example, in many situations all a workspace may be doing is mapping a function (represented as a natural language message) over a list (a message with linked list structure), without the user knowing or caring about the content of the function and list.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/bbd6b53ee2143649e308e7fd3626beee40958434/1f468/images/presentations/2018-factored-cognition/2018-factored-cognition.015.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Now let&#x27;s try to be a bit more precise about the parts of the system we&#x27;ve seen.</p><p>One component is the <em>human policy</em>, which we treat as a stateless map from contexts (immutable versions of workspaces) to actions (such as asking a particular sub-question).</p><p>Coming up with a single such actions should take the human at most a few minutes.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/3524a411b2942a0466cb024df747fbd95bcee06a/46d8f/images/presentations/2018-factored-cognition/2018-factored-cognition.016.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>The other main component is the transition function, which consumes a context and an action and generates a set of new contexts.</p><p>For example, if the action is to ask a sub-question, there will be two new contexts:</p><ol><li>The successor of the parent context that now has an additional reference to a sub-question.</li><li>The initial context for the newly generated sub-question workspace.</li></ol><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/ca91584f7d8a81c8e01e28a9b798e5a613f33042/9e6ae/images/presentations/2018-factored-cognition/2018-factored-cognition.017.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Composed together, the human policy and the transition function define a kind of evaluator: A map from a context to a set of new contexts.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/2eaf5007e924d8a20d86f9c41bbd03b891b0f452/dfb68/images/presentations/2018-factored-cognition/2018-factored-cognition.018.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>In what follows, nodes (depicted as circles) refer to workspaces. Note that both inputs and outputs of workspaces can be messages with pointers, i.e. can be very large objects.</p><p>I&#x27;ll mostly collapse workspaces to just questions and answers, so that we can draw entire trees of workspaces more easily.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/e9c404a8d7d1e56589aa83aa973f2716166c7bc7/f3bb6/images/presentations/2018-factored-cognition/2018-factored-cognition.019.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>By iteratively applying the evaluator, we generate increasingly large trees of workspaces. Over the course of this process, the root question will become increasingly informed by answers to sub-computations, and should thus become increasingly correct and helpful. (What exactly happens depends on how the transition function is set up, and what instructions we give to the human users.)</p><p>This process is essentially identical to what Paul Christiano refers to as <a href=\"https://ai-alignment.com/policy-amplification-6a70cbee4f34\">amplification</a>: A single amplification step augments an agent (in our case, a human question-answerer) by giving it access to calls to itself. Multiple amplification steps generate trees of agents assisting each other.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/9e4fd698555f10555899d517ee75910d8c8bdd50/be128/images/presentations/2018-factored-cognition/2018-factored-cognition.020.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>I&#x27;ll now walk through a few examples of different types of thinking by recursive decomposition.</p><p>The longer-term goal behind these examples is to understand: How decomposable is cognitive work? That is, can amplification work - in general, or for specific problems, with or without strong bounds on the capability of the resulting system?</p><p>Perhaps the easiest non-trivial case is arithmetic: To multiply two numbers, we can use the rules of addition and multiplication to break down the multiplication into a few multiplications of smaller numbers and add up the results.</p><p>If we wanted to scale to very large numbers, we&#x27;d have to represent each number as a nested pointer structure instead of plain text as shown here.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/c7349fe967bfc0787cd849d2d6e67f05ed6b713a/81473/images/presentations/2018-factored-cognition/2018-factored-cognition.021.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>We can also implement other kinds of algorithms. Here, we&#x27;re given a sequence of numbers as a linked list and we sum it up one by one. This ends up looking pretty much the same as how you&#x27;d sum up a list of numbers in a purely functional programming language such as Lisp or Scheme.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/3f0238070d9db050a7fec7836db39ae8625e6986/dc0f1/images/presentations/2018-factored-cognition/2018-factored-cognition.022.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Indeed, we can implement <em>any</em> algorithm using this framework - it is computationally universal. One way to see this is to implement an evaluator for a programming language, e.g. following the example of the meta-circular evaluator in SICP.</p><p>As a consequence, if there&#x27;s a problem we can&#x27;t solve using this sort of framework, it&#x27;s not because the framework can&#x27;t run the program required to solve it. It&#x27;s because the framework can&#x27;t <em>come up with</em> the program by composing short-term tasks.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/fe992ccfc94ad37d9bafad7ef95f023016d02193/6a654/images/presentations/2018-factored-cognition/2018-factored-cognition.023.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Let&#x27;s start moving away from obviously algorithmic examples. This example shows how one could generate a Fermi estimate of a quantity by combining upper and lower bounds for the estimates of component quantities.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/d10c37469c2b316b533793902efb68bee9490bac/d17bf/images/presentations/2018-factored-cognition/2018-factored-cognition.024.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>This example hints at how one might implement conditioning for probability distributions. We could first generate a list of all possible outcomes together with their associated probabilities, then filter the list of outcomes to only include those that satisfy our condition, and renormalize the resulting (sub-)distribution such that the probabilities of all outcomes sum to one again.</p><p>The general principle here is that we&#x27;re happy to run very expensive computations as long as they&#x27;re semantically correct. What I&#x27;ve described for conditioning is more or less the textbook definition of exact inference, but in general that is computationally intractable for distributions with many variables. The reason we&#x27;re happy with expensive computations is that eventually we won&#x27;t instantiate them explicitly, but rather emulate them using cheap ML-based function approximators.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/192a2157901f6b49a3d324fc3acfaa379d804399/9125d/images/presentations/2018-factored-cognition/2018-factored-cognition.025.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>If we want to use this framework to implement agents that can eventually exceed human capability, we can&#x27;t use most human object-level knowledge, but rather need to set up a process that can learn human-like abilities from data in a more scalable way.</p><p>Consider the example of understanding natural language: If we wanted to determine whether a pair of sentences is a contradiction, entailment, or neutral (as in the SNLI dataset), we could simply ask the human to judge - but this won&#x27;t scale to languages that none of the human judges know.</p><p>Alternatively, we can break down natural language understanding into (very) many small component tasks and try to solve the task without leveraging the humans&#x27; native language understanding facilities much. For example, we might start by computing the meaning of a sentence as a function of the meanings of all possible pairs of sub-phrases.</p><p>As in the case of probabilistic inference, this will be computationally intractable, and getting the decomposition right in the first place is substantially harder than solving the object-level task.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/91fe0385538a8f7ccc9afbeb1d3f72f3f4b1ffe5/b369e/images/presentations/2018-factored-cognition/2018-factored-cognition.026.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Here&#x27;s a class of problems that seems particularly challenging for factored cognition: Problems where people would usually learn concepts over an extended period of time.</p><p>Consider solving a problem that is posed halfway through a math textbook. Usually, the textbook reader would have solved many simpler problems up to this point and would have built up conceptual structures and heuristics that then allow them to solve this new problem. If we need to solve the problem by composing work done by a large collection of humans, none of which can spend more than 15 minutes on the task, we&#x27;ll have to replace this intuitive, implicit process with an externalized, explicit alternative.</p><p>It&#x27;s not entirely clear to me how to do that, but one way to start would be to build up knowledge about the propositions and entities that are part of the problem statement by effectively applying semantic parsing to the relevant parts of the textbook, so that we can later ask whether (e.g.) a proposition with meaning X implies a proposition with meaning Y, where both X and Y are large nested pointer structures that encode detailed meaning representations.</p><p>If this reminds you of Good Old-Fashioned AI, it is not by accident. We&#x27;re essentially trying to succeed where GOFAI failed, and our primary advantage is that we&#x27;re okay with exponentially expensive computations, because we&#x27;re not planning to ever run them directly. More on that soon.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/4cbf8add640c0b095448632528a771d70542f43a/674a6/images/presentations/2018-factored-cognition/2018-factored-cognition.027.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>So far, the workspaces we&#x27;ve looked at were quite sparse. All questions and answers were limited to a sentence or two. This &quot;low-bandwidth&quot; setting is not the only way to use the system - we could alternatively instruct the human users to provide more detail in their questions and to write longer answers.</p><p>For the purpose of automation, low bandwidth has advantages, both in the short term (where it makes automation easier) and in the long term (where it reduces a particular class of <a href=\"https://ai-alignment.com/security-amplification-f4931419f903\">potential security vulnerabilities</a>).</p><p>Empirical evidence from experiments with humans will need to inform this choice as well, and the correct answer is probably at least slightly more high-bandwidth than the examples shown so far.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/35633c014364b25655b50ab48b9d59927ac918ea/37f36/images/presentations/2018-factored-cognition/2018-factored-cognition.028.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Here&#x27;s a kind of reasoning that I feel relatively optimistic that we can implement using factored cognition: Causal reasoning, both learning causal structures from data as well as computing the results of interventions and counterfactuals.</p><p>The particular tree of workspaces shown here doesn&#x27;t really illustrate this, but I can imagine implementing Pearl-style algorithms for causal inference in a way where each step locally makes sense and slightly simplifies the overall problem.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/ad284ca72657907876d66c9b687dca2e4cd3577f/c35c5/images/presentations/2018-factored-cognition/2018-factored-cognition.029.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>The final example, meta-reasoning, is in some ways the most important one: If we want factored cognition to eventually produce <em>very</em> good solutions to problems - perhaps being competitive with any other systematic approach - then it&#x27;s not enough to rely on the users directly choosing a good object-level decomposition for the problem at hand. Instead, they&#x27;ll need to go meta and use the system to reason about what decompositions would work well, and how to find them.</p><p>One kind of general pattern for this is that users can ask something like &quot;What approach should we take to problem <code>#1</code>?&quot; as a first sub-problem, get back an answer <code>#2</code>, and then ask &quot;What is the result of executing approach <code>#2</code> to question <code>#1</code>?&quot; as a second sub-question. As we increase the budget for the meta-question, the object-level approach can change radically.</p><p>And, of course, we could also go meta twice, ask about approaches to solving the first meta-level problem, and the same consideration applies: Our meta-level approach to finding good object-level approaches could improve substantially as we invest more budget in meta-meta.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/26e245928a3b970e7bcce0999d85a891693c0402/8c22a/images/presentations/2018-factored-cognition/2018-factored-cognition.030.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>So far, I&#x27;ve shown one particular instantiation of factored cognition: a way to structure workspaces, a certain set of actions, and a corresponding implementation of the transition function that generates new workspace versions.</p><p>By varying each of these components, we can generate other ways to build systems in this space. For example, we might include actions for asking clarifying questions. I&#x27;ve written about these degrees of freedom on <a href=\"https://ought.org/projects/factored-cognition/taxonomy\">our taxonomy page</a>.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/b1d075f4b5a01a624e7f6606eb6dba5408399fbf/d595a/images/presentations/2018-factored-cognition/2018-factored-cognition.031.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Here&#x27;s one example of an alternate system. This is a straightforward Javascript port of parts of Paul Christiano&#x27;s <a href=\"https://github.com/paulfchristiano/alba\">ALBA implementation</a>.</p><p>Workspaces are structured as sequences of observations and actions. All actions are commands that the user types, including <code>ask</code>, <code>reply</code>, <code>view</code> (for expanding a pointer), and <code>reflect</code> (for getting a pointer to the current context).</p><p>The command-line version is <a href=\"https://github.com/oughtinc/hch\">available on Github</a>.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/ea07b6f0effc9d99a1f60ce79d074e898d2573b3/26745/images/presentations/2018-factored-cognition/2018-factored-cognition.032.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>A few days ago, we open-sourced <a href=\"https://github.com/oughtinc/patchwork\">Patchwork</a>, a new command-line app for recursive question-answering where we paid particular attention to build it in a way that is a good basis for multiple users and automation. To see a brief screencast, take a look at <a href=\"https://github.com/oughtinc/patchwork#patchwork\">the README</a>.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/e9951d4132effbdab226d651f10215695b888b39/1b2b2/images/presentations/2018-factored-cognition/2018-factored-cognition.033.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Suppose decomposition worked and we could solve difficult problems using factored cognition - how could we transition from only using human labor to partial automation and eventually full automation? I&#x27;ll discuss a few approaches, starting from very basic ideas that we can implement now and progressing to ones that will not be tractable using present-day ML.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/11ba537b4692e9da8c84334f4c204e28343b7ee5/a2713/images/presentations/2018-factored-cognition/2018-factored-cognition.034.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Let&#x27;s again consider a tree of workspaces, and in each workspace, one or more humans taking one or more actions.</p><p>For simplicity, I&#x27;ll pretend that there is just a single action per workspace. This allows me to equivocate nodes and actions below. Nothing substantial changes if there are multiple actions.</p><p>I&#x27;ll also pretend that all humans are essentially identical, which is obviously false, but allows me to consider the simpler problem of learning a single human policy from data.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/e992bf095cc21ff73ba9ebedba7f1e200df1d9c8/8a6fa/images/presentations/2018-factored-cognition/2018-factored-cognition.035.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>As a first step towards automation, we&#x27;ll <em>memoize</em> the human H. That is, whenever we would show a context to H, we first check whether we&#x27;ve shown this context to some other H before, and if so, we directly reuse the action that was taken previously.</p><p>This is a big win if many contexts are simple. For example, it may be very common to want to map a function over a list, and this operation will always involve the same kinds of sub-questions (check if the list is empty, if not get the first element, apply the function to the first element, etc). Ideally, we only do this sort of work once and then reuse it in the future. Memoization gets us part of the way there.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/096480f24d535a04dc10a3d7d5833530bd78c27a/1daac/images/presentations/2018-factored-cognition/2018-factored-cognition.036.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>A significant step up in difficulty, we can try to imitate the behavior of H even in cases where the new context doesn&#x27;t match any element of our &quot;training set&quot; exactly.</p><p>Of course, for general question-answering, we won&#x27;t be able to fully automate the human policy any time soon. This means that any imitation algorithms we do apply will need to make choices about whether a context under consideration is the sort of situation where they can make good predictions about what a human would do, or whether to abstain.</p><p>If algorithms can make multiple choices in sequence, we need algorithms that are well-calibrated about when their actions are appropriate, and that in particular have very few false positives. Otherwise, even a relatively low probability of false positives could cascade into sequences of inappropriate actions.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/408c80185343288042ebe145c2af4f3c451b9509/c369a/images/presentations/2018-factored-cognition/2018-factored-cognition.037.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>We&#x27;ve tried to isolate and study this particular problem - making well-calibrated predictions in AI-complete domains - in a separate project called <a href=\"https://ought.org/projects/judgments\">Predicting Slow Judgments</a>. So far, we&#x27;ve found it challenging to make non-trivial predictions about human responses for the dataset we&#x27;ve collected there.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/096480f24d535a04dc10a3d7d5833530bd78c27a/5d066/images/presentations/2018-factored-cognition/2018-factored-cognition.038.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>How useful would it be to be able to automate some fraction of human actions? If the total number of actions needed to solve a task is exponentially large (e.g. because we&#x27;re enumerating all potential sub-phrases of a paragraph of text), even being able to automate 90% of all actions wouldn&#x27;t be enough to make this approach computationally tractable. To get to tractability in that regime, we need to automate entire subtrees. (And we need to do so using an amount of training data that is not itself exponentially large - an important aspect that this talk won&#x27;t address at all.)</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/6a42c974c0f1d72be7b098f806d12f582484c415/cf7e2/images/presentations/2018-factored-cognition/2018-factored-cognition.039.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Let&#x27;s reconsider amplification. Recall that in this context, each node represents the question-answer behavior implemented by a workspace operated on by some agent (to start with, a human). This agent can pose sub-questions to other agents who may or may not themselves get to ask such sub-questions, as indicated by whether they have nodes below them or not.</p><p>Each step grows the tree of agents by one level, so after <em>n</em> steps, we have a tree of size <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"O(2^n)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span>. This process will become intractable before long.</p><p>(The next few slides describe Paul Christiano&#x27;s <a href=\"https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616\">Iterated Distillation and Amplification</a> approach to training ML systems.)</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/eafcb397f690d5b66678dc909c1cc44e474bf333/410d5/images/presentations/2018-factored-cognition/2018-factored-cognition.040.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Instead of iterating amplification, let&#x27;s pause after one step. We started out with a single agent (a human) and then built a composite system using multiple agents (also all humans). This composite system is slower than the one we started out with. This slowdown perhaps isn&#x27;t too bad for a single step, but it will add up over the course of multiple steps. To iterate amplification many times, we need to avoid this slowdown. What can we do?</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/9e0fe35b1471c0db3eb21476005ce3c52a502764/9d576/images/presentations/2018-factored-cognition/2018-factored-cognition.041.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>The basic idea is to train an ML-based agent to imitate the behavior of the composite system. A simple (but insufficient!) approach would be to generate training data - questions and answers - based on the behavior of the composite system, and to train a supervised learner using this dataset.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/3ae01b6a2ef4d50a6eb8416020912b36b2b196b7/d4558/images/presentations/2018-factored-cognition/2018-factored-cognition.042.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>In practice, this sort of training (&quot;distillation&quot;) would probably need to involve not just imitation, but more advanced techniques, including adversarial training and approaches to interpretability that allow the composite system (the &quot;overseer&quot;) to reason about the internals of its fast ML-based successor.</p><p>If we wanted to implement this training step in rich domains, we&#x27;d need ML techniques that are substantially better than the state of the art as of May 2018, and even then, some domains would almost certainly resist efficient distillation.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/7574b8b4ee4bcb049f331c4f14bac19bfb9262c2/76bfe/images/presentations/2018-factored-cognition/2018-factored-cognition.043.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>But, hypothetically, if we could implement faithful distillation, we would have a much better starting point for the next amplification step: We could compose together multiple instances of the fast ML-based learner, and the result would be a tree of agents that is only as large as the one we built in the first step (3 nodes, say), but exhibits the question-answer behavior of an agent that has multiple advisors, <em>each of which</em> as capable as the entire tree at the first step.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/a1f8506b7a3f22018cc39e36487c8b5401ee613a/f5157/images/presentations/2018-factored-cognition/2018-factored-cognition.045.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>We can repeat whatever training process we used in the first step to get a yet better distilled system that &quot;imitates&quot; the behavior of the overseer composed of the systems trained in the previous step.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/f14d3931093ce1675d156a212a58e0c29717c902/49b68/images/presentations/2018-factored-cognition/2018-factored-cognition.048.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>Through repeated amplification and distillation, we could hope to eventually satiate the representational and computational abilities of whatever ML system we&#x27;re using in the distillation step, while guiding it towards implementing question-answer behavior that corresponds to what H would do if they had a large number of well-resourced assistants.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/42fc30168878c61d64e354be998698c728ac8a5e/a1e7f/images/presentations/2018-factored-cognition/2018-factored-cognition.049.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>In practice, we might not want to implement this process as a series of distinct systems, and instead run self-play where a single system serves both as the overseer and the system-to-be-trained.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/9c3537f9663bb3e3140eae9ad3fe18dc1289f5a9/1e178/images/presentations/2018-factored-cognition/2018-factored-cognition.050.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>If Iterated Amplification and Distillation can work, we might be able to approximate the results of running some computations that would naively take exponential time: after <em>n</em> steps of amplification and distillation, we&#x27;d use a fast ML-based approximator to run computations that would take <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"O(2^n)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">O</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span> time if we instantiated them explicitly.</p><p>As a particularly interesting special case, this might include the kinds of human-guided computations that arise from people take actions in cognitive workspaces.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/94821e8abf48007005de7e20b0355e8560420523/a5fcc/images/presentations/2018-factored-cognition/2018-factored-cognition.051.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>There are many open questions for the scheme described above, both on whether we can make reasoning explicit, and on whether we can automate it efficiently even if it is made explicit. While I&#x27;ve talked a bit about automation, anything beyond basic automation is out of scope for Ought right now, so I&#x27;ll focus on open questions related to decomposition.</p><p>For decomposition, the two main questions we ask ourselves are:</p><ol><li>Can factored cognition recover the ability of a single human working over time for essentially all important tasks?</li><li>If so, can we exceed the capability of other systematic approaches to problem-solving if we just use sufficiently large budgets, i.e. compose sufficiently many small workspaces in sufficiently large trees? Equivalently, can we reach essentially arbitrarily high capability if we execute sufficiently many amplifications steps?</li></ol><p>Our plan is to study both of these questions using a set of challenge problems.</p><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/6d39cded49702c561e98a22fc098e05aebd40e8c/1c581/images/presentations/2018-factored-cognition/2018-factored-cognition.052.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>The idea behind these challenge problems is to pick problems that are particularly likely to stretch the capabilities of problem solving by decomposition:</p><ol><li>When people tackle tricky <em>math or programming puzzles</em>, they sometimes give up, go to bed, and the next day in the shower they suddenly know how to solve it. Can we solve such puzzles even if no single individual spends more than 15 minutes on the problem?</li><li>We&#x27;ve already seen a math textbook example earlier. We want to know more generally whether we can replicate the effects of learning over time, and are planning to study this using different kinds of <em>textbook problems</em>.</li><li>Similarly, when people reason about evidence, e.g. about whether a statement that a politician made is true, they seem to make incremental updates to opaque internal models and may use heuristics that they find difficult to verbalize. If we instead require all evidence to be aggregated explicitly, can we still match or exceed their <em>fact-checking</em> capabilities?</li><li>All examples of problems we&#x27;ve seen are one-off problems. However, ultimately we want to use automated systems to interact with a stateful world, e.g. through <em>dialog</em>. Abstractly, we know <a href=\"https://ought.org/projects/factored-cognition/taxonomy#interaction\">how to approach this situation</a>, but we&#x27;d like to try it in practice e.g. on personal questions such as &quot;Where should I go on vacation?&quot;.</li><li>For systems to scale to high capability, we&#x27;ve noted earlier that they will need to reason about cognitive strategies, not just object-level facts. <em>Prioritizing tasks </em>for a user might be a domain particularly suitable for testing this, since the same kind of reasoning (what to work on next) could be used on both object- and meta-level.</li></ol><p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/6fd397a17925c367c99a46436e707ce0f3e3ba5e/3d259/images/presentations/2018-factored-cognition/2018-factored-cognition.053.jpeg\" class=\"draft-image \" style=\"\" /></figure></p><p>If we make progress on the feasibility of factored cognition and come to believe that it might be able to match and eventually exceed &quot;normal&quot; thinking, we&#x27;d like to move towards learning more about how this process would play out.</p><p>What would the human policy - the map from contexts to actions - look like that would have these properties? What concepts would be part of this policy? For scaling to high capability, it probably can&#x27;t leverage most of the object-level knowledge people have. But what else? Abstract knowledge about how to reason? About causality, evidence, agents, logic? And how big would this policy be - could we effectively treat it as a lookup table, or are there many questions and answers in distinct domains that we could only really learn to imitate using sophisticated ML?</p><p>What would happen if we scaled up by iterating this learned human policy many times? What instructions would the humans that generate our training data need to follow for the resulting system to remain corrigible, even if run with extremely large amounts of computation (as might be the case if distillation works)? Would the behavior of the resulting system be chaotic, strongly dependent on its initial conditions, or could we be confident that there is a <a href=\"https://ai-alignment.com/corrigibility-3039e668638\">basin of attraction</a> that all careful ways of setting up such a system converge to?</p>",
    "user": {
      "username": "stuhlmueller",
      "slug": "stuhlmueller",
      "displayName": "stuhlmueller"
    }
  },
  {
    "_id": "wQofcJZzySGLqiw5N",
    "title": "Playing Politics",
    "slug": "playing-politics",
    "pageUrl": "https://www.lesswrong.com/posts/wQofcJZzySGLqiw5N/playing-politics",
    "postedAt": "2018-12-05T00:30:00.996Z",
    "baseScore": 97,
    "voteCount": 45,
    "commentCount": 45,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p><em>Epistemic Status: Guesses Based on Personal Experience</em></p>\n<p>Lately I’ve been going through a family of learning experiences in the world of <em>how to get things done cooperatively</em>.  It’s hard for me.  Even very basic things in this area have been stumping me, overwhelming me, leaving me way more tired and drained than I’d expect. My productivity has gone to hell and — worse — I didn’t even <em>notice </em>for a while.  This is hard stuff, and rarely written about by the people for whom it’s hard, so my hope is that processing in public helps someone. I generally think that data-sharing is good and helpful.</p>\n<p><strong>Collective Deliberation Isn’t Working For Me</strong></p>\n<p>At a conference, I was in a room full of people having a really good discussion. I wanted to get people together to have a follow-up discussion later — nothing elaborate, just a room with whiteboards and snacks and maybe moving towards some action items.</p>\n<p>What I did:</p>\n<ul>\n<li>Passed around a sheet for emails to sign up</li>\n<li>Sent out an email proposing the parameters of the event</li>\n<li>Waited for people to propose dates that worked for them.</li>\n</ul>\n<p>Radio silence.</p>\n<p>Somebody else suggested a poll where people could put down their preferred times and dates.  Out of thirteen people, five signed up.  Nobody volunteered “ok, we’re doing it on this date then,” so I did.  I reserved a conference room at my office and bought a bunch of snacks.</p>\n<p>The front door was locked on the weekend and my key card didn’t work even though it was supposed to, so I had to switch locations at the last minute. It wouldn’t have mattered anyhow, because one person showed up on time, and one other person several hours late.</p>\n<p>Conclusion: it is harder than I thought to get ten people to show up in a room and talk to each other.</p>\n<p>And I probably shouldn’t have expected an event to coalesce naturally from the mailing list.  I have a strong “egalitarian” instinct that if I’m trying to do something <em>with</em> a group and in some sense <em>for the benefit of everyone in the group, </em>then I shouldn’t be too “bossy” in terms of unilaterally declaring what we’re all going to do.  But if I leave it up to the group to discuss, it seems like they generally…don’t.</p>\n<p>I’m also on a policy committee for a community organization, and it’s been a whole lot of heartache because I want to change some things about our policies and internal processes, and the process of trying to communicate that has resulted in a <em>lot </em>of hurt feelings, mine and other people’s.</p>\n<p>The first thing I did was write up a document explaining why I thought the existing policies were harmful, and share it with the mailing list.  This resulted in <em>DRAMA</em> because people heard it as a personal accusation.  (I never meant to imply that my fellow committee members were bad people, but I felt strongly about the policy changes and my writing tone may have come out angrier than I intended.)</p>\n<p>In retrospect, I should never have led with complaints — I should have started by proposing solutions.  My intention had been to raise the issues I cared about while minimizing bossiness — this is an organization for the benefit of a larger community, and I’m only one member of a committee, so I thought it would <em>leave more degrees of freedom open to the group </em>to say “here’s why the existing policies have problems, what do you think we should do?” rather than “here’s how I’d suggest improving the existing policies.”  I thought this was the <em>considerate </em>way to communicate.  But from the committee’s perspective, it must have sounded like “You’re doing it wrong. Here’s a bunch more work you have to do to fix it. You’re welcome!”  They were actually much <em>more </em>receptive once I wrote up a revised set of policies that I’d be happier with.  Once again, being “unbossy” and hoping that collaborative discussion would resolve the issue was a <em>total failure, </em>because people had less bandwidth to engage in discussion than I’d anticipated.</p>\n<p><strong>Private Discussions Are A Flawed Solution</strong></p>\n<p>I’ve noticed that in a lot of deliberative bodies or organizations, the real decision-making doesn’t happen in groups.  (<em>Meanwhile Madison is grappling with the fact that/ Not every issue can be settled in committee.</em>)  The people who have “real power” meet in private and hash things out off the record.  Nobody really shares their full thoughts on the internet or on an email list.  It’s not necessarily “secrecy”, but it’s secrecy-adjacent.</p>\n<p>I know this is how things are frequently done, but it bothers me.  When an issue is <em>officially the jurisdiction of a committee</em>, everyone on the committee is equally entitled to be part of the discussion, and entitled to know what’s going on; having secret side conversations creates a hierarchy between those “in the know” and those who aren’t.  (<em>No-one else was in the room where it happened/ the room where it happened/ the room where it happened.)</em>  Still more, when your project is supposed to <em>be for the sake of, and with the participation of, a broader community</em>, it seems like fairness demands being transparent with that community.</p>\n<p>Maybe this is just the geek-kid issue, or what people today tend to call the <a href=\"http://www.plausiblydeniable.com/opinion/gsf.html\">geek social fallacies.</a>  I’m deeply uncomfortable when I see what looks like an elite subgroup, a group of “cool kids” or “VIPs” or whatever, talking behind closed doors because <em>hoi polloi </em>just wouldn’t understand. I mean, yes, sometimes people <em>wouldn’t </em>understand!  I get it. There do exist people who will be offended by my honest opinion (god knows), or who literally aren’t bright enough or knowledgeable enough to contribute to a discussion.  I understand why it’s easier to talk in private with people who are already more-or-less on the same page.  But still…there’s a pattern that gives me the willies. It’s “elites get to know what’s going on, randos are kept out of the loop,” and even when somebody says that I qualify as an elite, not a rando, it still bothers me, because I’m much more comfortable <em>having rights </em>than <em>being favored</em>.</p>\n<p>This is part of what gives me a bad feeling about the discourse around “demon threads” (that is: big, addictive, internet debates) and in praise of “<a href=\"https://www.lesswrong.com/posts/LxrpCKQPbdpSsitBy/taking-it-private-short-circuiting-demon-threads-working\">taking things private</a>“, where tensions will be easier to defuse.  There are real costs to acrimonious debate, in time and emotional energy, and I appreciate that people are trying to find ways to reduce those costs.  But I feel nervous about anything that looks like it’s trying to <em>sweep real conflicts under the rug</em>.  It’s like “don’t fight in front of the children” — except that in this case the members of the public are being placed in the role of “the children,” whether or not we want to be.</p>\n<p>I occasionally find myself in situations where I feel I’m being asked to take a sort of Straussian stance — <em>if you want to get important things done, you can’t be totally transparent about what you’re doing, because the general public will stop you</em>.  I’m not sure these people are wrong.  But I really hope they are.  I have a bad feeling about maintaining information asymmetries as a general policy.  I have a dangerous temperamental temptation towards concealment — it’s just “minor” stuff like trying to hide my failures, but in the long run, that’s neither ethical nor practical — so I’ve developed a counter-tendency towards transparency, as a sort of partial safeguard.  If I tell people what I’m up to, early and often, I can’t slip down the road of dishonesty.</p>\n<p><strong>Therapeutic Language: Another Flawed Solution</strong></p>\n<p>Peace is good, all things being equal. Fighting <em>hurts</em>.  And many fights are unnecessary, borne of misunderstanding more than actual disagreement. I’ve seen this a lot firsthand.  It’s much more likely that someone <em>literally doesn’t comprehend </em>your idea than that they oppose it.</p>\n<p>And one of the most common types of misunderstanding is when people <em>falsely assume you are damning them as a person</em>.  This is something I learned from <a href=\"https://malcolmocean.com/\">Malcolm Ocean</a>, who gave me the first really clear explanation I ever got as to what people are doing when they use <a href=\"https://www.cnvc.org/Training/nvc-chapter-1\">NVC</a> or <a href=\"https://www.circlinginstitute.com/\">Circling </a>language or other types of very careful and mannered speech to avoid the perception of blame or judgment.  Surely, I asked him, sometimes you <em>do </em>need to judge?  To distinguish between good and bad behavior?  To enforce norms?</p>\n<p>After a while, we came up with this analogy:</p>\n<p>There’s a difference between saying “You’re fired” and “You’re fired, and also fuck you.”</p>\n<p>In the course of life, one absolutely <em>does </em>have to say things like “you’re fired.”  Or “you can’t behave like that in this space”, “this work does not merit publication”, or “I don’t want to go on a date with you.”  In other words, drawing boundaries is necessary for life.  But drawing boundaries doesn’t always have to involve <em>damning </em>someone, as though sending them to Hell, utterly <em>condemning their essential being</em>.  (What Madeleine l’Engle would call <em>X-ing</em>.)  One can fire a person from a job, or reject their manuscript, or turn them down romantically, without saying <em>it is bad that you exist and you should hate yourself</em>.  One can even, I believe, convict someone of a crime, or kill them in self-defense, without damning them, while wishing that they had not done the thing that forced you to draw an extremely severe boundary.</p>\n<p>Boundaries are necessary; self-defense is necessary; damning people might <em>not </em>be necessary, and I’m inclined to believe it isn’t.</p>\n<p>And yet, people <em>do </em>damn each other, very frequently; and even more frequently, as a result of these bad experiences, they <em>assume </em>they’re being damned when they’re merely being criticized.  “You did a thing with negative consequences” gets read as “your essence is stained, you are a Terrible Person, it’s time to hate yourself.”  So, as an imperfect attempt to forestall these misunderstandings, people have developed these extremely artificial locutions that, yes, make you sound like a therapist, and, yes, aren’t as natural as just speaking in plain language.  But the hope is that they create enough distance to allow people to avoid <em>immediately </em>jumping to the conclusion that you’re accusing them of being Generally Terrible and Worthy of Eternal Hellfire.</p>\n<p>Of course, the human mind being devious and wily at figuring out how to make us miserable, it’s possible to be easily set off by therapeutic language itself!  It turns out I have such a sensitivity.  “You’re insinuating that I’m having bad feelings — this means you’re saying that I’m Weak and Can’t Hack It and need Special Treatment — which means you’re calling me Generally Terrible!  Screw you!”   (This isn’t completely irrational; it is the appropriate norm for situations like work or school, where hiding physical and mental pain is expected and where people <em>are </em>penalized for failing to do so.)</p>\n<p>Now, of course, I <em>do </em>have bad feelings sometimes, being a human.  And, a lot of the time, the person using therapeutic language is trying to <em>deal productively with </em>that fact of the matter, rather than condemning me for it — they’ve moved on to Step 2, What Do We Do Now, while I’m still on Step 1, Is Sarah Terrible Y/N?</p>\n<p>But you really can’t have good conversations while anyone’s still on Step 1.  If you haven’t yet resolved “Do You Think I’m Terrible?” with a resounding “No,” then <em>every other conversation that’s nominally about some topic will actually be about the vital issue of Do You Think I’m Terrible?</em></p>\n<p>And, because the human mind is devious, Step 1 doesn’t stay resolved; you have to <em>keep reaffirming it</em>, because people will forget.  You have to put what seems like a <em>colossal </em>amount of unsubtle effort into saying “I like you and I think you’re good” in order to keep discussions from becoming about “I’m good and not terrible! See, I’ll prove it!”</p>\n<p>I have not <em>mastered </em>this art, or even close, but I basically agree with the need for it.</p>\n<p>I have totally observed people being blunt and irreverent without hurting others’ feelings and while getting very productive discussions done — but I think what’s going on is not that these people <em>don’t </em>validate each other, but that they validate each other <em>very well</em> through different <em>means </em>than therapeutic language.  Some people can get away with speaking styles that are very “offensive” by conventional standards, but that’s because they <em>also </em>show deep affection and regard for the people they’re talking to.</p>\n<p>I think there are people who are more robust than others at independently maintaining a sense that they’re Okay and Good and Liked and Valid (and that’s great!) but I don’t think this in any way <em>disproves the need </em>for validation, any more than the existence of plants proves that organisms don’t need chemical energy.</p>\n<p><b>Nobody (Exactly) Agrees With You</b></p>\n<p>I’ve been struggling a bunch with the fact that people seem to disagree <em>fractally and at every turn</em>.  It’s really, really hard to get exact alignment on worldviews and desires, to the point that I’m beginning to doubt it’s possible.  I see someone who seems to see <em>part </em>of the world the same way I do, and I go “can we talk? can we be buds? can we be twinsies? are we on the same team?” and then I realize “oh, no, outside of this tiny little area, they…really don’t agree with me at all.  Dammit.”</p>\n<p>It would be nice to have <em>someone to talk to who was basically the same person as you</em>, right?  Someone you could just melt into,  the way all of humanity melted into a single sea of neon-orange thought-fluid in <a href=\"https://wiki.evageeks.org/LCL\">that anime.</a></p>\n<p>But, in my experience, that just keeps not happening.  Friendship and mutual respect, sure, I’m very fortunate to have lots of that; but merging doesn’t happen.  There’s always me, or the other person, saying “no, not exactly” instead of “yes, and”.</p>\n<p>Is it just that I’m unusual?  Surely people who build movements get people to agree with each other?</p>\n<p>The thing is, I’m starting to suspect they <em>don’t</em>.  I recently went to TEDWomen, and saw a bunch of talks about activism and organizing, including by such luminaries as Dolores Huerta and Marian Wright Edelman.  And here are some takeaways I got from them:</p>\n<ul>\n<li>Activists view the main goal as <em>fighting apathy</em>, that is, getting people to participate, literally <em>activating </em>people.  Getting people to show up to vote or show up to a protest or to raise issues in conversations.</li>\n<li><em>Everybody </em>in a coalition supports <em>everybody </em>else. It’s very “all for one and one for all.” They explicitly talk about how you shouldn’t allow anyone to frame things as “the environment” vs “women’s issues” vs “labor issues” vs “immigration” — everyone’s encouraged to push for everyone’s agenda together, for every sub-group in the progressive coalition.</li>\n<li>Activists <em>endorse </em>being moved more by individual stories and art and emotional appeals than by facts and figures.  They don’t just talk about how “emotional appeals work better on the public” but they talk about how emotional appeals and personal connections work <em>on themselves</em>.</li>\n</ul>\n<p>If you think of everybody’s beliefs as a forest of trees, where consequences branch out from premises, then “trying to get agreement” is building trees as big as they can get and trying to hash out what’s going on when two people’s trees differ. What seems to be going on in an activist frame is <em>not building out the trees very big at all</em>, only getting agreement on rather basic things like “children shouldn’t live in poverty” and trying to move straight to voting and fundraising and other object-level actions, without really hashing out in much detail “ok, what ways of avoiding child poverty are effective and/or morally acceptable?”  They recognize that getting people to participate at all is difficult (in my shoes, they would have invested a lot more effort in getting people to show up to the event), and they don’t seem to even try to get people to <em>agree </em>in a deep sense, to agree on world-models and general principles and moral foundations.</p>\n<p>Just because everyone is shouting the same slogan doesn’t mean they <em>really agree with each other</em>.  They agree on the slogan.  It might mean different things to different people.  That’s not necessarily a bad thing, but it’s worth being aware that it isn’t true unity.</p>\n<p>The Greek for “with one accord” is <a href=\"https://biblehub.com/greek/3661.htm\">ὁμοθυμαδόν</a>, which appears frequently in the New Testament; it means literally “same passion” or “same spirit”, the seat of courage and emotion that lives in the heart.  “Unanimity” is an exact translation into Latin — “one spirit.”  You can have large groups of people who <em>feel </em>the same, who are filled with the same passion.  It is much harder for all those people to have the same <em>belief structure, </em>to stay on the same page on the nitty-gritty details.  Just getting groups of people to “weak unanimity,” namely, active participation, good will, and agreement on ideal goals, is a challenging full-time job by itself — and it doesn’t even <em>touch </em>getting worldview alignment.</p>\n<p><strong>The Cost of Complaint</strong></p>\n<p>One weird and maybe trivial thing that’s been nagging at me is trying to get a handle on the underlying worldview expressed by the <em>Incredibles </em>movies.  Yeah, it’s pop culture, but there’s clearly an attempt to communicate a moral, and it’s a <em>weird </em>one.</p>\n<p>Sure, there’s the inspiring, defiant pro-superhero note of “people shouldn’t be pressured to hide their excellence”, which often gets labeled Randian (but could just as easily be Nietzchean or <em>Harrison Bergeron-</em>esque).</p>\n<p>But it gets weird when you look at the villains.  The villains of both movies are genius technologists.  Syndrome, the villain of the first movie, is a bitter, pimpled male nerd, resentful of superheroes’ elevated status, who wants to provide technology to give everyone superpowers.  Evelyn Deaver, the villain of the second movie, is a bitter, urbane, worldly feminist, a technologist who dislikes the way technology has “dumbed down” its users, resentful of the public’s passive reliance on screens and superheroes.  For plot reasons, of course, both supervillains pull dangerous stunts that put the public at risk, and need to be stopped by the superheroes.  But their motivations are actually <em>empowering humanity, </em>weirdly enough.  Syndrome is, effectively, a transhumanist, while Evelyn is an “ethical techie” type reminiscent of the people at the <a href=\"http://humanetech.com/\">Center for Humane Technology</a>.  Their obsession is using their talents and hard work to make all people <em>more self-reliant and capable of greater things</em> — a mission that would actually sit well with Rand or Nietzsche, and, outside the world of the films, could easily work as a heroic cause.</p>\n<p>What’s wrong with the villains, in the world of <em>The Incredibles</em>, is that they’re <em>grouchy</em>.  They’re social critics. They complain.</p>\n<p>Notice that, before we know she’s a villain, Evelyn tries to get Mrs. Incredible to <em>commiserate </em>about sexism; the heroine doesn’t take the bait, and points out that Evelyn is also standing in her brother’s shadow.  Before <em>his </em>villainous reveal, Syndrome is a whiny kid who wants to be Mr. Incredible’s sidekick.  And the initial controversy that drove superheroes underground was a suicidal man who sued Mr. Incredible for saving his life.</p>\n<p>Also, notice that Brad Bird is taking a <em>very </em>firm stance in favor of optimism and against gloom, in the <em>Incredibles </em>movies and others; his movies overtly defend his creative choice to keep things positive and brightly colored in a world where critical acclaim usually comes<em> </em>in shades of gray. (The antagonist in <em>Ratatouille, </em>not accidentally, is a restaurant critic.)  I think it’s really that simple: Brad Bird likes unity and positivity, and doesn’t like complaining.  Critics like the <em>New Yorker’s </em>Richard Brody are right to <a href=\"https://www.newyorker.com/culture/richard-brody/review-the-authoritarian-populism-of-incredibles-2\">see a threat</a> in the movies — their real enemy is <em>criticism</em>.</p>\n<p>(If you look at Brad Bird’s <a href=\"https://www.vulture.com/2018/06/director-brad-bird-on-the-incredibles-2-john-lasseter.html\">actual words</a>, he isn’t any kind of a libertarian or Randian, and says so; he’s a centrist, he’s big on finding common ground, staying positive, focusing on unity, and so on.)</p>\n<p>It’s almost impossible to talk about the world intelligently while refraining from any complaint.  Try finding a blog to read that <em>never criticizes society, </em>from any direction.  Where you find interesting and articulate people, you’ll find people who express dissatisfaction with things as they are.  There’s no <em>principled </em>way to say “hey I think everyone’s pretty much right,” because people don’t remotely agree with each other if you ask about any details at all.</p>\n<p>And yet, people (like Bird, but also like me, and like many) get heartsick when we’re exposed to too much complaint or disagreement.  Moods are contagious, and criticism <em>is </em>very often depressing, for all we try to tell ourselves that it’s merely an intellectual awareness.  Sometimes I feel like “for god’s sake, World, for once could you give me a social context where <em>literally nobody expresses dislike or disapproval about anything</em>?  Could we have a Happy Zone please?”</p>\n<p>But I’m genuinely not sure if that’s possible.  It may be a feature of language or logic itself that it’s hard to talk at all if you restrict yourself firmly to avoiding critical speech.  I certainly would have a hard time sticking strictly to Happy Zone rules.</p>\n<p>I don’t have solutions here.  I’m just trying to figure things out.  It ought to be possible, I think, to deliberate and collaborate <em>with </em>people, allowing “the group” to decide, rather than just deciding what <em>I </em>want individually and letting people collaborate with me to the extent that it sounds good to them.  I know how to be an individualist; I’m trying to learn how to also do the collective thing, “voice” rather than “exit”.  But I’m just stumped by the fact that <em>people want different things, and think different things, and actual, far-reaching unity doesn’t seem to exist</em>.</p>",
    "user": {
      "username": "sarahconstantin",
      "slug": "sarahconstantin",
      "displayName": "sarahconstantin"
    }
  },
  {
    "_id": "DYCNuqMpCfQtPcPSo",
    "title": "Open and Welcome Thread December 2018",
    "slug": "open-and-welcome-thread-december-2018",
    "pageUrl": "https://www.lesswrong.com/posts/DYCNuqMpCfQtPcPSo/open-and-welcome-thread-december-2018",
    "postedAt": "2018-12-04T22:20:53.076Z",
    "baseScore": 26,
    "voteCount": 10,
    "commentCount": 23,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>If it’s worth saying, but not worth its own post, then it goes here.</p><p>Also, if you are new to LessWrong and want to introduce yourself, this is the place to do it. Personal stories, anecdotes, or just general comments on how you found us and what you hope to get from the site and community are welcome. If you want to explore the community more, I recommend <a href=\"https://www.lesswrong.com/library\">reading the Library</a>, <a href=\"https://www.lesswrong.com/?view=curated\">checking recent Curated posts</a>, and <a href=\"https://www.lesswrong.com/community\">seeing if there are any meetups in your area</a>.</p><hr class=\"dividerBlock\"/><p>As well as trying out combining welcome threads and open threads, I thought I&#x27;d try highlighting some frontpage comments I found especially insightful in the last month, for further discussion:</p><ul><li>Scott Garrabrant wrote a <a href=\"https://www.lesswrong.com/posts/p7x32SEt43ZMC9r7r/embedded-agents#b23WPjbNtijsawDpC\">comment</a> on how Embedded Agency and Agent Foundations research are like science in relation to ML approaches to AI alignment which are more like engineering. The comment helped me think about how I go about formalising and solving problems more generally.</li><li>Rohin Shah wrote a <a href=\"https://www.lesswrong.com/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment#3ECKoYzFNW2ZqS6km\">comment</a> on basic definitions of the alignment problem, contrasting a motivation-competence split versus a definition-optimization split. (It is then followed by a convo on definitions between Paul and Wei which gets pretty deep into the weeds - I&#x27;d love to read a summary here from anyone else who followed along.)</li></ul>",
    "user": {
      "username": "Benito",
      "slug": "benito",
      "displayName": "Ben Pace"
    }
  },
  {
    "_id": "Pe3aqWXJWLHoB6vc4",
    "title": "Alignment Newsletter #35",
    "slug": "alignment-newsletter-35",
    "pageUrl": "https://www.lesswrong.com/posts/Pe3aqWXJWLHoB6vc4/alignment-newsletter-35",
    "postedAt": "2018-12-04T01:10:01.209Z",
    "baseScore": 15,
    "voteCount": 3,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>Find all Alignment Newsletter resources <a href=\"http://rohinshah.com/alignment-newsletter/\">here</a>. In particular, you can <a href=\"http://eepurl.com/dqMSZj\">sign up</a>, or look through this <a href=\"https://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit?usp=sharing\">spreadsheet</a> of all summaries that have ever been in the newsletter.</p><p>This week we don&#x27;t have any explicit highlights, but remember to treat the sequences as though they were highlighted!</p><h1>Technical AI alignment</h1><h3>Iterated amplification sequence</h3><p><a href=\"https://www.alignmentforum.org/posts/fkLYhTQteAu5SinAc/corrigibility\">Corrigibility</a> <em>(Paul Christiano)</em>: A corrigible agent is one which helps its operator, even with tasks that would change the agent itself, such as correcting mistakes in AI design. Consider a good act-based agent, which chooses actions according to our preferences over that action. Since we have a short-term preference for corrigibility, the act-based agent should be corrigible. For example, if we are trying to turn off the agent, the agent will turn off because that&#x27;s what we would prefer -- it is easy to infer that the overseer would not prefer that agents stop the overseer from shutting them down. Typically we only believe that the agent would stop us from shutting it down if it makes <em>long-term plans</em>, in which case being operational is instrumentally useful, but with act-based agents the agent only optimizes for its overseer&#x27;s short term preferences. One potential objection is that the notion of corrigibility is not easy to learn, but it seems not that hard to answer the question &quot;Is the operator being misled&quot;, and in any case we can try this with simple systems, and the results should <em>improve</em> with more capable systems, since as you get smarter you are more capable of predicting the overseer.</p><p>In addition, even if an agent has a slightly wrong notion of the overseer&#x27;s values, it seems like it will <em>improve</em> over time. It is not hard to infer that the overseer wants the agent to make its approximation of the overseer&#x27;s values more accurate. So, as long as the agent has enough of the overseer&#x27;s preferences to be corrigible, it will try to learn about the preferences it is wrong about and will become more and more aligned over time. In addition, any slight value drifts caused by eg. amplification will tend to be fixed over time, at least on average.</p><p><strong>Rohin&#x27;s opinion:</strong> I really like this formulation of corrigibility, which I find quite different from <a href=\"https://intelligence.org/2014/10/18/new-report-corrigibility/\">MIRI&#x27;s paper</a>. This seems a lot more in line with the kind of reasoning that I want from an AI system, and it seems like iterated amplification or something like it could plausibly succeed at achieving this sort of corrigible behavior.</p><p><a href=\"https://www.alignmentforum.org/posts/HqLxuZ4LhaFhmAHWk/iterated-distillation-and-amplification\">Iterated Distillation and Amplification</a> <em>(Ajeya Cotra)</em>: This is the first in a series of four posts describing the iterated amplification framework in different ways. This post focuses on the repetition of two steps. In amplification, we take a fast aligned agent and turn it into a slow but more capable aligned agent, by allowing a human to coordinate many copies of the fast agent in order to make better decisions. In distillation, we take a slow aligned agent and turn it a fast aligned agent (perhaps by training a neural net to imitate the judgments of the slow agent). This is similar to AlphaGoZero, in which MCTS can be thought of as amplification, while distillation consists of updating the neural net to predict the outputs of the MCTS.</p><p>This allows us to get both alignment and powerful capabilities, whereas usually the two trade off against each other. High capabilities implies a sufficiently broad mandate to search for good behaviors, allowing our AI systems to find novel behaviors that we never would have thought of, which could be bad if the objective was slightly wrong. On the other hand, high alignment typically requires staying within the realm of human behavior, as in imitation learning, which prevents the AI from finding novel solutions.</p><p>In addition to distillation and amplification robustly preserving alignment, we also need to ensure that given a human as a starting point, iterated distillation and amplification can scale to arbitrary capabilities. We would also want it be about as cost-efficient as alternatives. This seems to be true at test time, when we are simply executing a learned model, but it could be that training is much more expensive.</p><p><strong>Rohin&#x27;s opinion:</strong> This is a great simple explanation of the scheme. I don&#x27;t have much to say about the idea since I&#x27;ve talked about iterated amplification so much in this newsletter already.</p><p><a href=\"https://www.alignmentforum.org/posts/PRaxzmDJdvie46ahL/benign-model-free-rl\">Benign model-free RL</a> <em>(Paul Christiano)</em>: This post is very similar to the previous one, just with different language: distillation is now implemented through reward modeling with robustness. The point of robustness is to ensure that the distilled agent is benign even outside of the training distribution (though it can be incompetent). There&#x27;s also an analysis of the costs of the scheme. One important note is that this approach only works for model-free RL systems -- we&#x27;ll need something else for eg. model-based RL, if it enables capabilities that we can&#x27;t get with model-free RL.</p><h3>Value learning sequence</h3><p><a href=\"https://www.alignmentforum.org/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior\">Intuitions about goal-directed behavior</a> and <a href=\"https://www.alignmentforum.org/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-imply-goal-directed-behavior\">Coherence arguments do not imply goal-directed behavior</a> <em>(Rohin Shah)</em> (summarized by Richard): Rohin discusses the &quot;misspecified goal argument for AI risk&quot;: that even a small misspecification in goals can lead to adversarial behaviour in advanced AI. He argues that whether behaviour is goal-directed depends on whether it generalises to new situations in ways that are predictable given that goal. He also raises the possibility that thinking of an agent as goal-directed becomes less useful the more we understand about how it works. If true, this would weaken the misspecified goal argument.</p><p>In the next post, Rohin argues against the claim that &quot;simply knowing that an agent is intelligent lets us infer that it is goal-directed&quot;. He points out that all behaviour can be rationalized as expected utility maximisation over world-histories - but this may not meet our criteria for goal-directed behaviour, and slightly misspecifying such a utility function may well be perfectly safe. What&#x27;s more interesting - and dangerous - is expected utility maximisation over world-states - but he claims that we shouldn&#x27;t assume that advanced AI will have this sort of utility function, unless we have additional information (e.g. that it has a utility function simple enough to be explicitly represented). There are plenty of intelligent agents which aren&#x27;t goal-directed - e.g. ones which are very good at inference but only take trivial actions.</p><p><strong>Richard&#x27;s opinion:</strong> I broadly agree with Rohin&#x27;s points in these posts, and am glad that he&#x27;s making these arguments explicit. However, while goal-directedness is a tricky property to reason about, I think it&#x27;s still useful to consider it a property of an agent rather than a property of our model of that agent. It&#x27;s true that when we have a detailed explanation of how an agent works, we&#x27;re able to think of cases in which its goal-directedness breaks down (e.g. adversarial examples). However, when these examples are very rare, they don&#x27;t make much practical difference (e.g. knowing that AlphaGo has a blind spot in certain endgames might not be very helpful in beating it, because you can&#x27;t get to those endgames).</p><h3>Agent foundations</h3><p><a href=\"https://link.springer.com/article/10.1007/s11238-018-9679-3\">Robust program equilibrium</a> <em>(Caspar Oesterheld)</em></p><p><a href=\"https://www.alignmentforum.org/posts/MgLeAWSeLbzx8mkZ2/bounded-oracle-induction\">Bounded Oracle Induction</a> <em>(Diffractor)</em></p><p><a href=\"https://www.alignmentforum.org/posts/QjYnuGTFcWCQudLsh/oracle-induction-proofs\">Oracle Induction Proofs</a> <em>(Diffractor)</em></p><h3>Learning human intent</h3><p><a href=\"http://arxiv.org/abs/1811.07882\">Guiding Policies with Language via Meta-Learning</a> <em>(John D. Co-Reyes)</em> (summarized by Richard): The authors train an agent to perform tasks specified in natural language, with a &quot;correction&quot; after each attempt (also in natural language). They formulate this as a meta-learning problem: for each instruction, several attempt-correction cycles are allowed. Each attempt takes into account previous attempts to achieve the same instruction by passing each previous trajectory and its corresponding correction through a CNN, then using the mean of all outputs as an input to a policy module.</p><p>In their experiments, all instructions and corrections are generated automatically, and test-time performance is evaluated as a function of how many corrections are allowed. In one experiment, the tasks is to navigate rooms to reach a goal, where the correction is the next subgoal required. Given 4 corrections, their agent outperforms a baseline which was given all 5 subgoals at the beginning of the task. In another experiment, the task is to move a block to an ambiguously-specified location, and the corrections narrow down the target area; their trained agent scores 0.9, as opposed to 0.96 for an agent given the exact target location.</p><p><strong>Richard&#x27;s opinion:</strong> This paper explores an important idea: correcting poorly-specified instructions using human-in-the-loop feedback. The second task in particular is a nice toy example of iterative preference clarification. I&#x27;m not sure whether their meta-learning approach is directly relevant to safety, particularly because each correction is only &quot;in scope&quot; for a single episode, and also only occurs after a bad attempt has finished. However, the broad idea of correction-based learning seems promising.</p><h3>Interpretability</h3><p><a href=\"http://arxiv.org/abs/1811.07807\">Deeper Interpretability of Deep Networks</a> <em>(Tian Xu et al)</em></p><p><a href=\"https://arxiv.org/abs/1811.10597\">GAN Dissection: Visualizing and Understanding Generative Adversarial Networks</a> <em>(David Bau et al)</em></p><p><a href=\"http://arxiv.org/abs/1811.10154\">Please Stop Explaining Black Box Models for High Stakes Decisions</a> <em>(Cynthia Rudin)</em></p><p><a href=\"http://arxiv.org/abs/1811.09720\">Representer Point Selection for Explaining Deep Neural Networks</a> <em>(Chih-Kuan Yeh, Joon Sik Kim et al)</em></p><h3>Adversarial examples</h3><p><a href=\"https://arxiv.org/abs/1811.09716\">Robustness via curvature regularization, and vice versa</a> <em>(Moosavi-Dezfooli et al)</em> (summarized by Dan H): This paper proposes a distinct way to increase adversarial perturbation robustness. They take an adversarial example generated with the FGSM, compute the gradient of the loss for the clean example and the gradient of the loss for the adversarial example, and they penalize this difference. Decreasing this penalty relates to decreasing the loss surface curvature. The technique works slightly worse than adversarial training.</p><h3>Uncertainty</h3><p><a href=\"http://proceedings.mlr.press/v80/kumar18a/kumar18a.pdf\">Trainable Calibration Measures For Neural Networks From Kernel Mean Embeddings</a> <em>(Aviral Kumar et al)</em></p><h3>Forecasting</h3><p><a href=\"http://mediangroup.org/gpu.html\">How rapidly are GPUs improving in price performance?</a> <em>(gallabytes)</em></p><p><a href=\"https://aiimpacts.org/diabetic-retinopathy-as-a-case-study-in-time-for-ai-to-cross-the-range-of-human-performance/\">Time for AI to cross the human performance range in diabetic retinopathy</a> <em>(Aysja Johnson)</em></p><h1>Near-term concerns</h1><h3>Fairness and bias</h3><p><a href=\"http://arxiv.org/abs/1811.10104\">50 Years of Test (Un)fairness: Lessons for Machine Learning</a> <em>(Ben Hutchinson)</em></p><h1>AI strategy and policy</h1><p><a href=\"https://arxiv.org/abs/1811.10840v1\">Robust Artificial Intelligence and Robust Human Organizations</a> <em>(Thomas G. Dietterich)</em></p><p><a href=\"https://futureoflife.org/2018/11/26/handful-of-countries-including-the-us-and-russia-hamper-discussions-to-ban-killer-robots-at-un/\">Handful of Countries – Including the US and Russia – Hamper Discussions to Ban Killer Robots at UN</a></p><h1>Other progress in AI</h1><h3>Exploration</h3><p><a href=\"http://eng.uber.com/go-explore/\">Montezuma’s Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems </a><em>(Adrien Ecoffet et al)</em> (summarized by Richard): This blog post showcases an agent which achieves high scores in Montezuma’s Revenge and Pitfall by keeping track of a frontier of visited states (and the trajectories which led to them). In each training episode, a state is chosen from the frontier, the environment is reset to that state, and then the agent randomly explores further and updates the frontier. The authors argue that this addresses the tendency of intrinsic motivation algorithms to forget about promising areas they&#x27;ve already explored. To make state storage tractable, each state is stored as a downsampled 11x8 image.</p><p>The authors note that this solution exploits the determinism of the environment, which makes it brittle. So they then use imitation learning to learn a policy from demonstrations by the original agent. The resulting agents score many times higher than state-of-the-art on Montezuma’s Revenge and Pitfall.</p><p><strong>Richard&#x27;s opinion:</strong> I’m not particularly impressed by this result, for a couple of reasons. Firstly, I think that exploiting determinism by resetting the environment (or even just memorising trajectories) fundamentally changes the nature of the problem posed by hard Atari games. Doing so allows us to solve them in the same ways as any other search problem - we could, for instance, just use the AlphaZero algorithm to train a value network. In addition, the headline results are generated by hand-engineering features like x-y coordinates and room number, a technique that has been eschewed by most other attempts. When you take those features away, their agent’s total reward on Pitfall falls back to 0.</p><p><strong>Read more:</strong> <a href=\"https://www.alexirpan.com/2018/11/27/go-explore.html\">Quick Opinions on Go-Explore</a></p><p><a href=\"http://arxiv.org/abs/1811.11298\">Prioritizing Starting States for Reinforcement Learning</a> <em>(Arash Tavakoli, Vitaly Levdik et al)</em></p><h3>Reinforcement learning</h3><p><a href=\"http://arxiv.org/abs/1811.07819\">Learning Actionable Representations with Goal-Conditioned Policies</a> <em>(Dibya Ghosh)</em></p><p><a href=\"http://arxiv.org/abs/1811.11359\">Unsupervised Control Through Non-Parametric Discriminative Rewards</a> <em>(David Warde-Farley)</em></p><h3>Hierarchical RL</h3><p><a href=\"http://arxiv.org/abs/1811.09656\">Hierarchical visuomotor control of humanoids</a> <em>(Josh Merel, Arun Ahuja et al)</em></p>",
    "user": {
      "username": "rohinmshah",
      "slug": "rohinmshah",
      "displayName": "Rohin Shah"
    }
  },
  {
    "_id": "gRcBten4TBmCNduMZ",
    "title": "Peanut Butter",
    "slug": "peanut-butter",
    "pageUrl": "https://www.lesswrong.com/posts/gRcBten4TBmCNduMZ/peanut-butter",
    "postedAt": "2018-12-03T19:30:56.907Z",
    "baseScore": 28,
    "voteCount": 8,
    "commentCount": 3,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>This is part 2 of my interview with <a href=\"https://deluks917.wordpress.com/\">deluks917</a>. In <a href=\"https://putanumonit.com/2018/11/20/deluks917-on-online-communities/\">part 1 we discussed</a> online weirdos, free thinkers, and Uyghurs. Stay tuned for part 3 on archetypes, the will to knowledge and Moana.</p><p>We talk about materialism, Buddhism, how to deal with the inevitable suffering that comes from attachment, and how peanut butter ties everything together. I have a lot of thoughts on all of the above! </p><p><a href=\"https://putanumonit.com/2018/11/27/peanut-butter/\">The original post on Putanumonit</a> has internal links you can use to jump from the interview to my thoughts and back. On LW you&#x27;ll have to read through the interview and get to my thoughts at the bottom.</p><p><strong>Jacob: Before I get your view on materialism, I want to share a story from last week.</strong></p><p><strong>It was my wife’s birthday, and she wanted to go to a fancy restaurant. A</strong> <strong><em>very</em></strong> <strong>fancy restaurant. We both wore nice outfits, and the meal cost more than everything we wore, including shoes.</strong></p><p><strong>At the start of the meal, we had large blue plates with golden trim in front of us. Then they put another round plate on top of that, and a smaller round plate, a square white plate, an even smaller round plate, and a tiny elongated plate on top of that. And on that pedestal of gleaming porcelain was an inch-long Italian pastry of some sort. Then they brought a cloth napkin wrapped around a slice of lemon and a twig of rosemary and poured water from a silver pitcher on the napkin so that it would be freshly wet.</strong></p><p><strong>I found the entire thing absurd, excess for the sake of excess. And I thought about you – as skeptical as I am of this sort of thing, you are far more skeptical. I remember coming over with a bottle of wine and asking if you had any wine glasses, and you said: “no, I’m not really into materialism” as if wine glasses were obviously superfluous. </strong></p><p>deluks917: It seems that there is a path that almost everyone walks down. You get new things, and then you hedonically adapt. You start to feel like you can’t give up certain material comforts. Suddenly you’re willing to compromise your morals to get the money you feel you need.</p><p>People constantly want new cars, new clothes. Things they would’ve been very happy with in the past no longer make them happy. If I looked at my current life through the eyes of a subsistence farmer, shouldn’t I just be filled with joy at this material abundance?</p><p>It’s not just material things. You see people who are successful professors at good universities who are very upset because they won’t win the Nobel prize.</p><p>And I see it in myself in some avenues. I play a lot of online games. On one game, Seven Wonders, I had an ELO score of 380 for a long while. Then I had a good run and I got to 440, then 450. But I played for six more hours and was down to 420. I felt like a failure. 420 seemed unbearably low, and just the day before it was higher than my all-time peak. I feel like I need an exorcist to get rid of these demons.</p><p><strong>Are you happy with the material progress that’s just the result of technology improving for everybody? Or do you want to avoid all progress, stepping off the hedonic treadmill entirely?</strong></p><p>I don’t think in terms of going up the treadmill or optimizing it. Shouldn’t we just realize how amazing things are materially and hold hands and sing praises to God? Things are just so great materially. And the elephant is trying to ruin this.</p><p>So I’m trying to push the other way. Could I live <em>less</em> well materially and still be happy?</p><p>I give away money to EA causes and I save a decent amount. My financial goal is to have enough money that if something happened to me I could continue living a reasonable life. I’m not trying to become a monk. But it would be just wonderful to retire on 30K a year in the middle of nowhere. And if I had enough to retire I would do something in the EA community or work directly on AI risk.</p><p><strong>It seems like your life is pretty well optimized, which is something a lot of rationalists are trying to do. This could mean getting more of something, or just being happy with less. As far as diet is concerned, you’re definitely following the second path. Can you describe your diet?</strong></p><p>By calories, my diet is about half peanut butter. The other half is things like beans and rice. I cook them in vegetable stock and add some ancho chili powder. It’s very easy, I cook either 4 pounds or 8 pounds of beans at a time and throw it in the fridge.</p><p><strong>You lost a lot of weight following the principles outlined in Stephan Guyenet’s</strong> <strong><em>The Hungry Brain</em>. Did you try an especially non-palatable diet or just a boring one?</strong></p><p>What I have right now is a boring diet. The other diet, the one I lost 40 pounds on, is much more strict. In the other diet, you’re not supposed to eat food that is tasty. I ate the “natural” kind of peanut butter with no emulsifiers in it where the oil separates. I really don’t like that kind of peanut butter. You just eat plain pasta, maybe you can put salt on it. You eat plain bread, just bread.</p><p><strong>Are you doing your current diet for health reasons? Or does it just appeal to you to eat rice, beans, and peanut butter forever?</strong></p><p>Peanut butter is good. If you handed it to a hunter-gatherer they’d be pretty impressed. It sure beats mongongo nuts you cook in sand. You can never get the sand off the nuts, man!</p><p>Sometimes I’ll eat other things, but this sort of thing is most of it. It’s fine, it tastes great.</p><p>I look at it from a Buddhist point of view. Think about the actual inputs to your brain: you have taste receptors, a picture you’re seeing, and your own thoughts which are an input in some sense. If you just meditate on what it’s really like to eat, the signal is pretty sparse. A lot of what happens is created in the mind. This is how you get people drinking fancy wine but they can’t tell it apart from a cheap version in a blind taste test.</p><p>So as a matter of daily experience, it’s not that much better to eat other foods. I enjoy my diet quite a bit. It doesn’t get you to 100% of the hedonic enjoyment of eating normal food but it’s pretty enjoyable.</p><p><strong>I’ve been</strong> <a href=\"https://putanumonit.com/2018/10/19/valley-of-bad-mindfulness/\">trying to learn about Buddhism</a><strong>, and it seems to be ultimately about detaching yourself from your desires. My initial reaction is that it’s not somewhere I want to get to. I enjoy having a moderate number of unfulfilled desires for things like better food, interesting people, beautiful women. But I could be wrong about that.</strong></p><p><strong>Where are you with Buddhism? Are you on the way to nirvana?</strong></p><p>Desires will cause you suffering. That’s just what desires do, from a Buddhist point of view. But given this, whether you should have desires remains an interesting question.</p><p>I certainly desire that AI doesn’t destroy the world. This desire actually causes me a lot of suffering. I worry about AI all the time. But me caring about that probably reduces total suffering if you integrate over everyone in the world.</p><p><strong>It seems that most of what Buddhism is against are elephant-based desires, not reducing existential risk to humanity. Satisfying those desires brings some happiness to you personally, but ultimately that satisfaction is not permanent and its impermanence brings suffering. So Buddhism says that it’s better to just get rid of your elephant desires completely.</strong></p><p><strong>Do you agree with this characterization and with the conclusion?</strong></p><p>On some level, I’m attached to my sense of self. Even though the self is not really a fundamentally coherent concept. To me the basic claims of the Buddhist worldview are true. They are right about suffering and desires. I guess the question is: would you wirehead?</p><p>Authentic Buddhism is basically a guide to wireheading. If you follow Buddhism really strictly, you become a monk who sits and meditates all day and eats one bowl of rice. There’s no magic, you’re not restoring karma to the universe, you have just found a way to be very happy. But it’s not obvious to say what part of yourself you’re willing to part with to be happy.</p><p><strong>You think that nirvana is happiness or just a lack of suffering?</strong></p><p>If you are sufficiently enlightened, you should be very fulfilled in the usual sense.</p><p><strong>The way I see it, there are short-term pleasures and short-term suffering, and I probably spend more time in the latter. Other people definitely seem to. So you oscillate between very positive and very negative experiences, and nirvana replaces that with a steady baseline. That baseline is above 0 – being a monk in nirvana is better than death. But wireheading could be way better, it can be a permanent baseline that’s as high as my current peak experiences.</strong></p><p>Being an enlightened monk is a very enjoyable experience on a deep level. You can get a taste of this if you meditate. The traditional Buddhist position is that you should not get attached to enjoyable meditation experiences. But if you meditate enough you will probably have some deeply enjoyable experiences just sitting on a rock.</p><p><strong>How much do you meditate?</strong></p><p>30 minutes a day, on average.</p><p><strong>Would you wirehead if I just handed you a device that would put you there?</strong></p><p>No, I don’t think I would wirehead. And that’s the reason why I’m not super interested in reaching traditional Buddhist nirvana.</p><p><strong>Because it would change who you are too much?</strong></p><p>If my only goal was to remove my suffering this would be a great experience. But I’m not overly motivated to put myself in a position where I’m just having a great time all the time. If everything was fine in the world then maybe I would take the wireheading, but maybe not. Being really happy all the time just isn’t that appealing to me.</p><p>Think about the conclusions that Buddhism or Derek Parfit come to with regards to the self. All you’re really achieving is that <em>someone </em>is having a great time, all the time. The ties that bind you to your sense of self are also the things causing you suffering. I am not sure I want to sever those ties.</p><p><strong>Going back to peanut butter: it’s delicious, but it’s hard for almost everyone to accept eating nothing else forever. This is a surprisingly good metaphor for monogamy.</strong></p><p><strong>And yet, we are both non-monogamous. Do you see the irony in being poly, given that romance is the one area where non-monks are trying to curb their pursuit of more and be happy with limiting their choices? What wisdom does Buddhism have to offer about romance?</strong></p><p>You got me here! There is a conflict. Traditional Buddhism is skeptical of sex and romance. If you are a monk you are supposed to refrain from all sexual and romantic attachments. Even masturbation is forbidden. There is a very funny quote from the Vinya, the rules for monks:</p><blockquote>It would be better that your penis be stuck into the mouth of a poisonous snake than into a woman’s vagina. It would be better that your penis be stuck into the mouth of a black viper than into a woman’s vagina. It would be better that your penis be stuck into a pit of burning embers, blazing and glowing, than into a woman’s vagina.<br/>Why is that? For that reason you would undergo death or death-like suffering, but you would not on that account, at the break-up of the body, after death, fall into deprivation, the bad destination, the abyss, hell.</blockquote><p>Of course, the celibacy rules are intended for monks and not lay people. But traditional Buddhism recommends that everyone weaken their attachment to sexual pleasure. Polyamory is a very bad idea from this perspective. Switching to poly is moving in the opposite of the wise direction and will almost certainly cause me quite a lot of suffering.</p><p>I am certainly interested in reducing my own suffering, but it’s not my top goal. I am ultimately motivated by a desire to know what is true. Polyamory is such an interesting idea. I have many poly friends who seem to be doing great, and they seem to do better the fewer restrictions they place on their relationships. Maybe monogamy is just a dogma we can let go of. That is a crazy thought.</p><p>I just have to find out if its true for me. If I have to suffer to see this for myself so be it.</p><p><em><a href=\"https://putanumonit.com/2018/06/20/miller-3-polyamory-mating/\">[Geoffrey Miller’s (and my) thoughts on polyamory as a frontier for scientific exploration]</a></em></p><p><strong>Do you feel that the life you are living is close to your ideal? Have you’ve figured things out?</strong></p><p>I think that I have gotten some of the easier things correct. But even if you look at <a href=\"https://www.lesswrong.com/posts/LgavAYtzFQZKg95WC/extreme-rationality-it-s-not-that-great\">Scott’s writings from 2009</a>, the greatest demon is akrasia or lack of focus. And this is the reason why all the self-help ideas on LessWrong don’t work truly amazingly. I think they work, but they’re not as transformative as people thought they would be.</p><p>Akrasia is a tough demon. If I could slay that monster I’d be happy with my life.</p><p><strong>What’s the plan for slaying it? What are you trying?</strong></p><p>I’ve tried a lot of things. Right now I’m thinking of a new approach.</p><p>Throughout the day you vary a lot in how conscious or lucid you are. When you are especially lucid, instead of trying to be directly productive you can instead try to push your less conscious self in the right direction. When you’re the rider, instead of reading the book you should push the elephant towards the library. Then if you take a nap you’ll wake up near the library.</p><p>I’m not quite sure how useful this insight will be, but I hope it turns out to be wise.</p><p><em>[<a href=\"http://rationality.org/\">The Center for Applied Rationality</a></em> <em>teaches many techniques a rider can use to train their elephant. A promising one is</em> <em><a href=\"https://www.lesswrong.com/posts/wJutA2czyFg6HbYoW/what-are-trigger-action-plans-taps\">Trigger Action Planning</a>, which <a href=\"https://www.lesswrong.com/events/tvA9wBLQbotcrFe9z/trigger-action-planning-workshop\">I&#x27;m running a workshop on</a> in NYC.]</em></p><p>Another idea is <a href=\"http://www.focusing.org/\">Gendlin’s focusing</a>. Based on focusing I have realized some feelings like tiredness are not really ‘real’. They’re just a felt sense of not wanting to keep programming. If the two of us were about to play a video game or a board game I liked, I wouldn’t feel tired. So it’s not a real tiredness, just an aversive felt sense.</p><p>And there are bad feedback loops that I don’t know how to break. If I talk to some people online about rationality I’ll get some validation and it will seem important. Whereas if I spend some time learning to program or doing my job, the immediate feedback is not going to be as good. My brain knows this on a deep level. So I’m thinking a lot about how to break this cycle.</p><p><strong>Thank you for doing this interview, you certainly gave me a lot to think about!</strong></p><h1>Follow up thoughts</h1><h2>The Hedonic Treadmill</h2><p>In my guide to getting rich slowly, <a href=\"https://putanumonit.com/2017/02/10/get-rich-slowly/#treadmill\">I wrote that my goal</a> is to improve my material lifestyle by 2% a year. I still stand by it. I think that even slow progress (coupled with technology getting better) feels a lot better than stagnation. Sharp increases in lifestyle don’t increase happiness by a commensurate amount and can’t be ratcheted back without causing suffering. Effective Altruism is a wonderful way to use the extra money for happiness and fulfillment while keeping material lifestyle inflation in check.</p><p>A 2% increase in consumption is certainly achievable if you’re willing to work and do sensible financial planning, especially if your starting point is modest. For other pursuits it’s harder – I don’t know how to guarantee that I sleep with partners who are 2% sexier every year for the rest of my life. </p><h3>Diet</h3><p><a href=\"https://putanumonit.com/2018/03/30/internal-diet-crux/\">How’s my diet going</a>? The experiment was to see if I could lose even 4 pounds, and I predicted that if I can achieve that I’ll be even more motivated to stick to counting calories and daily intermittent fasting. Instead, I quickly lost 5 pounds and then plateaued there, which is really demotivating.</p><p>After interviewing deluks917 I bought some peanuts and carrots and had just that for lunch for four days straight. It worked great in suppressing my appetite and keeping my diet on track, but also really bummed me out. I will keep trying to replace more of what I eat every day with routine staples, but I don’t think I’ll ever get to having nothing but 3 items in my fridge.</p><h3>Taste Tests</h3><p>My friend Spencer<a href=\"https://www.facebook.com/spencer.greenberg/posts/10104081932162022\"> ran a blind tasting experiment</a> which confirmed that people have no idea what they’re drinking and that cheap stuff often tastes better. My favorite blind tasting red wine, which was also the crowd’s favorite, was the <a href=\"https://www.totalwine.com/wine/red-wine/pinot-noir/mirassou-pinot-noir/p/97753750\">Mirassou pinot noir</a> which you can get for under $10.</p><h3>Attachments</h3><p>The issue of attachment brings together Buddhist and Stoic philosophy, although they ultimately arrive at different answers. Buddhism sees the suffering as inevitable – whatever you enjoy will ultimately bring you grief because it is impermanent. It is better to give it up from the start. Stoicism is more optimistic, holding that it’s possible to sever the feeling of attachment without necessarily giving up the object of desire itself.</p><p>Stoicism endorses eating peanut butter, but not forever – just long enough to dispel the fear of losing access to a rich diet. Once you realize how great peanut butter is, you can better enjoy the steaks and caviar. The same is true for things like relationships. I know people who have wasted lifetimes in codependent, abusive, or just unsatisfying relationships because they couldn’t get over the fear of being alone. Getting rid of the fear doesn’t require giving up relationships.</p><p>The Stoic Seneca advised:</p><blockquote>Set aside a certain number of days, during which you shall be content with the scantiest and cheapest fare, with coarse and rough dress, saying to yourself the while: ‘Is this the condition that I feared?’</blockquote><p>When I was 22, I worked 12 hours a day for Israeli minimum wage in a small city in the desert far from my friends. I was single and shared an apartment with 2 roommates and six cats. And I was far from unhappy because I had just finished four years of mandatory military service and civilian life of any sort was an upgrade.</p><p>Today I work 9 hours a day for a whole lot more money, and I live with my wife in New York City surrounded by friends. This is way better, and I wouldn’t give any of it up without a fight. But I know that I can lose it all and the world won’t end.</p><p>Perhaps this is how Buddhism and Stoicism can be integrated – learning to meditate not in order to permanently dissolve all suffering but to know that meditation works and that negative mental states are not all that scary. </p><h3>Suffering</h3><p>I can think of two compelling criticisms of the core Buddhist project: avoiding suffering by letting go of desires.</p><p>The first is utopian – instead of changing our minds to be content with an unsatisfactory universe, let’s change the universe to satisfy our desires. We used to be sick, hungry, and cold; we invented antibiotics, peanut butter, and the Uniqlo ultralight down jacket. Mental illness? Loneliness? Death? We’re working on it. As a benefit, actually solving problems instead of meditating them away helps those who aren’t meditators instead of throwing them under the bus.</p><p>The second answer is… I’ll let Zarathustra speak for himself:</p><blockquote>“We have discovered happiness”—say the last men, and blink thereby.<br/>One still worketh, for work is a pastime. But one is careful lest the pastime should hurt one.<br/>One no longer becometh poor or rich; both are too burdensome. Who still wanteth to rule? Who still wanteth to obey? Both are too burdensome.<br/>Every one wanteth the same; every one is equal: he who hath other sentiments goeth voluntarily into the madhouse.<br/>“Formerly all the world was insane,”—say the subtlest of them,<br/>“We have discovered happiness,”—say the last men, and blink thereby.<br/><em><strong>–</strong></em> <em><a href=\"https://en.wikisource.org/wiki/Thus_Spake_Zarathustra\">Thus Spake Zarathustra</a></em> <em>by <a href=\"https://en.wikisource.org/wiki/Author:Friedrich_Nietzsche\">Friedrich Nietzsche</a>.</em></blockquote><p>The Nietzschian response to Buddhism is that suffering is necessary, for nothing else drives creativity and transformation. The universe is full of dust and gas that feel no suffering, along with a handful of humans who alone are capable of suffering and also doing anything whatsoever. Would we give it up to become more like rocks?</p><p>I find wisdom in all three approaches. Some suffering is an illusion that should be dispelled, some can be solved by applied technology, and some must be borne to propel us to greatness.</p><p>Telling which is which is left as an exercise for the reader.</p>",
    "user": {
      "username": "Jacobian",
      "slug": "jacob-falkovich",
      "displayName": "Jacob Falkovich"
    }
  },
  {
    "_id": "3Y28GJ5qXYdAkiJCt",
    "title": "Rationality Café No. 7 - Bring and Share\n",
    "slug": "rationality-cafe-no-7-bring-and-share",
    "pageUrl": "https://www.lesswrong.com/events/3Y28GJ5qXYdAkiJCt/rationality-cafe-no-7-bring-and-share",
    "postedAt": "2018-12-03T17:26:11.880Z",
    "baseScore": 1,
    "voteCount": 1,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p> Hey gang - <br/>We&#x27;re doing another one of our sporadic but hugely enjoyable meetups. Kind of early this month but we thought it was worth it to avoid losing people for Christmas. <br/></p><p>Pinkmans is getting busier and busier - any ideas about where we could go instead are very welcomed. Museum cafe is only open till 16:30, so we could meet at 14:30 and still get 2 hours in.<br/></p><p>As it&#x27;s Christmassy, we don&#x27;t want to stress you out too much at all. So as a laid back event, you should all bring a &quot;Thing&quot; to share with the rest of the group that has improved your life, or will improve your life going forward. The thing can be an item/concept/idea/tool. Be prepared to talk about it for 5+ minutes or explain it. Feel free to bring a couple of items.<br/></p><p>For example, Nick is going to bring the concept of Sign Language and teach you all the alphabet and a few words. Nick&#x27;s also going to explain why SL is great and everyone should learn it.<br/></p><p>As always, afterwards we&#x27;ll normally devolve into discussion topics that are likely to happen: Less Wrong, Slate Star Codex, Harry Potter and the Methods of Rationality, Worm, Free Will, Efficiency, Podcasts, Politics, Effective Altruism.<br/></p><p>Also this is going to be Nick&#x27;s last directly organised Cafe as he&#x27;s moving to Plymouth. If you would be interested in helping organise it it&#x27;s pretty easy. Just bring it up at the Cafe.</p><p></p>",
    "user": {
      "username": "thegreatnick",
      "slug": "thegreatnick",
      "displayName": "thegreatnick"
    }
  },
  {
    "_id": "urE7CGJ7Rj24jiKPX",
    "title": "EA Bristol December Social\n",
    "slug": "ea-bristol-december-social",
    "pageUrl": "https://www.lesswrong.com/events/urE7CGJ7Rj24jiKPX/ea-bristol-december-social",
    "postedAt": "2018-12-03T17:22:24.057Z",
    "baseScore": 1,
    "voteCount": 1,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p> </p><p>Winter&#x27;s drawing in, so this month we&#x27;re getting comfortable in the Cosy Club. Hope to see you for our regular evening of good conversation, good drinks, and good people! <br/></p><p>Newcomers always welcome; see the links below for introductory material.<br/></p><p>----<br/></p><p>How can we make the world a better place? Which charities do the most good? What are the biggest risks facing humanity and what can we do about it?<br/></p><p>Effective Altruism is a philosophy and social movement that applies evidence and reason to determining the most effective ways to improve the world.<br/></p><p>You can find out more about Effective Altruism at the following pages:<br/></p><p><a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.effectivealtruism.org%2F%3Ffbclid%3DIwAR0lmzrOmB9mbb4cmRbIsLWkdDeMCsPob-H7CXlD3w1doayDzxj8Kr0UX6k&h=AT0p_3AiXsMLYz_9xVeGnGHRYnxqte48OAahtMDn_UajztmZp6qCzDf7xbll7z8MU9SSKumcY05A9dwfiY1Iu8NJ6EFHEI0Rp7TTGrndB8lmrmrGtH583G1oRSZP9ARD\">https://www.effectivealtruism.org/</a> - Main Effective Altruism page<br/><a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fwhatiseffectivealtruism.com%2F%3Ffbclid%3DIwAR1myT49j8m56KrX8ST0pHTbobFT2eXbhE2-0LLcooL8Oe0var-jg1rTTpY&h=AT3IYF_MjbzcBr1df2d6NeO2hyfsvNWN0TC6AObqmnLLDSng3_a_EBfGsEdbRwFtTnT03v0TWhCiKMmwoPGOJVRjiZIDtbotCJw_j9OMRwF-gKw2hlK248trSRjJCHlK\">https://whatiseffectivealtruism.com/</a> - The Effective Altruism FAQ<br/><a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FEffective_altruism%3Ffbclid%3DIwAR1DvpB6VnpP2J_-QoDTo_znEcPt-rU2cFym1_-m9QD86z_TNNtRbqVYQR0&h=AT3ODNu7dc99N2Bvj-ceWDV3Q79EmjJVGPXe_6P6CrMWIURUDAyPhbgV0xnCHYAvli4ews0HXJgrMsh0QV1Wzriug5zM6o_4UNDVM110FoqC2WBXrfRnpVH0VfRKRQBZ\">https://en.wikipedia.org/wiki/Effective_altruism</a> - Wikipedia page<br/><a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.centreforeffectivealtruism.org%2F%3Ffbclid%3DIwAR3RCrrKyuS_GgzCNmDw95Xc2WNruideWXNDXhfj-3gwsBmlQoUvQ-GPWgc&h=AT3ji2M3HYu9T_hdTaUdU2mQlO_W7OV_h0erGwrcq5rII8-ASMAEkPC9bdQvmd_Haj0QbMjmAUM0LZ3tAERD06dCcTCTdZIrq9i56AfadmUxIQGLhGf9-pc3TzxDDFjv\">https://www.centreforeffectivealtruism.org/</a> - Leading EA organisation based in Oxford<br/><a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fea-foundation.org%2F%3Ffbclid%3DIwAR1kr9HwkcU56ZT0GcIzb0Ntdq_BhNtUxKFtYuZChqfdY2EALsjAPiEQGwY&h=AT0njm-FBElN68KO8tAP4KaGewQ5EFlYrFNtjCv6mtSMeeNS5WW-z_CtPNpelOQMA-5bij3lIqz9Ng9tTHyzvVbgY-BfZH_IMpkmC8X67OU8kN6nobMOVqEjyiW7qEsA\">https://ea-foundation.org/</a> - Think tank and project incubator working at the intersection of ethics and science<br/><a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.givingwhatwecan.org%2F%3Ffbclid%3DIwAR2cl_1P-RqddDPoLTlescJ3Zpo_kLGfhQPN-Y7W8jKXdszZvdaLSqt_wvA&h=AT1iPAgiRQ5x2PoA_oQHnBNQmXTJaD4_GctSP6LYfwSV1oA-HDhWkSVcWQ0GImUddS9KmsK_KUpmj4p8YKxYmUSovhFrEAWA_Eo9lwO-T-kvxmeBJ8nzP0EigEc8E_pR\">https://www.givingwhatwecan.org/</a> - Help the world by donating 10% of your income to the most effective charities<br/><a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2F80000hours.org%2F%3Ffbclid%3DIwAR19-rLvcPKUgsP4iPeeXxg6LzDhEOrsgHa2gz6oIkhiSYybu9xu2UcdJOk&h=AT1RmcL-rY1VHx0cYVnQH3ZDhw5MU1-j_DDd21t4719ULFyiRJORq9tz1Gn5RPnZZqNryzo03VDIFg5uJmT0qGUHtWjHaxa_t9CMEepZsmber7BE8dt2g_a6_r_MUFaS\">https://80000hours.org/</a> - How to use your career to help the world most effectively<br/><a href=\"https://l.facebook.com/l.php?u=http%3A%2F%2Feffective-altruism.com%2F%3Ffbclid%3DIwAR1HqpuzT-68R2ZXOikd4UyPQAMptKduMKAJQEPyHP9USB11SnU_KgoUJ1s&h=AT0C0Jnckj8F8VScpF8UIQPcKiD2-TGYW9I6Cwkrr10WcTAJ_zn6eUxbuGxXu_o2cXaWfpMTTvN_r6kx89wvaP2XnZyKI_BxzBM3Eh8Ygs9RYtyuGJbNnKZ99MLFbrAU\">http://effective-altruism.com/</a> - Effective Altruism Forum</p>",
    "user": {
      "username": "thegreatnick",
      "slug": "thegreatnick",
      "displayName": "thegreatnick"
    }
  },
  {
    "_id": "mT6jmW5ZabH4mdSqe",
    "title": "No option to report spam",
    "slug": "no-option-to-report-spam",
    "pageUrl": "https://www.lesswrong.com/posts/mT6jmW5ZabH4mdSqe/no-option-to-report-spam",
    "postedAt": "2018-12-03T13:40:58.514Z",
    "baseScore": 37,
    "voteCount": 14,
    "commentCount": 13,
    "meta": true,
    "question": false,
    "url": null,
    "htmlBody": "<p>I&#x27;ve been seeing quite a bit more spam to the site recently. It might be worthwhile to add an option to report spam instead of the current situation where we can only downvote.</p>",
    "user": {
      "username": "Chris_Leong",
      "slug": "chris_leong",
      "displayName": "Chris_Leong"
    }
  },
  {
    "_id": "QffdgbTEGNRoXMews",
    "title": "Book Review - Other Minds: The Octopus, the Sea, and the Deep Origins of Consciousness",
    "slug": "book-review-other-minds-the-octopus-the-sea-and-the-deep",
    "pageUrl": "https://www.lesswrong.com/posts/QffdgbTEGNRoXMews/book-review-other-minds-the-octopus-the-sea-and-the-deep",
    "postedAt": "2018-12-03T08:00:00.000Z",
    "baseScore": 36,
    "voteCount": 16,
    "commentCount": 18,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<html><head><style type=\"text/css\">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></head><body><p><em>In this post:</em></p>\n<ul>\n<li><em>A true-false test about octopuses</em></li>\n<li><em>What is it like to be an octopus?</em></li>\n<li><em>An exercise in updating on surprising facts</em></li>\n<li><em>Experiments related to animal suffering and consciousness</em></li>\n<li><em>The evolution of aging</em></li>\n<li><em>Should you read</em> Other Minds: The Octopus, the Sea, and the Deep Origins of Consciousness?</li>\n</ul>\n<h2>I. Introduction</h2>\n<p>Peter Godfrey-Smith's <em>Other Minds: the Octopus, the Sea, and the Deep Origins of Consciousness</em> is a phenomenal mishmash of octopus- and consciousness-related topics. It deals with everything from the evolution of octopuses, to their social life, to animal consciousness (including octopus consciousness), to evolutionary theories of aging, and more. All of this is tied together by a palpable fascination with octopuses, which manifests itself in rich descriptions of Godfrey-Smith's own experiences scuba-diving off the coast of Australia to observe them.</p>\n<p>The book attempts to fit discussion of an impressive amount of interesting topics all into one slim volume. On the one hand, this is great, as each topic is fascinating in its own right, and several are relevant to EA/rationality. On the other hand, fitting in so many topics is a difficult task which the book only halfway pulls off. There wasn’t enough room to discuss each topic in as much depth as they deserved, and the breadth of topics meant that the book felt somewhat unorganized and disunified. The book as a whole didn’t seem to have any central claim; it was simply a collection of interesting facts, observations, musings, and theories that somehow relate to either octopuses, consciousness, or both, plus a bunch of fascinating first-hand descriptions of octopus behavior.</p>\n<p>Do I recommend the book? Yes and no. For general interest, definitely--it’s an interesting, enjoyable read; but for rationalists and EAs, there are probably better things to read on each topic the book discusses that would go into more depth, so it may not be the most effective investment of time for learning about, say, theories of animal consciousness. So in the rest of this review I’ve tried to 80:20 the book a bit, pulling out the insights that I found most interesting and relevant to EA/rationalism (as well as adding my own musings here and there). Because of this, the review is both quite long, and unavoidably reflects a bit of the disjointedness of the book itself--the sections can largely be read independently of each other.</p>\n<hr>\n<p>Before I begin, a true-false test. For the following statements about octopuses, write down whether you think they are true or false, and how confident you are in your response. We'll come back to these later in the review, and the answers will be at the end:</p>\n<ol>\n<li>Octopuses can squirt jets of ink as an escape tactic.</li>\n<li>Octopuses have color vision.</li>\n<li>Octopuses have bilateral symmetry.</li>\n<li>Octopuses can camouflage themselves nearly perfectly by changing the color and texture of their skin to match whatever surface or object they are trying to blend into.</li>\n<li>Octopuses can fit through any hole or gap bigger than their eye.</li>\n<li>Octopuses can recognize individual humans.</li>\n<li>Most octopus species live for more than 20 years.</li>\n<li>Octopuses are mostly solitary animals.</li>\n<li>Octopuses have been known to use shards of glass from shattered bottles on the seafloor as weapons to fight other octopuses.</li>\n</ol>\n<p>Answers below.</p>\n<hr>\n<h2>II. What is it like to be an octopus?</h2>\n<p>The nervous system of an octopus is structured quite differently than a mammalian nervous system. The octopus not only has a high concentration of neurons in its head (a “brain”), but also clusters of neurons throughout their body, particularly in each arm. At one point Godfrey-Smith notes that the number of neurons in the octopus's central brain is only a little over half of the number of neurons in the rest of its body (67). This means that each arm must have, roughly, 1/4 as many neurons as the central brain. What might it feel like to have 8 other \"brains\" in your body, each 1/4 the size of your \"main brain\"?[^1]</p>\n<p>Most likely, it feels completely different than we're even capable of imagining. Godfrey-Smith doesn’t discuss the question, and there are some philosophers, such as Daniel Dennett, who would deny that it even makes sense. Nevertheless, there are two main points of reference that I can think of that humans might use to imagine something of what this would feel like.</p>\n<p>First, we humans have what is sometimes called a \"<a href=\"https://www.ted.com/talks/heribert_watzke_the_brain_in_your_gut#t-894711\">gut brain</a>\": <a href=\"https://en.wikipedia.org/wiki/Enteric_nervous_system\">~500 million neurons in our digestive tracts</a>, <a href=\"https://www.ted.com/talks/heribert_watzke_the_brain_in_your_gut/transcript#t-528989\">about as many as in a cat's brain</a>, that control our digestive process. What does it feel like to have this brain? Well, hunger signals, the gag reflex, and some <a href=\"https://www.ted.com/talks/heribert_watzke_the_brain_in_your_gut/transcript#t-528989\">emotions (via the limbic system)</a>, are controlled by this brain, so roughly, it feels like what those mental states feel like. Perhaps the octopus has similar signals that come from its arm brains. These likely feel nothing like the signals coming from our gut brain, and would of course have different functions: while our gut brain sends us hunger or disgust reflexes, the octopus's arm signals might function something like our propioceptive sense, informing the \"main brain\" about the location of the arms, and maybe also relaying touch and taste/smell information from the respective sense organs located on the arms.</p>\n<p>Or perhaps the arm brains don't just relay sensory information from the arms, perhaps they also play a part in controlling the arms' movements (Godfrey-Smith posits that this is the main reason for why the octopus's nervous system is so distributed). Sticking with the gut-brain/arm-brain analogy for the moment, what does it feel like for the gut-brain to control the stomach? That is, what does it feel like to digest food? ... Often like nothing at all. We sometimes don't even notice digestion occurring, and we certainly don't have any detailed sensation of what's going on when it does. So perhaps the octopus just tells its arms where to go in broad strokes, and they take it from there, similar to how our gut-brain just \"takes it from there.\"</p>\n<p>The idea that the arm brains control the movement of the arms also brings me to my second comparison: <a href=\"https://en.wikipedia.org/wiki/Split-brain\">split-brain syndrome</a>. Split-brain results when the corpus callosum is surgically severed (usually as a treatment for epilepsy). After this operation, patients can function mostly normally, but it can be experimentally determined that the two sides of their body function somewhat independently of each other. For example:</p>\n<blockquote>\n<p>a patient with split brain is shown a picture of a chicken foot and a snowy field in separate visual fields and asked to choose from a list of words the best association with the pictures. The patient would choose a chicken to associate with the chicken foot and a shovel to associate with the snow; however, when asked to reason why the patient chose the shovel, the response would relate to the chicken (e.g. \"the shovel is for cleaning out the chicken coop\"). (<a href=\"https://en.wikipedia.org/wiki/Split-brain\">source</a>) (<a href=\"http://thebrain.mcgill.ca/flash/capsules/experience_bleu06.html\">original source</a>)</p>\n</blockquote>\n<p>Notoriously, the two sides can sometimes get into conflict, as when one split-brain patient violently <a href=\"https://en.wikipedia.org/wiki/Split-brain\">shook</a> his wife with his left hand while trying to stop himself with his right hand.</p>\n<p>What does it feel like for that to happen? Well, I don't actually know. But I wonder, does this person feel touch sensations from their left hand? If so, are these produced by the same processes that patch over our blind spot, or are they actual touch sensations? Suppose you put a split-brain patient’s left hand behind an opaque barrier; would they be able to tell when something touched it?</p>\n<p>Assuming that the answer to this question is \"no,\" another possibility for what having \"arm-brains\" could feel like is \"basically nothing\": just as the split-brain patient only knows what their left side is doing through external cues, like seeing it move, and doesn't have any control over it, so too with the octopus's arms. It doesn't really \"feel\" what's going on in its arms, the arms themselves know what's going on and that's enough.</p>\n<h2>III. An exercise in updating on surprising facts</h2>\n<p>[Note: this section is largely recycled from one of my <a href=\"https://www.lesswrong.com/posts/GpeMNbmNcGH4b4X7m/ikaxas-shortform-feed#AwLBToBrytRGfujyy\">shortform posts</a>; if you’ve already read that, this will be mostly redundant. If you haven’t read that, simply read on.]</p>\n<p>I confess, the purpose of the true-false test at the beginning of this review was largely to disguise one question in particular, so that the mere asking of it didn’t provide Bayesian evidence that the answer should be surprising: \"6.) Octopuses can recognize individual humans.\" Take a moment to look back at your answer to that question. What does your model say about whether octopuses should be able to recognize individual humans? Why can humans recognize other individual humans, and what does that say about whether octopuses should be able to?</p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p>As it turns out, octopuses can recognize individual humans. For example, in the book it's mentioned that at one lab, one of the octopuses had a habit of squirting jets of water at one particular researcher. Take a moment to <a href=\"https://www.lesswrong.com/posts/WnheMGAka4fL99eae/hindsight-devalues-science\">let it sink in</a> how surprising this is: octopuses, which 1.) are mostly nonsocial animals, 2.) have a completely different nervous system structure that evolved on a completely different branch of the tree of life, and 3.) have no evolutionary history of interaction with humans, can recognize individual humans, and differentiate them from other humans. I'm pretty sure humans have a hard time differentiating between individual octopuses.</p>\n<p>And since things are <a href=\"https://www.lesswrong.com/rationality/mind-projection-fallacy\">not inherently surprising</a>, <a href=\"https://www.lesswrong.com/posts/f6ZLxEWaankRZ2Crv/probability-is-in-the-mind\">only surprising to models</a>, this means my world model (and yours, if you were surprised by this) needs to be updated. First, generate a couple of updates you might make to your model after finding this out. I'll wait...</p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p>...</p>\n<p>Now that you've done that, here's what I came up with:</p>\n<p>(1) Perhaps the ability to recognize individuals isn't as tied to being a social animal as I had thought\n(2) Perhaps humans are easier to tell apart than I thought (i.e. humans have more distinguishing features, or these distinguishing features are larger/more visually noticeable, etc., than I thought)\n(3) Perhaps the ability to distinguish individual humans doesn't require a specific psychological module, as I had thought, but rather falls out of a more general ability to distinguish objects from each other (Godfrey-Smith mentions this possibility in the book).\n(4) Perhaps I'm overimagining how fine-grained the octopus's ability to distinguish humans is. I.e. maybe that person was the only one in the lab with a particular hair color or something, and they can't distinguish the rest of the people. (Though note, another example given in the book was that one octopus liked to squirt new people, people it hadn't seen regularly in the lab before. This wouldn't mesh very well with the \"octopuses can only make coarse-grained distinctions between people\" hypothesis.)</p>\n<p>To be clear, those were my first thoughts; I don't think all of them are correct. As per my <a href=\"https://www.lesswrong.com/posts/GpeMNbmNcGH4b4X7m/ikaxas-shortform-feed#AwLBToBrytRGfujyy\">shortform post</a> about this, I'm mostly leaning towards answer (2) being the correct update -- maybe the reason octopuses can recognize humans but not the other way around is mostly because individual humans are just more visually distinct from each other than individual octopuses, in that humans have a wider array of distinguishing features or these features are larger or otherwise easier to notice. But of course, these answers are neither mutually exclusive nor exhaustive. For example, I think answer (3) also probably has something to do with it. I suspect that humans probably have a specific module for recognizing humans, but it seems clear that octopuses couldn't have such a module, so it must not be strictly necessary in order to tell humans apart. Maybe a general object-recognizing capability plus however visually distinct humans are from each other is enough.[^2]</p>\n<p>I'd also love to hear in the comments what updates other people had from this.</p>\n<h2>IV. Animal consciousness</h2>\n<p>Something else from the book that I found interesting concerns animal consciousness/subjective experience. I suspect this is old hat for those who have done any significant research into animal suffering, but it added a couple more gears to my model of animal consciousness, so I'll share it here for those whose models were similarly gear-less.\nRemember <a href=\"https://en.wikipedia.org/wiki/Blindsight\">blindsight</a> (where people who are blind due to damage in their visual cortex can perform better than chance at vision tasks, because the rest of their brain still gets visual information, even though they don’t have access to it consciously)? A pair of vision scientists (Milner and Goodale) believe, roughly, that that's what's going on in frogs all the time. What convinced them of this is an experiment performed by David Ingle in which he was able to surgically reverse some, but not all, of the visual abilities of some froggy test subjects. Namely, when his frogs saw a fly in one side of their visual field, they would snap as if it were on the other, but they were able to go around barriers perfectly normally. Milner and Goodale take this as evidence that the frog doesn't have an integrated visual experience at all. They write:</p>\n<blockquote>\n<p>So what did these rewired frogs \"see\"? There is no sensible answer to this. The question only makes sense if you believe that the brain has a single visual representation of the outside world that governs all of an animal's behavior. Ingle's experiments reveal that this cannot possibly be true. (Milner and Goodale 2005, qtd. in Peter Godfrey-Smith, Other Minds, 2016, p. 80)</p>\n</blockquote>\n<p>Godfrey-Smith then goes on to discuss Milner and Goodale's view:</p>\n<blockquote>\n<p>Once you accept that a frog does not have a unified representation of the world, and instead has a number of separate streams that handle different kinds of sensing, there is no need to ask what the frog sees: in Milner and Goodale's words, \"the puzzle disappears.\"\nPerhaps one puzzle disappears, but another is raised. What does it feel like to be a frog perceiving the world in this situation? I think Milner and Goodale are suggesting that it feels like nothing. There is no experience here because the machinery of vision in frogs is not doing the sorts of things it does in us that give rise to subjective experience. (Godfrey-Smith, pp. 89-90)[^3]</p>\n</blockquote>\n<p>Though he doesn't mention it, there seems to me to be an obvious reply here: the phenomenon of blindsight reveals that there are parts of our visual processing that don't feel like anything to us (or perhaps, as Godfrey-Smith prefers, they feel like something, just not like vision), but this clearly doesn't change the fact that we (most of us) do have visual experience. Why couldn't something similar be going on in the frogs? They have a visual field, but they also have other visual processing going on as well which doesn't make it into their visual field.</p>\n<p>Let me try to explain this thought a bit better. One thing that the human blindsight subject described in Other Minds (known as \"DF\") was able to do was put letters through a mail-slot placed at different angles. Now those of us who have normal sight presumably still do all the same processing as those with blindsight, plus some extra. So, imagine someone performing brain surgery on a person with normal vision, which affected whatever brain circuitry allows people to align a letter at the correct angle to get it through a mail-slot. At the risk of arguing based on <a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">evidence I haven't seen yet</a>, one way I could imagine the scenario playing out is the following: this person would wake up and find that, though their visual experience was the same as before, for some reason they couldn't manage to fit letters through mail-slots anymore. They would experience this in a similar way as someone with exceptionally poor balance experiences their inability to walk a tightrope--it's not as though they can't see where the rope is, they just can't manage to put their feet in the right place to stay on it. I'd guess that the same thing would happen for the person, and that the same thing is happening for the frog. Respectively: it's not as though the person can't see where the mail-slot is, they just can't manage to get the letter through it, and it's not as though the frog can't see where the fly is, it just can't seem to get its tongue to move in the right direction to catch it.[^4]</p>\n<p>In any case, even if we discount this argument, does Milner and Goodale's argument amount to an argument that most animals don't have inner lives, and in particular that they don't feel pain?</p>\n<p>Not so, Godfrey-Smith wants to argue. He includes some discussion of various theories of consciousness/subjective experience and how early or late it arose,[^5] but what interested me was an experiment that tried to test whether an animal, in this case a Zebrafish, actually feels pain, or is only performing instinctive behaviors that look to us like pain.</p>\n<p>The experiment goes like this: There are two environments, A and B, and the fish is known to prefer A to B. The experimenter injects the fish with a chemical thought to be painful. Then, the experimenter dissolves painkiller in environment B, and lets the fish choose again which environment it prefers. With the painkiller and the painful chemical, the fish prefers environment B (though with the painful chemical and no painkiller, it still prefers A). The fish seems to be choosing environment B in order to relieve its pain, and this isn't the kind of situation that the fish could have an evolved reflex to react to. Since the fish is behaving as we would expect it to if it felt pain and the opposite of how we would expect it to if it didn't feel pain, and a reflex can't be the explanation, this is evidence that the fish feels pain, rather than simply seeming to feel pain.</p>\n<p>What excited me about this was the idea that we could use experiments to tell something about the inner lives of animals. Even though I've been thoroughly disabused of the idea of a philosophical zombie,[^6] I still had the idea that subjective experience is something that can't really be tested \"from the outside.\" Reading about these experiments made me much more optimistic that experiments could be useful to help determine whether and which animals are moral patients.</p>\n<h2>V. Aging</h2>\n<p>Another fact that might surprise (and perhaps sadden) you: octopuses, for the most part, only live about 2 years. One might think that intelligence is most advantageous when you <a href=\"https://www.youtube.com/watch?v=r7sRMTWNr_U\">live long enough</a> to benefit from the things you learn with it. Nevertheless, octopuses only live about 2 years. Why is this? Godfrey-Smith posits that octopuses evolved intelligence not for the benefits of long-term learning, but simply to control their highly-amorphous bodies. Since an octopus’s body can move so freely, it takes a very large nervous system to control it, which gave rise to what intelligence they possess. Even so, once they had intelligence, shouldn’t this have caused selection pressure towards longer lives? I’m still confused on this count, but this does lead us to another question: why do most living organisms age in the first place? There are organisms that don’t, at least on the timescales we’ve observed them on so far, so why are there any that do? What evolutionary benefit does aging provide, could it provide? One would think that aging, at least once an organism had reached maturity, would be strictly disadvantageous and thus selected against, so why do we mostly observe organisms that age and die?</p>\n<p>Godfrey-Smith surveys several standard theories, but the one he presents as most likely to be correct (originated by Peter Medawar and George Williams) is as follows. Imagine an organism that didn’t age; once it reached its prime, it remained that way, able to survive and reproduce indefinitely until it died of e.g. predation, disease, a falling rock, or some other external cause, all of which I’ll call “accidental death.” If we assume the average probability of dying by accidental death is constant each year, then the organism’s probability <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span></span> of surviving to age n decreases as n increases. Thus, for large enough <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span>, <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span></span> approaches 0, meaning that there is some age <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span> which the organism is almost certain to die before reaching, even without aging. Now imagine that the organism has a mutation with effects that are positive before age <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span>, but negative after age <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span>. Such a mutation would have almost no selection pressure against it, since the organism would almost certainly die of accidental death before its negative effects could manifest. Thus, such mutations could accumulate, and the few organisms that did survive to age <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span> would start to show those negative effects.</p>\n<p>The truth is more general than that. In general, as <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p_n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span></span> gets lower, so does the selection pressure against any mutation whose negative effects only appear after age <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span>. This theory predicts that organisms should exhibit a slow and steady increase of negative symptoms caused by mutations whose negative side effects only show up later, and an age which almost no individuals survive beyond, which is what we in fact observe.</p>\n<p>Still though, why should there be any <em>positive</em> pressure towards these mutations, even if there’s little pressure against them? Because, as I mentioned, at least some of these mutations might have positive effects that show up earlier bound up with the negative effects that show up later. This positive selection pressure, combined with the reduced negative selection pressure due to their negative effects only showing up late, after most with the mutation have already died due to accidental death, is enough to get these mutations to fixation. Godfrey-Smith uses the analogy, originally due to George Williams, of putting money in a savings account to be accessed when you’re 120 years old. You’ll almost certainly be dead by then, so it’s rather pointless to save for that far off. In the same way, it’s evolutionarily pointless for organisms to pass up mutations that have positive effects now and negative effects later when those negative effects only show up after the animal is almost certain to be dead by accidental death. So organisms take those mutations, and most do not survive to pay the price; aging is what happens to those who do.</p>\n<p>If this is the correct evolutionary account of why aging occurs, it has an interesting implication for anti-aging research: there might be certain routes to eliminating aging that come with unforeseen downsides. If we were to eliminate aging by finding the genes that produce these negative side effects and turning them off (please forgive my utter ignorance of genetics and the science of aging), this could also rob us of whatever benefits those genes provided earlier in life that caused them to be adopted in the first place. This is not to say that we should not pursue anti-aging research (in fact I’m strongly in favor of it), but just that we should be on the lookout for this kind of trap, and avoid it if we can.</p>\n<hr>\n<h2># Appendix: Answers to True-False Questions</h2>\n<ol>\n<li>Octopuses can squirt jets of ink as an escape tactic. <em>True</em></li>\n<li>Octopuses have color vision. <em>False</em></li>\n<li>Octopuses have bilateral symmetry. <em>True</em></li>\n<li>Octopuses can camouflage themselves nearly perfectly by changing the color and texture of their skin to match whatever surface or object they are trying to blend into. <em>True</em></li>\n<li>Octopuses can fit through any hole or gap bigger than their eye. <em>True</em></li>\n<li>Octopuses can recognize individual humans. <em>True</em></li>\n<li>Most octopus species live for more than 20 years. <em>False</em></li>\n<li>Octopuses are mostly solitary animals. <em>True</em></li>\n<li>Octopuses have been known to use shards of glass from shattered bottles on the seafloor as weapons to fight other octopuses. <em>As far as I know, false</em></li>\n</ol>\n<hr>\n<h3>Notes:</h3>\n<p>[^1]: To give a sense of the relationship between the octopus's central brain and its arms, here are some quotes from the book:</p>\n<blockquote>\n<p>How does an octopus's brain relate to its arms? Early work, looking at both behavior and anatomy, gave the impression that the arms enjoyed considerable independence. The channel of nerves that leads from each arm back to the central brain seemed pretty slim. Some behavioral studies gave the impression that octopuses did not even track where their own arms might be. As Roger Hanlon and John Messenger put it in their book Cephalopod Behavior, the arms seemed \"curiously divorced\" from the brain, at least in the control of basic motions. (67)</p>\n</blockquote>\n<blockquote>\n<p>Some sort of mixture of localized and top-down control might be operating. The best experimental work I know that bears on this topic comes out of Binyamin Hochner's laboratory at the Hebrew University of Jerusalem. A 2011 paper by Tamar Gutnick, Ruth Byrne, and Michael Kuba, along with Hochner, described a very clever experiment. They asked whether an octopus could learn to guide a single arm along a maze-like path to a specific place in order to obtain food. The task was set up in such a way that the arm's own chemical sensors would not suffice to guide it to the food; the arm would have to leave the water at one point to reach the target location. But the maze walls were transparent, so the target location could be seen. The octopus would have to guide an arm through the maze with its eyes.\nIt took a long while for the octopuses to learn to do this, but in the end, nearly all of the octopuses that were tested succeeded. The eyes can guide the arms. At the same time, the paper also noted that when the octopuses are doing well with this task, the arm that's finding the food appears to do its own local exploration as it goes, crawling and feeling around. So it seems that two forms of control are working in tandem: there is central control of the arm's overall path, via the eyes, combined with a fine-tuning of the search by the arm itself. (68-69)</p>\n</blockquote>\n<p>[^2]: So why do I still think humans have a specific module for it? Here’s one possible reason: I'm guessing octopuses can't recognize human faces--they probably use other cues, though nothing in the book speaks to this one way or the other. If that's the case, then it might be true both that a general object-differentiating capability is enough to recognize individual humans, but that to recognize faces requires a specific module. If I found out that octopuses could recognize human faces specifically, not just individual humans by other means than face-recognition, I would strongly update in favor of humans having no specific face- or other-person-recognition module. In the same vein, the fact that people can lose the ability to recognize faces without it affecting any other visual capacities (known as <a href=\"https://en.wikipedia.org/wiki/Prosopagnosia\">prosopagnosia</a> or \"face-blindness\") suggests that a single module is responsible for that ability.</p>\n<p>[^3]: After reading Daniel Dennett’s Consciousness Explained, it’s actually not at all clear to me why Godfrey-Smith interprets Milner and Goodale this way. It seems more natural to suppose that they’re suggesting something similar to Dennett’s denial of the “Cartesian Theater” (the idea that there is somewhere where “it all comes together” in the brain, in some sort of “inner movie” to use Chalmers’ phrase) and his replacement, the “Multiple Drafts Model” (which I don’t feel confident to summarize here).</p>\n<p>[^4]: Another way this might play out is if the frog saw the fly and only the fly as reversed in its visual field, rather like a hallucination. I don't see any reason why that would be impossible.</p>\n<p>[^5]: Godfrey-Smith actually makes a distinction between \"subjective experience\" and \"consciousness.\" The way Godfrey-Smith uses these words, when we say that something has \"subjective experience,\" we're just saying that there is something that it feels like to be that thing, while the claim that something has \"consciousness\" is in some unspecified way stronger. So consciousness is a subset of subjective experience. He speculates that subjective experience arose fairly early, in the form of things like hunger signals and pain, while consciousness arose later and involves things like memory, a \"global workspace,\" integrated experience, etc.</p>\n<p>[^6]: See Dennett, <a href=\"https://www.amazon.com/Intuition-Pumps-Other-Tools-Thinking/dp/0393082067/\"><em>Intuition Pumps and Other Tools for Thinking</em></a>, Ch. 55 “Zombies and Zimboes,” and Eliezer Yudkowsky’s essay <a href=\"https://www.readthesequences.com/Zombies-Zombies\">“Zombies! Zombies?”</a></p>\n</body></html>",
    "user": {
      "username": "Ikaxas",
      "slug": "ikaxas",
      "displayName": "Vaughn Papenhausen"
    }
  },
  {
    "_id": "NxF5G6CJiof6cemTw",
    "title": "Coherence arguments do not entail goal-directed behavior",
    "slug": "coherence-arguments-do-not-entail-goal-directed-behavior",
    "pageUrl": "https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior",
    "postedAt": "2018-12-03T03:26:03.563Z",
    "baseScore": 134,
    "voteCount": 60,
    "commentCount": 69,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>One of the most pleasing things about probability and expected utility theory is that there are many <em>coherence arguments</em> that suggest that these are the “correct” ways to reason. If you deviate from what the theory prescribes, then you must be executing a <em>dominated strategy</em>. There must be some other strategy that never does any worse than your strategy, but does strictly better than your strategy with certainty in at least one situation. There’s a good explanation of these arguments <a href=\"https://arbital.com/p/expected_utility_formalism/?l=7hh\">here</a>.</p><p>We shouldn’t expect mere humans to be able to notice any failures of coherence in a superintelligent agent, since if we could notice these failures, so could the agent. So we should expect that <a href=\"https://arbital.com/p/optimized_agent_appears_coherent/\">powerful agents appear coherent to us</a>. (Note that it is possible that the agent doesn’t fix the failures because it would not be worth it -- in this case, the argument says that we will not be able to notice any <em>exploitable</em> failures.)</p><p>Taken together, these arguments suggest that we should model an agent much smarter than us as an expected utility (EU) maximizer. And many people agree that EU maximizers are dangerous. So does this mean we’re doomed? I don’t think so: it seems to me that the problems about EU maximizers that we’ve identified are actually about <em><a href=\"https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma\">goal-directed behavior</a></em> or <em>explicit reward maximizers.</em> The coherence theorems say nothing about whether an AI system must look like one of these categories. This suggests that we could try building an AI system that can be modeled as an EU maximizer, yet doesn’t fall into one of these two categories, and so doesn’t have all of the problems that we worry about.</p><p>Note that there are two different flavors of arguments that the AI systems we build will be goal-directed agents (which are dangerous if the goal is even slightly wrong):</p><ul><li>Simply knowing that an agent is intelligent lets us infer that it is goal-directed. (EDIT: See <a href=\"https://www.lesswrong.com/posts/DkcdXsP56g9kXyBdq/coherence-arguments-imply-a-force-for-goal-directed-behavior?commentId=LvmHoLEhKLJzBrgrZ\">these</a> <a href=\"https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent#Gu4syyKBsRSQkktkJ\">comments</a> for more details on this argument.)</li><li>Humans are particularly likely to build goal-directed agents.</li></ul><p>I will only be arguing against the first claim in this post, and will talk about the second claim in the next post.</p><h2>All behavior can be rationalized as EU maximization</h2><p>Suppose we have access to the entire policy of an agent, that is, given any universe-history, we know what action the agent will take. Can we tell whether the agent is an EU maximizer?</p><p>Actually, <em>no matter what the policy is</em>, we can view the agent as an EU maximizer. The construction is simple: the agent can be thought as optimizing the utility function U, where U(h, a) = 1 if the policy would take action a given history h, else 0. Here I’m assuming that U is defined over histories that are composed of states/observations and actions. The actual policy gets 1 utility at every timestep; any other policy gets less than this, so the given policy perfectly maximizes this utility function. This construction has been given before, eg. at the bottom of page 6 of <a href=\"https://arxiv.org/abs/1811.07871\">this paper</a>. (I think I’ve seen it before too, but I can’t remember where.)</p><p>But wouldn’t this suggest that the VNM theorem has no content? Well, we assumed that we were looking at the <em>policy </em>of the agent, which led to a universe-history <em>deterministically</em>. We didn’t have access to any probabilities. Given a particular action, we knew exactly what the next state would be. Most of the axioms of the VNM theorem make reference to lotteries and probabilities -- if the world is deterministic, then the axioms simply say that the agent must have transitive preferences over outcomes. Given that we can only observe the agent choose one history over another, we can trivially construct a transitive preference ordering by saying that the chosen history is higher in the preference ordering than the one that was not chosen. This is essentially the construction we gave above.</p><p>What then is the purpose of the VNM theorem? It tells you how to behave <em>if you have probabilistic beliefs about the world</em>, as well as a <em>complete and consistent preference ordering over outcomes</em>. This turns out to be not very interesting when “outcomes” refers to “universe-histories”. It can be more interesting when “outcomes” refers to world <em>states</em> instead (that is, snapshots of what the world looks like at a particular time), but utility functions over states/snapshots can’t capture everything we’re interested in, and there’s no reason to take as an assumption that an AI system will have a utility function over states/snapshots.</p><h2>There are no coherence arguments that say you must have goal-directed behavior</h2><p>Not all behavior can be thought of as <a href=\"https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma\">goal-directed</a> (primarily because I allowed the category to be defined by fuzzy intuitions rather than something more formal). Consider the following examples:</p><ul><li>A robot that constantly twitches</li><li>The agent that always chooses the action that starts with the letter “A”</li><li>The agent that follows the policy &lt;policy&gt; where for every history the corresponding action in &lt;policy&gt; is generated randomly.</li></ul><p>These are not goal-directed by my “definition”. However, they can all be modeled as expected utility maximizers, and there isn’t any particular way that you can exploit any of these agents. Indeed, it seems hard to model the twitching robot or the policy-following agent as having any preferences at all, so the notion of “exploiting” them doesn’t make much sense.</p><p>You could argue that neither of these agents are <em>intelligent</em>, and we’re only concerned with superintelligent AI systems. I don’t see why these agents could not in principle be intelligent: perhaps the agent knows how the world would evolve, and how to intervene on the world to achieve different outcomes, but it does not act on these beliefs. Perhaps if we peered into the inner workings of the agent, we could find some part of it that allows us to predict the future very accurately, but it turns out that these inner workings did not affect the chosen action at all. Such an agent is in principle possible, and it seems like it is intelligent.</p><p>(If not, it seems as though you are <em>defining</em> intelligence to also be goal-driven, in which case I would frame my next post as arguing that we may not want to build superintelligent AI, because there are other things we could build that are as useful without the corresponding risks.)</p><p>You could argue that while this is possible in principle, no one would ever build such an agent. I wholeheartedly agree, but note that this is now an argument based on particular empirical facts about humans (or perhaps agent-building processes more generally). I’ll talk about those in the next post; here I am simply arguing that merely knowing that an agent is intelligent, with no additional empirical facts about the world, does not let you infer that it has goals.</p><p>As a corollary, since all behavior can be modeled as maximizing expected utility, but not all behavior is goal-directed, it is not possible to conclude that an agent is goal-driven if you only know that it can be modeled as maximizing some expected utility. However, if you know that an agent is maximizing the expectation of an <em>explicitly represented</em> utility function, I would expect that to lead to goal-driven behavior most of the time, since the utility function must be relatively simple if it is explicitly represented, and <em>simple</em> utility functions seem particularly likely to lead to goal-directed behavior.</p><h2>There are no coherence arguments that say you must have preferences</h2><p>This section is another way to view the argument in the previous section, with “goal-directed behavior” now being operationalized as “preferences”; it is not saying anything new.</p><p>Above, I said that the VNM theorem assumes both that you use probabilities and that you have a preference ordering over outcomes. There are lots of good reasons to assume that a good reasoner will use probability theory. However, there’s not much reason to assume that there is a preference ordering over outcomes. The twitching robot, “A”-following agent, and random policy agent from the last section all seem like they don’t have preferences (in the English sense, not the math sense).</p><p>Perhaps you could define a preference ordering by saying “if I gave the agent lots of time to think, how would it choose between these two histories?” However, you could apply this definition to <em>anything</em>, including eg. a thermostat, or a rock. You might argue that a thermostat or rock can’t “choose” between two histories; but then it’s unclear how to define how an AI “chooses” between two histories without that definition also applying to thermostats and rocks.</p><p>Of course, you could always define a preference ordering based on the AI’s observed behavior, but then you’re back in the setting of the first section, where <em>all</em> observed behavior can be modeled as maximizing an expected utility function and so saying “the AI is an expected utility maximizer” is vacuous.</p><h2>Convergent instrumental subgoals are about goal-directed behavior</h2><p>One of the classic reasons to worry about expected utility maximizers is the presence of convergent instrumental subgoals, detailed in Omohundro’s paper <a href=\"https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">The Basic AI Drives</a>. The paper itself is clearly talking about goal-directed AI systems:</p><p><em>To say that a system of any design is an “artificial intelligence”, we mean that it has goals which it tries to accomplish by acting in the world.</em></p><p>It then argues (among other things) that such AI systems will want to “be rational” and so will distill their goals into utility functions, which they then maximize. And once they have utility functions, they will protect them from modification.</p><p>Note that this starts from the assumption of goal-directed behavior and <em>derives</em> that the AI will be an EU maximizer along with the other convergent instrumental subgoals. The coherence arguments all imply that AIs will be EU maximizers for some (possibly degenerate) utility function; they don’t prove that the AI must be goal-directed.</p><h2>Goodhart’s Law is about goal-directed behavior</h2><p>A common argument for worrying about AI risk is that we know that a superintelligent AI system will look to us like an EU maximizer, and if it maximizes a utility function that is even slightly wrong we could get catastrophic outcomes.</p><p>By now you probably know my first response: that <em>any</em> behavior can be modeled as an EU maximizer, and so this argument proves too much, suggesting that any behavior causes catastrophic outcomes. But let’s set that aside for now.</p><p>The second part of the claim comes from arguments like <a href=\"https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile\">Value is Fragile</a> and <a href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\">Goodhart’s Law</a>. However, if we consider utility functions that assign value 1 to some histories and 0 to others, then if you accidentally assign a history where I needlessly stub my toe a 1 instead of a 0, that’s a slightly wrong utility function, but it isn’t going to lead to catastrophic outcomes.</p><p>The worry about utility functions that are <em>slightly wrong</em> holds water when the utility functions are wrong about some <em>high-level</em> concept, like whether humans care about their experiences reflecting reality. This is a very rarefied, particular distribution of utility functions, that are all going to lead to goal-directed or agentic behavior. As a result, I think that the argument is better stated as “if you have a slightly incorrect goal, you can get catastrophic outcomes”. And there aren’t any coherence arguments that say that agents must have goals.</p><h2>Wireheading is about explicit reward maximization</h2><p>There are <a href=\"https://intelligence.org/files/LearningValue.pdf\">a</a> <a href=\"http://people.idsia.ch/~ring/AGI-2011/Paper-B.pdf\">few</a> <a href=\"https://arxiv.org/abs/1605.03142\">papers</a> that talk about the problems that arise with a very powerful system with a reward function or utility function, most notably wireheading. The argument that AIXI will seize control of its reward channel falls into this category. In these cases, typically the AI system is considering making a change to the system by which it evaluates goodness of actions, and the goodness of the change is evaluated by the system <em>after the change</em>. Daniel Dewey argues in <a href=\"https://intelligence.org/files/LearningValue.pdf\">Learning What to Value</a> that if the change is evaluated by the system <em>before</em> the change, then these problems go away.</p><p>I think of these as problems with <em>reward</em> maximization, because typically when you phrase the problem as maximizing reward, you are maximizing the sum of rewards obtained in all timesteps, no matter how those rewards are obtained (i.e. even if you self-modify to make the reward maximal). It doesn’t seem like AI systems have to be built this way (though admittedly I do not know how to build AI systems that reliably avoid these problems).</p><h2>Summary</h2><p>In this post I’ve argued that many of the problems we typically associate with expected utility maximizers are actually problems with goal-directed agents or with explicit reward maximization. Coherence arguments only entail that a superintelligent AI system will look like an expected utility maximizer, but this is actually a vacuous constraint, and there are many potential utility functions for which the resulting AI system is neither goal-directed nor explicit-reward-maximizing. This suggests that we could try to build AI systems of this type, in order to sidestep the problems that we have identified so far.</p>",
    "user": {
      "username": "rohinmshah",
      "slug": "rohinmshah",
      "displayName": "Rohin Shah"
    }
  },
  {
    "_id": "PRaxzmDJdvie46ahL",
    "title": "Benign model-free RL",
    "slug": "benign-model-free-rl-1",
    "pageUrl": "https://www.lesswrong.com/posts/PRaxzmDJdvie46ahL/benign-model-free-rl-1",
    "postedAt": "2018-12-02T04:10:45.205Z",
    "baseScore": 15,
    "voteCount": 6,
    "commentCount": 1,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>In my <a href=\"https://medium.com/ai-control/directions-and-desiderata-for-ai-control-b60fca0da8f4\">last post</a>, I described three research areas in AI control that I see as central: reward learning, robustness, and deliberation.</p><p>In this post I argue that these three pieces may be <i>sufficient</i> to get a <a href=\"https://medium.com/ai-control/benign-ai-e4eb6ec6d68e#.ugg3x77ws\">benign</a> and competitive version of model-free reinforcement learning. I think this is an important intermediate goal of solving AI control.</p><p>This post doesn’t discuss <a href=\"https://medium.com/ai-control/aligned-search-366f983742e9#.rq3auppf0\">benign model-based RL</a> at all, which I think is another key obstacle for <a href=\"https://medium.com/ai-control/prosaic-ai-control-b959644d79c2#.d46mjxf3f\">prosaic AI control</a>.</p><p>(<i>This post overlaps extensively with my</i> <a href=\"https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf#.m3m81zgrd\"><i>post on ALBA</i></a><i>, but I hope this one will be much clearer. Technically, ALBA is an implementation of the general strategy outlined in this post. I think the general strategy is much more important than that particular implementation.</i>)</p><h1>Ingredients</h1><h2>Reward learning and robustness</h2><p>Given a <a href=\"https://medium.com/ai-control/benign-ai-e4eb6ec6d68e#.ugg3x77ws\">benign</a> agent H, <a href=\"https://medium.com/ai-control/the-reward-engineering-problem-30285c779450#.pmofowr9x\">reward learning</a> allows us to construct a reward function <i>r</i> that can be used to train a weaker benign agent A. If our training process is robust, the resulting agent A will remain benign off of the training distribution (though it may be <i>incompetent</i> off of the training distribution).</p><p>Schematically, we can think of reward learning + robustness as a widget which takes a slow, benign process H and produces a fast, benign process A</p><figure style=\"width:12.26%;\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/51fd4ed26c95d664604f36b0ae03e3b41ba49f1a0ffc1487.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/51fd4ed26c95d664604f36b0ae03e3b41ba49f1a0ffc1487.png/w_82 82w\"></figure><p>A’s capabilities should be roughly the “intersection” of H’s capabilities and our RL algorithms’ competence. That is, A should be able to perform a task whenever <i>both</i> H can perform that task and our RL algorithms can learn to perform that task.</p><p>In these pictures, the vertical axis corresponds intuitively to “capability,” with higher agents being more capable. But in reality I’m thinking of the possible capabilities as forming a complete <a href=\"https://en.wikipedia.org/wiki/Lattice_%28order%29\">lattice</a>. That is, a generic pair of levels of capabilities is incomparable, with neither strictly dominating the other.</p><h2>Amplification</h2><p>If we iteratively apply reward learning and robustness, we will obtain a sequence of weaker and weaker agents. To get anywhere, we need some mechanism that lets us produce a <i>stronger</i> agent.</p><p>The <a href=\"https://medium.com/ai-control/policy-amplification-6a70cbee4f34\">capability amplification problem</a> is to start with a weak agent A and a human expert H, and to produce a significantly more capable agent Hᴬ. The more capable agent can take a lot longer to think, all we care about is that it <i>eventually</i> arrives at better decisions than A. The key challenge is ensuring that Hᴬ remains benign, i.e. that the system doesn’t acquire new preferences as it becomes more capable.</p><p>An example approach is to provide A as an assistant to H. We can give H an hour to deliberate, and let it consult A thousands of times during that hour. Hᴬ’s output is then whatever H outputs at the end of that process. Because H is consulting A a large number of times, we can hope that the resulting system will be much smarter than A. Of course, the resulting system will be thousands of times more computationally expensive than A, but that’s fine.</p><p>In general, <a href=\"https://medium.com/ai-control/meta-execution-27ba9b34d377#.isdz38ftn\">meta-execution</a> is my current preferred approach to capability amplification.</p><p>Schematically, we can think of amplification as a widget which takes a fast, <a href=\"https://medium.com/ai-control/benign-ai-e4eb6ec6d68e#.ugg3x77ws\">benign</a> process A and produces a slow, benign process Hᴬ:</p><figure style=\"width:35.89%;\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/870ca2ca9fd69becee8ae1648b749d44865fdce50a451c9a.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/870ca2ca9fd69becee8ae1648b749d44865fdce50a451c9a.png/w_144 144w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/870ca2ca9fd69becee8ae1648b749d44865fdce50a451c9a.png/w_224 224w\"></figure><h2>Putting it&nbsp;together</h2><p>With these two widgets in hand, we can iteratively produce a sequence of increasingly competent agents:</p><figure style=\"width:71.2%;\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/aac26587420008aa733e24c46e851f3578dccf491aced47d.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/aac26587420008aa733e24c46e851f3578dccf491aced47d.png/w_89 89w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/aac26587420008aa733e24c46e851f3578dccf491aced47d.png/w_169 169w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/aac26587420008aa733e24c46e851f3578dccf491aced47d.png/w_249 249w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/aac26587420008aa733e24c46e851f3578dccf491aced47d.png/w_329 329w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/aac26587420008aa733e24c46e851f3578dccf491aced47d.png/w_409 409w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/aac26587420008aa733e24c46e851f3578dccf491aced47d.png/w_489 489w\"></figure><p>That is, we start with our benign expert H. We then learn a reward function and train an agent A, which is less capable than H but can run much faster. By running many instances of A, we obtain a more powerful agent Hᴬ, which is approximately as expensive as H.</p><p>We can then repeat the process, using Hᴬ to train an agent A⁺ which runs as fast as A but is more capable. By running A⁺ for a long time we obtain a still more capable agent Hᴬ⁺, and the cycle repeats.</p><h2>Collapsing the recursion</h2><p>I’ve described an explicit sequence of increasingly capable agents. This is the most convenient framework for analysis, but actually implementing a sequence of distinct agents might introduce significant overhead. It also feels at odds with current practice, such that I would be intuitively surprised to actually see it work out.</p><p>Instead, we can collapse the entire sequence to a single agent:</p><figure style=\"width:24.6%;\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/86c8e2eb362e15724180ac92c28a6c1e7fb3add1b275b6b4.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/86c8e2eb362e15724180ac92c28a6c1e7fb3add1b275b6b4.png/w_82 82w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/86c8e2eb362e15724180ac92c28a6c1e7fb3add1b275b6b4.png/w_162 162w\"></figure><p>In this version there is a single agent A which is simultaneously being trained and being used to define a reward function.</p><p>Alternatively, we can view this as a sequential scheme with a strong initialization: there is a separate agent at each time <i>t</i>, who oversees the agent at time <i>t</i>+1, but each agent is initialized using the previous one’s state.</p><p>This version of the scheme is more likely to be efficient, and it feels much closer to a practical framework for RL. (I originally suggested a similar scheme <a href=\"https://ai-alignment.com/implementing-our-considered-judgment-6c715a239b3e\">here</a>.)</p><p>However, in addition to complicating the analysis, it also introduces additional challenges and risks. For example, if Hᴬ actually consults A, then there are unattractive equilibria in which A manipulates the reward function, and the manipulated reward function rewards manipulation. Averting this problem either requires H to sometimes avoid depending on A, or else requires us to sometimes run against an old version of A (a trick sometimes used to stabilize self-play). Both of these techniques implicitly reintroduce the iterative structure of the original scheme, though they may do so with lower computational overhead.</p><p>We will have an even more serious problem if our approach to reward learning relied on throttling the learning algorithm. When we work with an explicit sequence of agents, we can ensure that their capabilities improve gradually. It’s not straightforward to do something analogous in the single agent case.</p><p>Overall I think this version of the scheme is more likely to be practical. But it introduces several additional complications, and I think it’s reasonable to start by considering the explicit sequential form until we have a solid grasp of it.</p><h1>Analysis</h1><p>I’ll make two critical claims about this construction. Neither claim has yet been formalized, and it’s not clear whether it will be possible to formalize them completely.</p><h2>Claim #1: All of these agents are&nbsp;<a href=\"https://medium.com/ai-control/benign-ai-e4eb6ec6d68e#.ugg3x77ws\">benign</a>.</h2><p>This is plausible by induction:</p><ul><li>The original expert H is benign by definition.</li><li>If we start with a benign overseer H, and have working solutions to reward learning + robustness, then the trained agent A is benign.</li><li>If we start with a benign agent A, and have a woking solution to capability amplification, then the amplified agent Hᴬ will be benign.</li></ul><p>There are important subtleties in this argument; for example, an agent may be benign with high probability, and the error probability may increase exponentially as we proceed through the induction. Dealing with these subtleties will require careful definitions, and in some cases adjustments to the algorithm. For example, in the case of increasing failure probabilities, we need to <a href=\"https://medium.com/ai-control/reliability-amplification-a96efa115687\">strengthen the statement of amplification</a> to avoid the problem.</p><h2>Claim #2: The final agent has state-of-the-art performance.</h2><p>This is plausible if our building blocks satisfy several desirable properties.</p><p>First, capability amplification should be able to cross every level non-maximal level of capability. That is, for every level of capability, it is possible to start with an agent A who is below that level, and end up with an agent Hᴬ which is above that level:</p><figure style=\"width:85.09%;\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ff6ceac39d7183daf77492a97677d063e33ad437f9a08eb5.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ff6ceac39d7183daf77492a97677d063e33ad437f9a08eb5.png/w_98 98w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ff6ceac39d7183daf77492a97677d063e33ad437f9a08eb5.png/w_178 178w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ff6ceac39d7183daf77492a97677d063e33ad437f9a08eb5.png/w_258 258w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ff6ceac39d7183daf77492a97677d063e33ad437f9a08eb5.png/w_338 338w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ff6ceac39d7183daf77492a97677d063e33ad437f9a08eb5.png/w_418 418w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ff6ceac39d7183daf77492a97677d063e33ad437f9a08eb5.png/w_498 498w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ff6ceac39d7183daf77492a97677d063e33ad437f9a08eb5.png/w_578 578w\"></figure><p>For every possible place we could put the dotted line — every possible capability level — there must be some agent A for whom the orange arrow crosses that dotted line. Otherwise we would never be able to get to the other side of that dotted line, i.e. we would never be able to surpass that level of capability.</p><p>Second, capability amplification should be monotonic (if A is at least as capable as B then Hᴬ should be at least as capable as Hᴮ).</p><p>Third, reward learning should yield an agent whose capabilities are at least the infimum of our RL algorithm’s capabilities and the overseer’s capabilities, even if we train robustly.</p><p>Now given a sequence of increasingly powerful fast agents we can take the supremum of their capabilities. Those agents will all be weaker than our RL algorithms and so the supremum is not the maximal capability, so we can consider a starting point from which capability amplification would cross that supremum. By hypothesis the sequence must eventually cross this starting point, and at that point amplification will push it above the supremum (and reward learning will keep it above the supremum). Making this argument carefully shows that the supremum is the state of the art for RL algorithms and that we attain the supremum after some finite number of steps. (Though all of this is based on a leaky abstraction of “capabilities.”)</p><h1>Cost</h1><p>I think this proposal will be most helpful if it imposes minimal additional overhead. My main goal is to develop algorithms with sublinear overhead, i.e. for which the fraction of overhead converges to 0 as the underlying algorithms become stronger.</p><p>The cost of this scheme depends on the quantitative properties of our basic building blocks:</p><h2>Factor #1: How much do reward learning and robustness slow down training?</h2><p>During RL, we need to evaluate the agent A many times. If we want to use a learned reward function we may need to evaluate A more times. And if we want to train a policy which remains benign off of the training distribution, we may need to evaluate A more times (e.g. since we may need to do adversarial training). Ideally that overhead will shrink as our algorithms become more powerful.</p><p>I think this is plausible but far from certain (for now it is uncertain whether reward learning and robustness are even plausible). Some reassuring factors:</p><ul><li>Reward learning / adversarial training can actually improve the performance of our system — the computational time spent on them might actually be well-spent even from a capabilities perspective</li><li>The difficulty of the “additional learning problem” we are trying to solve in each case (e.g. the concept of “defer to human control”) may not scale up linearly with the complexity of the underlying domain.</li></ul><h2>Factor #2: how many times do we have to invoke the overseer during training?</h2><p>In addition to calling the agent A, we will need to call the overseer H in order to get information about the reward function. Because the overseer is much more expensive than the agent, we would like to minimize the number of times we call the overseer. This can be quantified by the ratio between the number of calls to H and the number of calls to A. For example, we may need to call H once for every hundred calls to A.</p><h2>Factor #3: how expensive is capability amplification?</h2><p>Capability amplification is possible only because we allow the agent Hᴬ to think for much longer than A. But “much longer” could represent a range of values: is Hᴬ a hundred times more expensive to evaluate than A? A thousand? A million?</p><p>Roughly speaking, factors #2 and #3 should be multiplied together to get the overhead from reward learning: factor #2 tells us how many times we have to call the overseer, while factor #3 tells us how expensive the overseer is.</p><p>The total overhead is thus (Factor #1) + (Factor #2) * (Factor #3). As an example, I’d be happy with values like 10% + 0.01% × 1000 = 20%.</p><h2>Factor #4: do we need to train many separate&nbsp;agents?</h2><p>If we need to use a sequence of N increasingly capable agents, then we would naively increase our training time by a factor of N. Naively, this would dominate the overhead, and in order for the scheme to be workable I think we would need to avoid it. I see a few plausible approaches:</p><ul><li>We could use the collapsed version with a single agent.</li><li>We could use some other initialization or parameter-sharing scheme to effectively reuse the computational work done in training earlier agents.</li><li>The earlier agents could require significantly less training time than the final agent, e.g. because they are less capable. For example, if each agent takes only 20% as long to train as the following one, then the total overhead is only 25%.</li></ul><p>These mechanisms can work together; for example, each agent may require some amount of non-reusable computation, but that amount may be reduced by a clever initialization scheme.</p><h1>Conclusion</h1><p>I’ve outlined an approach to AI control for model-free RL. I think there is a very good chance, perhaps as high as 50%, that this basic strategy can eventually be used to train benign state-of-the-art model-free RL agents. Note that this strategy also applies to techniques like evolution that have historically been considered really bad news for control.</p><p>That said, the scheme in this post is still extremely incomplete. I have recently prioritized building a practical implementation of these ideas, rather than continuing to work out conceptual issues. That does not mean that I think the conceptual issues are worked out conclusively, but it does mean that I think we’re at the point where we’d benefit from empirical information about what works in practice (which is a long way from how I felt about AI control 3 years ago!)</p><p>I think the largest technical uncertainty with this scheme is whether we can achieve enough robustness to avoid malign behavior in general.</p><p>This scheme does not apply to any components of our system which <a href=\"https://medium.com/ai-control/not-just-learning-e3bfb5a1f96e#.mvuanlogj\">aren’t learned end-to-end</a>. The idea is to use this training strategy for any internal components of our system which use model-free RL. In parallel, we need to develop aligned variants of each other algorithmic technique that plays a role in our AI systems. In particular, I think that model-based RL with extensive planning is a likely sticking point for this program, and so is a natural topic for further conceptual research.</p><hr><p><i>This was originally posted </i><a href=\"https://ai-alignment.com/benign-model-free-rl-4aae8c97e385\"><i>here</i></a><i> on 19th March, 2017.</i></p>",
    "user": {
      "username": "paulfchristiano",
      "slug": "paulfchristiano",
      "displayName": "paulfchristiano"
    }
  },
  {
    "_id": "PHL8qvX4QpM6sXKfK",
    "title": "Anyone use the \"read time\" on Post Items?",
    "slug": "anyone-use-the-read-time-on-post-items",
    "pageUrl": "https://www.lesswrong.com/posts/PHL8qvX4QpM6sXKfK/anyone-use-the-read-time-on-post-items",
    "postedAt": "2018-12-01T23:16:23.249Z",
    "baseScore": 19,
    "voteCount": 6,
    "commentCount": 11,
    "meta": true,
    "question": false,
    "url": null,
    "htmlBody": "<p>Someone recently mentioned that they found the &quot;read time&quot; on home page post-items to be more of annoying clutter than a helpful indicator of whether to read something.</p><p>It occurred to me that I don&#x27;t personally use them, and in general it&#x27;s important for each UI element to be pulling it&#x27;s weight. I&#x27;m curious how many people actively like/use them?</p><p>[Update: enough people have chimed in that I&#x27;m reasonably confident it&#x27;s a good feature to have]</p>",
    "user": {
      "username": "Raemon",
      "slug": "raemon",
      "displayName": "Raemon"
    }
  },
  {
    "_id": "Qg8R4YSbNertsj57g",
    "title": "November 2018 gwern.net newsletter",
    "slug": "november-2018-gwern-net-newsletter",
    "pageUrl": "https://www.lesswrong.com/posts/Qg8R4YSbNertsj57g/november-2018-gwern-net-newsletter",
    "postedAt": "2018-12-01T13:57:00.661Z",
    "baseScore": 35,
    "voteCount": 8,
    "commentCount": 0,
    "meta": false,
    "question": false,
    "url": "https://www.gwern.net/newsletter/2018/11",
    "htmlBody": null,
    "user": {
      "username": "gwern",
      "slug": "gwern",
      "displayName": "gwern"
    }
  },
  {
    "_id": "DfcywmqRSkBaCB6Ma",
    "title": "Intuitions about goal-directed behavior",
    "slug": "intuitions-about-goal-directed-behavior",
    "pageUrl": "https://www.lesswrong.com/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior",
    "postedAt": "2018-12-01T04:25:46.560Z",
    "baseScore": 55,
    "voteCount": 27,
    "commentCount": 15,
    "meta": false,
    "question": false,
    "url": null,
    "htmlBody": "<p>One broad argument for AI risk is the Misspecified Goal argument: </p><blockquote><strong>The Misspecified Goal Argument for AI Risk:</strong> Very intelligent AI systems will be able to make long-term plans in order to achieve their goals, and if their goals are even slightly misspecified then the AI system will become adversarial and work against us. </blockquote><p>My main goal in this post is to make conceptual clarifications and suggest how they affect the Misspecified Goal argument, without making any recommendations about what we should actually do. Future posts will argue more directly for a particular position. As a result, I will not be considering other arguments for focusing on AI risk even though I find some of them more compelling.</p><p>I think of this as a concern about <em>long-term goal-directed behavior</em>. Unfortunately, it&#x2019;s not clear how to categorize behavior as goal-directed vs. not. Intuitively, any agent that searches over actions and chooses the one that best achieves some measure of &#x201C;goodness&#x201D; is goal-directed (though there are exceptions, such as the agent that selects actions that begin with the letter &#x201C;A&#x201D;). (ETA: I also think that agents that show goal-directed behavior because they are looking at some other agent are not goal-directed themselves -- see this <a href=\"https://www.alignmentforum.org/posts/9zpT9dikrrebdq3Jf/will-humans-build-goal-directed-agents#FdQsD6Q78SZQeXa64\">comment</a>.) However, this is not a necessary condition: many humans are goal-directed, but there is no goal baked into the brain that they are using to choose actions.</p><p>This is related to the concept of <a href=\"https://www.lesswrong.com/posts/D7EcMhL26zFNbJ3ED/optimization\">optimization</a>, though with intuitions around optimization we typically assume that we know the agent&#x2019;s preference ordering, which I don&#x2019;t want to assume here. (In fact, I don&#x2019;t want to assume that the agent even <em>has</em> a preference ordering.)</p><p>One potential formalization is to say that goal-directed behavior is any behavior that can be modelled as maximizing expected utility for some utility function; in the next post I will argue that this does not properly capture the behaviors we are worried about. In this post I&#x2019;ll give some intuitions about what &#x201C;goal-directed behavior&#x201D; means, and how these intuitions relate to the Misspecified Goal argument.</p><h1>Generalization to novel circumstances</h1><p>Consider two possible agents for playing some game, let&#x2019;s say TicTacToe. The first agent looks at the state and the rules of the game, and uses the <a href=\"https://en.wikipedia.org/wiki/Minimax#Minimax_algorithm_with_alternate_moves\">minimax algorithm</a> to find the optimal move to play. The second agent has a giant lookup table that tells it what move to play given any state. Intuitively, the first one is more &#x201C;agentic&#x201D; or &#x201C;goal-driven&#x201D;, while the second one is not. But both of these agents play the game in exactly the same way!</p><p>The difference is in how the two agents <em>generalize to new situations</em>. Let&#x2019;s suppose that we suddenly change the rules of TicTacToe -- perhaps now the win condition is reversed, so that anyone who gets three in a row loses. The minimax agent is still going to be optimal at this game, whereas the lookup-table agent will lose against any opponent with half a brain. The minimax agent looks like it is &#x201C;trying to win&#x201D;, while the lookup-table agent does not. (You could say that the lookup-table agent is &#x201C;trying to take actions according to &lt;policy&gt;&#x201D;, but this is a weird complicated goal so maybe it doesn&#x2019;t count.)</p><p>In general, when we say that an agent is pursuing some goal, this is meant to allow us to predict how the agent will generalize to some novel circumstance. This sort of reasoning is critical for the Goal-Directed argument for AI risk. For example, we worry that an AI agent will prevent us from turning it off, because that would prevent it from achieving its goal: &#x201C;You can&apos;t fetch the coffee if you&apos;re dead.&#x201D; This is a prediction about what an AI agent would do in the novel circumstance where a human is trying to turn the agent off.</p><p>This suggests a way to characterize these sorts of goal-directed agents: there is some goal such that the agent&#x2019;s behavior <em>in new circumstances</em> can be predicted by figuring out which behavior best achieves the goal. There&apos;s a lot of complexity in the space of goals we consider: something like &quot;human well-being&quot; should count, but &quot;the particular policy &lt;x&gt;&quot; and &#x201C;pick actions that start with the letter A&#x201D; should not. When I use the word goal I mean to include only the first kind, even though I currently don&#x2019;t know theoretically how to distinguish between the various cases.</p><p>Note that this is in stark contrast to existing AI systems, which are particularly bad at generalizing to new situations.</p><span><figure><img src=\"https://res.cloudinary.com/dq3pms5lt/image/upload/q_auto/v1543642073/meme_pued2q.png\" class=\"draft-image center\" style=\"width:40%\"></figure></span><br><p>Honestly, I&#x2019;m surprised it&#x2019;s only 90%. [1]</p><h1>Empowerment</h1><p>We could also look at whether or not the agent acquires more power and resources. It seems likely that an agent that is optimizing for some goal over the long term would want more power and resources in order to more easily achieve that goal. In addition, the agent would probably try to improve its own algorithms in order to become more intelligent.</p><p>This feels like a <em>consequence</em> of goal-directed behavior, and not its defining characteristic, because it is about being able to achieve a <em>wide variety</em> of goals, instead of a particular one. Nonetheless, it seems crucial to the broad argument for AI risk presented above, since an AI system will probably need to first accumulate power, resources, intelligence, etc. in order to cause catastrophic outcomes.</p><p>I find this concept most useful when thinking about the problem of inner optimizers, where in the course of optimization through a rich space you stumble across a member of the space that is itself doing optimization, but for a related but still misspecified metric. Since the inner optimizer is being &#x201C;controlled&#x201D; by the outer optimization process, it is probably not going to cause major harm unless it is able to &#x201C;take over&#x201D; the outer optimization process, which sounds a lot like accumulating power. (This discussion is extremely imprecise and vague; see <a href=\"https://arxiv.org/abs/1906.01820\">Risks from Learned Optimization</a> for a more thorough discussion.)</p><h1>Our understanding of the behavior</h1><p>There is a general pattern in which as soon as we understand something, it becomes something lesser. As soon as we understand rainbows, they are relegated to the <a href=\"https://www.lesswrong.com/posts/x4dG4GhpZH2hgz59x/joy-in-the-merely-real\">&#x201C;dull catalogue of common things&#x201D;</a>. This suggests a somewhat cynical explanation of our concept of &#x201C;intelligence&#x201D;: an agent is considered intelligent if we do not know how to achieve the outcomes it does using the resources that it has (in which case our best model for that agent may be that it is pursuing some goal, reflecting our tendency to anthropomorphize). That is, our evaluation about intelligence is a statement about our epistemic state. Some examples that follow this pattern are:</p><ul><li>As soon as we understand how some AI technique solves a challenging problem, it is <a href=\"https://www.zdnet.com/article/ai-tends-to-lose-its-definition-once-it-becomes-commonplace-sas/\">no longer considered AI</a>. Before we&#x2019;ve solved the problem, we imagine that we need some sort of &#x201C;intelligence&#x201D; that is pointed towards the goal and solves it: the only method we have of predicting what this AI system will do is to think about what a system that tries to achieve the goal would do. Once we understand how the AI technique works, we have more insight into what it is doing and can make more detailed predictions about where it will work well, where it tends to make mistakes, etc. and so it no longer seems like &#x201C;intelligence&#x201D;. Once you know that OpenAI Five is trained by self-play, you can predict that they haven&#x2019;t seen certain behaviors like standing still to turn invisible, and probably won&#x2019;t work well there.</li><li>Before we understood the idea of natural selection and evolution, we would look at the complexity of nature and ascribe it to intelligent design; once we had the <a href=\"https://en.wikipedia.org/wiki/Price_equation\">mathematics</a> (and even just the qualitative insight), we could make much more detailed predictions, and nature no longer seemed like it required intelligence. For example, we can predict the timescales on which we can expect evolutionary changes, which we couldn&#x2019;t do if we just modeled evolution as optimizing reproductive fitness.</li><li>Many phenomena (eg. rain, wind) that we now have scientific explanations for were previously explained to be the result of some anthropomorphic deity.</li><li>When someone performs a feat of mental math, or can tell you instantly what day of the week a random date falls on, you might be impressed and think them very intelligent. But if they explain to you <a href=\"http://mathforum.org/dr.math/faq/faq.calendar.html\">how they did it</a>, you may find it much less impressive. (Though of course these feats are selected to seem more impressive than they are.)</li></ul><p>Note that an alternative hypothesis is that humans equate intelligence with mystery; as we learn more and remove mystery around eg. evolution, we automatically think of it as less intelligent.</p><p>To the extent that the Misspecified Goal argument relies on this intuition, the argument feels a lot weaker to me. If the Misspecified Goal argument rested entirely upon this intuition, then it would be asserting that <em>because</em> we are ignorant about what an intelligent agent would do, we should assume that it is optimizing a goal, which means that it is going to accumulate power and resources and lead to catastrophe. In other words, it is arguing that assuming that an agent is intelligent <em>definitionally</em> means that it will accumulate power and resources. This seems clearly wrong; it is possible in principle to have an intelligent agent that nonetheless does not accumulate power and resources.</p><p>Also, the argument is <em>not</em> saying that <em>in practice</em> most intelligent agents accumulate power and resources. It says that we have no better model to go off of other than &#x201C;goal-directed&#x201D;, and then pushes this model to extreme scenarios where we should have a lot more uncertainty.</p><p>To be clear, I do <em>not</em> think that anyone would endorse the argument as stated. I am suggesting as a possibility that the Misspecified Goal argument relies on us incorrectly equating superintelligence with &#x201C;pursuing a goal&#x201D; because we use &#x201C;pursuing a goal&#x201D; as a default model for anything that can do interesting things, even if that is not the best model to be using.</p><h1>Summary</h1><p>Intuitively, goal-directed behavior can lead to catastrophic outcomes with a sufficiently intelligent agent, because the optimal behavior for even a slightly misspecified goal can be very bad according to the true goal. However, it&#x2019;s not clear exactly what we mean by goal-directed behavior. Often, an algorithm that searches over possible actions and chooses the one with the highest &#x201C;goodness&#x201D; will be goal-directed, but this is neither necessary nor sufficient.</p><p>&#x201C;From the outside&#x201D;, it seems like a goal-directed agent is characterized by the fact that we can predict the agent&#x2019;s behavior in new situations by assuming that it is pursuing some goal, and as a result it is acquires power and resources. This can be interpreted either as a statement about our epistemic state (we know so little about the agent that our best model is that it pursues a goal, even though this model is not very accurate or precise) or as a statement about the agent (predicting the behavior of the agent in new situations based on pursuit of a goal actually has very high precision and accuracy). These two views have very different implications on the validity of the Misspecified Goal argument for AI risk.</p><hr class=\"dividerBlock\"><p>[1] This is an entirely made-up number.</p>",
    "user": {
      "username": "rohinmshah",
      "slug": "rohinmshah",
      "displayName": "Rohin Shah"
    }
  }
]