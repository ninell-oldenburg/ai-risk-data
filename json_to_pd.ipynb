{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f64b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99702b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e062666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize scraped text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Remove extra whitespace and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08fdaccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text_with_links(url, retries=3, delay=1):\n",
    "    \"\"\"\n",
    "    Scrape text content and links from a URL with improved error handling.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to scrape\n",
    "        retries (int): Number of retry attempts\n",
    "        delay (float): Delay between retries\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'text', 'links', 'title', and 'status'\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            logger.info(f\"Scraping {url} (attempt {attempt + 1})\")\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Extract title\n",
    "            title_tag = soup.find(\"title\")\n",
    "            title = clean_text(title_tag.get_text()) if title_tag else \"No title\"\n",
    "            \n",
    "            # Try multiple selectors for content\n",
    "            content_selectors = [\n",
    "                \"div.post-body\",\n",
    "                \"article\",\n",
    "                \"main\",\n",
    "                \"div.content\",\n",
    "                \"div.entry-content\",\n",
    "                \".post-content\",\n",
    "                \"#content\"\n",
    "            ]\n",
    "            \n",
    "            post_div = None\n",
    "            for selector in content_selectors:\n",
    "                post_div = soup.select_one(selector)\n",
    "                if post_div:\n",
    "                    break\n",
    "            \n",
    "            if not post_div:\n",
    "                # Fallback: use body but exclude nav, header, footer\n",
    "                post_div = soup.find(\"body\")\n",
    "                if post_div:\n",
    "                    # Remove navigation, headers, footers, sidebars\n",
    "                    for tag in post_div.find_all([\"nav\", \"header\", \"footer\", \"aside\", \"script\", \"style\"]):\n",
    "                        tag.decompose()\n",
    "            \n",
    "            if not post_div:\n",
    "                return {\n",
    "                    \"text\": \"\",\n",
    "                    \"links\": [],\n",
    "                    \"title\": title,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": \"No content found\"\n",
    "                }\n",
    "            \n",
    "            # Extract text and links\n",
    "            text_content = []\n",
    "            links = []\n",
    "            \n",
    "            # Get all text nodes and links\n",
    "            for elem in post_div.descendants:\n",
    "                if elem.name == \"a\" and elem.get(\"href\"):\n",
    "                    link_text = clean_text(elem.get_text())\n",
    "                    if link_text:  # Only include links with text\n",
    "                        href = elem[\"href\"]\n",
    "                        # Convert relative URLs to absolute\n",
    "                        if href.startswith(('http://', 'https://')):\n",
    "                            full_url = href\n",
    "                        else:\n",
    "                            full_url = urljoin(url, href)\n",
    "                        \n",
    "                        links.append({\n",
    "                            \"text\": link_text,\n",
    "                            \"url\": full_url\n",
    "                        })\n",
    "                elif hasattr(elem, 'string') and elem.string and elem.string.strip():\n",
    "                    text_content.append(elem.string.strip())\n",
    "            \n",
    "            # Combine text content\n",
    "            full_text = clean_text(\" \".join(text_content))\n",
    "            \n",
    "            return {\n",
    "                \"text\": full_text,\n",
    "                \"links\": links,\n",
    "                \"title\": title,\n",
    "                \"status\": \"success\",\n",
    "                \"word_count\": len(full_text.split()) if full_text else 0\n",
    "            }\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.warning(f\"Timeout for {url} on attempt {attempt + 1}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Request error for {url} on attempt {attempt + 1}: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error for {url} on attempt {attempt + 1}: {e}\")\n",
    "        \n",
    "        if attempt < retries - 1:\n",
    "            time.sleep(delay * (attempt + 1))  # Exponential backoff\n",
    "    \n",
    "    return {\n",
    "        \"text\": \"\",\n",
    "        \"links\": [],\n",
    "        \"title\": \"Failed to scrape\",\n",
    "        \"status\": \"failed\",\n",
    "        \"error\": f\"Failed after {retries} attempts\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "076498ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_individual_file(content, url, output_folder, file_format=\"txt\"):\n",
    "    \"\"\"Save scraped content to individual file.\"\"\"\n",
    "    try:\n",
    "        # Create safe filename from URL\n",
    "        parsed_url = urlparse(url)\n",
    "        filename = f\"{parsed_url.netloc}_{parsed_url.path}\".replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "        filename = re.sub(r'[<>:\"|?*]', '_', filename)  # Remove invalid characters\n",
    "        filename = filename[:100]  # Limit length\n",
    "        \n",
    "        if file_format == \"txt\":\n",
    "            filepath = output_folder / f\"{filename}.txt\"\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"URL: {url}\\n\")\n",
    "                f.write(f\"Title: {content['title']}\\n\")\n",
    "                f.write(f\"Word Count: {content.get('word_count', 0)}\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "                f.write(content['text'])\n",
    "                \n",
    "                if content['links']:\n",
    "                    f.write(\"\\n\\n\" + \"=\"*50 + \"\\n\")\n",
    "                    f.write(\"LINKS FOUND:\\n\")\n",
    "                    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "                    for link in content['links']:\n",
    "                        f.write(f\"Text: {link['text']}\\n\")\n",
    "                        f.write(f\"URL: {link['url']}\\n\\n\")\n",
    "        \n",
    "        elif file_format == \"json\":\n",
    "            filepath = output_folder / f\"{filename}.json\"\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\n",
    "                    \"source_url\": url,\n",
    "                    \"scraped_content\": content,\n",
    "                    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving file for {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "541e651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_urls_from_json(json_base_folder, csv_base_folder, save_individual_files=True, \n",
    "                          individual_files_folder=None, file_format=\"txt\", batch_size=10):\n",
    "    \"\"\"\n",
    "    Enhanced processing function with individual file saving option.\n",
    "    \n",
    "    Args:\n",
    "        json_base_folder (Path): Folder containing JSON files organized by year\n",
    "        csv_base_folder (Path): Output folder for CSV files\n",
    "        save_individual_files (bool): Whether to save individual text files\n",
    "        individual_files_folder (Path): Folder for individual files (optional)\n",
    "        file_format (str): 'txt' or 'json' for individual files\n",
    "        batch_size (int): Number of URLs to process before saving progress\n",
    "    \"\"\"\n",
    "    # Convert to Path objects\n",
    "    json_base_folder = Path(json_base_folder)\n",
    "    csv_base_folder = Path(csv_base_folder)\n",
    "    \n",
    "    if save_individual_files:\n",
    "        if individual_files_folder is None:\n",
    "            individual_files_folder = csv_base_folder / \"scraped_content\"\n",
    "        individual_files_folder = Path(individual_files_folder)\n",
    "        individual_files_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_successful = 0\n",
    "    \n",
    "    # Process each year folder\n",
    "    for year_folder in sorted(json_base_folder.iterdir()):\n",
    "        if not year_folder.is_dir():\n",
    "            continue\n",
    "            \n",
    "        year = year_folder.name\n",
    "        csv_year_folder = csv_base_folder / year\n",
    "        csv_year_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Processing year: {year}\")\n",
    "        \n",
    "        # Process each month file\n",
    "        for json_file in sorted(year_folder.glob(\"*.json\")):\n",
    "            month = json_file.stem\n",
    "            logger.info(f\"Processing {year}/{month}\")\n",
    "            \n",
    "            try:\n",
    "                with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                if not data:\n",
    "                    logger.info(f\"No data in {json_file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                df = pd.json_normalize(data)\n",
    "                \n",
    "                if \"pageUrl\" not in df.columns:\n",
    "                    logger.error(f\"No 'pageUrl' column in {json_file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize new columns\n",
    "                df[\"scraped_text\"] = \"\"\n",
    "                df[\"scraped_links\"] = \"\"\n",
    "                df[\"scrape_status\"] = \"\"\n",
    "                df[\"word_count\"] = 0\n",
    "                df[\"scrape_error\"] = \"\"\n",
    "                \n",
    "                # Process URLs in batches\n",
    "                for i, row in df.iterrows():\n",
    "                    url = row[\"pageUrl\"]\n",
    "                    content = scrape_text_with_links(url)\n",
    "                    \n",
    "                    # Update DataFrame\n",
    "                    df.at[i, \"scraped_text\"] = content[\"text\"]\n",
    "                    df.at[i, \"scraped_links\"] = json.dumps(content[\"links\"]) if content[\"links\"] else \"\"\n",
    "                    df.at[i, \"scrape_status\"] = content[\"status\"]\n",
    "                    df.at[i, \"word_count\"] = content.get(\"word_count\", 0)\n",
    "                    df.at[i, \"scrape_error\"] = content.get(\"error\", \"\")\n",
    "                    \n",
    "                    total_processed += 1\n",
    "                    if content[\"status\"] == \"success\":\n",
    "                        total_successful += 1\n",
    "                    \n",
    "                    # Save individual file if requested\n",
    "                    if save_individual_files and content[\"status\"] == \"success\":\n",
    "                        year_individual_folder = individual_files_folder / year\n",
    "                        year_individual_folder.mkdir(parents=True, exist_ok=True)\n",
    "                        saved_file = save_individual_file(content, url, year_individual_folder, file_format)\n",
    "                        if saved_file:\n",
    "                            logger.info(f\"Saved individual file: {saved_file}\")\n",
    "                    \n",
    "                    # Progress update\n",
    "                    if (i + 1) % batch_size == 0:\n",
    "                        logger.info(f\"Processed {i + 1}/{len(df)} URLs from {month}\")\n",
    "                        # Save progress\n",
    "                        csv_file = csv_year_folder / f\"{month}.csv\"\n",
    "                        df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "                    \n",
    "                    # Rate limiting\n",
    "                    time.sleep(1.5)  # Be respectful to servers\n",
    "                \n",
    "                # Final save for this month\n",
    "                csv_file = csv_year_folder / f\"{month}.csv\"\n",
    "                df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "                \n",
    "                success_rate = sum(df[\"scrape_status\"] == \"success\") / len(df) * 100\n",
    "                logger.info(f\"âœ… Saved {csv_file} with {len(df)} posts ({success_rate:.1f}% success rate)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {json_file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    logger.info(f\"COMPLETE: Processed {total_processed} URLs, {total_successful} successful ({total_successful/total_processed*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1bdbd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:18:28,259 - INFO - Processing year: 2016\n",
      "2025-08-30 09:18:28,261 - INFO - Processing 2016/2016-01\n",
      "2025-08-30 09:18:28,283 - INFO - Scraping https://www.lesswrong.com/posts/HpsExWAYKHA6xNy76/ai-safety-in-the-age-of-neural-networks-and-stanislaw-lem (attempt 1)\n",
      "2025-08-30 09:18:28,893 - INFO - Saved individual file: lw_csv/scraped_content/2016/www.lesswrong.com__posts_HpsExWAYKHA6xNy76_ai-safety-in-the-age-of-neural-networks-and-stanislaw-lem.txt\n",
      "2025-08-30 09:18:30,399 - INFO - Scraping https://www.lesswrong.com/posts/i6LDrasYK2y6GgQuW/meetup-lw-melb-rationality-dojo-including-critical-thoughts (attempt 1)\n",
      "2025-08-30 09:18:30,654 - INFO - Saved individual file: lw_csv/scraped_content/2016/www.lesswrong.com__posts_i6LDrasYK2y6GgQuW_meetup-lw-melb-rationality-dojo-including-critical-though.txt\n",
      "2025-08-30 09:18:32,159 - INFO - Scraping https://www.lesswrong.com/posts/2ZxBjuv88cgmSjjbc/identifying-bias-a-bayesian-analysis-of-suspicious-agreement (attempt 1)\n",
      "2025-08-30 09:18:33,353 - INFO - Saved individual file: lw_csv/scraped_content/2016/www.lesswrong.com__posts_2ZxBjuv88cgmSjjbc_identifying-bias-a-bayesian-analysis-of-suspicious-agreem.txt\n",
      "2025-08-30 09:18:34,859 - INFO - Scraping https://www.lesswrong.com/posts/bLMmKZDTqmdyinTsc/link-how-a-lamp-took-away-my-reading-and-a-box-brought-it (attempt 1)\n",
      "2025-08-30 09:18:35,317 - INFO - Saved individual file: lw_csv/scraped_content/2016/www.lesswrong.com__posts_bLMmKZDTqmdyinTsc_link-how-a-lamp-took-away-my-reading-and-a-box-brought-it.txt\n",
      "2025-08-30 09:18:36,819 - INFO - Scraping https://www.lesswrong.com/posts/xCrhCkPaexhE2FZKP/moderator-action-the_lion-and-the_lion2-are-banned (attempt 1)\n",
      "2025-08-30 09:18:37,561 - INFO - Saved individual file: lw_csv/scraped_content/2016/www.lesswrong.com__posts_xCrhCkPaexhE2FZKP_moderator-action-the_lion-and-the_lion2-are-banned.txt\n",
      "2025-08-30 09:18:39,067 - INFO - Scraping https://www.lesswrong.com/posts/5bd75cc58225bf06703750c6/another-toy-model-of-the-control-problem (attempt 1)\n",
      "2025-08-30 09:18:39,742 - INFO - Saved individual file: lw_csv/scraped_content/2016/www.lesswrong.com__posts_5bd75cc58225bf06703750c6_another-toy-model-of-the-control-problem.txt\n",
      "2025-08-30 09:18:41,247 - INFO - Scraping https://www.lesswrong.com/posts/4qPy8jwRxLg9qWLiG/yoshua-bengio-on-ai-progress-hype-and-risks (attempt 1)\n",
      "2025-08-30 09:18:41,717 - INFO - Saved individual file: lw_csv/scraped_content/2016/www.lesswrong.com__posts_4qPy8jwRxLg9qWLiG_yoshua-bengio-on-ai-progress-hype-and-risks.txt\n",
      "2025-08-30 09:18:43,221 - INFO - Scraping https://www.lesswrong.com/posts/3KAFLdJKeheRJLrFf/positive-utility-in-an-infinite-universe (attempt 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m CSV_BASE_FOLDER\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Run the scraping process\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mprocess_urls_from_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_base_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mJSON_BASE_FOLDER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_base_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCSV_BASE_FOLDER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_individual_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False if you don't want individual files\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtxt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# or \"json\"\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Save progress every N URLs\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 68\u001b[0m, in \u001b[0;36mprocess_urls_from_json\u001b[0;34m(json_base_folder, csv_base_folder, save_individual_files, individual_files_folder, file_format, batch_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     67\u001b[0m     url \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpageUrl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 68\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_text_with_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Update DataFrame\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscraped_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m, in \u001b[0;36mscrape_text_with_links\u001b[0;34m(url, retries, delay)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     28\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/urllib3/connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure your paths\n",
    "JSON_BASE_FOLDER = Path(\"lw_json\")  # Folder with year subfolders\n",
    "CSV_BASE_FOLDER = Path(\"lw_csv\")       # Output folder for CSVs\n",
    "    \n",
    "# Create output directory\n",
    "CSV_BASE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "# Run the scraping process\n",
    "process_urls_from_json(\n",
    "    json_base_folder=JSON_BASE_FOLDER,\n",
    "    csv_base_folder=CSV_BASE_FOLDER,\n",
    "    save_individual_files=True,  # Set to False if you don't want individual files\n",
    "    file_format=\"txt\",           # or \"json\"\n",
    "    batch_size=10               # Save progress every N URLs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ad231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
