{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d232f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.colors as cls\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import glob\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548260a",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d8d859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Nature format configuration loaded\n",
      "  Single column width: 3.5\"\n",
      "  1.5 column width: 4.7\"\n",
      "  Double column width: 7.2\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"=== NATURE FORMAT CONFIGURATION ===\"\"\"\n",
    "# Set these once at the beginning for all plots in the notebook\n",
    "\n",
    "# Font settings\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']  # Fallback fonts\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['axes.labelsize'] = 8\n",
    "plt.rcParams['axes.titlesize'] = 9\n",
    "plt.rcParams['xtick.labelsize'] = 7\n",
    "plt.rcParams['ytick.labelsize'] = 7\n",
    "plt.rcParams['legend.fontsize'] = 7\n",
    "plt.rcParams['legend.title_fontsize'] = 7\n",
    "plt.rcParams['figure.titlesize'] = 9\n",
    "\n",
    "# Line and spine settings\n",
    "plt.rcParams['axes.linewidth'] = 0.5\n",
    "plt.rcParams['lines.linewidth'] = 1.0\n",
    "plt.rcParams['patch.linewidth'] = 0.5\n",
    "plt.rcParams['xtick.major.width'] = 0.5\n",
    "plt.rcParams['ytick.major.width'] = 0.5\n",
    "\n",
    "# Grid settings\n",
    "plt.rcParams['grid.linewidth'] = 0.5\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "plt.rcParams['savefig.pad_inches'] = 0.05\n",
    "\n",
    "# Remove top and right spines by default\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "# Nature figure widths (in inches) - these stay the same\n",
    "SINGLE_COL = 3.5   # 89 mm\n",
    "ONE_HALF_COL = 4.7  # 120 mm\n",
    "DOUBLE_COL = 7.2   # 183 mm\n",
    "\n",
    "print(\"✓ Nature format configuration loaded\")\n",
    "print(f\"  Single column width: {SINGLE_COL}\\\"\")\n",
    "print(f\"  1.5 column width: {ONE_HALF_COL}\\\"\")\n",
    "print(f\"  Double column width: {DOUBLE_COL}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1989b",
   "metadata": {},
   "source": [
    "# Load CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b94269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from all CSV files...\n",
      "Found 224 LessWrong CSV files\n",
      "Loaded 45981 total LessWrong posts from 224 files\n",
      "Found 126 Alignment Forum CSV files\n",
      "Loaded 4230 total Alignment Forum posts from 126 files\n",
      "\n",
      "Combined total: 50211 posts across both platforms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load all CSV files from the folder structure\"\"\"\n",
    "print(\"Loading data from all CSV files...\")\n",
    "\n",
    "# Load LessWrong data\n",
    "forum = 'lesswrong'\n",
    "lw_files = glob.glob(f\"../src/processed_data/{forum}/03_with_topics/**/*.csv\", recursive=True)\n",
    "print(f\"Found {len(lw_files)} LessWrong CSV files\")\n",
    "\n",
    "lw_data = []\n",
    "for file in lw_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        # Extract year and month from filename\n",
    "        parts = Path(file).stem.split('-')\n",
    "        if len(parts) >= 2:\n",
    "            df['year'] = int(parts[0])\n",
    "            df['month'] = int(parts[1])\n",
    "        lw_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "if not lw_data:\n",
    "    print(\"No LessWrong data found! Check your file paths.\")\n",
    "else:\n",
    "    lesswrong_df = pd.concat(lw_data, ignore_index=True)\n",
    "    print(f\"Loaded {len(lesswrong_df)} total LessWrong posts from {len(lw_data)} files\")\n",
    "    \n",
    "    # Clean and prepare LessWrong data\n",
    "    lesswrong_df['postedAt'] = pd.to_datetime(lesswrong_df['postedAt'], errors='coerce')\n",
    "    lesswrong_df['baseScore'] = pd.to_numeric(lesswrong_df['baseScore'], errors='coerce').fillna(0)\n",
    "    lesswrong_df['commentCount'] = pd.to_numeric(lesswrong_df['commentCount'], errors='coerce').fillna(0)\n",
    "\n",
    "# Load Alignment Forum data\n",
    "forum = 'alignment_forum'\n",
    "af_files = glob.glob(f\"../src/processed_data/{forum}/03_with_topics/**/*.csv\", recursive=True)\n",
    "print(f\"Found {len(af_files)} Alignment Forum CSV files\")\n",
    "\n",
    "af_data = []\n",
    "for file in af_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        # Extract year and month from filename\n",
    "        parts = Path(file).stem.split('-')\n",
    "        if len(parts) >= 2:\n",
    "            df['year'] = int(parts[0])\n",
    "            df['month'] = int(parts[1])\n",
    "        af_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "if not af_data:\n",
    "    print(\"No Alignment Forum data found! Check your file paths.\")\n",
    "else:\n",
    "    alignment_forum_df = pd.concat(af_data, ignore_index=True)\n",
    "    print(f\"Loaded {len(alignment_forum_df)} total Alignment Forum posts from {len(af_data)} files\")\n",
    "    \n",
    "    # Clean and prepare Alignment Forum data\n",
    "    alignment_forum_df['postedAt'] = pd.to_datetime(alignment_forum_df['postedAt'], errors='coerce')\n",
    "    alignment_forum_df['baseScore'] = pd.to_numeric(alignment_forum_df['baseScore'], errors='coerce').fillna(0)\n",
    "    alignment_forum_df['commentCount'] = pd.to_numeric(alignment_forum_df['commentCount'], errors='coerce').fillna(0)\n",
    "\n",
    "# Optional: Create combined dataframe with platform identifier\n",
    "combined_df = pd.concat([\n",
    "    lesswrong_df.assign(platform='LessWrong'),\n",
    "    alignment_forum_df.assign(platform='Alignment Forum')\n",
    "], ignore_index=True)\n",
    "print(f\"\\nCombined total: {len(combined_df)} posts across both platforms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed5aa9",
   "metadata": {},
   "source": [
    "---\n",
    "# Forum Nodes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fc392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, min_val=0.0, max_val=1.0, n=256):\n",
    "    \"\"\"Helper function to truncate colormap.\"\"\"\n",
    "    new_cmap = LinearSegmentedColormap.from_list(\n",
    "        f'trunc({cmap.name},{min_val:.2f},{max_val:.2f})',\n",
    "        cmap(np.linspace(min_val, max_val, n))\n",
    "    )\n",
    "    return new_cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015cb30",
   "metadata": {},
   "source": [
    "## Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=== TOPIC ANALYSIS ===\"\"\"\n",
    "\n",
    "# Plot for both platforms\n",
    "for platform_name, platform_df in [(\"LessWrong\", lesswrong_df), (\"Alignment Forum\", alignment_forum_df)]:\n",
    "    \n",
    "    print(f\"\\n--- Analyzing {platform_name} ---\")\n",
    "    \n",
    "    # Filter out Misc topics\n",
    "    filtered_df = platform_df[platform_df[\"topic_label\"] != 'Misc: No Topic'].copy()\n",
    "    \n",
    "    if len(filtered_df) == 0:\n",
    "        print(f\"No topics found for {platform_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Get topic counts\n",
    "    topic_counts = filtered_df[\"topic_label\"].value_counts()\n",
    "    \n",
    "    # --- Figure: Topic distribution pie chart (Nature format) ---\n",
    "    fig, ax = plt.subplots(figsize=(3.5, 3.5))  # Single column width (89mm)\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        topic_counts.values,\n",
    "        labels=topic_counts.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        counterclock=False,\n",
    "        colors=okabe_ito_palette[:len(topic_counts)],\n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},\n",
    "        textprops={'fontsize': 6}  # Small font for labels\n",
    "    )\n",
    "    \n",
    "    # Format percentage text\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontsize(6)\n",
    "        autotext.set_weight('bold')\n",
    "    \n",
    "    # Format labels\n",
    "    for text in texts:\n",
    "        text.set_fontsize(6)\n",
    "    \n",
    "    ax.set_title(f\"{platform_name}: Topic Distribution\", fontsize=8, pad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Topics found: {len(topic_counts)}\")\n",
    "    print(f\"Total posts: {len(filtered_df)}\")\n",
    "\n",
    "# --- Combined plot for both platforms (Nature double-column format) ---\n",
    "print(\"\\n--- Creating combined comparison ---\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.2, 3.5))  # Double column width\n",
    "\n",
    "for ax, (platform_name, platform_df) in zip([ax1, ax2], [(\"LessWrong\", lesswrong_df), (\"Alignment Forum\", alignment_forum_df)]):\n",
    "    filtered_df = platform_df[platform_df[\"topic_label\"] != 'Misc: No Topic'].copy()\n",
    "    \n",
    "    if len(filtered_df) == 0:\n",
    "        ax.text(0.5, 0.5, f'No topics for {platform_name}', \n",
    "                ha='center', va='center', fontsize=7)\n",
    "        ax.set_title(f\"{platform_name}\", fontsize=8)\n",
    "        continue\n",
    "    \n",
    "    topic_counts = filtered_df[\"topic_label\"].value_counts()\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        topic_counts.values,\n",
    "        labels=topic_counts.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        counterclock=False,\n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},\n",
    "        textprops={'fontsize': 6}\n",
    "    )\n",
    "    \n",
    "    # Format percentage text\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontsize(6)\n",
    "        autotext.set_weight('bold')\n",
    "    \n",
    "    # Format labels\n",
    "    for text in texts:\n",
    "        text.set_fontsize(6)\n",
    "    \n",
    "    ax.set_title(f\"{platform_name}\", fontsize=8, pad=10)\n",
    "\n",
    "# Add figure labels (a, b) as Nature requires\n",
    "fig.text(0.02, 0.98, 'a', fontsize=10, fontweight='bold', va='top')\n",
    "fig.text(0.52, 0.98, 'b', fontsize=10, fontweight='bold', va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save in multiple formats\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4caa3ec",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0aeb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=== MANUAL TOPIC VALIDATION ===\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "TOTAL_VALIDATION_SAMPLES = 200  # Adjust based on how many you want to validate\n",
    "RANDOM_SEED = 42  # For reproducibility\n",
    "\n",
    "# Combine both platforms\n",
    "combined_df = pd.concat([\n",
    "    lesswrong_df.assign(platform='LessWrong'),\n",
    "    alignment_forum_df.assign(platform='Alignment Forum')\n",
    "], ignore_index=True)\n",
    "\n",
    "# Filter out Misc topics\n",
    "validation_df = combined_df[combined_df[\"topic_label\"] != 'Misc: No Topic'].copy()\n",
    "\n",
    "print(f\"Total posts for validation: {len(validation_df)}\")\n",
    "print(f\"\\n=== Topic Distribution ===\")\n",
    "\n",
    "# Get topic counts and proportions\n",
    "topic_counts = validation_df[\"topic_label\"].value_counts()\n",
    "topic_proportions = topic_counts / len(validation_df)\n",
    "\n",
    "for topic, count in topic_counts.items():\n",
    "    proportion = count / len(validation_df)\n",
    "    n_samples = int(TOTAL_VALIDATION_SAMPLES * proportion)\n",
    "    print(f\"{topic}: {count} posts ({proportion*100:.1f}%) → {n_samples} validation samples\")\n",
    "\n",
    "# Stratified sampling\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "validation_samples = []\n",
    "\n",
    "for topic in topic_counts.index:\n",
    "    topic_df = validation_df[validation_df[\"topic_label\"] == topic]\n",
    "    \n",
    "    # Calculate number of samples for this topic (proportional)\n",
    "    n_samples = max(1, int(TOTAL_VALIDATION_SAMPLES * len(topic_df) / len(validation_df)))\n",
    "    \n",
    "    # Sample randomly\n",
    "    if len(topic_df) >= n_samples:\n",
    "        sampled = topic_df.sample(n=n_samples, random_state=RANDOM_SEED)\n",
    "    else:\n",
    "        sampled = topic_df  # If topic has fewer posts than needed samples\n",
    "    \n",
    "    validation_samples.append(sampled)\n",
    "\n",
    "validation_set = pd.concat(validation_samples, ignore_index=True)\n",
    "\n",
    "# Shuffle the validation set\n",
    "validation_set = validation_set.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "validation_set = validation_set.drop(columns=['htmlBody','slug,pageUrl','postedAt','baseScore',\n",
    "                                              'voteCount','commentCount','meta','question','user.username',\n",
    "                                              'user.slug','user.displayName','extracted_links','pageUrl','slug',\n",
    "                                              'user_gender','extracted_dois','topic_cluster_id','year','month'], errors='ignore')\n",
    "\n",
    "# Truncate text column to 500 chars if it exists\n",
    "if 'text' in validation_set.columns:\n",
    "    validation_set['text'] = validation_set['cleaned_htmlBody'].astype(str).apply(lambda x: x[:500] + \"...\" if len(x) > 500 else x)\n",
    "\n",
    "print(f\"\\n=== Validation Set Created ===\")\n",
    "print(f\"Total validation samples: {len(validation_set)}\")\n",
    "print(f\"\\nSamples per topic:\")\n",
    "print(validation_set[\"topic_label\"].value_counts().sort_index())\n",
    "\n",
    "# Add columns for manual validation\n",
    "validation_set['validation_correct'] = ''  # To be filled: 'yes', 'no', 'unsure'\n",
    "validation_set['validation_notes'] = ''\n",
    "validation_set['validation_alternative_topic'] = ''  # If incorrect, what should it be?\n",
    "\n",
    "# Save to CSV for validation\n",
    "output_file = 'topic_validation_set.csv'\n",
    "validation_set.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved validation set to: {output_file}\")\n",
    "\n",
    "# Display first few examples\n",
    "print(\"\\n=== First 5 Validation Examples ===\\n\")\n",
    "for idx, row in validation_set.head().iterrows():\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Sample {idx + 1}\")\n",
    "    print(f\"Platform: {row['platform']}\")\n",
    "    print(f\"Assigned Topic: {row['topic_label']}\")\n",
    "    print(f\"Title: {row.get('title', 'N/A')}\")\n",
    "    print(f\"\\nContent Preview (first 500 chars):\")\n",
    "    content = str(row.get('content', row.get('text', 'N/A')))\n",
    "    print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "    print()\n",
    "\n",
    "# Create a more readable HTML version for Jupyter\n",
    "def create_validation_html(df, max_samples=20):\n",
    "    html = \"\"\"\n",
    "    <style>\n",
    "        .validation-card {\n",
    "            border: 2px solid #333;\n",
    "            padding: 15px;\n",
    "            margin: 20px 0;\n",
    "            background-color: #f9f9f9;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        .topic-label {\n",
    "            background-color: #4CAF50;\n",
    "            color: white;\n",
    "            padding: 5px 10px;\n",
    "            border-radius: 3px;\n",
    "            display: inline-block;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        .platform {\n",
    "            background-color: #2196F3;\n",
    "            color: white;\n",
    "            padding: 5px 10px;\n",
    "            border-radius: 3px;\n",
    "            display: inline-block;\n",
    "            margin-left: 10px;\n",
    "        }\n",
    "        .content-preview {\n",
    "            background-color: white;\n",
    "            padding: 10px;\n",
    "            margin-top: 10px;\n",
    "            border-left: 3px solid #ddd;\n",
    "            font-family: monospace;\n",
    "            font-size: 0.9em;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    \n",
    "    for idx, row in df.head(max_samples).iterrows():\n",
    "        content = str(row.get('content', row.get('text', 'N/A')))\n",
    "        content_preview = content[:800] + \"...\" if len(content) > 800 else content\n",
    "        \n",
    "        html += f\"\"\"\n",
    "        <div class=\"validation-card\">\n",
    "            <h3>Sample {idx + 1}</h3>\n",
    "            <span class=\"topic-label\">{row['topic_label']}</span>\n",
    "            <span class=\"platform\">{row['platform']}</span>\n",
    "            <h4>{row.get('title', 'No Title')}</h4>\n",
    "            <div class=\"content-preview\">{content_preview}</div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "print(\"\\n=== Interactive Preview (first 20 samples) ===\")\n",
    "display(HTML(create_validation_html(validation_set, max_samples=20)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Open 'topic_validation_set.csv' in Excel/Google Sheets\")\n",
    "print(\"2. For each row, fill in:\")\n",
    "print(\"   - validation_correct: 'yes', 'no', or 'unsure'\")\n",
    "print(\"   - validation_notes: Any comments\")\n",
    "print(\"   - validation_alternative_topic: If 'no', what should the topic be?\")\n",
    "print(\"3. Save the file and load it back to calculate inter-rater reliability\")\n",
    "print(\"\\nOptional: Share the CSV with another rater for inter-rater reliability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c8744",
   "metadata": {},
   "source": [
    "## Topic Analysis with Engagement Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=== TOPIC ANALYSIS WITH ENGAGEMENT METRICS ===\"\"\"\n",
    "\n",
    "# Analyze both platforms\n",
    "for platform_name, platform_df in [(\"LessWrong\", lesswrong_df), (\"Alignment Forum\", alignment_forum_df)]:\n",
    "    \n",
    "    print(f\"\\n--- {platform_name}: Topic Engagement Analysis ---\")\n",
    "    \n",
    "    safe_name = platform_name.lower().replace(\" \", \"_\")\n",
    "        \n",
    "    if len(filtered_df) == 0:\n",
    "        print(f\"No topics found for {platform_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate engagement metrics by topic\n",
    "    topic_engagement = filtered_df.groupby('topic_label').agg({\n",
    "        'baseScore': ['sum', 'mean', 'median'],\n",
    "        'commentCount': ['sum', 'mean', 'median'],\n",
    "        '_id': 'count'  # Number of posts\n",
    "    }).round(1)\n",
    "    \n",
    "    # Flatten column names\n",
    "    topic_engagement.columns = ['_'.join(col).strip() for col in topic_engagement.columns.values]\n",
    "    topic_engagement = topic_engagement.rename(columns={'_id_count': 'post_count'})\n",
    "    \n",
    "    # Sort by total score\n",
    "    topic_engagement = topic_engagement.sort_values('baseScore_sum', ascending=False)\n",
    "    \n",
    "    print(\"\\nTopic Engagement Metrics:\")\n",
    "    print(topic_engagement)\n",
    "    \n",
    "    # 1. Total engagement by topic (stacked bar: score + comments)\n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3.5))\n",
    "    \n",
    "    x_pos = np.arange(len(topic_engagement))\n",
    "    \n",
    "    # Normalize to make comparable (score and comments on different scales)\n",
    "    # Use score as primary metric\n",
    "    ax.barh(x_pos, topic_engagement['baseScore_sum'], \n",
    "            color='#2a9d8f', edgecolor='white', linewidth=0.5,\n",
    "            label='Total Score')\n",
    "    \n",
    "    ax.set_yticks(x_pos)\n",
    "    ax.set_yticklabels(topic_engagement.index)\n",
    "    ax.set_xlabel('Total Base Score')\n",
    "    ax.set_title(f'{platform_name}: Total Engagement by Topic', pad=10)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x')\n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Average engagement per post by topic\n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3.5))\n",
    "    \n",
    "    x_pos = np.arange(len(topic_engagement))\n",
    "    sorted_by_mean = topic_engagement.sort_values('baseScore_mean', ascending=False)\n",
    "    \n",
    "    ax.barh(x_pos, sorted_by_mean['baseScore_mean'], \n",
    "            color='#e76f51', edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    ax.set_yticks(x_pos)\n",
    "    ax.set_yticklabels(sorted_by_mean.index)\n",
    "    ax.set_xlabel('Average Base Score per Post')\n",
    "    ax.set_title(f'{platform_name}: Average Engagement by Topic', pad=10)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Bubble chart: Posts vs Engagement (size = comment count)\n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3.5))\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        topic_engagement['post_count'],\n",
    "        topic_engagement['baseScore_mean'],\n",
    "        s=topic_engagement['commentCount_mean'] * 10,  # Size by avg comments\n",
    "        c=np.arange(len(topic_engagement)),  # Color by topic\n",
    "        cmap='viridis',\n",
    "        alpha=0.6,\n",
    "        edgecolors='white',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    \n",
    "    # Add topic labels\n",
    "    for idx, row in topic_engagement.iterrows():\n",
    "        ax.annotate(idx, \n",
    "                   (row['post_count'], row['baseScore_mean']),\n",
    "                   fontsize=6,\n",
    "                   ha='center',\n",
    "                   va='center')\n",
    "    \n",
    "    ax.set_xlabel('Number of Posts')\n",
    "    ax.set_ylabel('Average Base Score')\n",
    "    ax.set_title(f'{platform_name}: Topic Size vs Engagement\\n(bubble size = avg comments)', \n",
    "                 pad=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend for bubble size\n",
    "    sizes = [10, 50, 100]\n",
    "    labels = ['10 comments', '50 comments', '100 comments']\n",
    "    legend_elements = [plt.scatter([], [], s=s*10, c='gray', alpha=0.6, \n",
    "                                   edgecolors='white', linewidth=0.5) \n",
    "                      for s in sizes]\n",
    "    ax.legend(legend_elements, labels, \n",
    "             title='Avg Comments', \n",
    "             loc='lower right',\n",
    "             frameon=True,\n",
    "             edgecolor='black',\n",
    "             fancybox=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Engagement comparison: Score vs Comments\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(DOUBLE_COL, 3))\n",
    "    \n",
    "    x_pos = np.arange(len(topic_engagement))\n",
    "    \n",
    "    # Left: Total scores\n",
    "    ax1.barh(x_pos, topic_engagement['baseScore_sum'], \n",
    "            color='#2a9d8f', edgecolor='white', linewidth=0.5)\n",
    "    ax1.set_yticks(x_pos)\n",
    "    ax1.set_yticklabels(topic_engagement.index, fontsize=6)\n",
    "    ax1.set_xlabel('Total Base Score')\n",
    "    ax1.set_title('By Score', pad=10)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.grid(axis='x')\n",
    "    \n",
    "    # Right: Total comments\n",
    "    ax2.barh(x_pos, topic_engagement['commentCount_sum'], \n",
    "            color='#e9c46a', edgecolor='white', linewidth=0.5)\n",
    "    ax2.set_yticks(x_pos)\n",
    "    ax2.set_yticklabels([])  # Hide labels on right side\n",
    "    ax2.set_xlabel('Total Comments')\n",
    "    ax2.set_title('By Comments', pad=10)\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.grid(axis='x')\n",
    "    \n",
    "    fig.suptitle(f'{platform_name}: Topic Engagement Comparison', \n",
    "                fontsize=9, y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n=== Topic engagement analysis complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9f5f6",
   "metadata": {},
   "source": [
    "## Gender Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4720ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze gender distribution\"\"\"\n",
    "print(\"\\n=== GENDER ANALYSIS ===\")\n",
    "\n",
    "for platform_name, platform_df in [(\"LessWrong\", lesswrong_df), (\"Alignment Forum\", alignment_forum_df), (\"Combined\", combined_df)]:\n",
    "    \n",
    "    print(f\"\\n--- {platform_name} ---\")\n",
    "    \n",
    "    gender_counts = platform_df['user_gender'].value_counts()\n",
    "    print(f\"\\nGender Distribution:\")\n",
    "    for gender, count in gender_counts.items():\n",
    "        percentage = (count / len(platform_df)) * 100\n",
    "        print(f\"{gender.title()}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Gender posts pie chart - use SINGLE_COL for pie charts\n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, SINGLE_COL))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        gender_counts.values, \n",
    "        labels=gender_counts.index, \n",
    "        autopct='%1.1f%%', \n",
    "        startangle=90,\n",
    "        colors=[\"#e76f51\", \"#2a9d8f\", \"#e9c46a\"], \n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},\n",
    "        counterclock=False\n",
    "    )\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax.set_title(f'{platform_name}: Gender Distribution of Posts', pad=10)\n",
    "    safe_name = platform_name.lower().replace(\" \", \"_\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Gender users pie chart\n",
    "    unique_users = platform_df[['user.username', 'user_gender']].drop_duplicates()\n",
    "    user_gender_counts = unique_users['user_gender'].value_counts()\n",
    "    \n",
    "    print(f\"\\nUnique Users by Gender:\")\n",
    "    for gender, count in user_gender_counts.items():\n",
    "        percentage = (count / len(unique_users)) * 100\n",
    "        print(f\"{gender.title()}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, SINGLE_COL))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        user_gender_counts.values, \n",
    "        autopct='%1.1f%%', \n",
    "        startangle=90,\n",
    "        colors=[\"#e76f51\", \"#2a9d8f\", \"#e9c46a\"], \n",
    "        labels=user_gender_counts.index,\n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},\n",
    "        counterclock=False\n",
    "    )\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax.set_title(f'{platform_name}: Gender Distribution of Users', pad=10)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Gender distribution by topics - use DOUBLE_COL for bar charts\n",
    "    filtered_df = platform_df[platform_df[\"topic_label\"] != 'Misc: No Topic'].copy()\n",
    "    \n",
    "    if len(filtered_df) > 0:\n",
    "        gender_topic = filtered_df.groupby(['topic_label', 'user_gender']).size().unstack(fill_value=0)\n",
    "        gender_topic['total'] = gender_topic.sum(axis=1)\n",
    "        \n",
    "        gender_cols = {}\n",
    "        if 'gf' in gender_topic.columns:\n",
    "            gender_cols['female'] = 'gf'\n",
    "        elif 'female' in gender_topic.columns:\n",
    "            gender_cols['female'] = 'female'\n",
    "            \n",
    "        if 'gm' in gender_topic.columns:\n",
    "            gender_cols['male'] = 'gm'\n",
    "        elif 'male' in gender_topic.columns:\n",
    "            gender_cols['male'] = 'male'\n",
    "            \n",
    "        if '-' in gender_topic.columns:\n",
    "            gender_cols['unknown'] = '-'\n",
    "        elif 'unknown' in gender_topic.columns:\n",
    "            gender_cols['unknown'] = 'unknown'\n",
    "        \n",
    "        gender_perc_data = {}\n",
    "        for display_name, col_name in gender_cols.items():\n",
    "            if col_name in gender_topic.columns:\n",
    "                gender_perc_data[display_name] = (gender_topic[col_name] * 100) / gender_topic['total']\n",
    "        \n",
    "        gender_perc = pd.DataFrame(gender_perc_data)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3.5))\n",
    "        gender_perc.plot(kind=\"bar\", ax=ax,\n",
    "                         color=[\"#2a9d8f\", \"#e76f51\", \"#e9c46a\"],\n",
    "                         stacked=True,\n",
    "                         width=0.8,\n",
    "                         edgecolor='white')\n",
    "        \n",
    "        ax.set_title(f'{platform_name}: Gender Distribution by Topic', pad=10)\n",
    "        ax.set_ylabel('Percentage of Posts (%)')\n",
    "        ax.set_xlabel('Topic')\n",
    "        ax.legend(title='Gender', bbox_to_anchor=(1.02, 1), loc='upper left', \n",
    "                  frameon=True, edgecolor='black', fancybox=False)\n",
    "        ax.set_xticklabels(gender_perc.index, rotation=45, ha='right')\n",
    "        ax.grid(axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "print(\"\\n=== Gender analysis complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140fdcb6",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184106f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=== MANUAL GENDER VALIDATION ===\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "TOTAL_VALIDATION_SAMPLES = 200  # Adjust based on how many you want to validate\n",
    "RANDOM_SEED = 42  # For reproducibility\n",
    "\n",
    "# Combine both platforms\n",
    "combined_df = pd.concat([\n",
    "    lesswrong_df.assign(platform='LessWrong'),\n",
    "    alignment_forum_df.assign(platform='Alignment Forum')\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Total posts for validation: {len(combined_df)}\")\n",
    "print(f\"\\n=== Gender Distribution ===\")\n",
    "\n",
    "# Get gender counts and proportions\n",
    "gender_counts = combined_df[\"user_gender\"].value_counts()\n",
    "gender_proportions = gender_counts / len(combined_df)\n",
    "\n",
    "for gender, count in gender_counts.items():\n",
    "    proportion = count / len(combined_df)\n",
    "    n_samples = int(TOTAL_VALIDATION_SAMPLES * proportion)\n",
    "    print(f\"{gender}: {count} posts ({proportion*100:.1f}%) → {n_samples} validation samples\")\n",
    "\n",
    "# Stratified sampling by gender\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "validation_samples = []\n",
    "\n",
    "for gender in gender_counts.index:\n",
    "    gender_df = combined_df[combined_df[\"user_gender\"] == gender]\n",
    "    \n",
    "    # Calculate number of samples for this gender (proportional)\n",
    "    n_samples = max(1, int(TOTAL_VALIDATION_SAMPLES * len(gender_df) / len(combined_df)))\n",
    "    \n",
    "    # Sample randomly\n",
    "    if len(gender_df) >= n_samples:\n",
    "        sampled = gender_df.sample(n=n_samples, random_state=RANDOM_SEED)\n",
    "    else:\n",
    "        sampled = gender_df\n",
    "    \n",
    "    validation_samples.append(sampled)\n",
    "\n",
    "validation_set = pd.concat(validation_samples, ignore_index=True)\n",
    "\n",
    "# Shuffle the validation set\n",
    "validation_set = validation_set.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# Keep only essential columns for validation\n",
    "columns_to_keep = ['user.username', 'user_gender', 'platform', 'title', \n",
    "                   'text', 'content']\n",
    "validation_set = validation_set[[col for col in columns_to_keep if col in validation_set.columns]]\n",
    "\n",
    "# Drop htmlBody if it exists\n",
    "validation_set = validation_set.drop(columns=['htmlBody'], errors='ignore')\n",
    "\n",
    "# Truncate text columns to 500 chars\n",
    "if 'text' in validation_set.columns:\n",
    "    validation_set['text'] = validation_set['text'].astype(str).apply(lambda x: x[:500] + \"...\" if len(x) > 500 else x)\n",
    "if 'content' in validation_set.columns:\n",
    "    validation_set['content'] = validation_set['content'].astype(str).apply(lambda x: x[:500] + \"...\" if len(x) > 500 else x)\n",
    "\n",
    "print(f\"\\n=== Validation Set Created ===\")\n",
    "print(f\"Total validation samples: {len(validation_set)}\")\n",
    "print(f\"\\nSamples per gender:\")\n",
    "print(validation_set[\"user_gender\"].value_counts().sort_index())\n",
    "\n",
    "# Add columns for manual validation\n",
    "validation_set['validation_correct'] = ''  # To be filled: 'yes', 'no', 'unsure'\n",
    "validation_set['validation_notes'] = ''\n",
    "validation_set['validation_alternative_gender'] = ''  # If incorrect, what should it be?\n",
    "\n",
    "# Save to CSV for validation\n",
    "output_file = 'gender_validation_set.csv'\n",
    "validation_set.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved validation set to: {output_file}\")\n",
    "\n",
    "# Display first few examples\n",
    "print(\"\\n=== First 5 Validation Examples ===\\n\")\n",
    "for idx, row in validation_set.head().iterrows():\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Sample {idx + 1}\")\n",
    "    print(f\"Platform: {row['platform']}\")\n",
    "    print(f\"Username: {row['user.username']}\")\n",
    "    print(f\"Assigned Gender: {row['user_gender']}\")\n",
    "    print(f\"Title: {row.get('title', 'N/A')}\")\n",
    "    print(f\"\\nContent Preview (first 500 chars):\")\n",
    "    content = str(row.get('content', row.get('text', 'N/A')))\n",
    "    print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "    print()\n",
    "\n",
    "# Create a more readable HTML version for Jupyter\n",
    "def create_gender_validation_html(df, max_samples=20):\n",
    "    html = \"\"\"\n",
    "    <style>\n",
    "        .validation-card {\n",
    "            border: 2px solid #333;\n",
    "            padding: 15px;\n",
    "            margin: 20px 0;\n",
    "            background-color: #f9f9f9;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        .gender-label {\n",
    "            background-color: #e76f51;\n",
    "            color: white;\n",
    "            padding: 5px 10px;\n",
    "            border-radius: 3px;\n",
    "            display: inline-block;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        .platform {\n",
    "            background-color: #2196F3;\n",
    "            color: white;\n",
    "            padding: 5px 10px;\n",
    "            border-radius: 3px;\n",
    "            display: inline-block;\n",
    "            margin-left: 10px;\n",
    "        }\n",
    "        .username {\n",
    "            background-color: #9c27b0;\n",
    "            color: white;\n",
    "            padding: 5px 10px;\n",
    "            border-radius: 3px;\n",
    "            display: inline-block;\n",
    "            margin-left: 10px;\n",
    "        }\n",
    "        .content-preview {\n",
    "            background-color: white;\n",
    "            padding: 10px;\n",
    "            margin-top: 10px;\n",
    "            border-left: 3px solid #ddd;\n",
    "            font-family: monospace;\n",
    "            font-size: 0.9em;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    \n",
    "    for idx, row in df.head(max_samples).iterrows():\n",
    "        content = str(row.get('content', row.get('text', 'N/A')))\n",
    "        content_preview = content[:800] + \"...\" if len(content) > 800 else content\n",
    "        \n",
    "        html += f\"\"\"\n",
    "        <div class=\"validation-card\">\n",
    "            <h3>Sample {idx + 1}</h3>\n",
    "            <span class=\"gender-label\">{row['user_gender']}</span>\n",
    "            <span class=\"platform\">{row['platform']}</span>\n",
    "            <span class=\"username\">{row['user.username']}</span>\n",
    "            <h4>{row.get('title', 'No Title')}</h4>\n",
    "            <div class=\"content-preview\">{content_preview}</div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "print(\"\\n=== Interactive Preview (first 20 samples) ===\")\n",
    "display(HTML(create_gender_validation_html(validation_set, max_samples=20)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Open 'gender_validation_set.csv' in Excel/Google Sheets\")\n",
    "print(\"2. For each row, fill in:\")\n",
    "print(\"   - validation_correct: 'yes', 'no', or 'unsure'\")\n",
    "print(\"   - validation_notes: Any comments\")\n",
    "print(\"   - validation_alternative_gender: If 'no', what should the gender be?\")\n",
    "print(\"3. Save the file and load it back to calculate inter-rater reliability\")\n",
    "print(\"\\nOptional: Share the CSV with another rater for inter-rater reliability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7fe12",
   "metadata": {},
   "source": [
    "## Author Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f59460",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze author activity patterns\"\"\"\n",
    "print(\"\\n=== AUTHOR ANALYSIS ===\")\n",
    "\n",
    "# Analyze both platforms separately and combined\n",
    "for platform_name, platform_df in [(\"LessWrong\", lesswrong_df), (\"Alignment Forum\", alignment_forum_df), (\"Combined\", combined_df)]:\n",
    "    \n",
    "    print(f\"\\n--- {platform_name} ---\")\n",
    "    \n",
    "    author_counts = platform_df['user.username'].value_counts()\n",
    "    print(f\"\\nTotal unique authors: {len(author_counts):,}\")\n",
    "    print(f\"Authors with only 1 post: {(author_counts == 1).sum():,} ({((author_counts == 1).sum() / len(author_counts) * 100):.1f}%)\")\n",
    "    print(f\"Authors with 10+ posts: {(author_counts >= 10).sum():,}\")\n",
    "    print(f\"Authors with 50+ posts: {(author_counts >= 50).sum():,}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Active Authors:\")\n",
    "    for author, count in author_counts.head(10).items():\n",
    "        print(f\"  {author}: {count} posts\")\n",
    "    \n",
    "    safe_name = platform_name.lower().replace(\" \", \"_\")\n",
    "    \n",
    "    # Author activity distribution (bar chart)\n",
    "    post_ranges = ['1', '2-5', '6-10', '11-25', '26-50', '50+']\n",
    "    counts = [\n",
    "        (author_counts == 1).sum(),\n",
    "        ((author_counts >= 2) & (author_counts <= 5)).sum(),\n",
    "        ((author_counts >= 6) & (author_counts <= 10)).sum(),\n",
    "        ((author_counts >= 11) & (author_counts <= 25)).sum(),\n",
    "        ((author_counts >= 26) & (author_counts <= 50)).sum(),\n",
    "        (author_counts >= 50).sum()\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, 2.5))\n",
    "    bars = ax.bar(post_ranges, counts, color='#2a9d8f', edgecolor='white', linewidth=0.5)\n",
    "    ax.set_title(f'{platform_name}: Author Activity Distribution', pad=10)\n",
    "    ax.set_ylabel('Number of Authors')\n",
    "    ax.set_xlabel('Posts per Author')\n",
    "    ax.set_xticklabels(post_ranges, rotation=45, ha='right')\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height):,}',\n",
    "                ha='center', va='bottom', fontsize=6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Posts per author histogram (log scale)\n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, 2.5))\n",
    "    ax.hist(author_counts.values, bins=50, color='#e76f51', \n",
    "            edgecolor='white', alpha=0.8, linewidth=0.5)\n",
    "    ax.set_xlabel('Posts per Author')\n",
    "    ax.set_ylabel('Number of Authors (log scale)')\n",
    "    ax.set_title(f'{platform_name}: Distribution of Posts per Author', pad=10)\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(axis='y', which='both')  # Show grid for both major and minor ticks on log scale\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n=== Author analysis complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac03679",
   "metadata": {},
   "source": [
    "## Temporal Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze temporal trends\"\"\"\n",
    "print(\"\\n=== TEMPORAL TRENDS ===\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Analyze both platforms separately and combined\n",
    "for platform_name, platform_df in [(\"LessWrong\", lesswrong_df), (\"Alignment Forum\", alignment_forum_df), (\"Combined\", combined_df)]:\n",
    "    \n",
    "    print(f\"\\n--- {platform_name} ---\")\n",
    "    \n",
    "    # Posts by year\n",
    "    yearly_posts = platform_df.groupby('year').size()\n",
    "    print(f\"\\nPosts by Year:\")\n",
    "    for year, count in yearly_posts.items():\n",
    "        print(f\"  {year}: {count:,} posts\")\n",
    "    \n",
    "    safe_name = platform_name.lower().replace(\" \", \"_\")\n",
    "    \n",
    "    # Line plot of posts over time\n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 2.5))\n",
    "    ax.plot(yearly_posts.index, yearly_posts.values, \n",
    "            marker='o', linewidth=1.5, markersize=4, \n",
    "            color='#2a9d8f')\n",
    "    ax.set_title(f'{platform_name}: Posts by Year', pad=10)\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Number of Posts')\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Heatmap of posts by year and month\n",
    "    if 'month' in platform_df.columns:\n",
    "        heatmap_data = platform_df.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "        \n",
    "        # Ensure all months 1-12 are present\n",
    "        for month in range(1, 13):\n",
    "            if month not in heatmap_data.columns:\n",
    "                heatmap_data[month] = 0\n",
    "        heatmap_data = heatmap_data[sorted(heatmap_data.columns)]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(DOUBLE_COL, len(heatmap_data) * 0.3 + 1))\n",
    "        \n",
    "        # Use Nature-appropriate colormap\n",
    "        sns.heatmap(heatmap_data, annot=True, fmt='d', \n",
    "                    cmap='YlOrRd', \n",
    "                    ax=ax,\n",
    "                    cbar_kws={'label': 'Number of Posts'},\n",
    "                    linewidths=0.5,\n",
    "                    linecolor='white')\n",
    "        \n",
    "        ax.set_title(f'{platform_name}: Posts by Year and Month', pad=10)\n",
    "        ax.set_xlabel('Month')\n",
    "        ax.set_ylabel('Year')\n",
    "        \n",
    "        # Set month labels\n",
    "        month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        ax.set_xticklabels(month_labels, rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # Monthly activity (aggregated across all years)\n",
    "    if 'month' in platform_df.columns:\n",
    "        monthly_posts = platform_df.groupby('month').size()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(DOUBLE_COL, 2.5))\n",
    "        bars = ax.bar(monthly_posts.index, monthly_posts.values, \n",
    "                      color='#e76f51', edgecolor='white', linewidth=0.5)\n",
    "        ax.set_title(f'{platform_name}: Posts by Month (All Years)', pad=10)\n",
    "        ax.set_xlabel('Month')\n",
    "        ax.set_ylabel('Number of Posts')\n",
    "        ax.set_xticks(range(1, 13))\n",
    "        ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], \n",
    "                          rotation=45, ha='right')\n",
    "        ax.grid(axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "print(\"\\n=== Temporal trends analysis complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2f1d1",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b893dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze extracted links\"\"\"\n",
    "print(\"\\n=== LINK ANALYSIS ===\")\n",
    "\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_domain(url):\n",
    "    \"\"\"Extract domain from URL.\"\"\"\n",
    "    try:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc.lower()\n",
    "        # Remove www. prefix for cleaner grouping\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "    except:\n",
    "        return 'invalid_url'\n",
    "\n",
    "def analyze_links(platform_df, platform_name):\n",
    "    \"\"\"Analyze links for a given platform.\"\"\"\n",
    "    print(f\"\\n--- {platform_name} ---\")\n",
    "    \n",
    "    safe_name = platform_name.lower().replace(\" \", \"_\")\n",
    "    \n",
    "    # Extract all links\n",
    "    all_links = []\n",
    "    for entry in platform_df['extracted_links'].dropna():\n",
    "        if isinstance(entry, str):\n",
    "            links = [link.strip() for link in entry.split(';') if link.strip()]\n",
    "            all_links.extend(links)\n",
    "        elif isinstance(entry, list):\n",
    "            all_links.extend(entry)\n",
    "        else:\n",
    "            links = [link.strip() for link in str(entry).split(';') if link.strip()]\n",
    "            all_links.extend(links)\n",
    "    \n",
    "    # Final cleaning\n",
    "    clean_links = [l for l in all_links if l]\n",
    "    \n",
    "    if not clean_links:\n",
    "        print(f\"No links found for {platform_name}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total links extracted: {len(clean_links):,}\")\n",
    "    print(f\"Posts with links: {platform_df['extracted_links'].notna().sum():,}\")\n",
    "    \n",
    "    # Pattern analysis\n",
    "    doi_pattern = r'10\\.\\d{4,9}/[^\\s;<>\"]+'\n",
    "    arxiv_pattern = r'(arxiv\\.org/)'\n",
    "    \n",
    "    doi_links = [link for link in clean_links if re.search(doi_pattern, link, re.IGNORECASE)]\n",
    "    arxiv_links = [link for link in clean_links if re.search(arxiv_pattern, link, re.IGNORECASE)]\n",
    "    lesswrong_links = [link for link in clean_links if 'lesswrong' in link.lower()]\n",
    "    \n",
    "    # Extract domains\n",
    "    domains = [extract_domain(link) for link in clean_links]\n",
    "    domain_counts = pd.Series(domains).value_counts()\n",
    "    \n",
    "    # Category counts\n",
    "    youtube_count = len([d for d in domains if 'youtube' in d or 'youtu.be' in d])\n",
    "    wikipedia_count = len([d for d in domains if 'wikipedia' in d])\n",
    "    github_count = len([d for d in domains if 'github' in d])\n",
    "    ea_forum_count = len([d for d in domains if 'forum.effectivealtruism' in d])\n",
    "    alignment_forum_count = len([d for d in domains if 'alignmentforum' in d])\n",
    "    \n",
    "    print(f\"\\n=== LINK CATEGORIES ===\")\n",
    "    print(f\"DOI/Academic: {len(doi_links):,} ({len(doi_links)/len(clean_links)*100:.1f}%)\")\n",
    "    print(f\"ArXiv: {len(arxiv_links):,} ({len(arxiv_links)/len(clean_links)*100:.1f}%)\")\n",
    "    print(f\"LessWrong: {len(lesswrong_links):,} ({len(lesswrong_links)/len(clean_links)*100:.1f}%)\")\n",
    "    print(f\"YouTube: {youtube_count:,} ({youtube_count/len(clean_links)*100:.1f}%)\")\n",
    "    print(f\"Wikipedia: {wikipedia_count:,} ({wikipedia_count/len(clean_links)*100:.1f}%)\")\n",
    "    print(f\"GitHub: {github_count:,} ({github_count/len(clean_links)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n=== TOP 15 DOMAINS ===\")\n",
    "    for domain, count in domain_counts.head(15).items():\n",
    "        percentage = count/len(clean_links)*100\n",
    "        print(f\"  {domain}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualizations\n",
    "    \n",
    "    # 1. Top domains bar chart\n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, 3))\n",
    "    top_domains = domain_counts.head(10)\n",
    "    bars = ax.barh(range(len(top_domains)), top_domains.values, \n",
    "                   color='#2a9d8f', edgecolor='white', linewidth=0.5)\n",
    "    ax.set_yticks(range(len(top_domains)))\n",
    "    ax.set_yticklabels(top_domains.index)\n",
    "    ax.set_xlabel('Number of Links')\n",
    "    ax.set_title(f'{platform_name}: Top 10 Most Linked Domains', pad=10)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Link categories pie chart\n",
    "    categories = {\n",
    "        'DOI/Academic': len(doi_links),\n",
    "        'ArXiv': len(arxiv_links),\n",
    "        'LessWrong': len(lesswrong_links),\n",
    "        'YouTube': youtube_count,\n",
    "        'Wikipedia': wikipedia_count,\n",
    "        'EA Forum': ea_forum_count,\n",
    "        'Alignment Forum': alignment_forum_count,\n",
    "        'GitHub': github_count,\n",
    "    }\n",
    "    \n",
    "    # Calculate \"Other\" category\n",
    "    other_count = len(clean_links) - sum(categories.values())\n",
    "    if other_count > 0:\n",
    "        categories['Other'] = other_count\n",
    "    \n",
    "    # Filter out zero-count categories\n",
    "    categories = {k: v for k, v in categories.items() if v > 0}\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, SINGLE_COL))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        categories.values(), \n",
    "        labels=categories.keys(), \n",
    "        autopct='%1.1f%%', \n",
    "        startangle=90,\n",
    "        colors=['#2a9d8f', '#e76f51', '#e9c46a', '#264653', '#f4a261', \n",
    "                '#e63946', '#457b9d', '#a8dadc', '#cccccc'],\n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},\n",
    "        counterclock=False\n",
    "    )\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax.set_title(f'{platform_name}: Link Categories', pad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Academic vs Non-Academic\n",
    "    academic_count = len(doi_links) + len(arxiv_links)\n",
    "    non_academic_count = len(clean_links) - academic_count\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, SINGLE_COL))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        [academic_count, non_academic_count],\n",
    "        labels=['Academic\\n(DOI/ArXiv)', 'Non-Academic'],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=['#2a9d8f', '#e76f51'],\n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},\n",
    "        counterclock=False\n",
    "    )\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax.set_title(f'{platform_name}: Academic vs Non-Academic Links', pad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Sample links for verification\n",
    "    print(f\"\\n=== SAMPLE LINKS ===\")\n",
    "    if doi_links:\n",
    "        print(f\"Sample DOI links: {doi_links[:2]}\")\n",
    "    if arxiv_links:\n",
    "        print(f\"Sample ArXiv links: {arxiv_links[:2]}\")\n",
    "    if lesswrong_links:\n",
    "        print(f\"Sample LessWrong links: {lesswrong_links[:2]}\")\n",
    "\n",
    "# Run analysis for each platform\n",
    "for platform_name, platform_df in [(\"LessWrong\", lesswrong_df), (\"Alignment Forum\", alignment_forum_df), (\"Combined\", combined_df)]:\n",
    "    if 'extracted_links' in platform_df.columns:\n",
    "        analyze_links(platform_df, platform_name)\n",
    "    else:\n",
    "        print(f\"\\n--- {platform_name} ---\")\n",
    "        print(\"No 'extracted_links' column found\")\n",
    "\n",
    "print(\"\\n=== Link analysis complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ccbcf0",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a329a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate comprehensive summary reports\"\"\"\n",
    "\n",
    "def generate_summary_report(platform_df, platform_name):\n",
    "    \"\"\"Generate a detailed summary report for a platform.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"          {platform_name.upper()} DATA SUMMARY REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall Statistics\n",
    "    print(f\"\\n📊 OVERALL STATISTICS:\")\n",
    "    print(f\"   Total Posts: {len(platform_df):,}\")\n",
    "    print(f\"   Date Range: {platform_df['year'].min()}-{platform_df['year'].max()}\")\n",
    "    print(f\"   Unique Authors: {platform_df['user.username'].nunique():,}\")\n",
    "    if 'topic_label' in platform_df.columns:\n",
    "        unique_topics = platform_df[platform_df['topic_label'] != 'Misc: No Topic']['topic_label'].nunique()\n",
    "        print(f\"   Unique Topics: {unique_topics}\")\n",
    "    \n",
    "    # Author Insights\n",
    "    print(f\"\\n👥 AUTHOR INSIGHTS:\")\n",
    "    author_counts = platform_df['user.username'].value_counts()\n",
    "    print(f\"   One-time contributors: {(author_counts == 1).sum():,} ({((author_counts == 1).sum() / len(author_counts) * 100):.1f}%)\")\n",
    "    print(f\"   Regular authors (2-9 posts): {((author_counts >= 2) & (author_counts < 10)).sum():,}\")\n",
    "    print(f\"   Prolific authors (10-49 posts): {((author_counts >= 10) & (author_counts < 50)).sum():,}\")\n",
    "    print(f\"   Super authors (50+ posts): {(author_counts >= 50).sum():,}\")\n",
    "    print(f\"   Most prolific: {author_counts.index[0]} with {author_counts.iloc[0]:,} posts\")\n",
    "    \n",
    "    # Show top 5 authors\n",
    "    print(f\"   Top 5 authors:\")\n",
    "    for i, (author, count) in enumerate(author_counts.head(5).items(), 1):\n",
    "        print(f\"      {i}. {author}: {count:,} posts\")\n",
    "    \n",
    "    # Topic Insights\n",
    "    if 'topic_label' in platform_df.columns:\n",
    "        print(f\"\\n🏷️  TOPIC INSIGHTS:\")\n",
    "        topic_df = platform_df[platform_df['topic_label'] != 'Misc: No Topic']\n",
    "        topic_counts = topic_df['topic_label'].value_counts()\n",
    "        if len(topic_counts) > 0:\n",
    "            print(f\"   Most popular topic: {topic_counts.index[0]} ({topic_counts.iloc[0]:,} posts, {topic_counts.iloc[0]/len(topic_df)*100:.1f}%)\")\n",
    "            print(f\"   Topics with 100+ posts: {(topic_counts >= 100).sum()}\")\n",
    "            print(f\"   Topics with 1000+ posts: {(topic_counts >= 1000).sum()}\")\n",
    "            print(f\"   Topic distribution:\")\n",
    "            for topic, count in topic_counts.items():\n",
    "                percentage = (count / len(topic_df)) * 100\n",
    "                print(f\"      {topic}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Gender Distribution\n",
    "    print(f\"\\n🚻 GENDER DISTRIBUTION:\")\n",
    "    if 'user_gender' in platform_df.columns:\n",
    "        gender_counts = platform_df['user_gender'].value_counts()\n",
    "        for gender, count in gender_counts.items():\n",
    "            percentage = (count / len(platform_df)) * 100\n",
    "            print(f\"   {gender.title()}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Unique users by gender\n",
    "        unique_users = platform_df[['user.username', 'user_gender']].drop_duplicates()\n",
    "        user_gender_counts = unique_users['user_gender'].value_counts()\n",
    "        print(f\"   Unique users by gender:\")\n",
    "        for gender, count in user_gender_counts.items():\n",
    "            percentage = (count / len(unique_users)) * 100\n",
    "            print(f\"      {gender.title()}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Engagement Metrics\n",
    "    print(f\"\\n📈 ENGAGEMENT METRICS:\")\n",
    "    print(f\"   Average score: {platform_df['baseScore'].mean():.1f}\")\n",
    "    print(f\"   Median score: {platform_df['baseScore'].median():.1f}\")\n",
    "    print(f\"   Average comments: {platform_df['commentCount'].mean():.1f}\")\n",
    "    print(f\"   Median comments: {platform_df['commentCount'].median():.1f}\")\n",
    "    print(f\"   Total comments: {platform_df['commentCount'].sum():,}\")\n",
    "    \n",
    "    # Top posts\n",
    "    top_score_idx = platform_df['baseScore'].idxmax()\n",
    "    top_comments_idx = platform_df['commentCount'].idxmax()\n",
    "    print(f\"   Highest scoring post ({platform_df.loc[top_score_idx, 'baseScore']:.0f} points):\")\n",
    "    print(f\"      '{platform_df.loc[top_score_idx, 'title'][:60]}...'\")\n",
    "    print(f\"      by {platform_df.loc[top_score_idx, 'user.username']}\")\n",
    "    print(f\"   Most commented post ({platform_df.loc[top_comments_idx, 'commentCount']:.0f} comments):\")\n",
    "    print(f\"      '{platform_df.loc[top_comments_idx, 'title'][:60]}...'\")\n",
    "    print(f\"      by {platform_df.loc[top_comments_idx, 'user.username']}\")\n",
    "    \n",
    "    # Score and comment distribution\n",
    "    high_score = (platform_df['baseScore'] >= 100).sum()\n",
    "    very_high_score = (platform_df['baseScore'] >= 500).sum()\n",
    "    print(f\"   Posts with 100+ score: {high_score:,} ({high_score/len(platform_df)*100:.1f}%)\")\n",
    "    print(f\"   Posts with 500+ score: {very_high_score:,} ({very_high_score/len(platform_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Temporal Trends\n",
    "    print(f\"\\n📅 TEMPORAL TRENDS:\")\n",
    "    yearly_posts = platform_df.groupby('year').size()\n",
    "    peak_year = yearly_posts.idxmax()\n",
    "    print(f\"   Peak posting year: {peak_year} with {yearly_posts[peak_year]:,} posts\")\n",
    "    if len(yearly_posts) > 1:\n",
    "        growth = ((yearly_posts.iloc[-1] / yearly_posts.iloc[0] - 1) * 100)\n",
    "        print(f\"   Growth from {yearly_posts.index[0]} to {yearly_posts.index[-1]}: {growth:+.1f}%\")\n",
    "    \n",
    "    # Year-by-year breakdown\n",
    "    print(f\"   Posts by year:\")\n",
    "    for year, count in yearly_posts.items():\n",
    "        print(f\"      {year}: {count:,} posts\")\n",
    "    \n",
    "    # Monthly patterns (if available)\n",
    "    if 'month' in platform_df.columns:\n",
    "        monthly_posts = platform_df.groupby('month').size()\n",
    "        peak_month = monthly_posts.idxmax()\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        print(f\"   Most active month: {month_names[peak_month-1]} ({monthly_posts[peak_month]:,} posts total)\")\n",
    "    \n",
    "    # Link Analysis (if available)\n",
    "    if 'extracted_links' in platform_df.columns:\n",
    "        print(f\"\\n🔗 LINK ANALYSIS:\")\n",
    "        posts_with_links = platform_df['extracted_links'].notna().sum()\n",
    "        print(f\"   Posts with links: {posts_with_links:,} ({posts_with_links/len(platform_df)*100:.1f}%)\")\n",
    "        print(f\"   Posts without links: {len(platform_df) - posts_with_links:,} ({(len(platform_df) - posts_with_links)/len(platform_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Word count analysis (if available)\n",
    "    if 'wordCount' in platform_df.columns:\n",
    "        print(f\"\\n📝 CONTENT METRICS:\")\n",
    "        print(f\"   Average word count: {platform_df['wordCount'].mean():.0f}\")\n",
    "        print(f\"   Median word count: {platform_df['wordCount'].median():.0f}\")\n",
    "        short_posts = (platform_df['wordCount'] < 500).sum()\n",
    "        medium_posts = ((platform_df['wordCount'] >= 500) & (platform_df['wordCount'] < 2000)).sum()\n",
    "        long_posts = (platform_df['wordCount'] >= 2000).sum()\n",
    "        print(f\"   Short posts (<500 words): {short_posts:,} ({short_posts/len(platform_df)*100:.1f}%)\")\n",
    "        print(f\"   Medium posts (500-2000 words): {medium_posts:,} ({medium_posts/len(platform_df)*100:.1f}%)\")\n",
    "        print(f\"   Long posts (2000+ words): {long_posts:,} ({long_posts/len(platform_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Generate reports for each platform\n",
    "for platform_name, platform_df in [(\"LessWrong\", lesswrong_df), (\"Alignment Forum\", alignment_forum_df), (\"Combined\", combined_df)]:\n",
    "    generate_summary_report(platform_df, platform_name)\n",
    "\n",
    "# Comparative summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    COMPARATIVE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n📊 PLATFORM COMPARISON:\")\n",
    "print(f\"   LessWrong posts: {len(lesswrong_df):,}\")\n",
    "print(f\"   Alignment Forum posts: {len(alignment_forum_df):,}\")\n",
    "print(f\"   Total: {len(combined_df):,}\")\n",
    "\n",
    "print(f\"\\n👥 AUTHOR OVERLAP:\")\n",
    "lw_authors = set(lesswrong_df['user.username'].unique())\n",
    "af_authors = set(alignment_forum_df['user.username'].unique())\n",
    "overlap = lw_authors & af_authors\n",
    "print(f\"   LessWrong only: {len(lw_authors - af_authors):,}\")\n",
    "print(f\"   Alignment Forum only: {len(af_authors - lw_authors):,}\")\n",
    "print(f\"   Both platforms: {len(overlap):,}\")\n",
    "\n",
    "print(f\"\\n📈 ENGAGEMENT COMPARISON:\")\n",
    "print(f\"   LessWrong avg score: {lesswrong_df['baseScore'].mean():.1f}\")\n",
    "print(f\"   Alignment Forum avg score: {alignment_forum_df['baseScore'].mean():.1f}\")\n",
    "print(f\"   LessWrong avg comments: {lesswrong_df['commentCount'].mean():.1f}\")\n",
    "print(f\"   Alignment Forum avg comments: {alignment_forum_df['commentCount'].mean():.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                  SUMMARY REPORT COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a14617",
   "metadata": {},
   "source": [
    "---\n",
    "# OpenAlex Nodes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load OpenAlex data\"\"\"\n",
    "print(\"\\n=== LOADING OPENALEX DATA ===\")\n",
    "\n",
    "# Load OpenAlex data with same structure\n",
    "openalex_files = glob.glob(f\"../src/processed_data/openalex/02_with_gender/**/*.csv\", recursive=True)\n",
    "print(f\"Found {len(openalex_files)} OpenAlex CSV files\")\n",
    "\n",
    "openalex_data = []\n",
    "for file in openalex_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        # Extract year and month from filename\n",
    "        parts = Path(file).stem.split('-')\n",
    "        if len(parts) >= 2:\n",
    "            df['year'] = int(parts[0])\n",
    "            df['month'] = int(parts[1])\n",
    "        openalex_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "if not openalex_data:\n",
    "    print(\"No OpenAlex data found! Check your file paths.\")\n",
    "else:\n",
    "    openalex_df = pd.concat(openalex_data, ignore_index=True)\n",
    "    print(f\"Loaded {len(openalex_df)} total papers from {len(openalex_data)} files\")\n",
    "    \n",
    "    # Clean and prepare data\n",
    "    openalex_df['publication_date'] = pd.to_datetime(openalex_df['publication_date'], errors='coerce')\n",
    "    openalex_df['cited_by_count'] = pd.to_numeric(openalex_df['cited_by_count'], errors='coerce').fillna(0)\n",
    "    openalex_df['num_authors'] = pd.to_numeric(openalex_df['num_authors'], errors='coerce').fillna(0)\n",
    "    \n",
    "    print(f\"\\nOpenAlex Dataset Summary:\")\n",
    "    print(f\"  Total papers: {len(openalex_df):,}\")\n",
    "    print(f\"  Date range: {openalex_df['publication_year'].min()} - {openalex_df['publication_year'].max()}\")\n",
    "    print(f\"  Papers with DOI: {openalex_df['doi'].notna().sum():,}\")\n",
    "    print(f\"  Papers with topics: {openalex_df['topics'].notna().sum():,}\")\n",
    "    print(f\"  Papers with gender data: {openalex_df['author_genders'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7620b2",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Parsing OpenAlex Topics ===\")\n",
    "\n",
    "# Parse topics column (semicolon-separated string)\n",
    "def parse_topics(topic_str):\n",
    "    \"\"\"Parse topics from semicolon-separated string.\"\"\"\n",
    "    if pd.isna(topic_str):\n",
    "        return []\n",
    "    # Split by semicolon and clean whitespace\n",
    "    topics = [t.strip() for t in str(topic_str).split(';') if t.strip()]\n",
    "    return topics\n",
    "\n",
    "# Extract all topics\n",
    "print(\"Parsing topic data...\")\n",
    "openalex_df['parsed_topics'] = openalex_df['topics'].apply(parse_topics)\n",
    "openalex_df['num_topics'] = openalex_df['parsed_topics'].apply(len)\n",
    "\n",
    "print(f\"Papers with topics: {(openalex_df['num_topics'] > 0).sum():,}\")\n",
    "print(f\"Average topics per paper: {openalex_df['num_topics'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=== OPENALEX TOPIC ANALYSIS ===\"\"\"\n",
    "\n",
    "# Create expanded dataframe with one row per topic\n",
    "all_topics = []\n",
    "for idx, row in openalex_df.iterrows():\n",
    "    topics = row['parsed_topics']\n",
    "    for topic in topics:\n",
    "        all_topics.append({\n",
    "            'paper_id': row['id'],\n",
    "            'topic_name': topic,\n",
    "            'cited_by_count': row['cited_by_count'],\n",
    "            'num_authors': row['num_authors'],\n",
    "            'year': row['year'],\n",
    "            'publication_year': row['publication_year']\n",
    "        })\n",
    "\n",
    "topics_df = pd.DataFrame(all_topics)\n",
    "print(f\"Extracted {len(topics_df)} topic associations from {len(openalex_df)} papers\")\n",
    "\n",
    "if len(topics_df) == 0:\n",
    "    print(\"No topics found in OpenAlex data\")\n",
    "else:\n",
    "    print(f\"Unique topics: {topics_df['topic_name'].nunique()}\")\n",
    "    \n",
    "    # 1. Top Topics Distribution (show top 20)\n",
    "    topic_counts = topics_df['topic_name'].value_counts().head(20)\n",
    "    \n",
    "    print(f\"\\nTop 20 Topic Distribution:\")\n",
    "    for topic, count in topic_counts.items():\n",
    "        percentage = (count / len(topics_df)) * 100\n",
    "        print(f\"  {topic}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, max(3.5, len(topic_counts) * 0.25)))\n",
    "    bars = ax.barh(range(len(topic_counts)), topic_counts.values,\n",
    "                   color='#2a9d8f', edgecolor='white', linewidth=0.5)\n",
    "    ax.set_yticks(range(len(topic_counts)))\n",
    "    ax.set_yticklabels(topic_counts.index, fontsize=7)\n",
    "    ax.set_xlabel('Number of Papers')\n",
    "    ax.set_title('OpenAlex: Top 20 Research Topics', pad=10)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "               f' {int(width):,}',\n",
    "               ha='left', va='center', fontsize=6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Topic Distribution (Pie Chart - top 10 only)\n",
    "    top_10_topics = topics_df['topic_name'].value_counts().head(10)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, SINGLE_COL))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        top_10_topics.values,\n",
    "        labels=top_10_topics.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=['#2a9d8f', '#e76f51', '#e9c46a', '#264653', '#f4a261', \n",
    "                '#e63946', '#457b9d', '#a8dadc', '#cccccc', '#8d99ae'],\n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},\n",
    "        counterclock=False,\n",
    "        textprops={'fontsize': 6}\n",
    "    )\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(6)\n",
    "    \n",
    "    # Adjust label fontsize\n",
    "    for text in texts:\n",
    "        text.set_fontsize(6)\n",
    "    \n",
    "    ax.set_title('OpenAlex: Top 10 Research Topics', pad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Topic Citation Impact (top 20)\n",
    "    topic_engagement = topics_df.groupby('topic_name').agg({\n",
    "        'cited_by_count': ['sum', 'mean', 'median'],\n",
    "        'paper_id': 'count'\n",
    "    }).round(1)\n",
    "    \n",
    "    topic_engagement.columns = ['total_citations', 'avg_citations', 'median_citations', 'paper_count']\n",
    "    topic_engagement = topic_engagement.sort_values('total_citations', ascending=False).head(20)\n",
    "    \n",
    "    print(f\"\\nTop 20 Topics by Citation Metrics:\")\n",
    "    print(topic_engagement.to_string())\n",
    "    \n",
    "    # Total citations by topic\n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, max(3.5, len(topic_engagement) * 0.25)))\n",
    "    bars = ax.barh(range(len(topic_engagement)), topic_engagement['total_citations'],\n",
    "                   color='#e9c46a', edgecolor='white', linewidth=0.5)\n",
    "    ax.set_yticks(range(len(topic_engagement)))\n",
    "    ax.set_yticklabels(topic_engagement.index, fontsize=7)\n",
    "    ax.set_xlabel('Total Citations')\n",
    "    ax.set_title('OpenAlex: Most Cited Topics (Top 20)', pad=10)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Average citations by topic\n",
    "    topic_engagement_avg = topics_df.groupby('topic_name').agg({\n",
    "        'cited_by_count': 'mean',\n",
    "        'paper_id': 'count'\n",
    "    }).round(1)\n",
    "    topic_engagement_avg.columns = ['avg_citations', 'paper_count']\n",
    "    # Only topics with at least 5 papers\n",
    "    topic_engagement_avg = topic_engagement_avg[topic_engagement_avg['paper_count'] >= 5]\n",
    "    topic_engagement_avg = topic_engagement_avg.sort_values('avg_citations', ascending=False).head(20)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, max(3.5, len(topic_engagement_avg) * 0.25)))\n",
    "    bars = ax.barh(range(len(topic_engagement_avg)), topic_engagement_avg['avg_citations'],\n",
    "                   color='#e76f51', edgecolor='white', linewidth=0.5)\n",
    "    ax.set_yticks(range(len(topic_engagement_avg)))\n",
    "    ax.set_yticklabels(topic_engagement_avg.index, fontsize=7)\n",
    "    ax.set_xlabel('Average Citations per Paper')\n",
    "    ax.set_title('OpenAlex: Highest Impact Topics (≥5 papers, Top 20)', pad=10)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Topics over time (top 10 only)\n",
    "    top_10_for_temporal = topics_df['topic_name'].value_counts().head(10).index\n",
    "    topic_temporal = topics_df[topics_df['topic_name'].isin(top_10_for_temporal)].groupby(['year', 'topic_name']).size().reset_index(name='count')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3.5))\n",
    "    \n",
    "    for topic in top_10_for_temporal:\n",
    "        topic_data = topic_temporal[topic_temporal['topic_name'] == topic]\n",
    "        ax.plot(topic_data['year'], topic_data['count'], \n",
    "               marker='o', linewidth=1.5, markersize=4, label=topic)\n",
    "    \n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Number of Papers')\n",
    "    ax.set_title('OpenAlex: Top 10 Topics Over Time', pad=10)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "             frameon=True, edgecolor='black', fancybox=False,\n",
    "             fontsize=6)\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Bubble chart: Papers vs Citations (top 30 topics)\n",
    "    topic_bubble = topics_df.groupby('topic_name').agg({\n",
    "        'cited_by_count': 'mean',\n",
    "        'num_authors': 'mean',\n",
    "        'paper_id': 'count'\n",
    "    }).round(1)\n",
    "    topic_bubble.columns = ['avg_citations', 'avg_authors', 'paper_count']\n",
    "    topic_bubble = topic_bubble[topic_bubble['paper_count'] >= 3].sort_values('paper_count', ascending=False).head(30)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3.5))\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        topic_bubble['paper_count'],\n",
    "        topic_bubble['avg_citations'],\n",
    "        s=topic_bubble['avg_authors'] * 20,\n",
    "        c=np.arange(len(topic_bubble)),\n",
    "        cmap='viridis',\n",
    "        alpha=0.6,\n",
    "        edgecolors='white',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    \n",
    "    # Add labels for top 10 only\n",
    "    for idx, row in topic_bubble.head(10).iterrows():\n",
    "        label = idx if len(idx) <= 30 else idx[:27] + '...'\n",
    "        ax.annotate(label, \n",
    "                   (row['paper_count'], row['avg_citations']),\n",
    "                   fontsize=5,\n",
    "                   ha='center',\n",
    "                   va='center')\n",
    "    \n",
    "    ax.set_xlabel('Number of Papers')\n",
    "    ax.set_ylabel('Average Citations per Paper')\n",
    "    ax.set_title('OpenAlex: Topic Size vs Impact (Top 30)\\n(bubble size = avg authors)', pad=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend for bubble size\n",
    "    sizes = [int(topic_bubble['avg_authors'].min()), \n",
    "             int(topic_bubble['avg_authors'].mean()), \n",
    "             int(topic_bubble['avg_authors'].max())]\n",
    "    labels = [f'{s} authors' for s in sizes]\n",
    "    legend_elements = [plt.scatter([], [], s=s*20, c='gray', alpha=0.6,\n",
    "                                   edgecolors='white', linewidth=0.5)\n",
    "                      for s in sizes]\n",
    "    ax.legend(legend_elements, labels,\n",
    "             title='Avg Authors',\n",
    "             loc='upper right',\n",
    "             frameon=True,\n",
    "             edgecolor='black',\n",
    "             fancybox=False,\n",
    "             fontsize=6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n=== OpenAlex topic analysis complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4655e",
   "metadata": {},
   "source": [
    "## Gender Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== OPENALEX GENDER ANALYSIS ===\")\n",
    "\n",
    "# Parse author_genders column (semicolon-separated string)\n",
    "def parse_genders(gender_str):\n",
    "    \"\"\"Parse author genders from semicolon-separated string.\"\"\"\n",
    "    if pd.isna(gender_str):\n",
    "        return []\n",
    "    # Split by semicolon and clean whitespace, convert to lowercase\n",
    "    genders = [g.strip().lower() for g in str(gender_str).split(';') if g.strip()]\n",
    "    return genders\n",
    "\n",
    "# Extract gender information\n",
    "print(\"Parsing author gender data...\")\n",
    "openalex_df['parsed_genders'] = openalex_df['author_genders'].apply(parse_genders)\n",
    "openalex_df['num_genders'] = openalex_df['parsed_genders'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e99954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expanded dataframe with one row per author\n",
    "all_authors = []\n",
    "for idx, row in openalex_df.iterrows():\n",
    "    genders = row['parsed_genders']\n",
    "    for gender in genders:\n",
    "        all_authors.append({\n",
    "            'paper_id': row['id'],\n",
    "            'gender': gender,\n",
    "            'year': row['year'],\n",
    "            'cited_by_count': row['cited_by_count'],\n",
    "            'num_authors': row['num_authors'],\n",
    "            'publication_year': row['publication_year']\n",
    "        })\n",
    "\n",
    "authors_df = pd.DataFrame(all_authors)\n",
    "print(f\"Extracted {len(authors_df)} author records from {len(openalex_df)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(authors_df) == 0:\n",
    "    print(\"No gender data found in OpenAlex\")\n",
    "else:\n",
    "    # Clean gender labels\n",
    "    gender_counts = authors_df['gender'].value_counts()\n",
    "    print(f\"\\nAuthor Gender Distribution:\")\n",
    "    for gender, count in gender_counts.items():\n",
    "        percentage = (count / len(authors_df)) * 100\n",
    "        print(f\"  {gender.upper()}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Paper-level gender composition - IGNORE unknowns\n",
    "    openalex_df['has_female'] = openalex_df['parsed_genders'].apply(\n",
    "        lambda x: any(g in ['gf', 'female', 'f'] for g in x) if isinstance(x, list) else False\n",
    "    )\n",
    "    openalex_df['has_male'] = openalex_df['parsed_genders'].apply(\n",
    "        lambda x: any(g in ['gm', 'male', 'm'] for g in x) if isinstance(x, list) else False\n",
    "    )\n",
    "    # Only count as \"all unknown\" if there are NO known genders at all\n",
    "    openalex_df['has_any_known'] = openalex_df['has_female'] | openalex_df['has_male']\n",
    "    \n",
    "    print(f\"\\nPaper Gender Composition (ignoring unknown):\")\n",
    "    print(f\"  Papers with ≥1 female author: {openalex_df['has_female'].sum():,} ({openalex_df['has_female'].sum()/len(openalex_df)*100:.1f}%)\")\n",
    "    print(f\"  Papers with ≥1 male author: {openalex_df['has_male'].sum():,} ({openalex_df['has_male'].sum()/len(openalex_df)*100:.1f}%)\")\n",
    "    print(f\"  Papers with no known genders: {(~openalex_df['has_any_known']).sum():,} ({(~openalex_df['has_any_known']).sum()/len(openalex_df)*100:.1f}%)\")\n",
    "    \n",
    "    # 1. Overall gender distribution (authors) - including unknown for transparency\n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, SINGLE_COL))\n",
    "    \n",
    "    # Map to cleaner labels\n",
    "    label_map = {'gf': 'Female', 'gm': 'Male', 'unknown': 'Unknown', '-': 'Unknown'}\n",
    "    clean_labels = [label_map.get(g, g.title()) for g in gender_counts.index]\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        gender_counts.values,\n",
    "        labels=clean_labels,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=['#e76f51', '#2a9d8f', '#e9c46a'],\n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},\n",
    "        counterclock=False\n",
    "    )\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax.set_title('OpenAlex: Author Gender Distribution', pad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Gender trends over time\n",
    "    gender_temporal = authors_df.groupby(['year', 'gender']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    gender_temporal_pct = gender_temporal.div(gender_temporal.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3))\n",
    "    \n",
    "    for gender in gender_temporal_pct.columns:\n",
    "        display_label = label_map.get(gender, gender.title())\n",
    "        ax.plot(gender_temporal_pct.index, gender_temporal_pct[gender],\n",
    "               marker='o', linewidth=1.5, markersize=4, label=display_label)\n",
    "    \n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Percentage of Authors (%)')\n",
    "    ax.set_title('OpenAlex: Gender Representation Over Time', pad=10)\n",
    "    ax.legend(frameon=True, edgecolor='black', fancybox=False)\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Gender by topic (if topics available - top 15 topics only)\n",
    "    if len(topics_df) > 0:\n",
    "        # Merge topics with authors\n",
    "        topic_gender = topics_df.merge(\n",
    "            authors_df[['paper_id', 'gender']], \n",
    "            on='paper_id', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Get top 15 topics\n",
    "        top_topics_for_gender = topic_gender['topic_name'].value_counts().head(15).index\n",
    "        topic_gender_filtered = topic_gender[topic_gender['topic_name'].isin(top_topics_for_gender)]\n",
    "        \n",
    "        # Calculate gender distribution by topic\n",
    "        topic_gender_dist = topic_gender_filtered.groupby(['topic_name', 'gender']).size().unstack(fill_value=0)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        topic_gender_pct = topic_gender_dist.div(topic_gender_dist.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        # Sort by female percentage if available\n",
    "        if 'gf' in topic_gender_pct.columns:\n",
    "            topic_gender_pct = topic_gender_pct.sort_values('gf', ascending=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(DOUBLE_COL, max(4, len(topic_gender_pct) * 0.3)))\n",
    "        \n",
    "        # Create color mapping\n",
    "        color_map = {'gf': '#e76f51', 'gm': '#2a9d8f', 'unknown': '#e9c46a', '-': '#e9c46a'}\n",
    "        colors = [color_map.get(col, '#cccccc') for col in topic_gender_pct.columns]\n",
    "        \n",
    "        topic_gender_pct.plot(kind='barh', stacked=True, ax=ax,\n",
    "                              color=colors,\n",
    "                              width=0.8,\n",
    "                              edgecolor='white',\n",
    "                              linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Percentage of Authors (%)')\n",
    "        ax.set_ylabel('Topic')\n",
    "        ax.set_title('OpenAlex: Gender Distribution by Top 15 Topics', pad=10)\n",
    "        \n",
    "        # Clean up legend labels\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        clean_legend_labels = [label_map.get(l, l.title()) for l in labels]\n",
    "        ax.legend(handles, clean_legend_labels, title='Gender', \n",
    "                 bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                 frameon=True, edgecolor='black', fancybox=False)\n",
    "        ax.grid(axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Team composition analysis - IGNORE unknowns in classification\n",
    "    openalex_df['team_type'] = 'No Known Gender'\n",
    "    openalex_df.loc[openalex_df['has_female'] & openalex_df['has_male'], 'team_type'] = 'Mixed'\n",
    "    openalex_df.loc[openalex_df['has_female'] & ~openalex_df['has_male'], 'team_type'] = 'Female-only'\n",
    "    openalex_df.loc[~openalex_df['has_female'] & openalex_df['has_male'], 'team_type'] = 'Male-only'\n",
    "    \n",
    "    team_counts = openalex_df['team_type'].value_counts()\n",
    "    \n",
    "    print(f\"\\nTeam Composition (unknowns ignored):\")\n",
    "    for team_type, count in team_counts.items():\n",
    "        percentage = (count / len(openalex_df)) * 100\n",
    "        print(f\"  {team_type}: {count:,} papers ({percentage:.1f}%)\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, SINGLE_COL))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        team_counts.values,\n",
    "        labels=team_counts.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=['#2a9d8f', '#e76f51', '#e9c46a', '#264653'],\n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},\n",
    "        counterclock=False\n",
    "    )\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax.set_title('OpenAlex: Research Team Composition', pad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Average team size by gender\n",
    "    gender_team_size = authors_df.groupby('gender')['num_authors'].mean()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(SINGLE_COL, 2.5))\n",
    "    \n",
    "    display_labels = [label_map.get(g, g.title()) for g in gender_team_size.index]\n",
    "    colors_list = ['#e76f51' if 'gf' in g else '#2a9d8f' if 'gm' in g else '#e9c46a' \n",
    "                   for g in gender_team_size.index]\n",
    "    \n",
    "    bars = ax.bar(range(len(gender_team_size)), gender_team_size.values,\n",
    "                  color=colors_list,\n",
    "                  edgecolor='white',\n",
    "                  linewidth=0.5)\n",
    "    \n",
    "    ax.set_xticks(range(len(gender_team_size)))\n",
    "    ax.set_xticklabels(display_labels)\n",
    "    ax.set_ylabel('Average Team Size')\n",
    "    ax.set_title('OpenAlex: Avg Team Size by Author Gender', pad=10)\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.1f}',\n",
    "               ha='center', va='bottom', fontsize=7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Gender over time (absolute numbers)\n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3))\n",
    "    \n",
    "    for gender in gender_temporal.columns:\n",
    "        display_label = label_map.get(gender, gender.title())\n",
    "        ax.plot(gender_temporal.index, gender_temporal[gender],\n",
    "               marker='o', linewidth=1.5, markersize=4, label=display_label)\n",
    "    \n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Number of Authors')\n",
    "    ax.set_title('OpenAlex: Author Count by Gender Over Time', pad=10)\n",
    "    ax.legend(frameon=True, edgecolor='black', fancybox=False)\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n=== OpenAlex gender analysis complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1e8bb",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0828d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=== OPENALEX TEMPORAL ANALYSIS ===\"\"\"\n",
    "\n",
    "print(\"\\n=== OPENALEX TEMPORAL ANALYSIS ===\")\n",
    "\n",
    "# 1. Papers by year\n",
    "yearly_papers = openalex_df.groupby('publication_year').size()\n",
    "\n",
    "print(f\"\\nPapers by Year:\")\n",
    "for year, count in yearly_papers.items():\n",
    "    print(f\"  {year}: {count:,} papers\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3))\n",
    "ax.plot(yearly_papers.index, yearly_papers.values,\n",
    "        marker='o', linewidth=1.5, markersize=4, color='#2a9d8f')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of Papers')\n",
    "ax.set_title('OpenAlex: Publications Over Time', pad=10)\n",
    "ax.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 2. Citations over time\n",
    "yearly_citations = openalex_df.groupby('publication_year').agg({\n",
    "    'cited_by_count': ['sum', 'mean', 'median']\n",
    "}).round(1)\n",
    "yearly_citations.columns = ['total_citations', 'avg_citations', 'median_citations']\n",
    "\n",
    "print(f\"\\nCitations by Year:\")\n",
    "print(yearly_citations.to_string())\n",
    "\n",
    "# Total citations\n",
    "fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3))\n",
    "ax.plot(yearly_citations.index, yearly_citations['total_citations'],\n",
    "        marker='o', linewidth=1.5, markersize=4, color='#e76f51')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Total Citations')\n",
    "ax.set_title('OpenAlex: Total Citations Over Time', pad=10)\n",
    "ax.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Average citations\n",
    "fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3))\n",
    "ax.plot(yearly_citations.index, yearly_citations['avg_citations'],\n",
    "        marker='o', linewidth=1.5, markersize=4, color='#e9c46a', label='Mean')\n",
    "ax.plot(yearly_citations.index, yearly_citations['median_citations'],\n",
    "        marker='s', linewidth=1.5, markersize=4, color='#264653', label='Median')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Citations per Paper')\n",
    "ax.set_title('OpenAlex: Average Citation Impact Over Time', pad=10)\n",
    "ax.legend(frameon=True, edgecolor='black', fancybox=False)\n",
    "ax.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 3. Collaboration trends (authors per paper)\n",
    "yearly_collaboration = openalex_df.groupby('publication_year').agg({\n",
    "    'num_authors': ['mean', 'median']\n",
    "}).round(1)\n",
    "yearly_collaboration.columns = ['avg_authors', 'median_authors']\n",
    "\n",
    "print(f\"\\nCollaboration Trends:\")\n",
    "print(yearly_collaboration.to_string())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3))\n",
    "ax.plot(yearly_collaboration.index, yearly_collaboration['avg_authors'],\n",
    "        marker='o', linewidth=1.5, markersize=4, color='#2a9d8f', label='Mean')\n",
    "ax.plot(yearly_collaboration.index, yearly_collaboration['median_authors'],\n",
    "        marker='s', linewidth=1.5, markersize=4, color='#e76f51', label='Median')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Authors per Paper')\n",
    "ax.set_title('OpenAlex: Team Size Over Time', pad=10)\n",
    "ax.legend(frameon=True, edgecolor='black', fancybox=False)\n",
    "ax.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 4. Monthly heatmap (if month data available)\n",
    "if 'month' in openalex_df.columns:\n",
    "    heatmap_data = openalex_df.groupby(['publication_year', 'month']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Ensure all months 1-12 are present\n",
    "    for month in range(1, 13):\n",
    "        if month not in heatmap_data.columns:\n",
    "            heatmap_data[month] = 0\n",
    "    heatmap_data = heatmap_data[sorted(heatmap_data.columns)]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, len(heatmap_data) * 0.3 + 1))\n",
    "    \n",
    "    import seaborn as sns\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='d',\n",
    "                cmap='YlOrRd',\n",
    "                ax=ax,\n",
    "                cbar_kws={'label': 'Number of Papers'},\n",
    "                linewidths=0.5,\n",
    "                linecolor='white')\n",
    "    \n",
    "    ax.set_title('OpenAlex: Publications by Year and Month', pad=10)\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Year')\n",
    "    \n",
    "    # Set month labels\n",
    "    month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    ax.set_xticklabels(month_labels, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Monthly aggregated pattern\n",
    "    monthly_papers = openalex_df.groupby('month').size()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 2.5))\n",
    "    bars = ax.bar(monthly_papers.index, monthly_papers.values,\n",
    "                  color='#e76f51', edgecolor='white', linewidth=0.5)\n",
    "    ax.set_title('OpenAlex: Publications by Month (All Years)', pad=10)\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Number of Papers')\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "                      rotation=45, ha='right')\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# 5. Paper type distribution over time (if type column exists)\n",
    "if 'type' in openalex_df.columns and openalex_df['type'].notna().sum() > 0:\n",
    "    type_temporal = openalex_df.groupby(['publication_year', 'type']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Get top 5 types\n",
    "    top_types = openalex_df['type'].value_counts().head(5).index\n",
    "    type_temporal_filtered = type_temporal[top_types]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3.5))\n",
    "    \n",
    "    for pub_type in top_types:\n",
    "        ax.plot(type_temporal_filtered.index, type_temporal_filtered[pub_type],\n",
    "               marker='o', linewidth=1.5, markersize=4, label=pub_type)\n",
    "    \n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Number of Papers')\n",
    "    ax.set_title('OpenAlex: Publication Types Over Time', pad=10)\n",
    "    ax.legend(frameon=True, edgecolor='black', fancybox=False)\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# 6. Growth rate analysis\n",
    "yearly_papers_sorted = yearly_papers.sort_index()\n",
    "growth_rate = yearly_papers_sorted.pct_change() * 100\n",
    "\n",
    "print(f\"\\nYear-over-Year Growth Rate:\")\n",
    "for year, rate in growth_rate.items():\n",
    "    if not pd.isna(rate):\n",
    "        print(f\"  {year}: {rate:+.1f}%\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3))\n",
    "ax.bar(growth_rate.index, growth_rate.values,\n",
    "       color=['#2a9d8f' if x > 0 else '#e76f51' for x in growth_rate.values],\n",
    "       edgecolor='white', linewidth=0.5)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Growth Rate (%)')\n",
    "ax.set_title('OpenAlex: Year-over-Year Publication Growth', pad=10)\n",
    "ax.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 7. Cumulative papers over time\n",
    "cumulative_papers = yearly_papers_sorted.cumsum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(DOUBLE_COL, 3))\n",
    "ax.fill_between(cumulative_papers.index, cumulative_papers.values,\n",
    "                alpha=0.3, color='#2a9d8f')\n",
    "ax.plot(cumulative_papers.index, cumulative_papers.values,\n",
    "        linewidth=2, color='#2a9d8f')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Cumulative Number of Papers')\n",
    "ax.set_title('OpenAlex: Cumulative Publications', pad=10)\n",
    "ax.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 8. Combined metrics dashboard\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(DOUBLE_COL, 7))\n",
    "\n",
    "# Papers over time\n",
    "ax1.plot(yearly_papers.index, yearly_papers.values,\n",
    "         marker='o', linewidth=1.5, markersize=3, color='#2a9d8f')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Papers')\n",
    "ax1.set_title('Publications', pad=5)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Citations over time\n",
    "ax2.plot(yearly_citations.index, yearly_citations['avg_citations'],\n",
    "         marker='o', linewidth=1.5, markersize=3, color='#e76f51')\n",
    "ax2.set_xlabel('Year')\n",
    "ax2.set_ylabel('Avg Citations')\n",
    "ax2.set_title('Citation Impact', pad=5)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Collaboration trends\n",
    "ax3.plot(yearly_collaboration.index, yearly_collaboration['avg_authors'],\n",
    "         marker='o', linewidth=1.5, markersize=3, color='#e9c46a')\n",
    "ax3.set_xlabel('Year')\n",
    "ax3.set_ylabel('Avg Authors')\n",
    "ax3.set_title('Team Size', pad=5)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Growth rate\n",
    "ax4.bar(growth_rate.index, growth_rate.values,\n",
    "        color=['#2a9d8f' if x > 0 else '#e76f51' for x in growth_rate.values],\n",
    "        edgecolor='white', linewidth=0.5)\n",
    "ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax4.set_xlabel('Year')\n",
    "ax4.set_ylabel('Growth (%)')\n",
    "ax4.set_title('Annual Growth Rate', pad=5)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "fig.suptitle('OpenAlex: Temporal Trends Dashboard', fontsize=10, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n=== OpenAlex temporal analysis complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbee54e",
   "metadata": {},
   "source": [
    "---\n",
    "# Edges\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd6acf",
   "metadata": {},
   "source": [
    "## What coverage of DOIs does OpenAlex data provide for LessWrong posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "105dbf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: Preparing forum posts data\n",
      "================================================================================\n",
      "LessWrong AI Safety posts (Topic 0): 21579\n",
      "Alignment Forum posts (all): 4230\n",
      "\n",
      "Total combined AI Safety posts: 25809\n",
      "\n",
      "Loading OpenAlex data...\n",
      "✓ Loaded 41,231 OpenAlex papers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: Preparing forum posts data\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract AI Safety posts from LessWrong (topic 0)\n",
    "lesswrong_ai_safety = lesswrong_df[lesswrong_df['topic_cluster_id'] == 0].copy()\n",
    "print(f\"LessWrong AI Safety posts (Topic 0): {len(lesswrong_ai_safety)}\")\n",
    "\n",
    "# Get all Alignment Forum posts\n",
    "alignment_forum_all = alignment_forum_df.copy()\n",
    "print(f\"Alignment Forum posts (all): {len(alignment_forum_all)}\")\n",
    "\n",
    "# Combine them\n",
    "posts_df = pd.concat([\n",
    "    lesswrong_ai_safety.assign(source='LessWrong_Topic0'),\n",
    "    alignment_forum_all.assign(source='AlignmentForum')\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal combined AI Safety posts: {len(posts_df)}\")\n",
    "print()\n",
    "\n",
    "# Load OpenAlex papers\n",
    "print(\"Loading OpenAlex data...\")\n",
    "openalex_df = pd.read_csv('../data/nodes_openalex_works.csv')\n",
    "print(f\"✓ Loaded {len(openalex_df):,} OpenAlex papers\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a40a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 2: Extracting DOIs from post content\n",
      "================================================================================\n",
      "Columns to search for DOIs: ['htmlBody', 'cleaned_htmlBody', 'extracted_links']\n",
      "\n",
      "Searching column: htmlBody\n",
      "  ✓ Found 3,751 DOIs in 1,505 posts\n",
      "Searching column: cleaned_htmlBody\n",
      "  ✓ Found 1,023 DOIs in 240 posts\n",
      "Searching column: extracted_links\n",
      "  ✓ Found 3,302 DOIs in 1,380 posts\n",
      "\n",
      "================================================================================\n",
      "EXTRACTION SUMMARY:\n",
      "================================================================================\n",
      "  Posts with at least 1 DOI: 1,505 (5.8%)\n",
      "  Total DOIs extracted: 4,450\n",
      "  Average DOIs per post (for posts with DOIs): 2.96\n",
      "  Max DOIs in a single post: 138\n",
      "\n",
      "Distribution of DOI counts per post:\n",
      "  0 DOIs: 24,304 posts\n",
      "  1 DOIs: 868 posts\n",
      "  2 DOIs: 258 posts\n",
      "  3 DOIs: 113 posts\n",
      "  4 DOIs: 54 posts\n",
      "  5 DOIs: 46 posts\n",
      "  6 DOIs: 30 posts\n",
      "  7 DOIs: 27 posts\n",
      "  8 DOIs: 27 posts\n",
      "  9 DOIs: 5 posts\n",
      "\n",
      "Sample posts with DOIs:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Common sense as a prior\n",
      "   Source: LessWrong_Topic0\n",
      "   DOIs found: 1\n",
      "     - 10.1007/s11238-006-9004-4\n",
      "\n",
      "2. [link] Psychologists strike a blow for reproducibility\n",
      "   Source: LessWrong_Topic0\n",
      "   DOIs found: 1\n",
      "     - 10.1038/nature.2012.11535\n",
      "\n",
      "3. The Relevance of Advanced Vocabulary to Rationality\n",
      "   Source: LessWrong_Topic0\n",
      "   DOIs found: 1\n",
      "     - 10.1111/j.1600-0404.1995.tb07018.x/abstract\n",
      "\n",
      "4. Democracy and rationality\n",
      "   Source: LessWrong_Topic0\n",
      "   DOIs found: 1\n",
      "     - 10.1007/978-3-642-25510-6_13#page-1\n",
      "\n",
      "5. A Difficulty in the Concept of CEV\n",
      "   Source: LessWrong_Topic0\n",
      "   DOIs found: 2\n",
      "     - 10.2307/1828886\n",
      "     - 10.2307/1912309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: EXTRACT DOIs FROM POST CONTENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: Extracting DOIs from post content\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_dois_from_text(text):\n",
    "    \"\"\"\n",
    "    Extract all DOIs from any text content.\n",
    "    \n",
    "    Handles multiple formats:\n",
    "    - Plain DOI: 10.1234/example\n",
    "    - URL format: https://doi.org/10.1234/example\n",
    "    - dx.doi.org format: http://dx.doi.org/10.1234/example\n",
    "    - DOI with prefix: doi:10.1234/example\n",
    "    \n",
    "    Returns a list of normalized DOIs (lowercase, no URL prefix).\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # DOI regex pattern\n",
    "    # DOIs always start with \"10.\" followed by 4+ digits, then \"/\", then any characters\n",
    "    # We exclude whitespace and common delimiters that would end a DOI\n",
    "    doi_pattern = r'10\\.\\d{4,}/[^\\s,;|\\]}\\)\"\\'\\><\\n]+'\n",
    "    \n",
    "    # Find all DOIs in the text\n",
    "    dois = re.findall(doi_pattern, text)\n",
    "    \n",
    "    # Clean and normalize DOIs\n",
    "    cleaned_dois = []\n",
    "    for doi in dois:\n",
    "        # Remove common trailing punctuation that might be captured\n",
    "        doi = doi.rstrip('.,;:!?')\n",
    "        # Remove HTML tags if present (e.g., </a>)\n",
    "        doi = re.sub(r'<[^>]+>$', '', doi)\n",
    "        # Normalize to lowercase (DOIs are case-insensitive)\n",
    "        doi = doi.lower()\n",
    "        # Basic validation: DOI should still look like 10.xxxx/yyyy\n",
    "        if re.match(r'^10\\.\\d{4,}/.+', doi):\n",
    "            cleaned_dois.append(doi)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_dois = []\n",
    "    for doi in cleaned_dois:\n",
    "        if doi not in seen:\n",
    "            seen.add(doi)\n",
    "            unique_dois.append(doi)\n",
    "    \n",
    "    return unique_dois\n",
    "\n",
    "# Define columns to search for DOIs\n",
    "# Based on the sample data, we have: htmlBody, cleaned_htmlBody, extracted_links\n",
    "columns_to_search = []\n",
    "\n",
    "# Check which columns exist\n",
    "if 'htmlBody' in posts_df.columns:\n",
    "    columns_to_search.append('htmlBody')\n",
    "if 'cleaned_htmlBody' in posts_df.columns:\n",
    "    columns_to_search.append('cleaned_htmlBody')\n",
    "if 'extracted_links' in posts_df.columns:\n",
    "    columns_to_search.append('extracted_links')\n",
    "\n",
    "print(f\"Columns to search for DOIs: {columns_to_search}\")\n",
    "print()\n",
    "\n",
    "# Extract DOIs from each column\n",
    "for col in columns_to_search:\n",
    "    print(f\"Searching column: {col}\")\n",
    "    posts_df[f'dois_from_{col}'] = posts_df[col].apply(extract_dois_from_text)\n",
    "    dois_found = posts_df[f'dois_from_{col}'].apply(len).sum()\n",
    "    posts_with_dois = (posts_df[f'dois_from_{col}'].apply(len) > 0).sum()\n",
    "    print(f\"  ✓ Found {dois_found:,} DOIs in {posts_with_dois:,} posts\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Combine all DOIs into a single list per post\n",
    "def combine_doi_lists(*lists):\n",
    "    \"\"\"Combine multiple lists of DOIs, removing duplicates\"\"\"\n",
    "    combined = []\n",
    "    seen = set()\n",
    "    for doi_list in lists:\n",
    "        if isinstance(doi_list, list):\n",
    "            for doi in doi_list:\n",
    "                if doi not in seen:\n",
    "                    seen.add(doi)\n",
    "                    combined.append(doi)\n",
    "    return combined\n",
    "\n",
    "# Get all the DOI columns we created\n",
    "doi_columns = [f'dois_from_{col}' for col in columns_to_search]\n",
    "\n",
    "# Combine all extracted DOIs\n",
    "posts_df['all_extracted_dois'] = posts_df[doi_columns].apply(\n",
    "    lambda row: combine_doi_lists(*row), axis=1\n",
    ")\n",
    "posts_df['doi_count'] = posts_df['all_extracted_dois'].apply(len)\n",
    "\n",
    "# Summary statistics\n",
    "total_dois = posts_df['doi_count'].sum()\n",
    "posts_with_dois = (posts_df['doi_count'] > 0).sum()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"EXTRACTION SUMMARY:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Posts with at least 1 DOI: {posts_with_dois:,} ({posts_with_dois/len(posts_df)*100:.1f}%)\")\n",
    "print(f\"  Total DOIs extracted: {total_dois:,}\")\n",
    "\n",
    "if posts_with_dois > 0:\n",
    "    print(f\"  Average DOIs per post (for posts with DOIs): {posts_df[posts_df['doi_count'] > 0]['doi_count'].mean():.2f}\")\n",
    "    print(f\"  Max DOIs in a single post: {posts_df['doi_count'].max()}\")\n",
    "    print()\n",
    "    \n",
    "    # Show distribution\n",
    "    print(\"Distribution of DOI counts per post:\")\n",
    "    dist = posts_df['doi_count'].value_counts().sort_index().head(10)\n",
    "    for count, freq in dist.items():\n",
    "        print(f\"  {count} DOIs: {freq:,} posts\")\n",
    "    print()\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"Sample posts with DOIs:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, (idx, row) in enumerate(posts_df[posts_df['doi_count'] > 0].head(5).iterrows(), 1):\n",
    "        title = row.get('title', 'N/A')\n",
    "        title = title[:70] + \"...\" if len(title) > 70 else title\n",
    "        print(f\"\\n{i}. {title}\")\n",
    "        print(f\"   Source: {row['source']}\")\n",
    "        print(f\"   DOIs found: {len(row['all_extracted_dois'])}\")\n",
    "        for doi in row['all_extracted_dois'][:3]:  # Show first 3 DOIs\n",
    "            print(f\"     - {doi}\")\n",
    "        if len(row['all_extracted_dois']) > 3:\n",
    "            print(f\"     ... and {len(row['all_extracted_dois']) - 3} more\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"⚠️  No DOIs found in any posts!\")\n",
    "    print(\"This might mean:\")\n",
    "    print(\"  - Posts don't contain DOI links\")\n",
    "    print(\"  - DOIs are in a different format than expected\")\n",
    "    print(\"  - Need to check other columns\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45a4f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 3: Preparing OpenAlex DOI lookup\n",
      "================================================================================\n",
      "OpenAlex papers with DOIs: 35,822 (86.9%)\n",
      "Unique DOIs in OpenAlex: 35,822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: PREPARE OpenAlex DOI LOOKUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: Preparing OpenAlex DOI lookup\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def clean_openalex_doi(doi_str):\n",
    "    \"\"\"Clean OpenAlex DOI for matching\"\"\"\n",
    "    if pd.isna(doi_str):\n",
    "        return None\n",
    "    doi_str = str(doi_str).lower().strip()\n",
    "    # Remove URL prefixes if present\n",
    "    doi_str = doi_str.replace('https://doi.org/', '')\n",
    "    doi_str = doi_str.replace('http://dx.doi.org/', '')\n",
    "    doi_str = doi_str.replace('doi:', '')\n",
    "    return doi_str if doi_str else None\n",
    "\n",
    "openalex_df['doi_cleaned'] = openalex_df['openalex_doi'].apply(clean_openalex_doi)\n",
    "\n",
    "# Remove rows without DOIs\n",
    "openalex_with_doi = openalex_df[openalex_df['doi_cleaned'].notna()].copy()\n",
    "print(f\"OpenAlex papers with DOIs: {len(openalex_with_doi):,} ({len(openalex_with_doi)/len(openalex_df)*100:.1f}%)\")\n",
    "\n",
    "# Create a set for fast O(1) lookup\n",
    "openalex_doi_set = set(openalex_with_doi['doi_cleaned'].values)\n",
    "print(f\"Unique DOIs in OpenAlex: {len(openalex_doi_set):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc1246dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After extracting DOIs in Step 2, clean them immediately:\n",
    "def clean_doi(doi):\n",
    "    \"\"\"\n",
    "    Comprehensive DOI cleaning for matching.\n",
    "    Handles all the weird edge cases we've found.\n",
    "    \"\"\"\n",
    "    if not doi or pd.isna(doi):\n",
    "        return None\n",
    "    \n",
    "    doi = str(doi).strip()\n",
    "    \n",
    "    # Remove URL prefixes (in case they're there)\n",
    "    doi = doi.replace('https://doi.org/', '')\n",
    "    doi = doi.replace('http://doi.org/', '')\n",
    "    doi = doi.replace('https://dx.doi.org/', '')\n",
    "    doi = doi.replace('http://dx.doi.org/', '')\n",
    "    doi = doi.replace('doi:', '')\n",
    "    \n",
    "    # Remove URL fragments (e.g., #page-1, #.u14eh_ldvjm)\n",
    "    if '#' in doi:\n",
    "        doi = doi.split('#')[0]\n",
    "    \n",
    "    # Remove query parameters (e.g., ?uid=...)\n",
    "    if '?' in doi:\n",
    "        doi = doi.split('?')[0]\n",
    "    \n",
    "    # Remove &amp; and other HTML entities\n",
    "    doi = doi.replace('&amp;', '').replace('&amp', '')\n",
    "    \n",
    "    # Remove common path suffixes\n",
    "    doi = re.sub(r'/abstract$', '', doi)\n",
    "    doi = re.sub(r'/full$', '', doi)\n",
    "    doi = re.sub(r'/pdf$', '', doi)\n",
    "    doi = re.sub(r'/epdf$', '', doi)\n",
    "    doi = re.sub(r'/issuetoc$', '', doi)\n",
    "    \n",
    "    # Remove version indicators (v1.full, v2.full, etc.)\n",
    "    doi = re.sub(r'v\\d+\\.full$', '', doi)\n",
    "    \n",
    "    # Remove weird caret suffixes (.^node, .^b, .^f, etc.)\n",
    "    doi = re.sub(r'\\.\\^[a-z]+$', '', doi, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove bracket artifacts and anything after them\n",
    "    doi = re.sub(r'\\[[^\\]]*$', '', doi)\n",
    "    \n",
    "    # Remove trailing parentheses that look incomplete\n",
    "    if doi.endswith('('):\n",
    "        doi = doi[:-1]\n",
    "    \n",
    "    # Remove trailing punctuation\n",
    "    doi = doi.rstrip('/.,;:!?')\n",
    "    \n",
    "    # Lowercase for consistency\n",
    "    doi = doi.lower()\n",
    "    \n",
    "    return doi.strip() if doi else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9133bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df['all_extracted_dois'] = posts_df['all_extracted_dois'].apply(\n",
    "    lambda doi_list: [clean_doi(doi) for doi in doi_list if clean_doi(doi)]\n",
    ")\n",
    "\n",
    "# Remove any None/empty values\n",
    "posts_df['all_extracted_dois'] = posts_df['all_extracted_dois'].apply(\n",
    "    lambda doi_list: [doi for doi in doi_list if doi]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a3dfc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "openalex_df['doi_cleaned'] = openalex_df['openalex_doi'].apply(clean_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f87bf9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 4: Matching forum DOIs with OpenAlex DOIs\n",
      "================================================================================\n",
      "✓ Matching complete!\n",
      "  Total DOIs extracted: 4,450\n",
      "  Total DOIs matched in OpenAlex: 191\n",
      "  Match rate: 4.3%\n",
      "\n",
      "  Posts with at least 1 match: 134\n",
      "  Posts with DOIs but no match: 1,371\n",
      "\n",
      "Sample unmatched DOIs (for debugging):\n",
      "  - 10.1007/s11238-006-9004-4\n",
      "  - 10.1038/nature.2012.11535\n",
      "  - 10.1111/j.1600-0404.1995.tb07018.x\n",
      "  - 10.1007/978-3-642-25510-6_13\n",
      "  - 10.2307/1828886\n",
      "  - 10.2307/1912309\n",
      "  - 10.1080/00048408112340011\n",
      "  - 10.2307/20115662\n",
      "  - 10.2307/40267481\n",
      "  - 10.1007/s11229-011-0022-6\n",
      "  - 10.2307/20115662\n",
      "  - 10.2307/40267481\n",
      "  - 10.1146/annurev-psych-120710-100350\n",
      "  - 10.1142/s1793843011000686\n",
      "  - 10.3758/s13423-013-0384-5\n",
      "  - 10.2307/2677856\n",
      "  - 10.1111/j.1467-8543.2009.00723.x\n",
      "  - 10.2307/2677856\n",
      "  - 10.3389/fnbeh.2013.00206\n",
      "  - 10.1016/j.aop.2014.04.021\n",
      "  - 10.1103/physrev.106.620\n",
      "  - 10.1103/physrev.108.171\n",
      "  - 10.3389/fnana.2012.00032\n",
      "  - 10.3389/fnana.2012.00032\n",
      "  - 10.3389/fnana.2012.00032\n",
      "  - 10.1111/j.1755-2567.1996.tb00529.x\n",
      "  - 10.1080/09668136.2013.824140\n",
      "  - 10.2307/4132491\n",
      "  - 10.2307/4132491\n",
      "  - 10.1080/07399019108964994\n",
      "  - 10.1111/ijcp.12026\n",
      "  - 10.1002/pst.v13.1\n",
      "  - 10.1111/jssr.12046\n",
      "  - 10.1007/s002650100346\n",
      "  - 10.1007/s00265-001-0420-8\n",
      "  - 10.1098/rspb.2010.1045\n",
      "  - 10.1006/anbe.2001.1925\n",
      "  - 10.1007/978-3-540-27836-8_40\n",
      "  - 10.1073/pnas.1620732114\n",
      "  - 10.1186/s12961-017-0192-x\n",
      "  - 10.1177/2515245917747646\n",
      "  - 10.1177/23780231211024421\n",
      "  - 10.1177/0146167207301028\n",
      "  - 10.1038/s42256-019-0048-x\n",
      "  - 10.1038/s42256-019-0048-x\n",
      "  - 10.1007/s12685-013-0088-9\n",
      "  - 10.1007/978-94-007-3932-1\n",
      "  - 10.1007/978-94-017-2527-9_3\n",
      "  - 10.48550/arxiv.2203.02155\n",
      "  - 10.1007/978-94-007-3932-1\n",
      "  - 10.1371/journal.pbio.1002106\n",
      "  - 10.1073/pnas.1903070116\n",
      "  - 10.1103/physrevresearch.4.013201\n",
      "  - 10.1145/3135932.3135941\n",
      "  - 10.1214/21-ss133.full\n",
      "  - 10.1137/19m1308943\n",
      "  - 10.3389/fpsyg.2020.01723\n",
      "  - 10.3389/neuro.09.031.2009\n",
      "  - 10.1101/2022.09.29.509744\n",
      "  - 10.1007/978-3-662-47992-6\n",
      "  - 10.1145/3345252.3345292\n",
      "  - 10.1049/ise2.12052\n",
      "  - 10.1088/2634-4386/ac4a83\n",
      "  - 10.1007/s00429-017-1382-6\n",
      "  - 10.3389/fncel.2016.00239\n",
      "  - 10.1002/cne.21974\n",
      "  - 10.1212/01.wnl.0000166914.38327.bb\n",
      "  - 10.1088/0957-4484/24/38/384004\n",
      "  - 10.1038/s41563-021-01099-9\n",
      "  - 10.1038/s41598-019-51330-6\n",
      "  - 10.1002/aisy.202000096\n",
      "  - 10.1002/aelm.201800909\n",
      "  - 10.7554/elife.49673\n",
      "  - 10.1073/pnas.2107022118\n",
      "  - 10.1038/s41586-021-04223-6\n",
      "  - 10.3389/fnins.2018.00774\n",
      "  - 10.3389/fnins.2020.00119\n",
      "  - 10.1038/s41598-021-91786-z\n",
      "  - 10.1038/s41586-019-1677-2\n",
      "  - 10.1038/s42256-020-0159-4\n",
      "  - 10.23919/vlsit.2019.8776502\n",
      "  - 10.1109/tcsii.2021.3068764\n",
      "  - 10.1038/s41565-020-0655-z\n",
      "  - 10.1038/s41586-018-0180-5\n",
      "  - 10.3389/fncom.2021.674154\n",
      "  - 10.1145/3296957.3173177\n",
      "  - 10.1088/2632-072x/ac3ad3\n",
      "  - 10.48550/arxiv.2206.07682\n",
      "  - 10.48550/arxiv.2010.14701\n",
      "  - 10.48550/arxiv.2001.08361\n",
      "  - 10.1016/j.pneurobio.2017.07.002\n",
      "  - 10.1016/s0169-5347(98\n",
      "  - 10.1162/netn_a_00128\n",
      "  - 10.48550/arxiv.2204.06125\n",
      "  - 10.48550/arxiv.1810.04805\n",
      "  - 10.1038/s41586-021-03819-2\n",
      "  - 10.1162/daed_a_01899\n",
      "  - 10.1126/sciadv.1501326\n",
      "  - 10.32470/ccn.2022.1081-0\n",
      "  - 10.1016/j.neunet.2019.03.005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: MATCH DOIs\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: Matching forum DOIs with OpenAlex DOIs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def match_dois(doi_list, openalex_set):\n",
    "    \"\"\"Match a list of DOIs against the OpenAlex set\"\"\"\n",
    "    if not doi_list:\n",
    "        return []\n",
    "    return [doi for doi in doi_list if doi in openalex_set]\n",
    "\n",
    "# Find matches for each post\n",
    "posts_df['matched_dois'] = posts_df['all_extracted_dois'].apply(\n",
    "    lambda dois: match_dois(dois, openalex_doi_set)\n",
    ")\n",
    "posts_df['matched_doi_count'] = posts_df['matched_dois'].apply(len)\n",
    "\n",
    "# Calculate statistics\n",
    "total_dois_extracted = posts_df['doi_count'].sum()\n",
    "total_dois_matched = posts_df['matched_doi_count'].sum()\n",
    "posts_with_matches = (posts_df['matched_doi_count'] > 0).sum()\n",
    "\n",
    "print(f\"✓ Matching complete!\")\n",
    "print(f\"  Total DOIs extracted: {total_dois_extracted:,}\")\n",
    "print(f\"  Total DOIs matched in OpenAlex: {total_dois_matched:,}\")\n",
    "\n",
    "if total_dois_extracted > 0:\n",
    "    match_rate = total_dois_matched / total_dois_extracted * 100\n",
    "    print(f\"  Match rate: {match_rate:.1f}%\")\n",
    "    print()\n",
    "    print(f\"  Posts with at least 1 match: {posts_with_matches:,}\")\n",
    "    print(f\"  Posts with DOIs but no match: {posts_with_dois - posts_with_matches:,}\")\n",
    "    \n",
    "    # Show some unmatched DOIs for debugging\n",
    "    if total_dois_matched < total_dois_extracted:\n",
    "        print()\n",
    "        print(\"Sample unmatched DOIs (for debugging):\")\n",
    "        unmatched = []\n",
    "        for dois, matched in zip(posts_df['all_extracted_dois'], posts_df['matched_dois']):\n",
    "            for doi in dois:\n",
    "                if doi not in matched:\n",
    "                    unmatched.append(doi)\n",
    "                    if len(unmatched) >= 100:\n",
    "                        break\n",
    "            if len(unmatched) >= 100:\n",
    "                break\n",
    "        for doi in unmatched[:200]:\n",
    "            print(f\"  - {doi}\")\n",
    "else:\n",
    "    print(\"  ⚠️  No DOIs found to match\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eeadbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 5: Collecting unmatched DOIs\n",
      "================================================================================\n",
      "Total unmatched DOI occurrences: 4,263\n",
      "Unique unmatched DOIs: 3,023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: Collecting unmatched DOIs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all DOIs that were extracted but not matched\n",
    "unmatched_dois = []\n",
    "\n",
    "for dois_extracted, dois_matched in zip(posts_df['all_extracted_dois'], posts_df['matched_dois']):\n",
    "    for doi in dois_extracted:\n",
    "        if doi not in dois_matched:\n",
    "            unmatched_dois.append(doi)\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "unique_unmatched = list(dict.fromkeys(unmatched_dois))\n",
    "\n",
    "print(f\"Total unmatched DOI occurrences: {len(unmatched_dois):,}\")\n",
    "print(f\"Unique unmatched DOIs: {len(unique_unmatched):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6b3e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 6: Cleaning DOIs for API lookup\n",
      "================================================================================\n",
      "Cleaned DOIs: 2,938\n",
      "\n",
      "Sample DOI cleaning:\n",
      "--------------------------------------------------------------------------------\n",
      "(First 10 DOIs were already clean)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: CLEAN DOIs FOR API LOOKUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 6: Cleaning DOIs for API lookup\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def clean_doi_for_api(doi):\n",
    "    \"\"\"\n",
    "    Clean DOI for OpenAlex API lookup.\n",
    "    Remove URL fragments, query parameters, and other artifacts.\n",
    "    \"\"\"\n",
    "    if not doi or pd.isna(doi):\n",
    "        return None\n",
    "    \n",
    "    doi = str(doi).strip()\n",
    "    \n",
    "    # Remove URL fragments (e.g., #page-1)\n",
    "    if '#' in doi:\n",
    "        doi = doi.split('#')[0]\n",
    "    \n",
    "    # Remove query parameters (e.g., ?uid=...)\n",
    "    if '?' in doi:\n",
    "        doi = doi.split('?')[0]\n",
    "    \n",
    "    # Remove &amp; and other HTML entities\n",
    "    doi = doi.replace('&amp;', '').replace('&amp', '')\n",
    "    \n",
    "    # Remove common suffixes\n",
    "    doi = re.sub(r'/abstract$', '', doi)\n",
    "    doi = re.sub(r'/full$', '', doi)\n",
    "    doi = re.sub(r'/pdf$', '', doi)\n",
    "    doi = re.sub(r'/epdf$', '', doi)\n",
    "    doi = re.sub(r'/issuetoc$', '', doi)  # NEW: journal table of contents\n",
    "    \n",
    "    # Remove version indicators (v1, v2, etc.) before /full\n",
    "    doi = re.sub(r'v\\d+\\.full$', '', doi)  # NEW: v1.full, v2.full\n",
    "    \n",
    "    # Remove weird caret suffixes (^node, ^b, ^f, etc.)\n",
    "    doi = re.sub(r'\\.\\^[a-z]+$', '', doi)  # NEW: .^node, .^b, etc.\n",
    "    \n",
    "    # Remove bracket artifacts and anything after them\n",
    "    doi = re.sub(r'\\[[^\\]]*$', '', doi)  # NEW: [6, [something\n",
    "    \n",
    "    # Remove trailing parentheses that look incomplete\n",
    "    # BUT be careful: DOIs can legitimately end with (##) for year\n",
    "    # Only remove if it looks broken (e.g., ends with just '(')\n",
    "    if doi.endswith('('):\n",
    "        doi = doi[:-1]\n",
    "    \n",
    "    # Remove trailing slashes, dots, commas\n",
    "    doi = doi.rstrip('/.,;:')\n",
    "    \n",
    "    return doi.strip()\n",
    "\n",
    "# Clean all DOIs\n",
    "cleaned_unmatched = [clean_doi_for_api(doi) for doi in unique_unmatched]\n",
    "unique_cleaned = list(dict.fromkeys(cleaned_unmatched))\n",
    "\n",
    "print(f\"Cleaned DOIs: {len(unique_cleaned):,}\")\n",
    "print()\n",
    "\n",
    "# Show some examples of cleaning\n",
    "print(\"Sample DOI cleaning:\")\n",
    "print(\"-\" * 80)\n",
    "examples_shown = 0\n",
    "for original, cleaned in zip(unique_unmatched[:10], cleaned_unmatched[:10]):\n",
    "    if original != cleaned:\n",
    "        examples_shown += 1\n",
    "        print(f\"{examples_shown}. Original: {original}\")\n",
    "        print(f\"   Cleaned:  {cleaned}\")\n",
    "if examples_shown == 0:\n",
    "    print(\"(First 10 DOIs were already clean)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcb37809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 7: Looking up DOIs in OpenAlex API (with topics & concepts)\n",
      "================================================================================\n",
      "Looking up DOIs in OpenAlex API...\n",
      "(This may take several minutes for thousands of DOIs)\n",
      "\n",
      "  Progress: 50/3022 (1.7%)\n",
      "  Progress: 100/3022 (3.3%)\n",
      "  Progress: 150/3022 (5.0%)\n",
      "  Progress: 200/3022 (6.6%)\n",
      "  Progress: 250/3022 (8.3%)\n",
      "  Progress: 300/3022 (9.9%)\n",
      "  Progress: 350/3022 (11.6%)\n",
      "  Progress: 400/3022 (13.2%)\n",
      "  Progress: 450/3022 (14.9%)\n",
      "  Progress: 500/3022 (16.5%)\n",
      "  Progress: 550/3022 (18.2%)\n",
      "  Progress: 600/3022 (19.9%)\n",
      "  Progress: 650/3022 (21.5%)\n",
      "  Progress: 700/3022 (23.2%)\n",
      "  Progress: 750/3022 (24.8%)\n",
      "  Progress: 800/3022 (26.5%)\n",
      "  Progress: 850/3022 (28.1%)\n",
      "  Progress: 900/3022 (29.8%)\n",
      "  Progress: 950/3022 (31.4%)\n",
      "  Progress: 1000/3022 (33.1%)\n",
      "  Progress: 1050/3022 (34.7%)\n",
      "  Progress: 1100/3022 (36.4%)\n",
      "  Progress: 1150/3022 (38.1%)\n",
      "  Progress: 1200/3022 (39.7%)\n",
      "  Progress: 1250/3022 (41.4%)\n",
      "  Progress: 1300/3022 (43.0%)\n",
      "  Progress: 1350/3022 (44.7%)\n",
      "  Progress: 1400/3022 (46.3%)\n",
      "  Progress: 1450/3022 (48.0%)\n",
      "  ⚠️  Error looking up 10.1007/bf00125671: HTTPSConnectionPool(host='api.openalex.org', port=443): Read timed out. (read timeout=10)\n",
      "  Progress: 1500/3022 (49.6%)\n",
      "  Progress: 1550/3022 (51.3%)\n",
      "  Progress: 1600/3022 (52.9%)\n",
      "  Progress: 1650/3022 (54.6%)\n",
      "  Progress: 1700/3022 (56.3%)\n",
      "  Progress: 1750/3022 (57.9%)\n",
      "  Progress: 1800/3022 (59.6%)\n",
      "  Progress: 1850/3022 (61.2%)\n",
      "  Progress: 1900/3022 (62.9%)\n",
      "  Progress: 1950/3022 (64.5%)\n",
      "  Progress: 2000/3022 (66.2%)\n",
      "  Progress: 2050/3022 (67.8%)\n",
      "  Progress: 2100/3022 (69.5%)\n",
      "  ⚠️  Error looking up 10.5250/fronjwomestud.33.1.0024: HTTPSConnectionPool(host='api.openalex.org', port=443): Read timed out. (read timeout=10)\n",
      "  ⚠️  Error looking up 10.1002/app5.273: HTTPSConnectionPool(host='api.openalex.org', port=443): Read timed out. (read timeout=10)\n",
      "  Progress: 2150/3022 (71.1%)\n",
      "  Progress: 2200/3022 (72.8%)\n",
      "  Progress: 2250/3022 (74.5%)\n",
      "  Progress: 2300/3022 (76.1%)\n",
      "  Progress: 2350/3022 (77.8%)\n",
      "  Progress: 2400/3022 (79.4%)\n",
      "  Progress: 2450/3022 (81.1%)\n",
      "  Progress: 2500/3022 (82.7%)\n",
      "  Progress: 2550/3022 (84.4%)\n",
      "  Progress: 2600/3022 (86.0%)\n",
      "  Progress: 2650/3022 (87.7%)\n",
      "  Progress: 2700/3022 (89.3%)\n",
      "  Progress: 2750/3022 (91.0%)\n",
      "  Progress: 2800/3022 (92.7%)\n",
      "  Progress: 2850/3022 (94.3%)\n",
      "  Progress: 2900/3022 (96.0%)\n",
      "  Progress: 2950/3022 (97.6%)\n",
      "  Progress: 3000/3022 (99.3%)\n",
      "\n",
      "✓ Lookup complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: LOOKUP DOIS IN OPENALEX\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 7: Looking up DOIs in OpenAlex API (with topics & concepts)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def lookup_doi_with_metadata(doi, email=None):\n",
    "    \"\"\"\n",
    "    Look up a DOI in OpenAlex and extract full metadata including:\n",
    "    - Basic info (title, year, type, citations)\n",
    "    - Topics (primary_topic, topics list)\n",
    "    - Concepts (with scores)\n",
    "    - Keywords (from various sources)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (paper_info_dict, topics_list, concepts_list, keywords_list) if found\n",
    "        (None, None, None, None) if not found\n",
    "    \"\"\"\n",
    "    # OpenAlex API endpoint\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    \n",
    "    # Construct the DOI URL for OpenAlex\n",
    "    doi_url = f\"https://doi.org/{doi}\"\n",
    "    \n",
    "    # Make request\n",
    "    params = {'filter': f'doi:{doi_url}'}\n",
    "    if email:\n",
    "        params['mailto'] = email\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if we found results\n",
    "        if data.get('results') and len(data['results']) > 0:\n",
    "            work = data['results'][0]\n",
    "            \n",
    "            # === BASIC INFO ===\n",
    "            openalex_id = work.get('id', '').replace('https://openalex.org/', '')\n",
    "            \n",
    "            # Extract primary location info\n",
    "            primary_location = None\n",
    "            if work.get('primary_location'):\n",
    "                source = work['primary_location'].get('source')\n",
    "                if source:\n",
    "                    primary_location = source.get('display_name')\n",
    "            \n",
    "            # Extract open access info\n",
    "            is_oa = False\n",
    "            if work.get('open_access'):\n",
    "                is_oa = work['open_access'].get('is_oa', False)\n",
    "            \n",
    "            paper_info = {\n",
    "                'doi': doi,\n",
    "                'openalex_id': openalex_id,\n",
    "                'title': work.get('title'),\n",
    "                'publication_year': work.get('publication_year'),\n",
    "                'type': work.get('type'),\n",
    "                'cited_by_count': work.get('cited_by_count', 0),\n",
    "                'primary_location': primary_location,\n",
    "                'open_access': is_oa,\n",
    "            }\n",
    "            \n",
    "            # === TOPICS (NEW in OpenAlex - replaces concepts) ===\n",
    "            topics_list = []\n",
    "            \n",
    "            # Primary topic\n",
    "            if work.get('primary_topic'):\n",
    "                pt = work['primary_topic']\n",
    "                topics_list.append({\n",
    "                    'openalex_id': openalex_id,\n",
    "                    'doi': doi,\n",
    "                    'topic_id': pt.get('id', '').replace('https://openalex.org/', ''),\n",
    "                    'topic_name': pt.get('display_name'),\n",
    "                    'topic_score': 1.0,  # Primary topic gets max score\n",
    "                    'is_primary': True,\n",
    "                    'subfield': pt.get('subfield', {}).get('display_name') if pt.get('subfield') else None,\n",
    "                    'field': pt.get('field', {}).get('display_name') if pt.get('field') else None,\n",
    "                    'domain': pt.get('domain', {}).get('display_name') if pt.get('domain') else None,\n",
    "                })\n",
    "            \n",
    "            # All topics with scores\n",
    "            if work.get('topics'):\n",
    "                for topic in work['topics']:\n",
    "                    topics_list.append({\n",
    "                        'openalex_id': openalex_id,\n",
    "                        'doi': doi,\n",
    "                        'topic_id': topic.get('id', '').replace('https://openalex.org/', ''),\n",
    "                        'topic_name': topic.get('display_name'),\n",
    "                        'topic_score': topic.get('score', 0),\n",
    "                        'is_primary': False,\n",
    "                        'subfield': topic.get('subfield', {}).get('display_name') if topic.get('subfield') else None,\n",
    "                        'field': topic.get('field', {}).get('display_name') if topic.get('field') else None,\n",
    "                        'domain': topic.get('domain', {}).get('display_name') if topic.get('domain') else None,\n",
    "                    })\n",
    "            \n",
    "            # === CONCEPTS (DEPRECATED but still available) ===\n",
    "            concepts_list = []\n",
    "            if work.get('concepts'):\n",
    "                for concept in work['concepts']:\n",
    "                    concepts_list.append({\n",
    "                        'openalex_id': openalex_id,\n",
    "                        'doi': doi,\n",
    "                        'concept_id': concept.get('id', '').replace('https://openalex.org/', ''),\n",
    "                        'concept_name': concept.get('display_name'),\n",
    "                        'concept_score': concept.get('score', 0),\n",
    "                        'concept_level': concept.get('level', 0),\n",
    "                    })\n",
    "            \n",
    "            # === KEYWORDS ===\n",
    "            keywords_list = []\n",
    "            if work.get('keywords'):\n",
    "                for kw in work['keywords']:\n",
    "                    keywords_list.append({\n",
    "                        'openalex_id': openalex_id,\n",
    "                        'doi': doi,\n",
    "                        'keyword': kw.get('display_name'),\n",
    "                        'keyword_score': kw.get('score', 0),\n",
    "                    })\n",
    "            \n",
    "            return paper_info, topics_list, concepts_list, keywords_list\n",
    "        else:\n",
    "            return None, None, None, None\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  ⚠️  Error looking up {doi}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Lookup all unmatched DOIs\n",
    "print(\"Looking up DOIs in OpenAlex API...\")\n",
    "print(\"(This may take several minutes for thousands of DOIs)\")\n",
    "print()\n",
    "\n",
    "# You can add your email here for faster rate limits (polite pool)\n",
    "YOUR_EMAIL = None  # Set to your email like: \"your.email@example.com\"\n",
    "\n",
    "found_papers = []\n",
    "all_topics = []\n",
    "all_concepts = []\n",
    "all_keywords = []\n",
    "not_found_dois = []\n",
    "batch_size = 50\n",
    "\n",
    "for i, doi in enumerate(unique_cleaned):\n",
    "    # Progress indicator\n",
    "    if (i + 1) % batch_size == 0:\n",
    "        print(f\"  Progress: {i + 1}/{len(unique_cleaned)} ({(i+1)/len(unique_cleaned)*100:.1f}%)\")\n",
    "    \n",
    "    paper_info, topics, concepts, keywords = lookup_doi_with_metadata(doi, email=YOUR_EMAIL)\n",
    "    \n",
    "    if paper_info:\n",
    "        found_papers.append(paper_info)\n",
    "        all_topics.extend(topics)\n",
    "        all_concepts.extend(concepts)\n",
    "        all_keywords.extend(keywords)\n",
    "    else:\n",
    "        not_found_dois.append(doi)\n",
    "    \n",
    "    # Rate limiting: Be polite to OpenAlex API\n",
    "    # With email (polite pool): 10 req/sec = 0.1s wait\n",
    "    # Without email: 6 req/sec = 0.17s wait\n",
    "    time.sleep(0.11 if YOUR_EMAIL else 0.17)\n",
    "\n",
    "print(f\"\\n✓ Lookup complete!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fedc3233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 8: Results Summary\n",
      "================================================================================\n",
      "Total DOIs looked up: 2,938\n",
      "  Found in OpenAlex: 2,107 (71.7%)\n",
      "  Not found: 915 (31.1%)\n",
      "\n",
      "📚 Newly discovered papers:\n",
      "--------------------------------------------------------------------------------\n",
      "  Total papers: 2,107\n",
      "  Total topics: 1,061\n",
      "  Total concepts: 4,425\n",
      "  Total keywords: 2,063\n",
      "\n",
      "  Papers by type:\n",
      "    article: 1509\n",
      "    preprint: 229\n",
      "    review: 178\n",
      "    book-chapter: 107\n",
      "    book: 49\n",
      "    editorial: 11\n",
      "    letter: 9\n",
      "    other: 8\n",
      "    report: 5\n",
      "    grant: 1\n",
      "\n",
      "  Publication years:\n",
      "    Range: 1739-2025\n",
      "    Median: 2017\n",
      "\n",
      "  Citation stats:\n",
      "    Total citations: 1,409,020\n",
      "    Mean: 668.7\n",
      "    Median: 74\n",
      "    Max: 86817\n",
      "\n",
      "🏷️  TOPIC ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n",
      "  Top 15 topics (by paper count):\n",
      "     238 papers | Neural dynamics and brain function\n",
      "     217 papers | Topic Modeling\n",
      "     189 papers | Psychology of Moral and Emotional Judgment\n",
      "     166 papers | Decision-Making and Behavioral Economics\n",
      "     119 papers | Evolutionary Game Theory and Cooperation\n",
      "     111 papers | Computability, Logic, AI Algorithms\n",
      "     103 papers | Epistemology, Ethics, and Metaphysics\n",
      "      99 papers | Natural Language Processing Techniques\n",
      "      91 papers | Neural and Behavioral Psychology Studies\n",
      "      88 papers | Reinforcement Learning in Robotics\n",
      "      88 papers | Experimental Behavioral Economics Studies\n",
      "      81 papers | Social and Intergroup Psychology\n",
      "      80 papers | Neural Networks and Applications\n",
      "      72 papers | Advanced Memory and Neural Computing\n",
      "      70 papers | Philosophy and History of Science\n",
      "\n",
      "  Domains:\n",
      "    2976 | Social Sciences\n",
      "    2812 | Physical Sciences\n",
      "    1782 | Life Sciences\n",
      "     375 | Health Sciences\n",
      "\n",
      "💡 CONCEPT ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n",
      "  Top 15 concepts (by paper count):\n",
      "    1342 papers | Computer science\n",
      "     911 papers | Psychology\n",
      "     773 papers | Artificial intelligence\n",
      "     636 papers | Philosophy\n",
      "     617 papers | Biology\n",
      "     594 papers | Mathematics\n",
      "     497 papers | Epistemology\n",
      "     428 papers | Neuroscience\n",
      "     427 papers | Political science\n",
      "     419 papers | Economics\n",
      "     418 papers | Physics\n",
      "     401 papers | Law\n",
      "     397 papers | Social psychology\n",
      "     304 papers | Sociology\n",
      "     290 papers | Machine learning\n",
      "\n",
      "  Highest average concept scores:\n",
      "    0.998 | Diamondoid\n",
      "    0.997 | Prophage\n",
      "    0.996 | KcsA potassium channel\n",
      "    0.994 | Absurdity\n",
      "    0.994 | Danaus\n",
      "    0.991 | Ostensive definition\n",
      "    0.990 | Ontic\n",
      "    0.990 | Human echolocation\n",
      "    0.987 | Mania\n",
      "    0.985 | Preorder\n",
      "\n",
      "🔑 KEYWORD ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n",
      "  Top 20 keywords (by paper count):\n",
      "      73 papers | Value (mathematics)\n",
      "      49 papers | Argument (complex analysis)\n",
      "      36 papers | Philosophy of language\n",
      "      31 papers | Representation\n",
      "      26 papers | Robustness\n",
      "      20 papers | Relevance\n",
      "      19 papers | Phenomenon\n",
      "      19 papers | Affect\n",
      "      18 papers | Stimulus (psychology)\n",
      "      18 papers | Benchmark (surveying)\n",
      "      17 papers | Variation (astronomy)\n",
      "      17 papers | Interpretability\n",
      "      17 papers | Impossibility\n",
      "      16 papers | Empirical Research\n",
      "      16 papers | Heuristics\n",
      "      16 papers | Leverage (statistics)\n",
      "      16 papers | Neuromorphic engineering\n",
      "      16 papers | Sequence (biology)\n",
      "      15 papers | Code (set theory)\n",
      "      15 papers | Computational model\n",
      "\n",
      "  Top 10 most cited newly found papers:\n",
      "  ------------------------------------------------------------------------------\n",
      "    86,817 | 1997 | Long Short-Term Memory\n",
      "    76,598 | 1948 | A Mathematical Theory of Communication\n",
      "    62,423 | 2017 | Attention Is All You Need\n",
      "    38,759 | 2018 | BERT: Pre-training of Deep Bidirectional Transformers for Languag...\n",
      "    35,300 | 2021 | Highly accurate protein structure prediction with AlphaFold\n",
      "    34,898 | 2017 | ImageNet classification with deep convolutional neural networks\n",
      "    31,700 | 2019 | None\n",
      "    26,168 | 1974 | Judgment under Uncertainty: Heuristics and Biases\n",
      "    23,508 | 1937 | The Nature of the Firm\n",
      "    20,525 | 1981 | The Evolution of Cooperation\n",
      "\n",
      "❌ Sample DOIs not found in OpenAlex:\n",
      "--------------------------------------------------------------------------------\n",
      "  - 10.2307/1828886\n",
      "  - 10.2307/20115662\n",
      "  - 10.2307/40267481\n",
      "  - 10.2307/2677856\n",
      "  - 10.3389/fnbeh.2013.00206/full\n",
      "  - 10.3389/fnana.2012.00032/full\n",
      "  - 10.2307/4132491\n",
      "  - 10.1111/ijcp.12026/full\n",
      "  - 10.1002/pst.v13.1/issuetoc\n",
      "  - 10.1038/s42256-019-0048-x[6\n",
      "  - 10.1214/21-ss133.full\n",
      "  - 10.3389/neuro.09.031.2009/full\n",
      "  - 10.1101/2022.09.29.509744v1.full\n",
      "  - 10.1016/s0169-5347(98\n",
      "  - 10.1088/2634-4386/ac4a83.^node\n",
      "  - 10.1007/s00429-017-1382-6.^b\n",
      "  - 10.3389/fncel.2016.00239.^f\n",
      "  - 10.1002/cne.21974.^d\n",
      "  - 10.1212/01.wnl.0000166914.38327.bb.^h\n",
      "  - 10.1088/0957-4484/24/38/384004.^g\n",
      "  ... and 895 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 8: Results Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "found_count = len(found_papers)\n",
    "not_found_count = len(not_found_dois)\n",
    "total = len(unique_cleaned)\n",
    "\n",
    "print(f\"Total DOIs looked up: {total:,}\")\n",
    "print(f\"  Found in OpenAlex: {found_count:,} ({found_count/total*100:.1f}%)\")\n",
    "print(f\"  Not found: {not_found_count:,} ({not_found_count/total*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "if found_count > 0:\n",
    "    # Create dataframes\n",
    "    new_papers_df = pd.DataFrame(found_papers)\n",
    "    topics_df = pd.DataFrame(all_topics) if all_topics else pd.DataFrame()\n",
    "    concepts_df = pd.DataFrame(all_concepts) if all_concepts else pd.DataFrame()\n",
    "    keywords_df = pd.DataFrame(all_keywords) if all_keywords else pd.DataFrame()\n",
    "    \n",
    "    print(\"📚 Newly discovered papers:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Total papers: {len(new_papers_df):,}\")\n",
    "    print(f\"  Total topics: {len(topics_df['topic_name'].unique()):,}\")\n",
    "    print(f\"  Total concepts: {len(concepts_df['concept_name'].unique()):,}\")\n",
    "    print(f\"  Total keywords: {len(keywords_df['keyword'].unique()):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Paper statistics\n",
    "    print(f\"  Papers by type:\")\n",
    "    if 'type' in new_papers_df.columns:\n",
    "        type_counts = new_papers_df['type'].value_counts()\n",
    "        for paper_type, count in type_counts.head(10).items():\n",
    "            print(f\"    {paper_type}: {count}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"  Publication years:\")\n",
    "    if 'publication_year' in new_papers_df.columns:\n",
    "        year_stats = new_papers_df['publication_year'].describe()\n",
    "        print(f\"    Range: {int(year_stats['min'])}-{int(year_stats['max'])}\")\n",
    "        print(f\"    Median: {int(year_stats['50%'])}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"  Citation stats:\")\n",
    "    if 'cited_by_count' in new_papers_df.columns:\n",
    "        citation_stats = new_papers_df['cited_by_count'].describe()\n",
    "        print(f\"    Total citations: {int(new_papers_df['cited_by_count'].sum()):,}\")\n",
    "        print(f\"    Mean: {citation_stats['mean']:.1f}\")\n",
    "        print(f\"    Median: {int(citation_stats['50%'])}\")\n",
    "        print(f\"    Max: {int(citation_stats['max'])}\")\n",
    "    print()\n",
    "    \n",
    "    # Topic statistics\n",
    "    if not topics_df.empty:\n",
    "        print(\"🏷️  TOPIC ANALYSIS:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Most common topics\n",
    "        topic_counts = topics_df.groupby('topic_name').size().sort_values(ascending=False)\n",
    "        print(f\"  Top 15 topics (by paper count):\")\n",
    "        for topic, count in topic_counts.head(15).items():\n",
    "            print(f\"    {count:4d} papers | {topic}\")\n",
    "        print()\n",
    "        \n",
    "        # Most common domains/fields\n",
    "        if 'domain' in topics_df.columns:\n",
    "            domain_counts = topics_df[topics_df['domain'].notna()]['domain'].value_counts()\n",
    "            print(f\"  Domains:\")\n",
    "            for domain, count in domain_counts.head(10).items():\n",
    "                print(f\"    {count:4d} | {domain}\")\n",
    "        print()\n",
    "    \n",
    "    # Concept statistics\n",
    "    if not concepts_df.empty:\n",
    "        print(\"💡 CONCEPT ANALYSIS:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Most common concepts\n",
    "        concept_counts = concepts_df.groupby('concept_name').size().sort_values(ascending=False)\n",
    "        print(f\"  Top 15 concepts (by paper count):\")\n",
    "        for concept, count in concept_counts.head(15).items():\n",
    "            print(f\"    {count:4d} papers | {concept}\")\n",
    "        print()\n",
    "        \n",
    "        # Average concept scores\n",
    "        avg_scores = concepts_df.groupby('concept_name')['concept_score'].mean().sort_values(ascending=False)\n",
    "        print(f\"  Highest average concept scores:\")\n",
    "        for concept, score in avg_scores.head(10).items():\n",
    "            print(f\"    {score:.3f} | {concept}\")\n",
    "        print()\n",
    "    \n",
    "    # Keyword statistics\n",
    "    if not keywords_df.empty:\n",
    "        print(\"🔑 KEYWORD ANALYSIS:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Most common keywords\n",
    "        keyword_counts = keywords_df.groupby('keyword').size().sort_values(ascending=False)\n",
    "        print(f\"  Top 20 keywords (by paper count):\")\n",
    "        for keyword, count in keyword_counts.head(20).items():\n",
    "            print(f\"    {count:4d} papers | {keyword}\")\n",
    "        print()\n",
    "    \n",
    "    # Top cited papers\n",
    "    print(\"  Top 10 most cited newly found papers:\")\n",
    "    print(\"  \" + \"-\" * 78)\n",
    "    top_cited = new_papers_df.nlargest(10, 'cited_by_count')\n",
    "    for idx, row in top_cited.iterrows():\n",
    "        title = str(row['title'])[:65] + \"...\" if len(str(row['title'])) > 65 else row['title']\n",
    "        print(f\"    {row['cited_by_count']:5,} | {row['publication_year']} | {title}\")\n",
    "    print()\n",
    "\n",
    "if not_found_count > 0:\n",
    "    print(\"❌ Sample DOIs not found in OpenAlex:\")\n",
    "    print(\"-\" * 80)\n",
    "    for doi in not_found_dois[:20]:\n",
    "        print(f\"  - {doi}\")\n",
    "    if not_found_count > 20:\n",
    "        print(f\"  ... and {not_found_count - 20} more\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b7a3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 5: Saving results\n",
      "================================================================================\n",
      "✓ Saved 2,107 papers to: openalex_papers_from_api.csv\n",
      "✓ Saved 7,945 topic associations to: openalex_topics_from_api.csv\n",
      "✓ Saved 29,627 concept associations to: openalex_concepts_from_api.csv\n",
      "✓ Saved 4,081 keyword associations to: openalex_keywords_from_api.csv\n",
      "✓ Saved expanded dataset (43,338 papers) to: nodes_openalex_works_expanded.csv\n",
      "✓ Saved 915 DOIs not found to: dois_not_in_openalex.csv\n",
      "\n",
      "================================================================================\n",
      "✅ LOOKUP COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "📦 New dataframes available:\n",
      "  - new_papers_df: Papers found via OpenAlex API\n",
      "  - topics_df: Topic classifications for each paper\n",
      "  - concepts_df: Concepts (deprecated) for each paper\n",
      "  - keywords_df: Keywords for each paper\n",
      "  - expanded_openalex: Your original + newly found papers\n",
      "\n",
      "📊 Analysis possibilities:\n",
      "  - Analyze which topics are most cited by AI Safety community\n",
      "  - See if certain domains/fields are over-represented\n",
      "  - Track concept evolution over time\n",
      "  - Identify key research areas through keyword clusters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: Saving results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if found_count > 0:\n",
    "    # Save newly found papers\n",
    "    new_papers_df.to_csv('openalex_papers_from_api.csv', index=False)\n",
    "    print(f\"✓ Saved {found_count:,} papers to: openalex_papers_from_api.csv\")\n",
    "    \n",
    "    # Save topics\n",
    "    if not topics_df.empty:\n",
    "        topics_df.to_csv('openalex_topics_from_api.csv', index=False)\n",
    "        print(f\"✓ Saved {len(topics_df):,} topic associations to: openalex_topics_from_api.csv\")\n",
    "    \n",
    "    # Save concepts\n",
    "    if not concepts_df.empty:\n",
    "        concepts_df.to_csv('openalex_concepts_from_api.csv', index=False)\n",
    "        print(f\"✓ Saved {len(concepts_df):,} concept associations to: openalex_concepts_from_api.csv\")\n",
    "    \n",
    "    # Save keywords\n",
    "    if not keywords_df.empty:\n",
    "        keywords_df.to_csv('openalex_keywords_from_api.csv', index=False)\n",
    "        print(f\"✓ Saved {len(keywords_df):,} keyword associations to: openalex_keywords_from_api.csv\")\n",
    "    \n",
    "    # Save expanded OpenAlex dataset\n",
    "    expanded_openalex = pd.concat([\n",
    "        openalex_df[['openalex_id', 'openalex_doi', 'title', 'publication_year', 'type', 'cited_by_count']],\n",
    "        new_papers_df[['openalex_id', 'doi', 'title', 'publication_year', 'type', 'cited_by_count']].rename(columns={'doi': 'openalex_doi'})\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    expanded_openalex.to_csv('nodes_openalex_works_expanded.csv', index=False)\n",
    "    print(f\"✓ Saved expanded dataset ({len(expanded_openalex):,} papers) to: nodes_openalex_works_expanded.csv\")\n",
    "\n",
    "if not_found_count > 0:\n",
    "    # Save DOIs that weren't found\n",
    "    not_found_df = pd.DataFrame({'doi': not_found_dois})\n",
    "    not_found_df.to_csv('dois_not_in_openalex.csv', index=False)\n",
    "    print(f\"✓ Saved {not_found_count:,} DOIs not found to: dois_not_in_openalex.csv\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ LOOKUP COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"📦 New dataframes available:\")\n",
    "if found_count > 0:\n",
    "    print(\"  - new_papers_df: Papers found via OpenAlex API\")\n",
    "    print(\"  - topics_df: Topic classifications for each paper\")\n",
    "    print(\"  - concepts_df: Concepts (deprecated) for each paper\")\n",
    "    print(\"  - keywords_df: Keywords for each paper\")\n",
    "    print(\"  - expanded_openalex: Your original + newly found papers\")\n",
    "print()\n",
    "print(\"📊 Analysis possibilities:\")\n",
    "print(\"  - Analyze which topics are most cited by AI Safety community\")\n",
    "print(\"  - See if certain domains/fields are over-represented\")\n",
    "print(\"  - Track concept evolution over time\")\n",
    "print(\"  - Identify key research areas through keyword clusters\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "37b4c52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing specific DOI: 10.1080/0952813x.2014.895105\n",
      "================================================================================\n",
      "\n",
      "Test DOI: 10.1080/0952813x.2014.895105\n",
      "Test DOI (repr): '10.1080/0952813x.2014.895105'\n",
      "Test DOI length: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST: Why didn't 10.1080/0952813x.2014.895105 match?\n",
    "====================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Testing specific DOI: 10.1080/0952813x.2014.895105\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# The DOI we're looking for\n",
    "test_doi = \"10.1080/0952813x.2014.895105\"\n",
    "\n",
    "print(f\"Test DOI: {test_doi}\")\n",
    "print(f\"Test DOI (repr): {repr(test_doi)}\")\n",
    "print(f\"Test DOI length: {len(test_doi)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d140a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK 1: Searching in OpenAlex dataframe\n",
      "--------------------------------------------------------------------------------\n",
      "Matches in original 'openalex_doi' column: 1\n",
      "\n",
      "✅ FOUND in OpenAlex!\n",
      "  OpenAlex ID: https://openalex.org/W2009210150\n",
      "  DOI: https://doi.org/10.1080/0952813x.2014.895105\n",
      "  DOI (repr): 'https://doi.org/10.1080/0952813x.2014.895105'\n",
      "  Title: The errors, insights and lessons of famous AI predictions – and what they mean for the future\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK 1: Is it in the OpenAlex dataframe?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"CHECK 1: Searching in OpenAlex dataframe\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Search in original openalex_doi column\n",
    "matches_original = openalex_df[openalex_df['openalex_doi'].str.contains(test_doi, na=False, regex=False)]\n",
    "print(f\"Matches in original 'openalex_doi' column: {len(matches_original)}\")\n",
    "\n",
    "if not matches_original.empty:\n",
    "    print(\"\\n✅ FOUND in OpenAlex!\")\n",
    "    for idx, row in matches_original.iterrows():\n",
    "        print(f\"  OpenAlex ID: {row['openalex_id']}\")\n",
    "        print(f\"  DOI: {row['openalex_doi']}\")\n",
    "        print(f\"  DOI (repr): {repr(row['openalex_doi'])}\")\n",
    "        print(f\"  Title: {row['title']}\")\n",
    "else:\n",
    "    print(\"❌ NOT FOUND in original column\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df798ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK 2: Checking cleaned DOI column\n",
      "--------------------------------------------------------------------------------\n",
      "Exact matches in 'doi_cleaned' column: 1\n",
      "\n",
      "✅ FOUND in cleaned column!\n",
      "  Cleaned DOI: 10.1080/0952813x.2014.895105\n",
      "  Cleaned DOI (repr): '10.1080/0952813x.2014.895105'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK 2: Is it in the cleaned DOI column?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"CHECK 2: Checking cleaned DOI column\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'doi_cleaned' in openalex_df.columns:\n",
    "    matches_cleaned = openalex_df[openalex_df['doi_cleaned'] == test_doi.lower()]\n",
    "    print(f\"Exact matches in 'doi_cleaned' column: {len(matches_cleaned)}\")\n",
    "    \n",
    "    if not matches_cleaned.empty:\n",
    "        print(\"\\n✅ FOUND in cleaned column!\")\n",
    "        for idx, row in matches_cleaned.iterrows():\n",
    "            print(f\"  Cleaned DOI: {row['doi_cleaned']}\")\n",
    "            print(f\"  Cleaned DOI (repr): {repr(row['doi_cleaned'])}\")\n",
    "    else:\n",
    "        print(\"❌ NOT FOUND with exact match\")\n",
    "        \n",
    "        # Try partial match\n",
    "        partial = openalex_df[openalex_df['doi_cleaned'].str.contains(test_doi, na=False, regex=False)]\n",
    "        print(f\"Partial matches: {len(partial)}\")\n",
    "        if not partial.empty:\n",
    "            print(\"Found with partial match:\")\n",
    "            for idx, row in partial.head(3).iterrows():\n",
    "                print(f\"  {row['doi_cleaned']}\")\n",
    "else:\n",
    "    print(\"⚠️  'doi_cleaned' column doesn't exist yet\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aa89ee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK 3: Checking if it's in the openalex_doi_set\n",
      "--------------------------------------------------------------------------------\n",
      "Test DOI (lowercase): 10.1080/0952813x.2014.895105\n",
      "Is in set? True\n",
      "✅ YES, it's in the set!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK 3: Is it in the openalex_doi_set?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"CHECK 3: Checking if it's in the openalex_doi_set\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'openalex_doi_set' in dir():\n",
    "    test_doi_lower = test_doi.lower()\n",
    "    \n",
    "    print(f\"Test DOI (lowercase): {test_doi_lower}\")\n",
    "    print(f\"Is in set? {test_doi_lower in openalex_doi_set}\")\n",
    "    \n",
    "    if test_doi_lower in openalex_doi_set:\n",
    "        print(\"✅ YES, it's in the set!\")\n",
    "    else:\n",
    "        print(\"❌ NOT in the set\")\n",
    "        \n",
    "        # Check if any similar DOIs are in the set\n",
    "        print(\"\\nSearching for similar DOIs in set...\")\n",
    "        similar = [doi for doi in openalex_doi_set if '10.1080/0952813x' in doi]\n",
    "        print(f\"Found {len(similar)} DOIs starting with 10.1080/0952813x:\")\n",
    "        for doi in list(similar)[:5]:\n",
    "            print(f\"  {doi}\")\n",
    "            print(f\"  {repr(doi)}\")\n",
    "else:\n",
    "    print(\"⚠️  openalex_doi_set not found in namespace\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b558b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK 4: Was this DOI extracted from forum posts?\n",
      "--------------------------------------------------------------------------------\n",
      "Found in 2 posts\n",
      "\n",
      "✅ YES, it was extracted from posts!\n",
      "\n",
      "Post: Research Agenda v0.9: Synthesising a human's preferences into a utilit...\n",
      "  Source: LessWrong_Topic0\n",
      "  All DOIs: ['10.1080/0952813x.2014.895105']\n",
      "\n",
      "Post: Research Agenda v0.9: Synthesising a human's preferences into a utilit...\n",
      "  Source: AlignmentForum\n",
      "  All DOIs: ['10.1080/0952813x.2014.895105']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK 4: Was it extracted from posts?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"CHECK 4: Was this DOI extracted from forum posts?\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "found_in_posts = []\n",
    "for idx, row in posts_df.iterrows():\n",
    "    if test_doi in row['all_extracted_dois']:\n",
    "        found_in_posts.append(row)\n",
    "        if len(found_in_posts) >= 3:  # Limit to first 3\n",
    "            break\n",
    "\n",
    "print(f\"Found in {len(found_in_posts)} posts\")\n",
    "\n",
    "if found_in_posts:\n",
    "    print(\"\\n✅ YES, it was extracted from posts!\")\n",
    "    for post in found_in_posts[:3]:\n",
    "        print(f\"\\nPost: {post['title'][:70]}...\")\n",
    "        print(f\"  Source: {post['source']}\")\n",
    "        print(f\"  All DOIs: {post['all_extracted_dois']}\")\n",
    "else:\n",
    "    print(\"❌ NOT found in any post\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d0721e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK 5: Simulating the matching process\n",
      "--------------------------------------------------------------------------------\n",
      "Original OpenAlex DOI: https://doi.org/10.1080/0952813x.2014.895105\n",
      "Cleaned OpenAlex DOI:  10.1080/0952813x.2014.895105\n",
      "Cleaned (repr):        '10.1080/0952813x.2014.895105'\n",
      "\n",
      "Extracted DOI:         10.1080/0952813x.2014.895105\n",
      "Extracted (lowercase): 10.1080/0952813x.2014.895105\n",
      "Extracted (repr):      '10.1080/0952813x.2014.895105'\n",
      "\n",
      "Should match? True\n",
      "\n",
      "✅ They match perfectly!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK 5: Simulate the matching process\n",
    "# ============================================================================\n",
    "\n",
    "print(\"CHECK 5: Simulating the matching process\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Recreate the cleaning function\n",
    "def clean_openalex_doi_test(doi_str):\n",
    "    \"\"\"Clean OpenAlex DOI for matching\"\"\"\n",
    "    if pd.isna(doi_str):\n",
    "        return None\n",
    "    doi_str = str(doi_str).lower().strip()\n",
    "    # Remove https://doi.org/ prefix if present\n",
    "    doi_str = doi_str.replace('https://doi.org/', '')\n",
    "    return doi_str if doi_str else None\n",
    "\n",
    "# Clean the specific OpenAlex entry\n",
    "test_openalex_doi = \"https://doi.org/10.1080/0952813x.2014.895105\"\n",
    "cleaned_openalex = clean_openalex_doi_test(test_openalex_doi)\n",
    "\n",
    "print(f\"Original OpenAlex DOI: {test_openalex_doi}\")\n",
    "print(f\"Cleaned OpenAlex DOI:  {cleaned_openalex}\")\n",
    "print(f\"Cleaned (repr):        {repr(cleaned_openalex)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Extracted DOI:         {test_doi}\")\n",
    "print(f\"Extracted (lowercase): {test_doi.lower()}\")\n",
    "print(f\"Extracted (repr):      {repr(test_doi.lower())}\")\n",
    "print()\n",
    "\n",
    "print(f\"Should match? {cleaned_openalex == test_doi.lower()}\")\n",
    "print()\n",
    "\n",
    "# Character-by-character comparison\n",
    "if cleaned_openalex != test_doi.lower():\n",
    "    print(\"❌ They don't match! Let's compare character by character:\")\n",
    "    s1 = cleaned_openalex\n",
    "    s2 = test_doi.lower()\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    \n",
    "    print(\"\\nPosition | Cleaned | Extracted | Match?\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(max_len):\n",
    "        c1 = s1[i] if i < len(s1) else '(end)'\n",
    "        c2 = s2[i] if i < len(s2) else '(end)'\n",
    "        match = '✓' if c1 == c2 else '✗'\n",
    "        print(f\"{i:8d} | {repr(c1):7s} | {repr(c2):9s} | {match}\")\n",
    "        if c1 != c2:\n",
    "            print(f\"         ^ MISMATCH at position {i}\")\n",
    "            break\n",
    "else:\n",
    "    print(\"✅ They match perfectly!\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39d66b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts that extracted this DOI: 2\n",
      "\n",
      "Post: Research Agenda v0.9: Synthesising a human's preferences int...\n",
      "  Extracted DOIs: ['10.1080/0952813x.2014.895105']\n",
      "  Matched DOIs: ['10.1080/0952813x.2014.895105']\n",
      "  Was it matched? True\n",
      "\n",
      "Post: Research Agenda v0.9: Synthesising a human's preferences int...\n",
      "  Extracted DOIs: ['10.1080/0952813x.2014.895105']\n",
      "  Matched DOIs: ['10.1080/0952813x.2014.895105']\n",
      "  Was it matched? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if this specific DOI is in matched_dois\n",
    "test_doi = \"10.1080/0952813x.2014.895105\"\n",
    "\n",
    "# Find posts that extracted this DOI\n",
    "posts_with_this_doi = posts_df[posts_df['all_extracted_dois'].apply(lambda x: test_doi in x)]\n",
    "\n",
    "print(f\"Posts that extracted this DOI: {len(posts_with_this_doi)}\")\n",
    "print()\n",
    "\n",
    "for idx, row in posts_with_this_doi.head(3).iterrows():\n",
    "    print(f\"Post: {row['title'][:60]}...\")\n",
    "    print(f\"  Extracted DOIs: {row['all_extracted_dois']}\")\n",
    "    print(f\"  Matched DOIs: {row['matched_dois']}\")\n",
    "    print(f\"  Was it matched? {test_doi in row['matched_dois']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa5d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_doi_from_link(link):\n",
    "    \"\"\"\n",
    "    Extract clean DOI from any URL, removing URL fragments and query params\n",
    "    \"\"\"\n",
    "    if not link or not isinstance(link, str):\n",
    "        return None\n",
    "    \n",
    "    # Find DOI pattern (10.xxxx/...)\n",
    "    doi_match = re.search(r'10\\.\\d{4,9}/[^\\s;<>\"?#]+', link, re.IGNORECASE)\n",
    "    if not doi_match:\n",
    "        return None\n",
    "    \n",
    "    doi = doi_match.group()\n",
    "    \n",
    "    # Remove common URL fragments that aren't part of the DOI\n",
    "    # These often appear after the DOI in URLs\n",
    "    doi = re.sub(r'/(abstract|full|pdf|epdf|summary)$', '', doi, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove trailing punctuation\n",
    "    doi = re.sub(r'[.,;:)\\]]+$', '', doi)\n",
    "    \n",
    "    return doi.strip().lower()\n",
    "\n",
    "def normalize_doi(doi_string):\n",
    "    \"\"\"Robust DOI normalization.\"\"\"\n",
    "    if pd.isna(doi_string) or not doi_string:\n",
    "        return None\n",
    "\n",
    "    doi = str(doi_string).strip()\n",
    "    doi = doi.lower()\n",
    "\n",
    "    # Regex for common prefix removal (critical for consistency)\n",
    "    doi = re.sub(r'^(http(s)?://(dx\\.)?doi\\.org/|doi:|info:doi/)', '', doi, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove non-breaking spaces and other subtle issues\n",
    "    doi = doi.replace('\\xa0', ' ').strip()\n",
    "    \n",
    "    # Optional: basic validation before returning\n",
    "    if re.match(r'10\\.\\d{4,9}/[^\\s]+$', doi):\n",
    "        return doi\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=== FETCH UNMATCHED DOIs FROM OPENALEX ===\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load unmatched posts with DOIs\n",
    "unmatched_with_doi = pd.read_csv('ai_safety_unmatched_with_doi.csv')\n",
    "\n",
    "print(f\"Fetching {len(unmatched_with_doi)} DOIs from OpenAlex...\")\n",
    "\n",
    "# Prepare list of DOIs to fetch\n",
    "dois_to_fetch = unmatched_with_doi['doi_clean'].dropna().unique().tolist()\n",
    "print(f\"Unique DOIs to fetch: {len(dois_to_fetch)}\")\n",
    "\n",
    "# Function to fetch from OpenAlex\n",
    "def fetch_openalex_work(doi):\n",
    "    \"\"\"Fetch a single work from OpenAlex by DOI\"\"\"\n",
    "    url = f\"https://api.openalex.org/works/https://doi.org/{doi}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {doi}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch works from OpenAlex\n",
    "fetched_works = []\n",
    "failed_dois = []\n",
    "\n",
    "for doi in tqdm(dois_to_fetch, desc=\"Fetching from OpenAlex\"):\n",
    "    work = fetch_openalex_work(doi)\n",
    "    if work:\n",
    "        fetched_works.append({\n",
    "            'doi': doi,\n",
    "            'openalex_id': work.get('id'),\n",
    "            'title': work.get('title'),\n",
    "            'publication_year': work.get('publication_year'),\n",
    "            'type': work.get('type'),\n",
    "            'cited_by_count': work.get('cited_by_count'),\n",
    "            'concepts': work.get('concepts', []),\n",
    "            'keywords': work.get('keywords', []),\n",
    "            'abstract': work.get('abstract_inverted_index'),\n",
    "            'primary_topic': work.get('primary_topic'),\n",
    "            'topics': work.get('topics', [])\n",
    "        })\n",
    "    else:\n",
    "        failed_dois.append(doi)\n",
    "    \n",
    "    # Rate limit: OpenAlex allows ~10 requests per second for polite pool\n",
    "    time.sleep(0.11)\n",
    "\n",
    "print(f\"\\n=== Fetch Results ===\")\n",
    "print(f\"Successfully fetched: {len(fetched_works)}\")\n",
    "print(f\"Failed to fetch: {len(failed_dois)}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "if len(fetched_works) > 0:\n",
    "    works_df = pd.DataFrame(fetched_works)\n",
    "    \n",
    "    # Extract concept names and scores\n",
    "    works_df['concept_names'] = works_df['concepts'].apply(\n",
    "        lambda concepts: [c.get('display_name') for c in concepts] if concepts else []\n",
    "    )\n",
    "    works_df['top_concepts'] = works_df['concepts'].apply(\n",
    "        lambda concepts: ', '.join([c.get('display_name') for c in concepts[:5]]) if concepts else ''\n",
    "    )\n",
    "    \n",
    "    # Extract topic names\n",
    "    works_df['topic_names'] = works_df['topics'].apply(\n",
    "        lambda topics: [t.get('display_name') for t in topics] if topics else []\n",
    "    )\n",
    "    works_df['top_topic'] = works_df['primary_topic'].apply(\n",
    "        lambda topic: topic.get('display_name') if topic else None\n",
    "    )\n",
    "    \n",
    "    # Extract keyword names\n",
    "    works_df['keyword_names'] = works_df['keywords'].apply(\n",
    "        lambda keywords: [k.get('display_name') for k in keywords] if keywords else []\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Topic Analysis ===\")\n",
    "    \n",
    "    # Count topics\n",
    "    all_topics = []\n",
    "    for topics in works_df['topic_names']:\n",
    "        all_topics.extend(topics)\n",
    "    \n",
    "    topic_counts = pd.Series(all_topics).value_counts().head(20)\n",
    "    print(\"\\nTop 20 Topics:\")\n",
    "    print(topic_counts)\n",
    "    \n",
    "    # Count concepts\n",
    "    all_concepts = []\n",
    "    for concepts in works_df['concept_names']:\n",
    "        all_concepts.extend(concepts)\n",
    "    \n",
    "    concept_counts = pd.Series(all_concepts).value_counts().head(20)\n",
    "    print(\"\\n\\nTop 20 Concepts:\")\n",
    "    print(concept_counts)\n",
    "    \n",
    "    # Count keywords\n",
    "    all_keywords = []\n",
    "    for keywords in works_df['keyword_names']:\n",
    "        all_keywords.extend(keywords)\n",
    "    \n",
    "    if len(all_keywords) > 0:\n",
    "        keyword_counts = pd.Series(all_keywords).value_counts().head(20)\n",
    "        print(\"\\n\\nTop 20 Keywords:\")\n",
    "        print(keyword_counts)\n",
    "    \n",
    "    # Save results\n",
    "    works_df.to_csv('unmatched_dois_openalex_data.csv', index=False)\n",
    "    print(\"\\n✓ Saved 'unmatched_dois_openalex_data.csv'\")\n",
    "    \n",
    "    # Merge back with original unmatched posts\n",
    "    unmatched_enriched = unmatched_with_doi.merge(\n",
    "        works_df[['doi', 'top_concepts', 'top_topic', 'publication_year', 'cited_by_count']],\n",
    "        left_on='doi_clean',\n",
    "        right_on='doi',\n",
    "        how='left'\n",
    "    )\n",
    "    unmatched_enriched.to_csv('unmatched_posts_with_openalex_topics.csv', index=False)\n",
    "    print(\"✓ Saved 'unmatched_posts_with_openalex_topics.csv'\")\n",
    "    \n",
    "else:\n",
    "    print(\"No works were successfully fetched\")\n",
    "\n",
    "if len(failed_dois) > 0:\n",
    "    pd.DataFrame({'doi': failed_dois}).to_csv('failed_dois.csv', index=False)\n",
    "    print(f\"✓ Saved {len(failed_dois)} failed DOIs to 'failed_dois.csv'\")\n",
    "\n",
    "print(\"\\n=== Fetch complete! ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYSIS 1: TOPICS DISTRIBUTION\n",
    "# =============================================================================\n",
    "import ast \n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1️⃣  TOPICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "detailed_papers = pd.read_csv('missing_ai_papers_detailed.csv')\n",
    "\n",
    "all_topics = []\n",
    "all_topic_ids = []\n",
    "all_subfields = []\n",
    "all_fields = []\n",
    "all_domains = []\n",
    "all_keywords = []\n",
    "\n",
    "for _, paper in detailed_papers.iterrows():\n",
    "    # Parse the all_topics column\n",
    "    topics = ast.literal_eval(paper['all_topics']) if pd.notna(paper['all_topics']) and isinstance(paper['all_topics'], str) else [] \n",
    "    keywords = paper['keywords'].split(\";\") if pd.notna(paper['keywords']) and isinstance(paper['keywords'], str) else []\n",
    "    for keyword in keywords:\n",
    "        all_keywords.append(keyword.strip())\n",
    "\n",
    "    for topic in topics:\n",
    "        all_topics.append(topic.get('name', ''))\n",
    "        all_topic_ids.append(topic.get('id', ''))\n",
    "        all_subfields.append(topic.get('subfield', ''))\n",
    "        all_fields.append(topic.get('field', ''))\n",
    "        all_domains.append(topic.get('domain', ''))\n",
    "\n",
    "topic_counts = Counter(all_topics)\n",
    "topic_id_counts = Counter(all_topic_ids)\n",
    "subfield_counts = Counter(all_subfields)\n",
    "field_counts = Counter(all_fields)\n",
    "domain_counts = Counter(all_domains)\n",
    "keywords_counts = Counter(all_keywords)\n",
    "\n",
    "print(\"\\nTop 20 Topics:\")\n",
    "for topic, count in topic_counts.most_common(20):\n",
    "    pct = count / len(detailed_papers) * 100\n",
    "    print(f\"  {topic}: {count} papers ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nTop 30 Keywords:\")\n",
    "for topic, count in keywords_counts.most_common(30):\n",
    "    pct = count / len(detailed_papers) * 100\n",
    "    print(f\"  {topic}: {count} papers ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nTop Topic IDs:\")\n",
    "for topic_id, count in topic_id_counts.most_common(20):\n",
    "    pct = count / len(detailed_papers) * 100\n",
    "    print(f\"  {topic_id}: {count} papers ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nTop Subfields:\")\n",
    "for subfield, count in subfield_counts.most_common(10):\n",
    "    pct = count / len(detailed_papers) * 100\n",
    "    print(f\"  {subfield}: {count} papers ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nTop Fields:\")\n",
    "for field, count in field_counts.most_common(10):\n",
    "    pct = count / len(detailed_papers) * 100\n",
    "    print(f\"  {field}: {count} papers ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937dea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FILTER: ARTIFICIAL INTELLIGENCE SUBFIELD\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔍  PAPERS WITH 'ARTIFICIAL INTELLIGENCE' SUBFIELD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ai_subfield_papers = []\n",
    "\n",
    "for _, paper in detailed_papers.iterrows():\n",
    "    # Parse the all_topics column\n",
    "    if pd.notna(paper['all_topics']):\n",
    "        topics_data = ast.literal_eval(paper['all_topics']) if isinstance(paper['all_topics'], str) else paper['all_topics']\n",
    "        \n",
    "        # Check if any topic has \"Artificial Intelligence\" as subfield\n",
    "        ai_topics = [t for t in topics_data if 'artificial intelligence' in t.get('subfield', '').lower()]\n",
    "        \n",
    "        if ai_topics:\n",
    "            ai_subfield_papers.append({\n",
    "                'title': paper['title'],\n",
    "                'doi': paper['doi'],\n",
    "                'year': paper['publication_year'],\n",
    "                'cited_by': paper['cited_by_count'],\n",
    "                'topics': [t['name'] for t in topics_data[:3]],\n",
    "                'ai_topics': [t['name'] for t in ai_topics]\n",
    "            })\n",
    "\n",
    "print(f\"\\nFound {len(ai_subfield_papers)} papers with 'Artificial Intelligence' subfield\")\n",
    "print(f\"({len(ai_subfield_papers)/len(detailed_papers)*100:.1f}% of all papers)\\n\")\n",
    "\n",
    "# Sort by citation count\n",
    "ai_subfield_papers.sort(key=lambda x: x['cited_by'], reverse=True)\n",
    "\n",
    "print(\"\\nPapers sorted by citations:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, paper in enumerate(ai_subfield_papers, 1):\n",
    "    print(f\"\\n{i}. {paper['title']}\")\n",
    "    print(f\"   Year: {paper['year']} | Citations: {paper['cited_by']}\")\n",
    "    print(f\"   DOI: {paper['doi']}\")\n",
    "    print(f\"   AI Topics: {', '.join(paper['ai_topics'])}\")\n",
    "    print(f\"   All Topics: {', '.join(paper['topics'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYSIS 2: CONCEPTS DISTRIBUTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2️⃣  CONCEPTS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_concepts = []\n",
    "\n",
    "for _, paper in detailed_papers.iterrows():\n",
    "    # Parse the concepts_json column\n",
    "    if pd.notna(paper['concepts_json']):\n",
    "        concepts = ast.literal_eval(paper['concepts_json']) if isinstance(paper['concepts_json'], str) else paper['concepts_json']\n",
    "        \n",
    "        for concept in concepts:\n",
    "            if concept.get('score', 0) > 0.3:  # Only concepts with decent confidence\n",
    "                all_concepts.append(concept['name'])\n",
    "\n",
    "concept_counts = Counter(all_concepts)\n",
    "\n",
    "print(\"\\nTop 30 Concepts (score > 0.3):\")\n",
    "for concept, count in concept_counts.most_common(30):\n",
    "    pct = count / len(detailed_papers) * 100\n",
    "    print(f\"  {concept}: {count} papers ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4ce74",
   "metadata": {},
   "source": [
    "## Unknown Genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_gender = combined_df[combined_df[\"user_gender\"] == '-']\n",
    "unknown_gender_counts = unknown_gender['user.username'].value_counts()\n",
    "# if (unknown_gender_counts < 5).any():\n",
    "unknwons = set(unknown_gender_counts.index)\n",
    "unknowns_lower = {name.lower() for name in unknwons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07226c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in unknowns_lower:\n",
    "    print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fbe2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../src/metadata/graphql_usernames.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        names_data = json.load(f)\n",
    "\n",
    "MALE_USERNAMES = names_data[\"MALE_USERNAMES\"]\n",
    "FEMALE_USERNAMES = names_data[\"FEMALE_USERNAMES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a172c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknowns_without_male = unknowns_lower.difference(names.MALE_NAMES)\n",
    "unknowns_without_female = unknowns_lower.difference(names.FEMALE_NAMES)\n",
    "print(f'Unkown names that are neither in FEMALE_NAMES nor MALE_NAMES: {len(unknowns_without_male.intersection(unknowns_without_female))}')\n",
    "print(unknowns_without_male.intersection(unknowns_without_female))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f33d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names_without_unknowns = names.MALE_NAMES.difference(unknowns_lower)\n",
    "print(sorted(male_names_without_unknowns))\n",
    "print(sorted(names.MALE_NAMES - male_names_without_unknowns))\n",
    "print(sorted(names.MALE_NAMES - names.MALE_USERNAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.names as names\n",
    "importlib.reload(names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc880e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_names_without_unknowns = names.FEMALE_NAMES.difference(unknowns_lower)\n",
    "print(female_names_without_unknowns)\n",
    "print(names.FEMALE_NAMES - female_names_without_unknowns)\n",
    "print(names.FEMALE_NAMES - names.FEMALE_USERNAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aaa819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import src.names as names\n",
    " # Force reload\n",
    "print(type(names.MALE_NAMES))  # Should be <class 'set'>\n",
    "print(len(names.MALE_NAMES))   # Check the size\n",
    "print('yashvardhan' in names.MALE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c71fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(names.MALE_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "purely_unknown = unknwons - names.MALE_USERNAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43499cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "purely_unknown_lower = {name.lower() for name in purely_unknown}\n",
    "male_names_lower = {name.lower() for name in names.MALE_USERNAMES}\n",
    "print(purely_unknown_lower.intersection(male_names_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb434f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = list(names.FEMALE_NAMES) + list(names.MALE_NAMES)\n",
    "all_names_sorted = sorted(all_names, key=len, reverse=True)\n",
    "\n",
    "gf = []\n",
    "gf_user = []\n",
    "gm = []\n",
    "gm_user = []\n",
    "\n",
    "for username in unknowns_lower:\n",
    "    for name in all_names_sorted:\n",
    "        if len(name) > 3 and name in username:\n",
    "            if name in names.FEMALE_NAMES:\n",
    "                print(f\"FEMALE: {username} with {name}\")\n",
    "                gf_user.append(username)\n",
    "                gf.append(name)\n",
    "            elif name in names.MALE_NAMES:\n",
    "                print(f\"MALE: {username} with {name}\")\n",
    "                gm_user.append(username)\n",
    "                gm.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aca382",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gf_user)\n",
    "print(len(gf_user))\n",
    "print(gm_user)\n",
    "print(len(gm_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fba4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_unknowns = unknowns_lower - set(gf_user) - set(gm_user)\n",
    "for name in current_unknowns:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = [1150.00, 1105.33, 1072.61, 1054.01, 1040.12, 1028.32, 1018.51, 1009.65, 1002.37]\n",
    "topics = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "log_likelihood = [-126311522.90,-125601505.71, -125062974.37, -124749334.57, -124511617.32,  -124307203.07, -124135376.41, -123978846.41, -123849036.87]\n",
    "plt.plot(topics, perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('time (s)')\n",
    "ax1.set_ylabel('exp', color=color)\n",
    "ax1.plot(topics, perplexity, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second Axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('sin', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(topics, log_likelihood, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581cf29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
